---
title: "金融科技"
subtitle: "机器学习模型"
author:
  - "汪小圈"
format:
  revealjs:
    slide-number: c/t
    controls: true
    progressbar: true
    hash: true
    transition: slide
    background-transition: fade
    theme: moon
    css: addons/extra.css
    revealjs-plugins:
      - chalkboard
      - menu
      - multiplex
      - notes
      - search
      - zoom
editor: visual
execute: 
  echo: true
  warning: false
  message: false
---

## 内容安排

- 模型

    - 线性回归模型回顾

    - 岭回归与Lasso回归

    - 简单分类模型回顾

    - 支持向量机

    - 神经网络

    - 树模型

- 模型评估与选择

---

## 数据建模

- 线性回归模型回顾

- 岭回归与Lasso回归

- 简单分类模型回顾

- 支持向量机

- 神经网络

- 树模型

    - 决策树

    - 随机森林

    - 提升树

---

## 回归问题描述

- 通过由 $K \times 1$维向量 $x$表示的 $K$个观测到的预测变量（特征）来预测结果 $y$

- 由训练数据 $\{y_i, x_i\}_{1}^{N}$找到如下关系中的未知函数 $f$

$$y_i = f(x_i) + \epsilon_i$$

- 假设训练集中有 $N$个观测，将观测值堆叠成

    - $N \times 1$维向量 $y = (y_1, y_2, ..., y_N)'$

    - $N \times K$维矩阵 $X = (x_1, x_2, ..., x_N)'$

    - $N \times 1$维向量 $\epsilon = (\epsilon_1, \epsilon_2, ..., \epsilon_N)'$

- 回归模型可以写为： $y = f(X) + \epsilon$

---

## 线性回归模型回顾

- 线性模型： $y = X \beta + \epsilon$，其中 $\beta$是回归系数向量

- 最小化误差平方和：

$$\min_{\beta} (y - X\beta)'(y - X\beta)$$

- 得到普通最小二乘(OLS)估计量 $\hat{\beta} = (X'X)^{-1} X'y$

- 高维环境下的过拟合问题：

    - 当 $K$相对于 $N$来说并不小甚至比 $N$更大的情况下，基于OLS估计值的预测通常是不可靠的

    - 协变量相对观测数量来说非常多时，OLS会调整 $\hat{\beta}$来拟合噪音而非真实信号

    - 虽然样本内 $R^2$可能非常高，但样本外预测的 $R^2$则往往非常低甚至小于零

- 解决方案：正则化(Regulization)

---

## 岭回归

- 在最小化误差平方和的基础上，补充了 $L^2$范数惩罚项 $\beta'\beta$：

$$\min_{\beta} \left[ \frac{1}{N} (y - X\beta)'(y - X\beta) + \lambda \beta'\beta \right] $$

- 其中超参数 $\lambda$用以控制惩罚的强度

- 估计结果为 $\hat{\beta} = (X'X + \lambda I_{K})^{-1} X'y$，其中 $I_{K}$是 $K \times K$单位矩阵

- 通过向 $X'X$添加对角矩阵（即"岭"），求逆运算时 $\lambda I_{K}$的存在将导致回归系数 $\hat{\beta}$向零收缩

- 若 $X$是正交矩阵，即 $X'X = I_{K}$，岭回归将OLS估计值中的每个回归系数向零等比例收缩，即 $\hat{\beta}_j = \hat{\beta}_{j,OLS}/(1+\lambda)$

---

## Lasso回归

- 在最小化误差平方和的基础上，补充了 $L^1$范数惩罚项 $||\beta||_{1} = \sum_{j=1}^{K} |\beta_j|$：

$$\min_{\beta} \left[ \frac{1}{N} (y - X\beta)'(y - X\beta) + \gamma \sum_{j=1}^{K} |\beta_j| \right]$$

- 其中超参数 $\gamma$用以控制惩罚的强度

- 无解析解，仅有数值解

- 若 $X$是正交矩阵，Lasso将OLS估计值向零移动一个固定量 $\lambda$，即 $\hat{\beta}_j = sgn(\hat{\beta}_{j,OLS})(|\hat{\beta}_{j,OLS}| - \gamma)_{+}$

---

## 弹性网

- 弹性网结合了岭回归和Lasso的惩罚项：

$$\min_{\beta} \left[ \frac{1}{N} (y - X\beta)'(y - X\beta) + \gamma_1 \sum_{j=1}^{K} |\beta_j| + \gamma_2 \beta'\beta \right]$$

- 与Lasso一样，弹性网会将一些回归系数设为零，但它并非像Lasso那样强调变量选择，而是会同时对回归系数采取一些类似岭回归的收缩

---

## 分类问题描述

.pull-left[

- 通过由 $K \times 1$维向量 $x$表示的 $K$个观测到的预测变量（特征）来预测结果 $y$

- 二分类：预测结果 $y \in \{0,1\}$

- 多分类：预测结果 $y \in \{C_1, ..., C_L\}$

    - 有些二分类模型可以直接推广到多分类

    - 利用二分类模型来解决多分类问题，基本思路是"拆解法"，即将多分类任务拆为若干个二分类任务求解

]

.pull-right[

![:scale 50%](MLpics/Classify 2.png)

![:scale 70%](MLpics/Classify m.png)

]

---

## 类别不平衡问题

分类任务中不同类别的训练样本数量差别很大

- 类别平衡时，预测值 $y > 0.5$(即 $y/(1-y) > 1$)判别为正例，否则为反例

- 类别不平衡时，假定正类样本数量 $m^{+}$较少，反类样本数量 $m^{-}$较多，则 $\frac{y}{1-y} > \frac{m^{+}}{m^{-}}$ 时预测为正例

- "再缩放"策略：当 $\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^{-}}{m^{+}} > 1$ 时预测为正例

解决方案：

- 直接对训练集里的反类样本进行"欠抽样"(undersampling)

- 对训练集里的正类样本进行"过抽样"(oversampling)

- 直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将再缩放的方法嵌入到其决策过程中，称为"阈值移动"(threshold-moving)

---

## Logistic / Probit回归

.pull-left[

广义线性模型：使用Sigmoid函数 $g(.)$将分类任务的 $y$与线性回归模型的预测值联系起来

$$y_i = g(z_i) = g(x_i' \beta + \epsilon_i)$$

常用Sigmoid函数：

- 对数几率函数 / 逻辑函数(logistic function)： $\frac{1}{1+e^{-z}}$

- 对数几率回归 / 逻辑回归：假设对数几率(log odds)是线性的 $\ln \frac{y_i}{1-y_i} = x_i' \beta + \epsilon_i$

- 标准正态分布累计密度函数(CDF)： $\Phi(z)$

- Probit回归： $y_i = \Phi(x_i' \beta + \epsilon_i)$

]

.pull-right[

![:scale 90%](MLpics/Logistic.jpg)

]

---

## 线性判别分析

.pull-left[

思想：设法将样本投影到直线上，使得同类样本的投影点尽可能接近、异类样本的投影点尽可能远离

- 给定数据集 $D = \{ (x_i, y_i)\}_{i=1}^N$， $y_i \in \{0,1\}$

- 令 $X_i$、 $\mu_i$、 $\Sigma_i$分别表示第 $i \in \{0,1\}$类样本的集合、均值向量、协方差矩阵

- 若将样本投影到直线 $\beta$上，两类样本的均值在直线的投影分别为 $\beta' \mu_0$和 $\beta' \mu_1$，两类样本的协方差分别为 $\beta' \Sigma_0 \beta$和 $\beta' \Sigma_1 \beta$

- 最优化问题：

$$\min_{\beta}~~ -||\beta'\mu_0 - \beta'\mu_1||^2$$
$$s.t.~~ \beta'(\Sigma_0 + \Sigma_1)\beta = 1$$

- 最优解： $\beta = (\Sigma_0 + \Sigma_1)^{-1}(\mu_0 - \mu_1)$

]

.pull-right[

![:scale 90%](MLpics/LDA.jpg)

]

---

## 支持向量机图示

![:scale 70%](MLpics/SVM.png)

---

## 支持向量机：线性可分

- 分类问题 $y_i \in \{-1,1\}$

- 超平面定义为 $\{ x : f(x) = x'\beta + \beta_0 = 0 \}$，其中 $||\beta||=1$

- 在样本类别线性可分的情况下，目标是找到最大间隔

$$\max_{\beta, \beta_0, ||\beta||=1} M$$

$$s.t.~~ y_i(x_i'\beta + \beta_0) \ge M,~ i=1, ..., N$$

- 等同于如下最优问题

$$\min_{\beta, \beta_0} ||\beta||$$

$$s.t.~~ y_i(x_i'\beta + \beta_0) \ge 1,~ i=1, ..., N$$

---

## 支持向量机：线性不可分

- 非线性映射：引入核函数 $h(x)$，最优化问题仅改变约束条件

$$\min_{\beta, \beta_0} ||\beta||$$
$$y_i(h(x_i)'\beta + \beta_0) \ge 1,~ i=1, ..., N$$

- 软间隔与正则化：假设类别间有重叠区域，允许划分超平面两侧有错误的分类，引入松弛变量(slack variables) $\xi_i = (\xi_1, ..., \xi_N)$，最优化问题仅改变约束条件

$$\max_{\beta, \beta_0, ||\beta||=1} M$$

$$y_i(x_i'\beta + \beta_0) \ge M (1-\xi_i)$$
等同于

$$\min_{\beta, \beta_0} ||\beta||$$

$$y_i(x_i'\beta + \beta_0) \ge 1-\xi_i$$

其中 $\xi_i \ge 0$， $\Sigma_{i=1}^N \xi_i \le constant$

---

## 支持向量回归

- 假设能容忍模型输出 $f(x)$与真实输出 $y$之间最多有 $\epsilon$的偏差，即仅当 $f(x)$与 $y$之间的差别绝对值大于 $\epsilon$时才计算损失

- 考虑线性回归问题： $f(x) = x'\beta + \beta_0$，最优化问题如下

$$\min_{\beta, \beta_0} \sum_{i=1}^N V_{\epsilon}(y_i - f(x_i)) + \frac{\lambda}{2}||\beta||^2$$

其中 $\lambda$是正则化常数， $V_{\epsilon}$是 $\epsilon$-不敏感损失( $\epsilon$-insensitive loss)

$$V_{\epsilon}(r) = \left\{
\begin{array}{ll}
0,&  if~|r| \le \epsilon \\
|r| - \epsilon,&  otherwise
\end{array}
\right.$$

- 非线性 $f(x)$则使用核函数 $\{h_m(x)\},~m=1,2,...,M$来逼近： $f(x) = \sum_{m=1}^M \beta_m h_m(x) + \beta_0$，最优化问题变为

$$\min_{\beta, \beta_0} \sum_{i=1}^N V_{\epsilon}(y_i - f(x_i)) + \frac{\lambda}{2}\sum_{m=1}^M \beta_m^2$$

---

## 神经元图示

![:scale 70%](MLpics/Neural Network.jpg)

---

## 多层神经网络的基本结构

.pull-left[

![:scale 85%](MLpics/Neural Network.png)

![:scale 85%](MLpics/Neural-Network-Architecture.png)

]

.pull-right[

- 组合函数往往是线性的

- 隐藏层的激活函数常使用修正线性单元(ReLU)

$$ReLU(u) = \left\{
\begin{array}{ll}
u,&  if~u > 0 \\
0,&  if~u \le 0
\end{array}
\right.$$

- 输出层的激活函数取决于因变量

    - 因变量为二分类：Sigmoid函数，如Logistic函数 $\frac{1}{1+\exp(-u)}$

    - 因变量为多分类：Softmax函数 $\frac{\exp(u_j)}{\sum_{j=1}^J \exp(u_j)}$

    - 因变量为连续变量：线性函数

]

---

## 最优化问题与优化方法

- 单个样本点的误差函数 $(x_i, y_i)$定义为均方误差 $E_i = (y_i - \hat{y_i})^2$

- 总样本的误差函数定义为 $E = \frac{1}{N} \sum_{i=1}^N E_i$

- 使用迭代算法搜寻最优参数 $w$

    - 递增学习模式（随机梯度下降）：每个训练样本输入神经网络后，参数都进行更新，此时使用单个观测的误差函数 $E_i$

    - 批学习模式（批次梯度下降）：所有训练样本输入神经网络后，参数才进行更新，此时使用的是总误差函数 $E$

---

## 反向传播算法(BP算法)

.pull-left[

- 将输入样本提供给输入层神经元

- 初始化连接权重参数 $w^{(0)}$

- 逐层将信号向前传至隐藏层、输出层，产生输出层结果 $\hat{y_i}$

- 计算输出层误差 $E_i$

- 将误差反向传播至隐藏层神经元

- 根据隐藏层神经元对权重进行更新 $w^{(1)}$

- 上述过程循环进行，直至达到某些停止条件为止

]

.pull-right[

![:scale 55%](MLpics/BP.png)

]

---

## BP算法的权重更新

- BP算法基于梯度下降策略

- 令 $w^{(m)}$表示迭代第 $m$步的某个参数 $w$的值

- 使用链式法则计算梯度 $\frac{\partial E_i}{\partial w}$，它给出了误差函数上升最快的方向，因此它的负值给出了误差函数下降最快的方向

- 权重更新规则是

$$w^{(m+1)} = w^{(m)} + \Delta w^{(m)}$$

$$\Delta w^{(m)} = -\eta \left. \frac{\partial E_i}{\partial w} \right|_{w = w^{(m)}}$$

- 其中 $\eta$是学习率，是沿着梯度方向更新的步长

- 如果 $\eta$太小，训练过程会很慢；如果 $\eta$太大， $w$可能围着最优值反复震荡，甚至会跳出最优值附近的范围

---

## 防止过拟合的方法

- 缩小网络规模：限制模型中可学习参数的数量（由层数和每层单元数决定）

- 添加权重正则化：思想是在误差目标函数中增加一个用于描述网络复杂度的部分，如权重系数的绝对值( $L^1$范数)或者权重系数的值的二次方( $L^2$范数)

- 添加dropout：应用于层的dropout包括在训练期间随机丢弃（设置为零）层的多个输出特征，dropout率往往设置为0.2-0.5

---

## 决策树示例

![:scale 90%](MLpics/Decision Tree Animal.png)

---

## 决策树的基本概念

.pull-left[

![:scale 90%](MLpics/Decision Tree.png)

]

.pull-right[

一棵决策树包含

- 一个根节点(root node)

- 若干个叶节点(leaf node)，包含决策结果

- 若干个非叶节点(decision node)，根据属性进行分枝

超参

- 树的最大深度(maximum depth)

- 叶节点包含的最小样本数量

]

---

## 决策树的生长

.pull-left[

递归的过程、贪心的算法

- 从根节点出发开始选择最优划分属性，确定分枝准则

- 在某枝再确定进一步最优划分属性和分枝准则

- 直至分枝至叶节点

达到叶节点的标准

- 当前节点包含的样本全属于同一类别，无需划分

- 当前节点所有样本在所有属性上取值相同，无法划分，将其类别设定为该节点所含样本最多的类别

- 当前节点包含的样本集合为空，不能划分，将其类别设定为其父节点所含样本最多的类别

]

.pull-right[

![:scale 45%](MLpics/Decision Tree Grow.jfif)

]

---

## 如何选择最优划分属性

希望决策树的分支节点包含的样本尽可能属于同一类别，即节点的"纯度"越来越高

给定样本集 $D$和属性集 $A$，选择属性 $a$使得节点的"纯度提升(Gain)"达到最大

$$\max_{a \in A} Gain(D,a)$$

![:scale 90%](MLpics/Decision Tree Category.png)

---

## 划分属性选择

::: {layout-ncol="3"}

### 信息增益

假定样本集合中第 $k$类样本所占比例为 $p_k$，信息熵(information entropy)是度量样本集纯度的指标

$$Ent(D) = -\sum_{k=1}^K p_k \log_2 p_k$$

$Ent(D)$的值越小，则 $D$的纯度越高

假设离散属性 $a$有 $J$个可能取值 $\{a^1, a^2, ..., a^J\}$，若使用 $a$来对样本集 $D$进行划分，会产生 $J$个分支节点，其中第 $j$个分支节点包含了 $D$中所有在属性 $a$上取值为 $a^j$的样本，记为 $D^j$。那么用属性 $a$对样本集 $D$尽心那个划分所获得的"信息增益"为

$$Gain(D, a) = Ent(D) - \sum_{j=1}^J \frac{|D^j|}{|D|}Ent(D^j)$$

其中分支节点的权重 $|D^j|/|D|$取决于分支节点的样本数量

在候选属性集合 $A$中，选择"信息增益"最大的属性作为最优划分属性

### 信息增益率

$$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a) = -\sum_{j=1}^J \frac{|D^j|}{|D|} \log_2 \frac{|D^j|}{|D|}$$

- 信息增益准则对可取值数目较多的属性有所偏好，而增益率准则对可取值数目较少的属性有所偏好

- C4.5算法先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的作为最优划分属性

### 基尼指数

样本集 $D$的基尼值为

$$Gini(D) = \sum_{k=1}^K \sum_{k \ne k'} p_k p_k' = 1- \sum_{k=1}^K p_k^2$$

$Gini(D)$的值越小，则 $D$的纯度越高

属性 $a$的基尼指数定义为

$$Gini\_index(D, a) = \sum_{j=1}^J \frac{|D^j|}{|D|} Gini(D^j)$$

在候选属性集合 $A$中，选择基尼指数最小的属性作为最优划分属性

:::

---

## 决策树的修剪：防止过拟合

.pull-left[

![:scale 90%](MLpics/Decision Tree Preprunning.png)

- 在分枝时评估划分前后的泛化性能，来确定是否需要进一步划分

- 减少决策树的训练时间

- 有欠拟合风险

]

.pull-right[

![:scale 90%](MLpics/Decision Tree Postprunning.png)

- 在已经生成的决策树上进行剪枝，从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代
替这棵子树是否有益

- 后剪枝通常比预剪枝决策树保留了更多的分支，欠拟合风险更小，泛化性能往往优于预剪枝决策树

]

---

## 回归树

.pull-left[

- 树把整个输入空间划分为 $M$片，那么预测函数是分片常数

$$f(x) = \sum_{m=1}^M c_m I(x \in R_m)$$

- 根据划分属性 $j$和划分点 $s$，定义划分后的两片 $R_1(j,s) = \{X|X_j \le s\}$和 $R_2(j,s) = \{X|X_j > s\}$

- 选择最优划分属性 $j$和划分点 $s$使得

$$\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2\right]$$

- 最小化误差平方和 $\sum(y_i - c_m)^2$得到每片的预测值是该片输出的均值 $\hat{c}_m = ave(y_i|x_i \in R_m(j,s))$

]

.pull-right[

![:scale 90%](MLpics/Regression Tree.png)

]

---

## Bagging

.pull-left[

![:scale 90%](MLpics/Bagging 1.png)

- 通过有放回抽样(Bootstrap)得到多个新的样本

- 根据每个样本分别建立模型

]

.pull-right[

![:scale 90%](MLpics/Bagging 2.png)

- 对所得的多个模型进行加总，分类问题常使用投票，回归问题常使用平均

]

---

## 随机森林

通过Bagging引入样本随机性之外，在选择划分变量时也引入随机性

![:scale 70%](MLpics/Random Forest.png)

---

## Boosting

![:scale 90%](MLpics/Boosting.png)

---

## AdaBoost

.pull-left[

- 思想：在新树中增加上一棵树中被误分类的样本的权重

![:scale 90%](MLpics/AdaBoost Algo.png)

]

.pull-right[

![:scale 90%](MLpics/Adaptive Boosting.png)

]

---

## Gradient Boosting Decision Tree (GBDT)

- 思想：新树拟合的目标是上一棵树的损失函数的负梯度的值

- 当损失函数选为均方误差时，拟合损失函数的负梯度即为拟合残差

![:scale 50%](MLpics/Gradient Boosting Algo.png)

---

## 模型评估：分类模型

- 错误率与精度

- 查准率、查全率与F1

- ROC与AUC

---

## 错误率与精度

对样本集 $D = {(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$

- 分类错误率定义为

$$E(f; D) = \frac{1}{N} \sum_{i=1}^N I(f(x_i) \ne y_i)$$

- 精度则定义为

$$acc(f; D) = \frac{1}{N} \sum_{i=1}^N I(f(x_i) = y_i) = 1 - E(f; D)$$

---

## 查准率、查全率与F1

![:scale 70%](MLpics/F1.jpg)

---

## ROC与AUC

.pull-left[

- 分类器：预测值大于分类阈值(如0.5)为正例，否则为反例

- 分类阈值由最大慢慢变小可得到ROC "受试者工作特征"(Receiver Operating Characteristic)曲线

- 纵轴是"真正例率(True Positive Rate, TPR)" $TPR = \frac{TP}{TP + FN}$

- 横轴是"假正例率(False Positive Rate, FPR)" $FPR = \frac{FP}{TN +FP}$

- AUC (Area Under ROC Curve)是ROC曲线下的面积

]

.pull-right[

![:scale 90%](MLpics/ROC.png)

]

---

## 模型评估：回归模型

- 均方误差： $\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$

- 均方根误差： $\sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2}$

- 平均绝对误差： $\frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|$

- 平均相对误差： $\frac{1}{N} \sum_{i=1}^N |\frac{y_i - \hat{y}_i}{y_i}|$

---

## 模型选择

.pull-left[

![:scale 90%](MLpics/Data Sub.png)

]

.pull-right[

步骤：

- 数据划分

- 在训练集上进行数据建模

- 运用验证集调整超参

- 使用测试集来选择模型

]

---

## 数据划分方法

.pull-left[

- 留出法(hold-out)

![:scale 65%](MLpics/Hold Out.png)

- 交叉验证法(cross validation)

![:scale 75%](MLpics/Cross Validation.png)

]

.pull-right[

- 自助法(bootstrapping)

    - 留出法和交叉验证法中总是保留一部分样本用于测试，会引入因训练样本规模比总样本小而导致的估计偏差

    - 对包含 $N$个样本的数据集 $D$进行有放回抽样：每次抽一个样本，一共抽 $N$次，得到数据集 $D'$，不被抽中的概率是 $(1-\frac{1}{N})^N \rightarrow 0.368$

    - 将 $D'$用作训练集， $D \setminus D'$用作测试集

![:scale 70%](MLpics/Bootstrap.jpg)

]
