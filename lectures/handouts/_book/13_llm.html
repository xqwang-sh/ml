<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; 文本分析3：大语言模型及其应用 – 数据挖掘与机器学习课程讲义</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./project1_LC.html" rel="next">
<link href="./12_embedding.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ed6da6eef3892af8a4b5ed59bfb951f5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13_llm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析3：大语言模型及其应用</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">数据挖掘与机器学习课程讲义</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">前言</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_ml_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">机器学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab02_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_model_assess.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_ts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">时间序列监督学习</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab03_titanic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">泰坦尼克号生存预测实践</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_credit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">信用评分理论基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">非监督学习技术概览及其金融应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">非监督学习：聚类 (Clustering)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">非监督学习：降维 (Dimensionality Reduction)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">文本分析1：词频法与向量空间</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析2：词向量与深度学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析3：大语言模型及其应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project1_LC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2a_tspred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">项目2A：上证综指收益率时间序列预测</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2b_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">项目2B：财务报表文本分析与企业风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">《数据挖掘与机器学习》期末考试复习指南</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#从静态向量到动态表示" id="toc-从静态向量到动态表示" class="nav-link active" data-scroll-target="#从静态向量到动态表示"><span class="header-section-number">16</span> 从静态向量到动态表示</a>
  <ul class="collapse">
  <li><a href="#word2vec的局限性" id="toc-word2vec的局限性" class="nav-link" data-scroll-target="#word2vec的局限性"><span class="header-section-number">16.1</span> Word2Vec的局限性</a></li>
  <li><a href="#上下文感知的词表示" id="toc-上下文感知的词表示" class="nav-link" data-scroll-target="#上下文感知的词表示"><span class="header-section-number">16.2</span> 上下文感知的词表示</a></li>
  <li><a href="#语言模型理解上下文的基础" id="toc-语言模型理解上下文的基础" class="nav-link" data-scroll-target="#语言模型理解上下文的基础"><span class="header-section-number">16.3</span> 语言模型：理解上下文的基础</a></li>
  <li><a href="#从elmo到bert的演进" id="toc-从elmo到bert的演进" class="nav-link" data-scroll-target="#从elmo到bert的演进"><span class="header-section-number">16.4</span> 从ELMo到BERT的演进</a>
  <ul class="collapse">
  <li><a href="#elmo-2018" id="toc-elmo-2018" class="nav-link" data-scroll-target="#elmo-2018"><span class="header-section-number">16.4.1</span> ELMo (2018)</a></li>
  <li><a href="#gpt-2018" id="toc-gpt-2018" class="nav-link" data-scroll-target="#gpt-2018"><span class="header-section-number">16.4.2</span> GPT (2018)</a></li>
  <li><a href="#bert-2018" id="toc-bert-2018" class="nav-link" data-scroll-target="#bert-2018"><span class="header-section-number">16.4.3</span> BERT (2018)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert原理深度解析" id="toc-bert原理深度解析" class="nav-link" data-scroll-target="#bert原理深度解析"><span class="header-section-number">17</span> BERT原理深度解析</a>
  <ul class="collapse">
  <li><a href="#transformer架构bert的基础" id="toc-transformer架构bert的基础" class="nav-link" data-scroll-target="#transformer架构bert的基础"><span class="header-section-number">17.1</span> Transformer架构：BERT的基础</a>
  <ul class="collapse">
  <li><a href="#自注意力机制" id="toc-自注意力机制" class="nav-link" data-scroll-target="#自注意力机制"><span class="header-section-number">17.1.1</span> 自注意力机制</a></li>
  <li><a href="#多头注意力" id="toc-多头注意力" class="nav-link" data-scroll-target="#多头注意力"><span class="header-section-number">17.1.2</span> 多头注意力</a></li>
  <li><a href="#位置编码" id="toc-位置编码" class="nav-link" data-scroll-target="#位置编码"><span class="header-section-number">17.1.3</span> 位置编码</a></li>
  <li><a href="#前馈神经网络" id="toc-前馈神经网络" class="nav-link" data-scroll-target="#前馈神经网络"><span class="header-section-number">17.1.4</span> 前馈神经网络</a></li>
  <li><a href="#transformer编码器结构" id="toc-transformer编码器结构" class="nav-link" data-scroll-target="#transformer编码器结构"><span class="header-section-number">17.1.5</span> Transformer编码器结构</a></li>
  </ul></li>
  <li><a href="#bert模型详解" id="toc-bert模型详解" class="nav-link" data-scroll-target="#bert模型详解"><span class="header-section-number">17.2</span> BERT模型详解</a>
  <ul class="collapse">
  <li><a href="#bert的输入表示" id="toc-bert的输入表示" class="nav-link" data-scroll-target="#bert的输入表示"><span class="header-section-number">17.2.1</span> BERT的输入表示</a></li>
  <li><a href="#bert的预训练任务" id="toc-bert的预训练任务" class="nav-link" data-scroll-target="#bert的预训练任务"><span class="header-section-number">17.2.2</span> BERT的预训练任务</a></li>
  <li><a href="#bert的模型变体" id="toc-bert的模型变体" class="nav-link" data-scroll-target="#bert的模型变体"><span class="header-section-number">17.2.3</span> BERT的模型变体</a></li>
  <li><a href="#bert的微调方式" id="toc-bert的微调方式" class="nav-link" data-scroll-target="#bert的微调方式"><span class="header-section-number">17.2.4</span> BERT的微调方式</a></li>
  </ul></li>
  <li><a href="#bert的内部工作机制" id="toc-bert的内部工作机制" class="nav-link" data-scroll-target="#bert的内部工作机制"><span class="header-section-number">17.3</span> BERT的内部工作机制</a>
  <ul class="collapse">
  <li><a href="#层次化语言知识" id="toc-层次化语言知识" class="nav-link" data-scroll-target="#层次化语言知识"><span class="header-section-number">17.3.1</span> 层次化语言知识</a></li>
  <li><a href="#注意力头的专业化" id="toc-注意力头的专业化" class="nav-link" data-scroll-target="#注意力头的专业化"><span class="header-section-number">17.3.2</span> 注意力头的专业化</a></li>
  <li><a href="#bert的表示空间" id="toc-bert的表示空间" class="nav-link" data-scroll-target="#bert的表示空间"><span class="header-section-number">17.3.3</span> BERT的表示空间</a></li>
  </ul></li>
  <li><a href="#bert的后续演进" id="toc-bert的后续演进" class="nav-link" data-scroll-target="#bert的后续演进"><span class="header-section-number">17.4</span> BERT的后续演进</a>
  <ul class="collapse">
  <li><a href="#预训练任务优化" id="toc-预训练任务优化" class="nav-link" data-scroll-target="#预训练任务优化"><span class="header-section-number">17.4.1</span> 预训练任务优化</a></li>
  <li><a href="#知识增强" id="toc-知识增强" class="nav-link" data-scroll-target="#知识增强"><span class="header-section-number">17.4.2</span> 知识增强</a></li>
  <li><a href="#模型架构改进" id="toc-模型架构改进" class="nav-link" data-scroll-target="#模型架构改进"><span class="header-section-number">17.4.3</span> 模型架构改进</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#从bert到大语言模型" id="toc-从bert到大语言模型" class="nav-link" data-scroll-target="#从bert到大语言模型"><span class="header-section-number">18</span> 从BERT到大语言模型</a>
  <ul class="collapse">
  <li><a href="#transformer架构的扩展" id="toc-transformer架构的扩展" class="nav-link" data-scroll-target="#transformer架构的扩展"><span class="header-section-number">18.1</span> Transformer架构的扩展</a>
  <ul class="collapse">
  <li><a href="#编码器-解码器结构" id="toc-编码器-解码器结构" class="nav-link" data-scroll-target="#编码器-解码器结构"><span class="header-section-number">18.1.1</span> 编码器-解码器结构</a></li>
  <li><a href="#仅解码器架构" id="toc-仅解码器架构" class="nav-link" data-scroll-target="#仅解码器架构"><span class="header-section-number">18.1.2</span> 仅解码器架构</a></li>
  <li><a href="#长距离建模" id="toc-长距离建模" class="nav-link" data-scroll-target="#长距离建模"><span class="header-section-number">18.1.3</span> 长距离建模</a></li>
  </ul></li>
  <li><a href="#大型语言模型的关键创新" id="toc-大型语言模型的关键创新" class="nav-link" data-scroll-target="#大型语言模型的关键创新"><span class="header-section-number">18.2</span> 大型语言模型的关键创新</a>
  <ul class="collapse">
  <li><a href="#规模扩展" id="toc-规模扩展" class="nav-link" data-scroll-target="#规模扩展"><span class="header-section-number">18.2.1</span> 规模扩展</a></li>
  <li><a href="#涌现能力" id="toc-涌现能力" class="nav-link" data-scroll-target="#涌现能力"><span class="header-section-number">18.2.2</span> 涌现能力</a></li>
  <li><a href="#提示工程与思维链推理" id="toc-提示工程与思维链推理" class="nav-link" data-scroll-target="#提示工程与思维链推理"><span class="header-section-number">18.2.3</span> 提示工程与思维链推理</a></li>
  </ul></li>
  <li><a href="#代表性大型语言模型" id="toc-代表性大型语言模型" class="nav-link" data-scroll-target="#代表性大型语言模型"><span class="header-section-number">18.3</span> 代表性大型语言模型</a>
  <ul class="collapse">
  <li><a href="#gpt系列" id="toc-gpt系列" class="nav-link" data-scroll-target="#gpt系列"><span class="header-section-number">18.3.1</span> GPT系列</a></li>
  <li><a href="#开源大型语言模型" id="toc-开源大型语言模型" class="nav-link" data-scroll-target="#开源大型语言模型"><span class="header-section-number">18.3.2</span> 开源大型语言模型</a></li>
  </ul></li>
  <li><a href="#大语言模型的金融应用" id="toc-大语言模型的金融应用" class="nav-link" data-scroll-target="#大语言模型的金融应用"><span class="header-section-number">18.4</span> 大语言模型的金融应用</a>
  <ul class="collapse">
  <li><a href="#信息提取与分析" id="toc-信息提取与分析" class="nav-link" data-scroll-target="#信息提取与分析"><span class="header-section-number">18.4.1</span> 信息提取与分析</a></li>
  <li><a href="#金融文本生成" id="toc-金融文本生成" class="nav-link" data-scroll-target="#金融文本生成"><span class="header-section-number">18.4.2</span> 金融文本生成</a></li>
  <li><a href="#无监督学习辅助" id="toc-无监督学习辅助" class="nav-link" data-scroll-target="#无监督学习辅助"><span class="header-section-number">18.4.3</span> 无监督学习辅助</a></li>
  </ul></li>
  <li><a href="#大语言模型的使用指南" id="toc-大语言模型的使用指南" class="nav-link" data-scroll-target="#大语言模型的使用指南"><span class="header-section-number">18.5</span> 大语言模型的使用指南</a>
  <ul class="collapse">
  <li><a href="#提示词工程基础" id="toc-提示词工程基础" class="nav-link" data-scroll-target="#提示词工程基础"><span class="header-section-number">18.5.1</span> 提示词工程基础</a></li>
  <li><a href="#提示词模板示例" id="toc-提示词模板示例" class="nav-link" data-scroll-target="#提示词模板示例"><span class="header-section-number">18.5.2</span> 提示词模板示例</a></li>
  <li><a href="#提示词优化技巧" id="toc-提示词优化技巧" class="nav-link" data-scroll-target="#提示词优化技巧"><span class="header-section-number">18.5.3</span> 提示词优化技巧</a></li>
  <li><a href="#常见应用场景示例" id="toc-常见应用场景示例" class="nav-link" data-scroll-target="#常见应用场景示例"><span class="header-section-number">18.5.4</span> 常见应用场景示例</a></li>
  <li><a href="#提示词工程最佳实践" id="toc-提示词工程最佳实践" class="nav-link" data-scroll-target="#提示词工程最佳实践"><span class="header-section-number">18.5.5</span> 提示词工程最佳实践</a></li>
  <li><a href="#注意事项" id="toc-注意事项" class="nav-link" data-scroll-target="#注意事项"><span class="header-section-number">18.5.6</span> 注意事项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#小结与进阶方向" id="toc-小结与进阶方向" class="nav-link" data-scroll-target="#小结与进阶方向"><span class="header-section-number">19</span> 小结与进阶方向</a>
  <ul class="collapse">
  <li><a href="#从静态向量到大语言模型的演进" id="toc-从静态向量到大语言模型的演进" class="nav-link" data-scroll-target="#从静态向量到大语言模型的演进"><span class="header-section-number">19.1</span> 从静态向量到大语言模型的演进</a></li>
  <li><a href="#无监督学习的新范式" id="toc-无监督学习的新范式" class="nav-link" data-scroll-target="#无监督学习的新范式"><span class="header-section-number">19.2</span> 无监督学习的新范式</a></li>
  <li><a href="#金融文本分析的未来方向" id="toc-金融文本分析的未来方向" class="nav-link" data-scroll-target="#金融文本分析的未来方向"><span class="header-section-number">19.3</span> 金融文本分析的未来方向</a></li>
  <li><a href="#进阶学习资源" id="toc-进阶学习资源" class="nav-link" data-scroll-target="#进阶学习资源"><span class="header-section-number">19.4</span> 进阶学习资源</a></li>
  <li><a href="#本讲小结" id="toc-本讲小结" class="nav-link" data-scroll-target="#本讲小结"><span class="header-section-number">19.5</span> 本讲小结</a></li>
  </ul></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料"><span class="header-section-number">20</span> 参考资料</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析3：大语言模型及其应用</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="从静态向量到动态表示" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> 从静态向量到动态表示</h1>
<section id="word2vec的局限性" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="word2vec的局限性"><span class="header-section-number">16.1</span> Word2Vec的局限性</h2>
<p>上一讲中，我们学习了Word2Vec等词向量技术，它通过分布式表示极大提升了NLP的表示能力。然而，静态词向量仍然存在明显局限性：</p>
<ol type="1">
<li><p><strong>一词一向量问题</strong>：每个词只对应一个固定的向量，无法处理一词多义。例如”苹果”在”我吃了一个苹果”和”苹果公司发布新产品”中的含义完全不同。</p></li>
<li><p><strong>上下文无关</strong>：词向量无法捕捉词语在特定上下文中的含义变化。例如”银行存款”和”河流的银行”中，“银行”的含义有很大差异。</p></li>
<li><p><strong>长距离依赖问题</strong>：无法捕捉句子中相距较远的词之间的依赖关系。例如”他说中文，因为他在中国生活了很多年”中，第二个”他”与第一个”他”指代相同。</p></li>
<li><p><strong>表达能力有限</strong>：固定维度的向量难以编码复杂的语言知识和语法结构。</p></li>
</ol>
<p>这些局限性促使研究者探索更先进的表示方法，能够根据上下文动态调整词语的表示。这一探索最终导致了BERT等基于Transformer的语言模型的诞生。</p>
</section>
<section id="上下文感知的词表示" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="上下文感知的词表示"><span class="header-section-number">16.2</span> 上下文感知的词表示</h2>
<p><strong>上下文感知的词表示</strong>（Contextualized Word Representations）是指词语的向量表示会根据其所处的上下文动态变化。与静态词向量不同，它具有以下特点：</p>
<ol type="1">
<li><strong>动态表示</strong>：同一个词在不同上下文中具有不同的向量表示</li>
<li><strong>语义消歧</strong>：能够根据上下文区分多义词的不同含义</li>
<li><strong>句法感知</strong>：能够捕捉词语在句子中的句法功能</li>
<li><strong>长距离依赖</strong>：能够建模句子中远距离词语之间的关系</li>
</ol>
<p>这种表示方法的核心思想是：<strong>一个词的含义不仅取决于它自身，更取决于它的上下文环境</strong>。</p>
</section>
<section id="语言模型理解上下文的基础" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="语言模型理解上下文的基础"><span class="header-section-number">16.3</span> 语言模型：理解上下文的基础</h2>
<p>上下文感知表示的关键在于<strong>语言模型</strong>（Language Model）。语言模型是一种能够计算文本序列概率的模型，其基本任务是预测序列中的下一个词：</p>
<p><span class="math display">\[P(w_t | w_1, w_2, ..., w_{t-1})\]</span></p>
<p>不同类型的语言模型处理上下文的方式不同：</p>
<ol type="1">
<li><p><strong>传统n-gram语言模型</strong>：只考虑有限历史，如<span class="math inline">\(P(w_t | w_{t-2}, w_{t-1})\)</span></p></li>
<li><p><strong>循环神经网络(RNN)语言模型</strong>：通过隐藏状态递归编码全部历史</p></li>
<li><p><strong>双向语言模型</strong>：同时考虑左侧和右侧上下文</p></li>
<li><p><strong>Transformer语言模型</strong>：通过注意力机制直接建模所有位置间的依赖关系</p></li>
</ol>
<p>预训练语言模型的出现为NLP带来了革命性变化，它通过在大规模语料上无监督预训练，学习通用的语言表示，然后再针对下游任务进行微调。</p>
</section>
<section id="从elmo到bert的演进" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="从elmo到bert的演进"><span class="header-section-number">16.4</span> 从ELMo到BERT的演进</h2>
<p>上下文感知的词表示技术的发展经历了几个里程碑：</p>
<section id="elmo-2018" class="level3" data-number="16.4.1">
<h3 data-number="16.4.1" class="anchored" data-anchor-id="elmo-2018"><span class="header-section-number">16.4.1</span> ELMo (2018)</h3>
<p>ELMo (Embeddings from Language Models) 是上下文词表示的早期尝试，由Peters等人在2018年提出。其特点包括：</p>
<ul>
<li>使用双层双向LSTM结构</li>
<li>将前向和后向语言模型结合</li>
<li>使用不同层的表示的加权组合作为最终表示</li>
<li>有效解决了一词多义问题</li>
</ul>
<p>ELMo的表示公式为：</p>
<p><span class="math display">\[ELMo_k^{task} = E(R_k; \Theta^{task}) = \gamma^{task} \sum_{j=0}^{L} s_j^{task} \mathbf{h}_{k,j}^{LM}\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{h}_{k,j}^{LM}\)</span>是第k个词在第j层的表示。</p>
</section>
<section id="gpt-2018" class="level3" data-number="16.4.2">
<h3 data-number="16.4.2" class="anchored" data-anchor-id="gpt-2018"><span class="header-section-number">16.4.2</span> GPT (2018)</h3>
<p>OpenAI的GPT (Generative Pre-Training) 模型采用了单向Transformer结构：</p>
<ul>
<li>仅使用前向语言模型（只看左侧上下文）</li>
<li>基于Transformer解码器架构</li>
<li>首次展示了大规模预训练+微调的范式</li>
</ul>
<p>GPT采用的预训练目标是预测下一个词：</p>
<p><span class="math display">\[L(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta)\]</span></p>
</section>
<section id="bert-2018" class="level3" data-number="16.4.3">
<h3 data-number="16.4.3" class="anchored" data-anchor-id="bert-2018"><span class="header-section-number">16.4.3</span> BERT (2018)</h3>
<p>BERT (Bidirectional Encoder Representations from Transformers) 由Google在2018年提出，成为上下文词表示的里程碑工作：</p>
<ul>
<li>使用双向Transformer编码器</li>
<li>采用掩码语言模型(Masked LM)预训练</li>
<li>同时使用下一句预测(NSP)任务</li>
<li>极大提升了NLP任务的性能上限</li>
</ul>
<p>BERT的预训练目标是预测被掩码的词：</p>
<p><span class="math display">\[L(\mathcal{D}) = \sum_{i \in \mathcal{M}} \log P(w_i | w_{\neg \mathcal{M}}; \Theta)\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{M}\)</span>是被掩码的词的位置集合。</p>
<p>这一演进体现了以下趋势： - 从浅层网络到深层Transformer架构 - 从单向上下文到双向上下文 - 从特征提取器到通用语言模型 - 从任务相关到预训练-微调范式</p>
<p>接下来，我们将深入理解BERT模型的内部工作原理。</p>
</section>
</section>
</section>
<section id="bert原理深度解析" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> BERT原理深度解析</h1>
<section id="transformer架构bert的基础" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="transformer架构bert的基础"><span class="header-section-number">17.1</span> Transformer架构：BERT的基础</h2>
<p>BERT建立在Transformer架构之上，这是由Vaswani等人在2017年提出的一种完全基于注意力机制的神经网络结构。在深入BERT之前，我们需要先理解Transformer的基本组件。</p>
<section id="自注意力机制" class="level3" data-number="17.1.1">
<h3 data-number="17.1.1" class="anchored" data-anchor-id="自注意力机制"><span class="header-section-number">17.1.1</span> 自注意力机制</h3>
<p><strong>自注意力</strong>（Self-Attention）是Transformer的核心组件，它允许模型在处理某个位置时，考虑序列中所有位置的信息。其计算过程如下：</p>
<ol type="1">
<li><p>将输入向量<span class="math inline">\(X\)</span>分别转换为查询(Query)、键(Key)和值(Value)三个矩阵： <span class="math display">\[Q = XW^Q, K = XW^K, V = XW^V\]</span></p></li>
<li><p>计算注意力得分并归一化： <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p></li>
<li><p>其中，<span class="math inline">\(\sqrt{d_k}\)</span>是缩放因子，用于防止梯度消失。</p></li>
</ol>
<p>自注意力机制的优势在于： - 可以捕捉任意距离的依赖关系 - 计算复杂度相对RNN低 - 允许并行计算</p>
</section>
<section id="多头注意力" class="level3" data-number="17.1.2">
<h3 data-number="17.1.2" class="anchored" data-anchor-id="多头注意力"><span class="header-section-number">17.1.2</span> 多头注意力</h3>
<p>为了增强模型的表示能力，Transformer使用了<strong>多头注意力</strong>（Multi-Head Attention）：</p>
<p><span class="math display">\[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\]</span></p>
<p>其中，<span class="math inline">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span></p>
<p>多头注意力允许模型: - 在不同子空间中学习不同的关注模式 - 同时关注位置和语义信息 - 提供更丰富的特征表示</p>
</section>
<section id="位置编码" class="level3" data-number="17.1.3">
<h3 data-number="17.1.3" class="anchored" data-anchor-id="位置编码"><span class="header-section-number">17.1.3</span> 位置编码</h3>
<p>由于自注意力机制本身不包含位置信息，Transformer引入了<strong>位置编码</strong>（Positional Encoding）来将序列顺序信息注入模型：</p>
<p><span class="math display">\[PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\]</span> <span class="math display">\[PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\]</span></p>
<p>其中，<span class="math inline">\(pos\)</span>是位置，<span class="math inline">\(i\)</span>是维度。</p>
</section>
<section id="前馈神经网络" class="level3" data-number="17.1.4">
<h3 data-number="17.1.4" class="anchored" data-anchor-id="前馈神经网络"><span class="header-section-number">17.1.4</span> 前馈神经网络</h3>
<p>Transformer中每个子层还包含一个<strong>前馈神经网络</strong>（Feed-Forward Network），由两个线性变换组成：</p>
<p><span class="math display">\[FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\]</span></p>
</section>
<section id="transformer编码器结构" class="level3" data-number="17.1.5">
<h3 data-number="17.1.5" class="anchored" data-anchor-id="transformer编码器结构"><span class="header-section-number">17.1.5</span> Transformer编码器结构</h3>
<p>一个完整的Transformer编码器层包含： 1. 多头自注意力机制 2. 层归一化（Layer Normalization） 3. 前馈神经网络 4. 残差连接（Residual Connection）</p>
<p>这些组件按以下方式组合： <span class="math display">\[\hat{h} = LayerNorm(x + MultiHeadAttention(x))\]</span> <span class="math display">\[h = LayerNorm(\hat{h} + FFN(\hat{h}))\]</span></p>
<p>BERT使用了Transformer的编码器部分，通常包含12层（BERT-Base）或24层（BERT-Large）。</p>
</section>
</section>
<section id="bert模型详解" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="bert模型详解"><span class="header-section-number">17.2</span> BERT模型详解</h2>
<p>BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，旨在学习深层的双向语言表示。</p>
<section id="bert的输入表示" class="level3" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="bert的输入表示"><span class="header-section-number">17.2.1</span> BERT的输入表示</h3>
<p>BERT的输入由三种嵌入的总和组成：</p>
<ol type="1">
<li><strong>词嵌入</strong>（Token Embeddings）：WordPiece词表中的词元对应的嵌入</li>
<li><strong>段嵌入</strong>（Segment Embeddings）：区分句子对中的第一句和第二句</li>
<li><strong>位置嵌入</strong>（Position Embeddings）：表示词元在序列中的位置</li>
</ol>
<p>每个输入序列以特殊标记<code>[CLS]</code>开始，以<code>[SEP]</code>分隔不同句子。</p>
</section>
<section id="bert的预训练任务" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="bert的预训练任务"><span class="header-section-number">17.2.2</span> BERT的预训练任务</h3>
<p>BERT通过两个无监督任务进行预训练：</p>
<ol type="1">
<li><strong>掩码语言模型（Masked Language Model，MLM）</strong>：
<ul>
<li>随机掩盖输入中15%的词元</li>
<li>其中80%用<code>[MASK]</code>替换，10%用随机词替换，10%保持不变</li>
<li>训练模型预测被掩盖的原始词元</li>
<li>这使得BERT能够学习双向上下文表示</li>
</ul></li>
<li><strong>下一句预测（Next Sentence Prediction，NSP）</strong>：
<ul>
<li>给定两个句子，预测第二句是否是第一句的真实后续</li>
<li>训练数据中50%是真实的连续句子，50%是随机句子对</li>
<li>这使得BERT能够理解句子间的关系</li>
</ul></li>
</ol>
</section>
<section id="bert的模型变体" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="bert的模型变体"><span class="header-section-number">17.2.3</span> BERT的模型变体</h3>
<p>BERT有两个主要变体：</p>
<ol type="1">
<li><strong>BERT-Base</strong>：
<ul>
<li>12层Transformer编码器</li>
<li>12个注意力头</li>
<li>768维隐藏层</li>
<li>1.1亿参数</li>
</ul></li>
<li><strong>BERT-Large</strong>：
<ul>
<li>24层Transformer编码器</li>
<li>16个注意力头</li>
<li>1024维隐藏层</li>
<li>3.4亿参数</li>
</ul></li>
</ol>
</section>
<section id="bert的微调方式" class="level3" data-number="17.2.4">
<h3 data-number="17.2.4" class="anchored" data-anchor-id="bert的微调方式"><span class="header-section-number">17.2.4</span> BERT的微调方式</h3>
<p>预训练后的BERT可以通过简单的任务特定层进行微调，适用于多种下游任务：</p>
<ol type="1">
<li><strong>序列级任务</strong>（如分类）：使用<code>[CLS]</code>标记的最终隐藏状态</li>
<li><strong>词元级任务</strong>（如NER）：使用每个词元的最终隐藏状态</li>
<li><strong>句子对任务</strong>（如问答）：同时输入问题和段落，识别答案跨度</li>
</ol>
<p>微调过程通常只需要少量标注数据和训练轮次，极大地降低了NLP任务的门槛。</p>
</section>
</section>
<section id="bert的内部工作机制" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="bert的内部工作机制"><span class="header-section-number">17.3</span> BERT的内部工作机制</h2>
<p>通过深入分析BERT的内部表示，研究者发现BERT的不同层捕捉了不同类型的语言知识：</p>
<section id="层次化语言知识" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="层次化语言知识"><span class="header-section-number">17.3.1</span> 层次化语言知识</h3>
<ol type="1">
<li><strong>底层</strong>（1-4层）：捕捉表面语法特征、词性、局部依赖等</li>
<li><strong>中层</strong>（5-8层）：编码短语级语义和共指关系</li>
<li><strong>高层</strong>（9-12层）：处理长距离依赖和更抽象的语义关系</li>
</ol>
</section>
<section id="注意力头的专业化" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="注意力头的专业化"><span class="header-section-number">17.3.2</span> 注意力头的专业化</h3>
<p>BERT的不同注意力头专注于不同类型的语言信息：</p>
<ol type="1">
<li><strong>语法头</strong>：关注句法依赖关系</li>
<li><strong>语义头</strong>：关注语义相关的词</li>
<li><strong>共指头</strong>：关注指代同一实体的表达</li>
</ol>
</section>
<section id="bert的表示空间" class="level3" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="bert的表示空间"><span class="header-section-number">17.3.3</span> BERT的表示空间</h3>
<p>BERT的表示空间表现出interesting的性质：</p>
<ol type="1">
<li><strong>各向异性</strong>：嵌入向量集中在狭窄的锥体中，而非均匀分布</li>
<li><strong>语义区分</strong>：相似概念在表示空间中形成聚类</li>
<li><strong>线性结构</strong>：某些语义关系可以通过向量差来表示</li>
</ol>
<p>这些特性使得BERT能够有效地编码复杂的语言知识，并为下游任务提供丰富的特征表示。</p>
</section>
</section>
<section id="bert的后续演进" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="bert的后续演进"><span class="header-section-number">17.4</span> BERT的后续演进</h2>
<p>BERT发布后，研究者提出了许多改进版本，主要集中在以下几个方向：</p>
<section id="预训练任务优化" class="level3" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="预训练任务优化"><span class="header-section-number">17.4.1</span> 预训练任务优化</h3>
<ol type="1">
<li><strong>RoBERTa</strong>：移除NSP任务，使用更大批量和更多数据训练</li>
<li><strong>ALBERT</strong>：参数共享和分解嵌入，降低模型大小</li>
<li><strong>ELECTRA</strong>：用判别式替换检测训练，提高效率</li>
</ol>
</section>
<section id="知识增强" class="level3" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="知识增强"><span class="header-section-number">17.4.2</span> 知识增强</h3>
<ol type="1">
<li><strong>KnowBERT</strong>：集成知识库信息</li>
<li><strong>ERNIE</strong>：加入实体和短语级掩码</li>
<li><strong>FinBERT</strong>：针对金融领域的专业知识训练</li>
</ol>
</section>
<section id="模型架构改进" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="模型架构改进"><span class="header-section-number">17.4.3</span> 模型架构改进</h3>
<ol type="1">
<li><strong>SpanBERT</strong>：掩盖连续的文本片段而非单个词</li>
<li><strong>XLNet</strong>：使用排列语言模型，解决掩码带来的预训练-微调不一致</li>
<li><strong>DeBERTa</strong>：解耦注意力机制，增强位置编码</li>
</ol>
<p>这些改进进一步推动了预训练语言模型的发展，为下一代更强大的模型如GPT系列奠定了基础。</p>
</section>
</section>
</section>
<section id="从bert到大语言模型" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> 从BERT到大语言模型</h1>
<section id="transformer架构的扩展" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="transformer架构的扩展"><span class="header-section-number">18.1</span> Transformer架构的扩展</h2>
<p>虽然BERT在NLP领域带来了巨大进步，但它仍然存在一些局限性，如无法进行生成任务和处理长文本。为了克服这些限制，研究者们对Transformer架构进行了多方面扩展。</p>
<section id="编码器-解码器结构" class="level3" data-number="18.1.1">
<h3 data-number="18.1.1" class="anchored" data-anchor-id="编码器-解码器结构"><span class="header-section-number">18.1.1</span> 编码器-解码器结构</h3>
<p><strong>编码器-解码器</strong>（Encoder-Decoder）结构是机器翻译等序列到序列任务的标准架构：</p>
<ol type="1">
<li><strong>编码器</strong>：处理输入序列，生成上下文表示</li>
<li><strong>解码器</strong>：基于编码器输出生成目标序列</li>
<li><strong>交叉注意力</strong>：解码器通过注意力机制访问编码器的输出</li>
</ol>
<p>代表模型： - <strong>T5</strong>：将所有NLP任务统一为文本到文本的转换 - <strong>BART</strong>：通过降噪自编码器预训练</p>
</section>
<section id="仅解码器架构" class="level3" data-number="18.1.2">
<h3 data-number="18.1.2" class="anchored" data-anchor-id="仅解码器架构"><span class="header-section-number">18.1.2</span> 仅解码器架构</h3>
<p><strong>仅解码器</strong>（Decoder-only）架构专注于生成任务，通过自回归方式预测下一个词：</p>
<ol type="1">
<li><strong>单向自注意力</strong>：每个位置只能看到其前面的位置</li>
<li><strong>自回归生成</strong>：逐词生成输出序列</li>
<li><strong>缩放规模</strong>：通过扩大模型规模提升能力</li>
</ol>
<p>代表模型： - <strong>GPT系列</strong>：从GPT-1到GPT-4，规模和能力不断增长 - <strong>LLaMA</strong>：开源的大型语言模型，有效降低了资源需求</p>
</section>
<section id="长距离建模" class="level3" data-number="18.1.3">
<h3 data-number="18.1.3" class="anchored" data-anchor-id="长距离建模"><span class="header-section-number">18.1.3</span> 长距离建模</h3>
<p>处理长文本的能力是大语言模型的关键挑战之一，研究者提出了多种解决方案：</p>
<ol type="1">
<li><strong>稀疏注意力</strong>：如Longformer，只关注局部窗口和全局标记</li>
<li><strong>循环机制</strong>：如Transformer-XL，跨段传递隐藏状态</li>
<li><strong>线性复杂度</strong>：如Linformer，通过低秩近似降低计算量</li>
<li><strong>扩展上下文窗口</strong>：如DeepSeek，将上下文窗口扩展到128K</li>
</ol>
</section>
</section>
<section id="大型语言模型的关键创新" class="level2" data-number="18.2">
<h2 data-number="18.2" class="anchored" data-anchor-id="大型语言模型的关键创新"><span class="header-section-number">18.2</span> 大型语言模型的关键创新</h2>
<p>大型语言模型（LLMs）相比传统BERT模型有几个关键创新：</p>
<section id="规模扩展" class="level3" data-number="18.2.1">
<h3 data-number="18.2.1" class="anchored" data-anchor-id="规模扩展"><span class="header-section-number">18.2.1</span> 规模扩展</h3>
<p>深度学习研究表明，模型规模与性能呈现”幂律”关系，增加参数量能带来显著性能提升：</p>
<ol type="1">
<li><strong>从亿到千亿参数</strong>：BERT-Large有3.4亿参数，而GPT-4估计有超过1万亿参数</li>
<li><strong>计算资源增长</strong>：训练大模型需要数千GPU/TPU，消耗数百万美元</li>
<li><strong>预训练数据扩展</strong>：从GB级语料到TB级语料</li>
</ol>
</section>
<section id="涌现能力" class="level3" data-number="18.2.2">
<h3 data-number="18.2.2" class="anchored" data-anchor-id="涌现能力"><span class="header-section-number">18.2.2</span> 涌现能力</h3>
<p>大语言模型最惊人的特性是<strong>涌现能力</strong>（Emergent Abilities）——在达到一定规模后突然出现的能力：</p>
<ol type="1">
<li><strong>指令跟随</strong>：理解并执行自然语言指令</li>
<li><strong>思维链推理</strong>：通过分步骤推理解决复杂问题</li>
<li><strong>上下文学习</strong>：从少量示例中学习新任务</li>
<li><strong>多模态理解</strong>：结合文本与图像等多种模态信息</li>
</ol>
</section>
<section id="提示工程与思维链推理" class="level3" data-number="18.2.3">
<h3 data-number="18.2.3" class="anchored" data-anchor-id="提示工程与思维链推理"><span class="header-section-number">18.2.3</span> 提示工程与思维链推理</h3>
<p>大语言模型的使用方式也发生了革命性变化：</p>
<ol type="1">
<li><strong>提示工程</strong>（Prompt Engineering）：
<ul>
<li>通过精心设计的提示引导模型行为</li>
<li>不同于传统的微调范式</li>
<li>允许灵活调整模型输出</li>
</ul></li>
<li><strong>思维链推理</strong>（Chain-of-Thought）：
<ul>
<li>让模型先生成推理过程，再给出结论</li>
<li>显著提高模型解决复杂问题的能力</li>
<li>公式：<span class="math inline">\(\text{Prompt} + \text{思考过程} \to \text{更准确的结果}\)</span></li>
</ul></li>
<li><strong>上下文学习</strong>（In-context Learning）：
<ul>
<li>在提示中包含示例，引导模型学习模式</li>
<li>无需参数更新，即可适应新任务</li>
<li>示例：给出几个情感分类示例，模型可泛化到新文本</li>
</ul></li>
</ol>
</section>
</section>
<section id="代表性大型语言模型" class="level2" data-number="18.3">
<h2 data-number="18.3" class="anchored" data-anchor-id="代表性大型语言模型"><span class="header-section-number">18.3</span> 代表性大型语言模型</h2>
<section id="gpt系列" class="level3" data-number="18.3.1">
<h3 data-number="18.3.1" class="anchored" data-anchor-id="gpt系列"><span class="header-section-number">18.3.1</span> GPT系列</h3>
<p>由OpenAI开发的GPT（Generative Pre-trained Transformer）系列是大型语言模型的代表：</p>
<ol type="1">
<li><strong>GPT-1</strong>（2018）：
<ul>
<li>1.17亿参数</li>
<li>首次展示预训练+微调范式</li>
<li>在多个NLP任务上获得突破</li>
</ul></li>
<li><strong>GPT-2</strong>（2019）：
<ul>
<li>15亿参数</li>
<li>展示了零样本学习能力</li>
<li>文本生成质量有显著提升</li>
</ul></li>
<li><strong>GPT-3</strong>（2020）：
<ul>
<li>1750亿参数</li>
<li>展示了惊人的少样本学习能力</li>
<li>可以执行之前未见过的任务</li>
</ul></li>
<li><strong>GPT-4</strong>（2023）：
<ul>
<li>参数规模未公开，估计超过1万亿</li>
<li>多模态能力，支持图像输入</li>
<li>接近人类专家水平的表现</li>
</ul></li>
</ol>
</section>
<section id="开源大型语言模型" class="level3" data-number="18.3.2">
<h3 data-number="18.3.2" class="anchored" data-anchor-id="开源大型语言模型"><span class="header-section-number">18.3.2</span> 开源大型语言模型</h3>
<p>除了GPT系列，开源社区也开发了多种高性能大语言模型：</p>
<ol type="1">
<li><strong>LLaMA系列</strong>：
<ul>
<li>由Meta AI开发</li>
<li>参数规模从7B到65B不等</li>
<li>性能接近闭源商业模型</li>
<li>衍生了许多优秀模型如Vicuna和Alpaca</li>
</ul></li>
<li><strong>国产大模型</strong>：
<ul>
<li><strong>ChatGLM</strong>：清华大学与智谱AI合作开发的双语模型</li>
<li><strong>DeepSeek</strong>：深度求索开发，专注长序列处理</li>
<li><strong>Qwen</strong>：阿里云开发，性能优异的开源模型</li>
</ul></li>
<li><strong>多模态模型</strong>：
<ul>
<li><strong>CLIP</strong>：连接图像和文本的表示学习，能够理解自然语言描述与图像的对应关系</li>
<li><strong>GPT-4V</strong>：具有视觉理解能力的GPT-4变体，可以分析图像内容并生成相关文本描述</li>
<li><strong>Gemini</strong>：Google的多模态大语言模型，能同时处理文本、图像、音频和视频</li>
<li><strong>Claude 3</strong>：Anthropic推出的多模态模型，具有较强的视觉理解和推理能力</li>
<li><strong>多模态Mixtral</strong>：Mistral AI的多模态版本，支持多种输入模态的处理</li>
</ul></li>
</ol>
<p>这些多模态模型极大地扩展了大语言模型的应用场景，使其能够在金融图表分析、文档理解、多媒体内容生成等方面发挥作用。</p>
</section>
</section>
<section id="大语言模型的金融应用" class="level2" data-number="18.4">
<h2 data-number="18.4" class="anchored" data-anchor-id="大语言模型的金融应用"><span class="header-section-number">18.4</span> 大语言模型的金融应用</h2>
<p>大语言模型在金融领域有广泛的应用潜力：</p>
<section id="信息提取与分析" class="level3" data-number="18.4.1">
<h3 data-number="18.4.1" class="anchored" data-anchor-id="信息提取与分析"><span class="header-section-number">18.4.1</span> 信息提取与分析</h3>
<ol type="1">
<li><strong>报告解析</strong>：
<ul>
<li>自动提取财报中的关键财务指标</li>
<li>总结长篇研报要点</li>
<li>识别风险披露声明</li>
</ul></li>
<li><strong>市场情感分析</strong>：
<ul>
<li>分析新闻报道的市场情绪</li>
<li>提取投资者情绪信号</li>
<li>预测市场波动</li>
</ul></li>
<li><strong>事件提取</strong>：
<ul>
<li>从财经新闻中识别重大事件</li>
<li>构建事件知识图谱</li>
<li>分析事件之间的因果关系</li>
</ul></li>
</ol>
</section>
<section id="金融文本生成" class="level3" data-number="18.4.2">
<h3 data-number="18.4.2" class="anchored" data-anchor-id="金融文本生成"><span class="header-section-number">18.4.2</span> 金融文本生成</h3>
<ol type="1">
<li><strong>研究报告生成</strong>：
<ul>
<li>基于数据自动生成财务分析</li>
<li>创建行业趋势报告</li>
<li>生成个股评论</li>
</ul></li>
<li><strong>监管合规</strong>：
<ul>
<li>生成合规声明和披露</li>
<li>检查文档是否符合监管要求</li>
<li>自动更新合规文件</li>
</ul></li>
<li><strong>客户交互</strong>：
<ul>
<li>智能金融顾问</li>
<li>个性化投资建议</li>
<li>金融知识普及</li>
</ul></li>
</ol>
</section>
<section id="无监督学习辅助" class="level3" data-number="18.4.3">
<h3 data-number="18.4.3" class="anchored" data-anchor-id="无监督学习辅助"><span class="header-section-number">18.4.3</span> 无监督学习辅助</h3>
<ol type="1">
<li><strong>文本聚类</strong>：
<ul>
<li>通过嵌入向量聚类发现主题</li>
<li>识别相似公告和报告</li>
<li>发现市场关注热点</li>
</ul></li>
<li><strong>异常检测</strong>：
<ul>
<li>识别异常金融叙述</li>
<li>发现财报中的可疑部分</li>
<li>预警潜在风险信号</li>
</ul></li>
<li><strong>主题提取</strong>：
<ul>
<li>无监督发现文档主题</li>
<li>总结长文本的核心观点</li>
<li>追踪主题随时间的演变</li>
</ul></li>
</ol>
</section>
</section>
<section id="大语言模型的使用指南" class="level2" data-number="18.5">
<h2 data-number="18.5" class="anchored" data-anchor-id="大语言模型的使用指南"><span class="header-section-number">18.5</span> 大语言模型的使用指南</h2>
<section id="提示词工程基础" class="level3" data-number="18.5.1">
<h3 data-number="18.5.1" class="anchored" data-anchor-id="提示词工程基础"><span class="header-section-number">18.5.1</span> 提示词工程基础</h3>
<p>提示词工程（Prompt Engineering）是有效使用大语言模型的关键技能。一个好的提示词应该：</p>
<ol type="1">
<li><strong>明确任务目标</strong>：
<ul>
<li>清晰说明期望的输出格式</li>
<li>指定具体的任务要求</li>
<li>设定适当的约束条件</li>
</ul></li>
<li><strong>提供上下文信息</strong>：
<ul>
<li>补充必要的背景知识</li>
<li>说明专业领域要求</li>
<li>提供相关的参考信息</li>
</ul></li>
<li><strong>设定角色定位</strong>：
<ul>
<li>指定模型的专业角色</li>
<li>明确回答的视角</li>
<li>确定输出的风格</li>
</ul></li>
</ol>
</section>
<section id="提示词模板示例" class="level3" data-number="18.5.2">
<h3 data-number="18.5.2" class="anchored" data-anchor-id="提示词模板示例"><span class="header-section-number">18.5.2</span> 提示词模板示例</h3>
<p>以下是一些常用的提示词模板：</p>
<ol type="1">
<li><strong>分析类任务</strong>：</li>
</ol>
<pre><code>作为一位专业的金融分析师，请分析以下[文本类型]中的关键信息：
[文本内容]

请从以下几个方面进行分析：
1. 主要观点
2. 关键数据
3. 潜在风险
4. 投资建议</code></pre>
<ol start="2" type="1">
<li><strong>总结类任务</strong>：</li>
</ol>
<pre><code>请对以下[文本类型]进行专业总结：
[文本内容]

要求：
- 提取核心要点
- 保持专业术语
- 突出关键数据
- 控制在[字数]以内</code></pre>
<ol start="3" type="1">
<li><strong>比较类任务</strong>：</li>
</ol>
<pre><code>请比较以下两份[文本类型]的异同：
[文本1]
[文本2]

请从以下维度进行分析：
1. 内容重点
2. 数据差异
3. 观点异同
4. 结论对比</code></pre>
</section>
<section id="提示词优化技巧" class="level3" data-number="18.5.3">
<h3 data-number="18.5.3" class="anchored" data-anchor-id="提示词优化技巧"><span class="header-section-number">18.5.3</span> 提示词优化技巧</h3>
<ol type="1">
<li><strong>迭代优化</strong>：
<ul>
<li>从简单提示开始</li>
<li>根据输出结果调整</li>
<li>逐步细化要求</li>
<li>持续改进提示词</li>
</ul></li>
<li><strong>约束条件设置</strong>：
<ul>
<li>限制输出长度</li>
<li>指定输出格式</li>
<li>设定专业程度</li>
<li>明确时间范围</li>
</ul></li>
<li><strong>示例引导</strong>：
<ul>
<li>提供参考样例</li>
<li>说明期望格式</li>
<li>展示专业术语</li>
<li>示范分析深度</li>
</ul></li>
</ol>
</section>
<section id="常见应用场景示例" class="level3" data-number="18.5.4">
<h3 data-number="18.5.4" class="anchored" data-anchor-id="常见应用场景示例"><span class="header-section-number">18.5.4</span> 常见应用场景示例</h3>
<ol type="1">
<li><strong>金融报告分析</strong>：</li>
</ol>
<pre><code>作为一位资深金融分析师，请分析以下财报摘要：
[财报内容]

请提供：
1. 关键财务指标分析
2. 同比/环比变化
3. 主要风险点
4. 投资建议

要求：
- 使用专业金融术语
- 数据精确到小数点后两位
- 重点突出异常变化</code></pre>
<ol start="2" type="1">
<li><strong>市场新闻解读</strong>：</li>
</ol>
<pre><code>请以专业投资顾问的身份，解读以下市场新闻：
[新闻内容]

请从以下角度分析：
1. 对市场的影响
2. 相关行业影响
3. 投资机会
4. 风险提示

要求：
- 结合当前市场环境
- 提供具体数据支持
- 给出可操作建议</code></pre>
<ol start="3" type="1">
<li><strong>政策文件分析</strong>：</li>
</ol>
<pre><code>请作为政策研究专家，分析以下政策文件：
[政策内容]

请重点关注：
1. 政策要点
2. 实施影响
3. 受益行业
4. 潜在风险

要求：
- 结合历史政策对比
- 分析实施难度
- 预测市场反应</code></pre>
</section>
<section id="提示词工程最佳实践" class="level3" data-number="18.5.5">
<h3 data-number="18.5.5" class="anchored" data-anchor-id="提示词工程最佳实践"><span class="header-section-number">18.5.5</span> 提示词工程最佳实践</h3>
<ol type="1">
<li><strong>明确性</strong>：
<ul>
<li>使用清晰、具体的指令</li>
<li>避免模糊的表述</li>
<li>设定明确的边界</li>
<li>指定具体的输出格式</li>
</ul></li>
<li><strong>专业性</strong>：
<ul>
<li>使用领域专业术语</li>
<li>保持分析深度</li>
<li>确保数据准确性</li>
<li>符合行业标准</li>
</ul></li>
<li><strong>结构化</strong>：
<ul>
<li>采用清晰的层次结构</li>
<li>使用编号或要点</li>
<li>保持逻辑连贯</li>
<li>便于后续处理</li>
</ul></li>
<li><strong>可扩展性</strong>：
<ul>
<li>设计可复用的模板</li>
<li>预留调整空间</li>
<li>考虑不同场景</li>
<li>便于批量处理</li>
</ul></li>
</ol>
</section>
<section id="注意事项" class="level3" data-number="18.5.6">
<h3 data-number="18.5.6" class="anchored" data-anchor-id="注意事项"><span class="header-section-number">18.5.6</span> 注意事项</h3>
<ol type="1">
<li><strong>数据安全</strong>：
<ul>
<li>避免输入敏感信息</li>
<li>注意数据脱敏</li>
<li>遵守隐私规定</li>
<li>保护商业机密</li>
</ul></li>
<li><strong>输出验证</strong>：
<ul>
<li>核实关键数据</li>
<li>检查逻辑一致性</li>
<li>验证专业术语</li>
<li>确保结论合理</li>
</ul></li>
<li><strong>持续优化</strong>：
<ul>
<li>收集使用反馈</li>
<li>更新提示词模板</li>
<li>适应新需求</li>
<li>提升使用效果</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="小结与进阶方向" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> 小结与进阶方向</h1>
<section id="从静态向量到大语言模型的演进" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="从静态向量到大语言模型的演进"><span class="header-section-number">19.1</span> 从静态向量到大语言模型的演进</h2>
<p>本讲我们从Word2Vec的局限性出发，介绍了BERT等Transformer模型的原理，以及大语言模型的应用：</p>
<ol type="1">
<li><strong>表示方法演进</strong>：从静态词向量到上下文感知的动态表示</li>
<li><strong>架构演进</strong>：从浅层神经网络到深层Transformer架构</li>
<li><strong>规模演进</strong>：从百万参数到千亿参数</li>
<li><strong>应用演进</strong>：从特征提取到端到端文本理解与生成</li>
</ol>
</section>
<section id="无监督学习的新范式" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="无监督学习的新范式"><span class="header-section-number">19.2</span> 无监督学习的新范式</h2>
<p>大语言模型为无监督学习带来了新的范式：</p>
<ol type="1">
<li><strong>零样本学习</strong>：无需额外标注数据，直接分类新数据</li>
<li><strong>上下文学习</strong>：通过提示中的示例引导模型学习模式</li>
<li><strong>涌现能力</strong>：模型规模增长带来质的飞跃</li>
<li><strong>提示工程</strong>：通过设计提示引导模型行为</li>
</ol>
</section>
<section id="金融文本分析的未来方向" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="金融文本分析的未来方向"><span class="header-section-number">19.3</span> 金融文本分析的未来方向</h2>
<p>大语言模型在金融文本分析中的未来方向包括：</p>
<ol type="1">
<li><strong>多模态融合</strong>：结合文本、数值、图表等多种数据</li>
<li><strong>实时适应</strong>：持续学习最新市场信息和政策变化</li>
<li><strong>可解释性增强</strong>：提高模型决策的透明度</li>
<li><strong>领域知识增强</strong>：融入更多金融专业知识</li>
<li><strong>检索增强生成(RAG)</strong>：
<ul>
<li>将大语言模型与金融专业知识库结合</li>
<li>实时检索最新财经数据和报告</li>
<li>减少幻觉，提高事实准确性</li>
<li>构建公司、行业、政策等专业知识图谱</li>
<li>为金融决策提供更可靠的信息支持</li>
</ul></li>
</ol>
</section>
<section id="进阶学习资源" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="进阶学习资源"><span class="header-section-number">19.4</span> 进阶学习资源</h2>
<ol type="1">
<li><strong>理论深入</strong>：
<ul>
<li>《Attention Is All You Need》 - Transformer原始论文</li>
<li>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</li>
</ul></li>
<li><strong>实践教程</strong>：
<ul>
<li>Hugging Face Transformers 库文档</li>
<li>OpenAI GPT API 文档</li>
</ul></li>
<li><strong>金融NLP资源</strong>：
<ul>
<li>FinBERT 和 FinGPT 项目</li>
<li>金融领域预训练模型集合</li>
</ul></li>
</ol>
</section>
<section id="本讲小结" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="本讲小结"><span class="header-section-number">19.5</span> 本讲小结</h2>
<p>本讲我们从Word2Vec的局限性出发，介绍了BERT和Transformer架构的原理，以及大语言模型在金融文本分析中的应用：</p>
<ol type="1">
<li>从静态词向量到动态上下文表示的演进</li>
<li>Transformer架构与自注意力机制的工作原理</li>
<li>BERT等预训练模型的内部结构和应用方法</li>
<li>大语言模型的关键创新与涌现能力</li>
<li>实践案例：使用BERT和大语言模型分析政府工作报告</li>
</ol>
<p>通过这些内容，我们理解了现代NLP技术在金融文本分析中的强大能力，以及如何将这些技术应用于实际金融分析任务。</p>
</section>
</section>
<section id="参考资料" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> 参考资料</h1>
<ol type="1">
<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</li>
<li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</li>
<li>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., … &amp; Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</li>
<li>Yang, Y., Uy, M. C. S., &amp; Huang, A. (2020). FinBERT: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097.</li>
<li>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., … &amp; Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./12_embedding.html" class="pagination-link" aria-label="文本分析2：词向量与深度学习基础">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析2：词向量与深度学习基础</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./project1_LC.html" class="pagination-link" aria-label="项目1：借贷违约风险评估">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>