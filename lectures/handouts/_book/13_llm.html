<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; 文本分析(三)：大语言模型及其应用 – 数据挖掘与机器学习课程讲义</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./project1_LC.html" rel="next">
<link href="./12_embedding.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ed6da6eef3892af8a4b5ed59bfb951f5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13_llm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析(三)：大语言模型及其应用</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">数据挖掘与机器学习课程讲义</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">前言</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_ml_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">机器学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab02_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_model_assess.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_ts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">时间序列监督学习</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab03_titanic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">泰坦尼克号生存预测实践</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_credit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">信用评分理论基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">非监督学习技术概览及其金融应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">非监督学习：聚类 (Clustering)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">非监督学习：降维 (Dimensionality Reduction)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">文本分析(一)：词频法与向量空间</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析(二)：词向量与深度学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析(三)：大语言模型及其应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project1_LC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2a_tspred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">项目2A：上证综指收益率时间序列预测</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2b_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">项目2B：财务报表文本分析与企业网络安全风险评估</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#从静态向量到动态表示" id="toc-从静态向量到动态表示" class="nav-link active" data-scroll-target="#从静态向量到动态表示"><span class="header-section-number">16</span> 从静态向量到动态表示</a>
  <ul class="collapse">
  <li><a href="#word2vec的局限性" id="toc-word2vec的局限性" class="nav-link" data-scroll-target="#word2vec的局限性"><span class="header-section-number">16.1</span> Word2Vec的局限性</a></li>
  <li><a href="#上下文感知的词表示" id="toc-上下文感知的词表示" class="nav-link" data-scroll-target="#上下文感知的词表示"><span class="header-section-number">16.2</span> 上下文感知的词表示</a></li>
  <li><a href="#语言模型理解上下文的基础" id="toc-语言模型理解上下文的基础" class="nav-link" data-scroll-target="#语言模型理解上下文的基础"><span class="header-section-number">16.3</span> 语言模型：理解上下文的基础</a></li>
  <li><a href="#从elmo到bert的演进" id="toc-从elmo到bert的演进" class="nav-link" data-scroll-target="#从elmo到bert的演进"><span class="header-section-number">16.4</span> 从ELMo到BERT的演进</a>
  <ul class="collapse">
  <li><a href="#elmo-2018" id="toc-elmo-2018" class="nav-link" data-scroll-target="#elmo-2018"><span class="header-section-number">16.4.1</span> ELMo (2018)</a></li>
  <li><a href="#gpt-2018" id="toc-gpt-2018" class="nav-link" data-scroll-target="#gpt-2018"><span class="header-section-number">16.4.2</span> GPT (2018)</a></li>
  <li><a href="#bert-2018" id="toc-bert-2018" class="nav-link" data-scroll-target="#bert-2018"><span class="header-section-number">16.4.3</span> BERT (2018)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bert原理深度解析" id="toc-bert原理深度解析" class="nav-link" data-scroll-target="#bert原理深度解析"><span class="header-section-number">17</span> BERT原理深度解析</a>
  <ul class="collapse">
  <li><a href="#transformer架构bert的基础" id="toc-transformer架构bert的基础" class="nav-link" data-scroll-target="#transformer架构bert的基础"><span class="header-section-number">17.1</span> Transformer架构：BERT的基础</a>
  <ul class="collapse">
  <li><a href="#自注意力机制" id="toc-自注意力机制" class="nav-link" data-scroll-target="#自注意力机制"><span class="header-section-number">17.1.1</span> 自注意力机制</a></li>
  <li><a href="#多头注意力" id="toc-多头注意力" class="nav-link" data-scroll-target="#多头注意力"><span class="header-section-number">17.1.2</span> 多头注意力</a></li>
  <li><a href="#位置编码" id="toc-位置编码" class="nav-link" data-scroll-target="#位置编码"><span class="header-section-number">17.1.3</span> 位置编码</a></li>
  <li><a href="#前馈神经网络" id="toc-前馈神经网络" class="nav-link" data-scroll-target="#前馈神经网络"><span class="header-section-number">17.1.4</span> 前馈神经网络</a></li>
  <li><a href="#transformer编码器结构" id="toc-transformer编码器结构" class="nav-link" data-scroll-target="#transformer编码器结构"><span class="header-section-number">17.1.5</span> Transformer编码器结构</a></li>
  </ul></li>
  <li><a href="#bert模型详解" id="toc-bert模型详解" class="nav-link" data-scroll-target="#bert模型详解"><span class="header-section-number">17.2</span> BERT模型详解</a>
  <ul class="collapse">
  <li><a href="#bert的输入表示" id="toc-bert的输入表示" class="nav-link" data-scroll-target="#bert的输入表示"><span class="header-section-number">17.2.1</span> BERT的输入表示</a></li>
  <li><a href="#bert的预训练任务" id="toc-bert的预训练任务" class="nav-link" data-scroll-target="#bert的预训练任务"><span class="header-section-number">17.2.2</span> BERT的预训练任务</a></li>
  <li><a href="#bert的模型变体" id="toc-bert的模型变体" class="nav-link" data-scroll-target="#bert的模型变体"><span class="header-section-number">17.2.3</span> BERT的模型变体</a></li>
  <li><a href="#bert的微调方式" id="toc-bert的微调方式" class="nav-link" data-scroll-target="#bert的微调方式"><span class="header-section-number">17.2.4</span> BERT的微调方式</a></li>
  </ul></li>
  <li><a href="#bert的内部工作机制" id="toc-bert的内部工作机制" class="nav-link" data-scroll-target="#bert的内部工作机制"><span class="header-section-number">17.3</span> BERT的内部工作机制</a>
  <ul class="collapse">
  <li><a href="#层次化语言知识" id="toc-层次化语言知识" class="nav-link" data-scroll-target="#层次化语言知识"><span class="header-section-number">17.3.1</span> 层次化语言知识</a></li>
  <li><a href="#注意力头的专业化" id="toc-注意力头的专业化" class="nav-link" data-scroll-target="#注意力头的专业化"><span class="header-section-number">17.3.2</span> 注意力头的专业化</a></li>
  <li><a href="#bert的表示空间" id="toc-bert的表示空间" class="nav-link" data-scroll-target="#bert的表示空间"><span class="header-section-number">17.3.3</span> BERT的表示空间</a></li>
  </ul></li>
  <li><a href="#bert的后续演进" id="toc-bert的后续演进" class="nav-link" data-scroll-target="#bert的后续演进"><span class="header-section-number">17.4</span> BERT的后续演进</a>
  <ul class="collapse">
  <li><a href="#预训练任务优化" id="toc-预训练任务优化" class="nav-link" data-scroll-target="#预训练任务优化"><span class="header-section-number">17.4.1</span> 预训练任务优化</a></li>
  <li><a href="#知识增强" id="toc-知识增强" class="nav-link" data-scroll-target="#知识增强"><span class="header-section-number">17.4.2</span> 知识增强</a></li>
  <li><a href="#模型架构改进" id="toc-模型架构改进" class="nav-link" data-scroll-target="#模型架构改进"><span class="header-section-number">17.4.3</span> 模型架构改进</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#从bert到大语言模型" id="toc-从bert到大语言模型" class="nav-link" data-scroll-target="#从bert到大语言模型"><span class="header-section-number">18</span> 从BERT到大语言模型</a>
  <ul class="collapse">
  <li><a href="#transformer架构的扩展" id="toc-transformer架构的扩展" class="nav-link" data-scroll-target="#transformer架构的扩展"><span class="header-section-number">18.1</span> Transformer架构的扩展</a>
  <ul class="collapse">
  <li><a href="#编码器-解码器结构" id="toc-编码器-解码器结构" class="nav-link" data-scroll-target="#编码器-解码器结构"><span class="header-section-number">18.1.1</span> 编码器-解码器结构</a></li>
  <li><a href="#仅解码器架构" id="toc-仅解码器架构" class="nav-link" data-scroll-target="#仅解码器架构"><span class="header-section-number">18.1.2</span> 仅解码器架构</a></li>
  <li><a href="#长距离建模" id="toc-长距离建模" class="nav-link" data-scroll-target="#长距离建模"><span class="header-section-number">18.1.3</span> 长距离建模</a></li>
  </ul></li>
  <li><a href="#大型语言模型的关键创新" id="toc-大型语言模型的关键创新" class="nav-link" data-scroll-target="#大型语言模型的关键创新"><span class="header-section-number">18.2</span> 大型语言模型的关键创新</a>
  <ul class="collapse">
  <li><a href="#规模扩展" id="toc-规模扩展" class="nav-link" data-scroll-target="#规模扩展"><span class="header-section-number">18.2.1</span> 规模扩展</a></li>
  <li><a href="#涌现能力" id="toc-涌现能力" class="nav-link" data-scroll-target="#涌现能力"><span class="header-section-number">18.2.2</span> 涌现能力</a></li>
  <li><a href="#提示工程与思维链推理" id="toc-提示工程与思维链推理" class="nav-link" data-scroll-target="#提示工程与思维链推理"><span class="header-section-number">18.2.3</span> 提示工程与思维链推理</a></li>
  </ul></li>
  <li><a href="#代表性大型语言模型" id="toc-代表性大型语言模型" class="nav-link" data-scroll-target="#代表性大型语言模型"><span class="header-section-number">18.3</span> 代表性大型语言模型</a>
  <ul class="collapse">
  <li><a href="#gpt系列" id="toc-gpt系列" class="nav-link" data-scroll-target="#gpt系列"><span class="header-section-number">18.3.1</span> GPT系列</a></li>
  <li><a href="#开源大型语言模型" id="toc-开源大型语言模型" class="nav-link" data-scroll-target="#开源大型语言模型"><span class="header-section-number">18.3.2</span> 开源大型语言模型</a></li>
  </ul></li>
  <li><a href="#大语言模型的金融应用" id="toc-大语言模型的金融应用" class="nav-link" data-scroll-target="#大语言模型的金融应用"><span class="header-section-number">18.4</span> 大语言模型的金融应用</a>
  <ul class="collapse">
  <li><a href="#信息提取与分析" id="toc-信息提取与分析" class="nav-link" data-scroll-target="#信息提取与分析"><span class="header-section-number">18.4.1</span> 信息提取与分析</a></li>
  <li><a href="#金融文本生成" id="toc-金融文本生成" class="nav-link" data-scroll-target="#金融文本生成"><span class="header-section-number">18.4.2</span> 金融文本生成</a></li>
  <li><a href="#无监督学习辅助" id="toc-无监督学习辅助" class="nav-link" data-scroll-target="#无监督学习辅助"><span class="header-section-number">18.4.3</span> 无监督学习辅助</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#基于bert的政府工作报告分析" id="toc-基于bert的政府工作报告分析" class="nav-link" data-scroll-target="#基于bert的政府工作报告分析"><span class="header-section-number">19</span> 基于BERT的政府工作报告分析</a>
  <ul class="collapse">
  <li><a href="#bert与word2vec的实验对比" id="toc-bert与word2vec的实验对比" class="nav-link" data-scroll-target="#bert与word2vec的实验对比"><span class="header-section-number">19.1</span> BERT与Word2Vec的实验对比</a>
  <ul class="collapse">
  <li><a href="#数据准备与预处理" id="toc-数据准备与预处理" class="nav-link" data-scroll-target="#数据准备与预处理"><span class="header-section-number">19.1.1</span> 数据准备与预处理</a></li>
  <li><a href="#加载bert模型" id="toc-加载bert模型" class="nav-link" data-scroll-target="#加载bert模型"><span class="header-section-number">19.1.2</span> 加载BERT模型</a></li>
  <li><a href="#多义词表示对比" id="toc-多义词表示对比" class="nav-link" data-scroll-target="#多义词表示对比"><span class="header-section-number">19.1.3</span> 多义词表示对比</a></li>
  <li><a href="#生成句子向量比较" id="toc-生成句子向量比较" class="nav-link" data-scroll-target="#生成句子向量比较"><span class="header-section-number">19.1.4</span> 生成句子向量比较</a></li>
  <li><a href="#关键词提取对比" id="toc-关键词提取对比" class="nav-link" data-scroll-target="#关键词提取对比"><span class="header-section-number">19.1.5</span> 关键词提取对比</a></li>
  </ul></li>
  <li><a href="#使用预训练模型进行主题分析" id="toc-使用预训练模型进行主题分析" class="nav-link" data-scroll-target="#使用预训练模型进行主题分析"><span class="header-section-number">19.2</span> 使用预训练模型进行主题分析</a>
  <ul class="collapse">
  <li><a href="#基于bert表示的文本聚类" id="toc-基于bert表示的文本聚类" class="nav-link" data-scroll-target="#基于bert表示的文本聚类"><span class="header-section-number">19.2.1</span> 基于BERT表示的文本聚类</a></li>
  <li><a href="#文本主题随时间的演变分析" id="toc-文本主题随时间的演变分析" class="nav-link" data-scroll-target="#文本主题随时间的演变分析"><span class="header-section-number">19.2.2</span> 文本主题随时间的演变分析</a></li>
  <li><a href="#语义相似度异常点检测" id="toc-语义相似度异常点检测" class="nav-link" data-scroll-target="#语义相似度异常点检测"><span class="header-section-number">19.2.3</span> 语义相似度异常点检测</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#使用大语言模型进行金融文本分析" id="toc-使用大语言模型进行金融文本分析" class="nav-link" data-scroll-target="#使用大语言模型进行金融文本分析"><span class="header-section-number">20</span> 使用大语言模型进行金融文本分析</a>
  <ul class="collapse">
  <li><a href="#使用finbert进行金融文本分类" id="toc-使用finbert进行金融文本分类" class="nav-link" data-scroll-target="#使用finbert进行金融文本分类"><span class="header-section-number">20.1</span> 使用FinBERT进行金融文本分类</a>
  <ul class="collapse">
  <li><a href="#安装与加载finbert" id="toc-安装与加载finbert" class="nav-link" data-scroll-target="#安装与加载finbert"><span class="header-section-number">20.1.1</span> 安装与加载FinBERT</a></li>
  <li><a href="#金融情感分析" id="toc-金融情感分析" class="nav-link" data-scroll-target="#金融情感分析"><span class="header-section-number">20.1.2</span> 金融情感分析</a></li>
  <li><a href="#对政府工作报告进行金融情感分析" id="toc-对政府工作报告进行金融情感分析" class="nav-link" data-scroll-target="#对政府工作报告进行金融情感分析"><span class="header-section-number">20.1.3</span> 对政府工作报告进行金融情感分析</a></li>
  </ul></li>
  <li><a href="#使用通用大语言模型的零样本分类" id="toc-使用通用大语言模型的零样本分类" class="nav-link" data-scroll-target="#使用通用大语言模型的零样本分类"><span class="header-section-number">20.2</span> 使用通用大语言模型的零样本分类</a>
  <ul class="collapse">
  <li><a href="#安装与设置大语言模型" id="toc-安装与设置大语言模型" class="nav-link" data-scroll-target="#安装与设置大语言模型"><span class="header-section-number">20.2.1</span> 安装与设置大语言模型</a></li>
  <li><a href="#零样本文本分类" id="toc-零样本文本分类" class="nav-link" data-scroll-target="#零样本文本分类"><span class="header-section-number">20.2.2</span> 零样本文本分类</a></li>
  <li><a href="#使用大模型生成金融文本摘要" id="toc-使用大模型生成金融文本摘要" class="nav-link" data-scroll-target="#使用大模型生成金融文本摘要"><span class="header-section-number">20.2.3</span> 使用大模型生成金融文本摘要</a></li>
  </ul></li>
  <li><a href="#使用大语言模型进行高级文本分析" id="toc-使用大语言模型进行高级文本分析" class="nav-link" data-scroll-target="#使用大语言模型进行高级文本分析"><span class="header-section-number">20.3</span> 使用大语言模型进行高级文本分析</a>
  <ul class="collapse">
  <li><a href="#提示式主题建模" id="toc-提示式主题建模" class="nav-link" data-scroll-target="#提示式主题建模"><span class="header-section-number">20.3.1</span> 提示式主题建模</a></li>
  <li><a href="#思维链分析" id="toc-思维链分析" class="nav-link" data-scroll-target="#思维链分析"><span class="header-section-number">20.3.2</span> 思维链分析</a></li>
  <li><a href="#嵌入空间与向量检索" id="toc-嵌入空间与向量检索" class="nav-link" data-scroll-target="#嵌入空间与向量检索"><span class="header-section-number">20.3.3</span> 嵌入空间与向量检索</a></li>
  </ul></li>
  <li><a href="#应用大语言模型的最佳实践" id="toc-应用大语言模型的最佳实践" class="nav-link" data-scroll-target="#应用大语言模型的最佳实践"><span class="header-section-number">20.4</span> 应用大语言模型的最佳实践</a>
  <ul class="collapse">
  <li><a href="#提示设计技巧" id="toc-提示设计技巧" class="nav-link" data-scroll-target="#提示设计技巧"><span class="header-section-number">20.4.1</span> 提示设计技巧</a></li>
  <li><a href="#金融特定优化" id="toc-金融特定优化" class="nav-link" data-scroll-target="#金融特定优化"><span class="header-section-number">20.4.2</span> 金融特定优化</a></li>
  <li><a href="#局限性与注意事项" id="toc-局限性与注意事项" class="nav-link" data-scroll-target="#局限性与注意事项"><span class="header-section-number">20.4.3</span> 局限性与注意事项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#小结与进阶方向" id="toc-小结与进阶方向" class="nav-link" data-scroll-target="#小结与进阶方向"><span class="header-section-number">21</span> 小结与进阶方向</a>
  <ul class="collapse">
  <li><a href="#从静态向量到大语言模型的演进" id="toc-从静态向量到大语言模型的演进" class="nav-link" data-scroll-target="#从静态向量到大语言模型的演进"><span class="header-section-number">21.1</span> 从静态向量到大语言模型的演进</a></li>
  <li><a href="#无监督学习的新范式" id="toc-无监督学习的新范式" class="nav-link" data-scroll-target="#无监督学习的新范式"><span class="header-section-number">21.2</span> 无监督学习的新范式</a></li>
  <li><a href="#金融文本分析的未来方向" id="toc-金融文本分析的未来方向" class="nav-link" data-scroll-target="#金融文本分析的未来方向"><span class="header-section-number">21.3</span> 金融文本分析的未来方向</a></li>
  <li><a href="#进阶学习资源" id="toc-进阶学习资源" class="nav-link" data-scroll-target="#进阶学习资源"><span class="header-section-number">21.4</span> 进阶学习资源</a></li>
  <li><a href="#本讲小结" id="toc-本讲小结" class="nav-link" data-scroll-target="#本讲小结"><span class="header-section-number">21.5</span> 本讲小结</a></li>
  </ul></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料"><span class="header-section-number">22</span> 参考资料</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析(三)：大语言模型及其应用</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="从静态向量到动态表示" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> 从静态向量到动态表示</h1>
<section id="word2vec的局限性" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="word2vec的局限性"><span class="header-section-number">16.1</span> Word2Vec的局限性</h2>
<p>上一讲中，我们学习了Word2Vec等词向量技术，它通过分布式表示极大提升了NLP的表示能力。然而，静态词向量仍然存在明显局限性：</p>
<ol type="1">
<li><p><strong>一词一向量问题</strong>：每个词只对应一个固定的向量，无法处理一词多义。例如”苹果”在”我吃了一个苹果”和”苹果公司发布新产品”中的含义完全不同。</p></li>
<li><p><strong>上下文无关</strong>：词向量无法捕捉词语在特定上下文中的含义变化。例如”银行存款”和”河流的银行”中，“银行”的含义有很大差异。</p></li>
<li><p><strong>长距离依赖问题</strong>：无法捕捉句子中相距较远的词之间的依赖关系。例如”他说中文，因为他在中国生活了很多年”中，第二个”他”与第一个”他”指代相同。</p></li>
<li><p><strong>表达能力有限</strong>：固定维度的向量难以编码复杂的语言知识和语法结构。</p></li>
</ol>
<p>这些局限性促使研究者探索更先进的表示方法，能够根据上下文动态调整词语的表示。这一探索最终导致了BERT等基于Transformer的语言模型的诞生。</p>
</section>
<section id="上下文感知的词表示" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="上下文感知的词表示"><span class="header-section-number">16.2</span> 上下文感知的词表示</h2>
<p><strong>上下文感知的词表示</strong>（Contextualized Word Representations）是指词语的向量表示会根据其所处的上下文动态变化。与静态词向量不同，它具有以下特点：</p>
<ol type="1">
<li><strong>动态表示</strong>：同一个词在不同上下文中具有不同的向量表示</li>
<li><strong>语义消歧</strong>：能够根据上下文区分多义词的不同含义</li>
<li><strong>句法感知</strong>：能够捕捉词语在句子中的句法功能</li>
<li><strong>长距离依赖</strong>：能够建模句子中远距离词语之间的关系</li>
</ol>
<p>这种表示方法的核心思想是：<strong>一个词的含义不仅取决于它自身，更取决于它的上下文环境</strong>。</p>
</section>
<section id="语言模型理解上下文的基础" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="语言模型理解上下文的基础"><span class="header-section-number">16.3</span> 语言模型：理解上下文的基础</h2>
<p>上下文感知表示的关键在于<strong>语言模型</strong>（Language Model）。语言模型是一种能够计算文本序列概率的模型，其基本任务是预测序列中的下一个词：</p>
<p><span class="math display">\[P(w_t | w_1, w_2, ..., w_{t-1})\]</span></p>
<p>不同类型的语言模型处理上下文的方式不同：</p>
<ol type="1">
<li><p><strong>传统n-gram语言模型</strong>：只考虑有限历史，如<span class="math inline">\(P(w_t | w_{t-2}, w_{t-1})\)</span></p></li>
<li><p><strong>循环神经网络(RNN)语言模型</strong>：通过隐藏状态递归编码全部历史</p></li>
<li><p><strong>双向语言模型</strong>：同时考虑左侧和右侧上下文</p></li>
<li><p><strong>Transformer语言模型</strong>：通过注意力机制直接建模所有位置间的依赖关系</p></li>
</ol>
<p>预训练语言模型的出现为NLP带来了革命性变化，它通过在大规模语料上无监督预训练，学习通用的语言表示，然后再针对下游任务进行微调。</p>
</section>
<section id="从elmo到bert的演进" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="从elmo到bert的演进"><span class="header-section-number">16.4</span> 从ELMo到BERT的演进</h2>
<p>上下文感知的词表示技术的发展经历了几个里程碑：</p>
<section id="elmo-2018" class="level3" data-number="16.4.1">
<h3 data-number="16.4.1" class="anchored" data-anchor-id="elmo-2018"><span class="header-section-number">16.4.1</span> ELMo (2018)</h3>
<p>ELMo (Embeddings from Language Models) 是上下文词表示的早期尝试，由Peters等人在2018年提出。其特点包括：</p>
<ul>
<li>使用双层双向LSTM结构</li>
<li>将前向和后向语言模型结合</li>
<li>使用不同层的表示的加权组合作为最终表示</li>
<li>有效解决了一词多义问题</li>
</ul>
<p>ELMo的表示公式为：</p>
<p><span class="math display">\[ELMo_k^{task} = E(R_k; \Theta^{task}) = \gamma^{task} \sum_{j=0}^{L} s_j^{task} \mathbf{h}_{k,j}^{LM}\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{h}_{k,j}^{LM}\)</span>是第k个词在第j层的表示。</p>
</section>
<section id="gpt-2018" class="level3" data-number="16.4.2">
<h3 data-number="16.4.2" class="anchored" data-anchor-id="gpt-2018"><span class="header-section-number">16.4.2</span> GPT (2018)</h3>
<p>OpenAI的GPT (Generative Pre-Training) 模型采用了单向Transformer结构：</p>
<ul>
<li>仅使用前向语言模型（只看左侧上下文）</li>
<li>基于Transformer解码器架构</li>
<li>首次展示了大规模预训练+微调的范式</li>
</ul>
<p>GPT采用的预训练目标是预测下一个词：</p>
<p><span class="math display">\[L(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta)\]</span></p>
</section>
<section id="bert-2018" class="level3" data-number="16.4.3">
<h3 data-number="16.4.3" class="anchored" data-anchor-id="bert-2018"><span class="header-section-number">16.4.3</span> BERT (2018)</h3>
<p>BERT (Bidirectional Encoder Representations from Transformers) 由Google在2018年提出，成为上下文词表示的里程碑工作：</p>
<ul>
<li>使用双向Transformer编码器</li>
<li>采用掩码语言模型(Masked LM)预训练</li>
<li>同时使用下一句预测(NSP)任务</li>
<li>极大提升了NLP任务的性能上限</li>
</ul>
<p>BERT的预训练目标是预测被掩码的词：</p>
<p><span class="math display">\[L(\mathcal{D}) = \sum_{i \in \mathcal{M}} \log P(w_i | w_{\neg \mathcal{M}}; \Theta)\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{M}\)</span>是被掩码的词的位置集合。</p>
<p>这一演进体现了以下趋势： - 从浅层网络到深层Transformer架构 - 从单向上下文到双向上下文 - 从特征提取器到通用语言模型 - 从任务相关到预训练-微调范式</p>
<p>接下来，我们将深入理解BERT模型的内部工作原理。</p>
</section>
</section>
</section>
<section id="bert原理深度解析" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> BERT原理深度解析</h1>
<section id="transformer架构bert的基础" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="transformer架构bert的基础"><span class="header-section-number">17.1</span> Transformer架构：BERT的基础</h2>
<p>BERT建立在Transformer架构之上，这是由Vaswani等人在2017年提出的一种完全基于注意力机制的神经网络结构。在深入BERT之前，我们需要先理解Transformer的基本组件。</p>
<section id="自注意力机制" class="level3" data-number="17.1.1">
<h3 data-number="17.1.1" class="anchored" data-anchor-id="自注意力机制"><span class="header-section-number">17.1.1</span> 自注意力机制</h3>
<p><strong>自注意力</strong>（Self-Attention）是Transformer的核心组件，它允许模型在处理某个位置时，考虑序列中所有位置的信息。其计算过程如下：</p>
<ol type="1">
<li><p>将输入向量<span class="math inline">\(X\)</span>分别转换为查询(Query)、键(Key)和值(Value)三个矩阵： <span class="math display">\[Q = XW^Q, K = XW^K, V = XW^V\]</span></p></li>
<li><p>计算注意力得分并归一化： <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p></li>
<li><p>其中，<span class="math inline">\(\sqrt{d_k}\)</span>是缩放因子，用于防止梯度消失。</p></li>
</ol>
<p>自注意力机制的优势在于： - 可以捕捉任意距离的依赖关系 - 计算复杂度相对RNN低 - 允许并行计算</p>
</section>
<section id="多头注意力" class="level3" data-number="17.1.2">
<h3 data-number="17.1.2" class="anchored" data-anchor-id="多头注意力"><span class="header-section-number">17.1.2</span> 多头注意力</h3>
<p>为了增强模型的表示能力，Transformer使用了<strong>多头注意力</strong>（Multi-Head Attention）：</p>
<p><span class="math display">\[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\]</span></p>
<p>其中，<span class="math inline">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span></p>
<p>多头注意力允许模型: - 在不同子空间中学习不同的关注模式 - 同时关注位置和语义信息 - 提供更丰富的特征表示</p>
</section>
<section id="位置编码" class="level3" data-number="17.1.3">
<h3 data-number="17.1.3" class="anchored" data-anchor-id="位置编码"><span class="header-section-number">17.1.3</span> 位置编码</h3>
<p>由于自注意力机制本身不包含位置信息，Transformer引入了<strong>位置编码</strong>（Positional Encoding）来将序列顺序信息注入模型：</p>
<p><span class="math display">\[PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\]</span> <span class="math display">\[PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\]</span></p>
<p>其中，<span class="math inline">\(pos\)</span>是位置，<span class="math inline">\(i\)</span>是维度。</p>
</section>
<section id="前馈神经网络" class="level3" data-number="17.1.4">
<h3 data-number="17.1.4" class="anchored" data-anchor-id="前馈神经网络"><span class="header-section-number">17.1.4</span> 前馈神经网络</h3>
<p>Transformer中每个子层还包含一个<strong>前馈神经网络</strong>（Feed-Forward Network），由两个线性变换组成：</p>
<p><span class="math display">\[FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\]</span></p>
</section>
<section id="transformer编码器结构" class="level3" data-number="17.1.5">
<h3 data-number="17.1.5" class="anchored" data-anchor-id="transformer编码器结构"><span class="header-section-number">17.1.5</span> Transformer编码器结构</h3>
<p>一个完整的Transformer编码器层包含： 1. 多头自注意力机制 2. 层归一化（Layer Normalization） 3. 前馈神经网络 4. 残差连接（Residual Connection）</p>
<p>这些组件按以下方式组合： <span class="math display">\[\hat{h} = LayerNorm(x + MultiHeadAttention(x))\]</span> <span class="math display">\[h = LayerNorm(\hat{h} + FFN(\hat{h}))\]</span></p>
<p>BERT使用了Transformer的编码器部分，通常包含12层（BERT-Base）或24层（BERT-Large）。</p>
</section>
</section>
<section id="bert模型详解" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="bert模型详解"><span class="header-section-number">17.2</span> BERT模型详解</h2>
<p>BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，旨在学习深层的双向语言表示。</p>
<section id="bert的输入表示" class="level3" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="bert的输入表示"><span class="header-section-number">17.2.1</span> BERT的输入表示</h3>
<p>BERT的输入由三种嵌入的总和组成：</p>
<ol type="1">
<li><strong>词嵌入</strong>（Token Embeddings）：WordPiece词表中的词元对应的嵌入</li>
<li><strong>段嵌入</strong>（Segment Embeddings）：区分句子对中的第一句和第二句</li>
<li><strong>位置嵌入</strong>（Position Embeddings）：表示词元在序列中的位置</li>
</ol>
<p>每个输入序列以特殊标记<code>[CLS]</code>开始，以<code>[SEP]</code>分隔不同句子。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://d2l.ai/_images/bert-input.svg" class="img-fluid figure-img"></p>
<figcaption>BERT输入表示</figcaption>
</figure>
</div>
</section>
<section id="bert的预训练任务" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="bert的预训练任务"><span class="header-section-number">17.2.2</span> BERT的预训练任务</h3>
<p>BERT通过两个无监督任务进行预训练：</p>
<ol type="1">
<li><strong>掩码语言模型（Masked Language Model，MLM）</strong>：
<ul>
<li>随机掩盖输入中15%的词元</li>
<li>其中80%用<code>[MASK]</code>替换，10%用随机词替换，10%保持不变</li>
<li>训练模型预测被掩盖的原始词元</li>
<li>这使得BERT能够学习双向上下文表示</li>
</ul></li>
<li><strong>下一句预测（Next Sentence Prediction，NSP）</strong>：
<ul>
<li>给定两个句子，预测第二句是否是第一句的真实后续</li>
<li>训练数据中50%是真实的连续句子，50%是随机句子对</li>
<li>这使得BERT能够理解句子间的关系</li>
</ul></li>
</ol>
</section>
<section id="bert的模型变体" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="bert的模型变体"><span class="header-section-number">17.2.3</span> BERT的模型变体</h3>
<p>BERT有两个主要变体：</p>
<ol type="1">
<li><strong>BERT-Base</strong>：
<ul>
<li>12层Transformer编码器</li>
<li>12个注意力头</li>
<li>768维隐藏层</li>
<li>1.1亿参数</li>
</ul></li>
<li><strong>BERT-Large</strong>：
<ul>
<li>24层Transformer编码器</li>
<li>16个注意力头</li>
<li>1024维隐藏层</li>
<li>3.4亿参数</li>
</ul></li>
</ol>
</section>
<section id="bert的微调方式" class="level3" data-number="17.2.4">
<h3 data-number="17.2.4" class="anchored" data-anchor-id="bert的微调方式"><span class="header-section-number">17.2.4</span> BERT的微调方式</h3>
<p>预训练后的BERT可以通过简单的任务特定层进行微调，适用于多种下游任务：</p>
<ol type="1">
<li><strong>序列级任务</strong>（如分类）：使用<code>[CLS]</code>标记的最终隐藏状态</li>
<li><strong>词元级任务</strong>（如NER）：使用每个词元的最终隐藏状态</li>
<li><strong>句子对任务</strong>（如问答）：同时输入问题和段落，识别答案跨度</li>
</ol>
<p>微调过程通常只需要少量标注数据和训练轮次，极大地降低了NLP任务的门槛。</p>
</section>
</section>
<section id="bert的内部工作机制" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="bert的内部工作机制"><span class="header-section-number">17.3</span> BERT的内部工作机制</h2>
<p>通过深入分析BERT的内部表示，研究者发现BERT的不同层捕捉了不同类型的语言知识：</p>
<section id="层次化语言知识" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="层次化语言知识"><span class="header-section-number">17.3.1</span> 层次化语言知识</h3>
<ol type="1">
<li><strong>底层</strong>（1-4层）：捕捉表面语法特征、词性、局部依赖等</li>
<li><strong>中层</strong>（5-8层）：编码短语级语义和共指关系</li>
<li><strong>高层</strong>（9-12层）：处理长距离依赖和更抽象的语义关系</li>
</ol>
</section>
<section id="注意力头的专业化" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="注意力头的专业化"><span class="header-section-number">17.3.2</span> 注意力头的专业化</h3>
<p>BERT的不同注意力头专注于不同类型的语言信息：</p>
<ol type="1">
<li><strong>语法头</strong>：关注句法依赖关系</li>
<li><strong>语义头</strong>：关注语义相关的词</li>
<li><strong>共指头</strong>：关注指代同一实体的表达</li>
</ol>
</section>
<section id="bert的表示空间" class="level3" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="bert的表示空间"><span class="header-section-number">17.3.3</span> BERT的表示空间</h3>
<p>BERT的表示空间表现出interessting的性质：</p>
<ol type="1">
<li><strong>各向异性</strong>：嵌入向量集中在狭窄的锥体中，而非均匀分布</li>
<li><strong>语义区分</strong>：相似概念在表示空间中形成聚类</li>
<li><strong>线性结构</strong>：某些语义关系可以通过向量差来表示</li>
</ol>
<p>这些特性使得BERT能够有效地编码复杂的语言知识，并为下游任务提供丰富的特征表示。</p>
</section>
</section>
<section id="bert的后续演进" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="bert的后续演进"><span class="header-section-number">17.4</span> BERT的后续演进</h2>
<p>BERT发布后，研究者提出了许多改进版本，主要集中在以下几个方向：</p>
<section id="预训练任务优化" class="level3" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="预训练任务优化"><span class="header-section-number">17.4.1</span> 预训练任务优化</h3>
<ol type="1">
<li><strong>RoBERTa</strong>：移除NSP任务，使用更大批量和更多数据训练</li>
<li><strong>ALBERT</strong>：参数共享和分解嵌入，降低模型大小</li>
<li><strong>ELECTRA</strong>：用判别式替换检测训练，提高效率</li>
</ol>
</section>
<section id="知识增强" class="level3" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="知识增强"><span class="header-section-number">17.4.2</span> 知识增强</h3>
<ol type="1">
<li><strong>KnowBERT</strong>：集成知识库信息</li>
<li><strong>ERNIE</strong>：加入实体和短语级掩码</li>
<li><strong>FinBERT</strong>：针对金融领域的专业知识训练</li>
</ol>
</section>
<section id="模型架构改进" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="模型架构改进"><span class="header-section-number">17.4.3</span> 模型架构改进</h3>
<ol type="1">
<li><strong>SpanBERT</strong>：掩盖连续的文本片段而非单个词</li>
<li><strong>XLNet</strong>：使用排列语言模型，解决掩码带来的预训练-微调不一致</li>
<li><strong>DeBERTa</strong>：解耦注意力机制，增强位置编码</li>
</ol>
<p>这些改进进一步推动了预训练语言模型的发展，为下一代更强大的模型如GPT系列奠定了基础。</p>
</section>
</section>
</section>
<section id="从bert到大语言模型" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> 从BERT到大语言模型</h1>
<section id="transformer架构的扩展" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="transformer架构的扩展"><span class="header-section-number">18.1</span> Transformer架构的扩展</h2>
<p>虽然BERT在NLP领域带来了巨大进步，但它仍然存在一些局限性，如无法进行生成任务和处理长文本。为了克服这些限制，研究者们对Transformer架构进行了多方面扩展。</p>
<section id="编码器-解码器结构" class="level3" data-number="18.1.1">
<h3 data-number="18.1.1" class="anchored" data-anchor-id="编码器-解码器结构"><span class="header-section-number">18.1.1</span> 编码器-解码器结构</h3>
<p><strong>编码器-解码器</strong>（Encoder-Decoder）结构是机器翻译等序列到序列任务的标准架构：</p>
<ol type="1">
<li><strong>编码器</strong>：处理输入序列，生成上下文表示</li>
<li><strong>解码器</strong>：基于编码器输出生成目标序列</li>
<li><strong>交叉注意力</strong>：解码器通过注意力机制访问编码器的输出</li>
</ol>
<p>代表模型： - <strong>T5</strong>：将所有NLP任务统一为文本到文本的转换 - <strong>BART</strong>：通过降噪自编码器预训练</p>
</section>
<section id="仅解码器架构" class="level3" data-number="18.1.2">
<h3 data-number="18.1.2" class="anchored" data-anchor-id="仅解码器架构"><span class="header-section-number">18.1.2</span> 仅解码器架构</h3>
<p><strong>仅解码器</strong>（Decoder-only）架构专注于生成任务，通过自回归方式预测下一个词：</p>
<ol type="1">
<li><strong>单向自注意力</strong>：每个位置只能看到其前面的位置</li>
<li><strong>自回归生成</strong>：逐词生成输出序列</li>
<li><strong>缩放规模</strong>：通过扩大模型规模提升能力</li>
</ol>
<p>代表模型： - <strong>GPT系列</strong>：从GPT-1到GPT-4，规模和能力不断增长 - <strong>LLaMA</strong>：开源的大型语言模型，有效降低了资源需求</p>
</section>
<section id="长距离建模" class="level3" data-number="18.1.3">
<h3 data-number="18.1.3" class="anchored" data-anchor-id="长距离建模"><span class="header-section-number">18.1.3</span> 长距离建模</h3>
<p>处理长文本的能力是大语言模型的关键挑战之一，研究者提出了多种解决方案：</p>
<ol type="1">
<li><strong>稀疏注意力</strong>：如Longformer，只关注局部窗口和全局标记</li>
<li><strong>循环机制</strong>：如Transformer-XL，跨段传递隐藏状态</li>
<li><strong>线性复杂度</strong>：如Linformer，通过低秩近似降低计算量</li>
<li><strong>扩展上下文窗口</strong>：如DeepSeek，将上下文窗口扩展到128K</li>
</ol>
</section>
</section>
<section id="大型语言模型的关键创新" class="level2" data-number="18.2">
<h2 data-number="18.2" class="anchored" data-anchor-id="大型语言模型的关键创新"><span class="header-section-number">18.2</span> 大型语言模型的关键创新</h2>
<p>大型语言模型（LLMs）相比传统BERT模型有几个关键创新：</p>
<section id="规模扩展" class="level3" data-number="18.2.1">
<h3 data-number="18.2.1" class="anchored" data-anchor-id="规模扩展"><span class="header-section-number">18.2.1</span> 规模扩展</h3>
<p>深度学习研究表明，模型规模与性能呈现”幂律”关系，增加参数量能带来显著性能提升：</p>
<ol type="1">
<li><strong>从亿到千亿参数</strong>：BERT-Large有3.4亿参数，而GPT-4估计有超过1万亿参数</li>
<li><strong>计算资源增长</strong>：训练大模型需要数千GPU/TPU，消耗数百万美元</li>
<li><strong>预训练数据扩展</strong>：从GB级语料到TB级语料</li>
</ol>
</section>
<section id="涌现能力" class="level3" data-number="18.2.2">
<h3 data-number="18.2.2" class="anchored" data-anchor-id="涌现能力"><span class="header-section-number">18.2.2</span> 涌现能力</h3>
<p>大语言模型最惊人的特性是<strong>涌现能力</strong>（Emergent Abilities）——在达到一定规模后突然出现的能力：</p>
<ol type="1">
<li><strong>指令跟随</strong>：理解并执行自然语言指令</li>
<li><strong>思维链推理</strong>：通过分步骤推理解决复杂问题</li>
<li><strong>上下文学习</strong>：从少量示例中学习新任务</li>
<li><strong>多模态理解</strong>：结合文本与图像等多种模态信息</li>
</ol>
</section>
<section id="提示工程与思维链推理" class="level3" data-number="18.2.3">
<h3 data-number="18.2.3" class="anchored" data-anchor-id="提示工程与思维链推理"><span class="header-section-number">18.2.3</span> 提示工程与思维链推理</h3>
<p>大语言模型的使用方式也发生了革命性变化：</p>
<ol type="1">
<li><strong>提示工程</strong>（Prompt Engineering）：
<ul>
<li>通过精心设计的提示引导模型行为</li>
<li>不同于传统的微调范式</li>
<li>允许灵活调整模型输出</li>
</ul></li>
<li><strong>思维链推理</strong>（Chain-of-Thought）：
<ul>
<li>让模型先生成推理过程，再给出结论</li>
<li>显著提高模型解决复杂问题的能力</li>
<li>公式：<span class="math inline">\(\text{Prompt} + \text{思考过程} \to \text{更准确的结果}\)</span></li>
</ul></li>
<li><strong>上下文学习</strong>（In-context Learning）：
<ul>
<li>在提示中包含示例，引导模型学习模式</li>
<li>无需参数更新，即可适应新任务</li>
<li>示例：给出几个情感分类示例，模型可泛化到新文本</li>
</ul></li>
</ol>
</section>
</section>
<section id="代表性大型语言模型" class="level2" data-number="18.3">
<h2 data-number="18.3" class="anchored" data-anchor-id="代表性大型语言模型"><span class="header-section-number">18.3</span> 代表性大型语言模型</h2>
<section id="gpt系列" class="level3" data-number="18.3.1">
<h3 data-number="18.3.1" class="anchored" data-anchor-id="gpt系列"><span class="header-section-number">18.3.1</span> GPT系列</h3>
<p>由OpenAI开发的GPT（Generative Pre-trained Transformer）系列是大型语言模型的代表：</p>
<ol type="1">
<li><strong>GPT-1</strong>（2018）：
<ul>
<li>1.17亿参数</li>
<li>首次展示预训练+微调范式</li>
<li>在多个NLP任务上获得突破</li>
</ul></li>
<li><strong>GPT-2</strong>（2019）：
<ul>
<li>15亿参数</li>
<li>展示了零样本学习能力</li>
<li>文本生成质量有显著提升</li>
</ul></li>
<li><strong>GPT-3</strong>（2020）：
<ul>
<li>1750亿参数</li>
<li>展示了惊人的少样本学习能力</li>
<li>可以执行之前未见过的任务</li>
</ul></li>
<li><strong>GPT-4</strong>（2023）：
<ul>
<li>参数规模未公开，估计超过1万亿</li>
<li>多模态能力，支持图像输入</li>
<li>接近人类专家水平的表现</li>
</ul></li>
</ol>
</section>
<section id="开源大型语言模型" class="level3" data-number="18.3.2">
<h3 data-number="18.3.2" class="anchored" data-anchor-id="开源大型语言模型"><span class="header-section-number">18.3.2</span> 开源大型语言模型</h3>
<p>除了GPT系列，开源社区也开发了多种高性能大语言模型：</p>
<ol type="1">
<li><strong>LLaMA系列</strong>：
<ul>
<li>由Meta AI开发</li>
<li>参数规模从7B到65B不等</li>
<li>性能接近闭源商业模型</li>
<li>衍生了许多优秀模型如Vicuna和Alpaca</li>
</ul></li>
<li><strong>国产大模型</strong>：
<ul>
<li><strong>ChatGLM</strong>：清华大学与智谱AI合作开发的双语模型</li>
<li><strong>DeepSeek</strong>：深度求索开发，专注长序列处理</li>
<li><strong>Qwen</strong>：阿里云开发，性能优异的开源模型</li>
</ul></li>
<li><strong>多模态模型</strong>：
<ul>
<li><strong>CLIP</strong>：连接图像和文本的表示学习</li>
<li><strong>GPT-4V</strong>：具有视觉理解能力的GPT-4变体</li>
<li><strong>Gemini</strong>：Google的多模态大语言模型</li>
</ul></li>
</ol>
</section>
</section>
<section id="大语言模型的金融应用" class="level2" data-number="18.4">
<h2 data-number="18.4" class="anchored" data-anchor-id="大语言模型的金融应用"><span class="header-section-number">18.4</span> 大语言模型的金融应用</h2>
<p>大语言模型在金融领域有广泛的应用潜力：</p>
<section id="信息提取与分析" class="level3" data-number="18.4.1">
<h3 data-number="18.4.1" class="anchored" data-anchor-id="信息提取与分析"><span class="header-section-number">18.4.1</span> 信息提取与分析</h3>
<ol type="1">
<li><strong>报告解析</strong>：
<ul>
<li>自动提取财报中的关键财务指标</li>
<li>总结长篇研报要点</li>
<li>识别风险披露声明</li>
</ul></li>
<li><strong>市场情感分析</strong>：
<ul>
<li>分析新闻报道的市场情绪</li>
<li>提取投资者情绪信号</li>
<li>预测市场波动</li>
</ul></li>
<li><strong>事件提取</strong>：
<ul>
<li>从财经新闻中识别重大事件</li>
<li>构建事件知识图谱</li>
<li>分析事件之间的因果关系</li>
</ul></li>
</ol>
</section>
<section id="金融文本生成" class="level3" data-number="18.4.2">
<h3 data-number="18.4.2" class="anchored" data-anchor-id="金融文本生成"><span class="header-section-number">18.4.2</span> 金融文本生成</h3>
<ol type="1">
<li><strong>研究报告生成</strong>：
<ul>
<li>基于数据自动生成财务分析</li>
<li>创建行业趋势报告</li>
<li>生成个股评论</li>
</ul></li>
<li><strong>监管合规</strong>：
<ul>
<li>生成合规声明和披露</li>
<li>检查文档是否符合监管要求</li>
<li>自动更新合规文件</li>
</ul></li>
<li><strong>客户交互</strong>：
<ul>
<li>智能金融顾问</li>
<li>个性化投资建议</li>
<li>金融知识普及</li>
</ul></li>
</ol>
</section>
<section id="无监督学习辅助" class="level3" data-number="18.4.3">
<h3 data-number="18.4.3" class="anchored" data-anchor-id="无监督学习辅助"><span class="header-section-number">18.4.3</span> 无监督学习辅助</h3>
<ol type="1">
<li><strong>文本聚类</strong>：
<ul>
<li>通过嵌入向量聚类发现主题</li>
<li>识别相似公告和报告</li>
<li>发现市场关注热点</li>
</ul></li>
<li><strong>异常检测</strong>：
<ul>
<li>识别异常金融叙述</li>
<li>发现财报中的可疑部分</li>
<li>预警潜在风险信号</li>
</ul></li>
<li><strong>主题提取</strong>：
<ul>
<li>无监督发现文档主题</li>
<li>总结长文本的核心观点</li>
<li>追踪主题随时间的演变</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="基于bert的政府工作报告分析" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> 基于BERT的政府工作报告分析</h1>
<section id="bert与word2vec的实验对比" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="bert与word2vec的实验对比"><span class="header-section-number">19.1</span> BERT与Word2Vec的实验对比</h2>
<p>在本节中，我们将使用政府工作报告数据，对比BERT与Word2Vec在文本表示上的差异。这一对比将帮助我们理解上下文感知表示相对于静态词向量的优势。</p>
<section id="数据准备与预处理" class="level3" data-number="19.1.1">
<h3 data-number="19.1.1" class="anchored" data-anchor-id="数据准备与预处理"><span class="header-section-number">19.1.1</span> 数据准备与预处理</h3>
<p>与上一讲类似，我们首先加载和预处理政府工作报告数据：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jieba</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载政府工作报告数据</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>govreport <span class="op">=</span> pd.read_csv(<span class="st">"labs/NLP/data/govreport.csv"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置中文显示</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.sans-serif'</span>] <span class="op">=</span> [<span class="st">'Songti SC'</span>]  <span class="co"># 用来正常显示中文标签</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.unicode_minus'</span>] <span class="op">=</span> <span class="va">False</span>  <span class="co"># 用来正常显示负号</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载停用词</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"labs/NLP/data/ChineseStopWords.txt"</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    stop_words <span class="op">=</span> {line.strip() <span class="cf">for</span> line <span class="kw">in</span> f}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 文本清洗并分词</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_text(text):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 去除标点符号和数字</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^\u4e00-\u9fa5]'</span>, <span class="st">' '</span>, text)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 分词</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> jieba.cut(text)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 去除停用词和空白</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word.strip() <span class="kw">and</span> word <span class="kw">not</span> <span class="kw">in</span> stop_words]</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 处理所有文档</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> []</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>years <span class="op">=</span> []</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> govreport.iterrows():</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> preprocess_text(row[<span class="st">'texts'</span>])</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    corpus.append(words)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    years.append(row[<span class="st">'Year'</span>])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"语料库包含</span><span class="sc">{</span><span class="bu">len</span>(corpus)<span class="sc">}</span><span class="ss">篇文档"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的Word2Vec模型（假设我们已经训练好了，这里直接加载）</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>w2v_model <span class="op">=</span> Word2Vec.load(<span class="st">"gov_report_word2vec.model"</span>)</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="加载bert模型" class="level3" data-number="19.1.2">
<h3 data-number="19.1.2" class="anchored" data-anchor-id="加载bert模型"><span class="header-section-number">19.1.2</span> 加载BERT模型</h3>
<p>我们将使用中文预训练BERT模型：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的中文BERT模型</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-chinese'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-chinese'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># 设置为评估模式</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义函数获取BERT词向量</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_bert_embeddings(text, tokenizer, model, max_length<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 对文本进行分词和转换</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span>max_length, padding<span class="op">=</span><span class="st">'max_length'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 获取BERT输出</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取最后一层的隐藏状态（词向量）</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    last_hidden_states <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取[CLS]标记的向量作为句子表示</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    cls_embedding <span class="op">=</span> last_hidden_states[:, <span class="dv">0</span>, :].numpy()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取所有词元的向量</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    token_embeddings <span class="op">=</span> last_hidden_states.numpy()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cls_embedding'</span>: cls_embedding,  <span class="co"># 句子表示</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">'token_embeddings'</span>: token_embeddings,  <span class="co"># 词元表示</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'tokens'</span>: tokenizer.convert_ids_to_tokens(inputs[<span class="st">'input_ids'</span>][<span class="dv">0</span>])  <span class="co"># 对应的词元</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    }</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="多义词表示对比" class="level3" data-number="19.1.3">
<h3 data-number="19.1.3" class="anchored" data-anchor-id="多义词表示对比"><span class="header-section-number">19.1.3</span> 多义词表示对比</h3>
<p>我们将对比BERT和Word2Vec在处理多义词上的能力差异：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 选择一个多义词进行分析，如"发展"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ambiguous_word <span class="op">=</span> <span class="st">"发展"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 从报告中找出含有该词的不同上下文</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>contexts <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, doc <span class="kw">in</span> <span class="bu">enumerate</span>(corpus):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将分词列表转换为文本</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">""</span>.join(doc)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 在原文中寻找该词</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ambiguous_word <span class="kw">in</span> text:</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 寻找包含目标词的短句</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        sentences <span class="op">=</span> re.split(<span class="vs">r'[，。！？；]'</span>, text)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ambiguous_word <span class="kw">in</span> sentence <span class="kw">and</span> <span class="dv">10</span> <span class="op">&lt;</span> <span class="bu">len</span>(sentence) <span class="op">&lt;</span> <span class="dv">50</span>:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                contexts.append((years[i], sentence))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(contexts) <span class="op">&gt;=</span> <span class="dv">5</span>:  <span class="co"># 只取5个上下文示例</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(contexts) <span class="op">&gt;=</span> <span class="dv">5</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Word2Vec的表示（静态）</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>w2v_vector <span class="op">=</span> w2v_model.wv[ambiguous_word]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. BERT的表示（动态，依赖上下文）</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>bert_vectors <span class="op">=</span> []</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year, context <span class="kw">in</span> contexts:</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 获取BERT表示</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    bert_result <span class="op">=</span> get_bert_embeddings(context, tokenizer, model)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 找到目标词的位置</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    target_positions <span class="op">=</span> [i <span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(bert_result[<span class="st">'tokens'</span>]) </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> ambiguous_word <span class="kw">in</span> token <span class="kw">and</span> <span class="st">'##'</span> <span class="kw">not</span> <span class="kw">in</span> token]</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> target_positions:</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取目标词在该上下文中的表示</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        target_idx <span class="op">=</span> target_positions[<span class="dv">0</span>]</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        bert_vector <span class="op">=</span> bert_result[<span class="st">'token_embeddings'</span>][<span class="dv">0</span>, target_idx, :]</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        bert_vectors.append((year, context, bert_vector))</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算BERT表示之间的相似度</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bert_vectors:</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    bert_similarities <span class="op">=</span> np.zeros((<span class="bu">len</span>(bert_vectors), <span class="bu">len</span>(bert_vectors)))</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bert_vectors)):</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bert_vectors)):</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            bert_similarities[i, j] <span class="op">=</span> cosine_similarity(</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>                bert_vectors[i][<span class="dv">2</span>].reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), </span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>                bert_vectors[j][<span class="dv">2</span>].reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>            )[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 可视化BERT表示的相似度矩阵</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    plt.imshow(bert_similarities, cmap<span class="op">=</span><span class="st">'YlOrRd'</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    plt.colorbar(label<span class="op">=</span><span class="st">'余弦相似度'</span>)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'"</span><span class="sc">{</span>ambiguous_word<span class="sc">}</span><span class="ss">"在不同上下文中的BERT表示相似度'</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 设置坐标轴标签</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>    context_labels <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>context[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">..."</span> <span class="cf">for</span> year, context, _ <span class="kw">in</span> bert_vectors]</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(context_labels)), context_labels, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    plt.yticks(<span class="bu">range</span>(<span class="bu">len</span>(context_labels)), context_labels)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Word2Vec中'</span><span class="sc">{</span>ambiguous_word<span class="sc">}</span><span class="ss">'的静态表示是固定的，无法区分不同上下文"</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"而BERT可以为'</span><span class="sc">{</span>ambiguous_word<span class="sc">}</span><span class="ss">'在不同上下文中生成不同的表示"</span>)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"相似度矩阵展示了这些表示之间的差异，相似度较低的对应不同语义用法"</span>)</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="生成句子向量比较" class="level3" data-number="19.1.4">
<h3 data-number="19.1.4" class="anchored" data-anchor-id="生成句子向量比较"><span class="header-section-number">19.1.4</span> 生成句子向量比较</h3>
<p>我们将对比Word2Vec和BERT生成的句子向量在文本相似度任务上的表现：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 为每个报告生成句子向量</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 基于Word2Vec (简单平均)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_w2v_sentence_vector(tokens, model):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    valid_tokens <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> t <span class="kw">in</span> model.wv]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> valid_tokens:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.zeros(model.vector_size)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean([model.wv[t] <span class="cf">for</span> t <span class="kw">in</span> valid_tokens], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 基于BERT的[CLS]标记</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_bert_sentence_vector(text, tokenizer, model):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> get_bert_embeddings(text, tokenizer, model)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result[<span class="st">'cls_embedding'</span>][<span class="dv">0</span>]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成向量</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>w2v_doc_vectors <span class="op">=</span> [get_w2v_sentence_vector(doc, w2v_model) <span class="cf">for</span> doc <span class="kw">in</span> corpus]</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>bert_doc_vectors <span class="op">=</span> []</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, _ <span class="kw">in</span> govreport.iterrows():</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 取报告的前512个字符（BERT输入长度限制）</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    text_sample <span class="op">=</span> govreport.iloc[i][<span class="st">'texts'</span>][:<span class="dv">512</span>]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    bert_vec <span class="op">=</span> get_bert_sentence_vector(text_sample, tokenizer, model)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    bert_doc_vectors.append(bert_vec)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算报告之间的相似度</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>w2v_similarities <span class="op">=</span> cosine_similarity(w2v_doc_vectors)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>bert_similarities <span class="op">=</span> cosine_similarity(bert_doc_vectors)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化报告相似度矩阵比较</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">7</span>))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Word2Vec相似度矩阵</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> axes[<span class="dv">0</span>].imshow(w2v_similarities, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Word2Vec: 报告相似度矩阵'</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(years)))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(<span class="bu">len</span>(years)))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticklabels(years)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels(years)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im1, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT相似度矩阵</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>im2 <span class="op">=</span> axes[<span class="dv">1</span>].imshow(bert_similarities, cmap<span class="op">=</span><span class="st">'Reds'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'BERT: 报告相似度矩阵'</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(years)))</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticks(<span class="bu">range</span>(<span class="bu">len</span>(years)))</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticklabels(years)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticklabels(years)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im2, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="关键词提取对比" class="level3" data-number="19.1.5">
<h3 data-number="19.1.5" class="anchored" data-anchor-id="关键词提取对比"><span class="header-section-number">19.1.5</span> 关键词提取对比</h3>
<p>比较BERT和Word2Vec在关键词提取任务上的差异：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 基于BERT的关键词提取</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_bert_keywords(text, tokenizer, model, top_n<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将文本分成短句</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> re.split(<span class="vs">r'[，。！？；]'</span>, text)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> [s <span class="cf">for</span> s <span class="kw">in</span> sentences <span class="cf">if</span> <span class="bu">len</span>(s) <span class="op">&gt;</span> <span class="dv">5</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> sentences:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 对每个句子获取词元表示</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    all_tokens <span class="op">=</span> []</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    all_embeddings <span class="op">=</span> []</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sentence <span class="kw">in</span> sentences[:<span class="dv">20</span>]:  <span class="co"># 限制句子数量</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> get_bert_embeddings(sentence, tokenizer, model)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> result[<span class="st">'tokens'</span>]</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> result[<span class="st">'token_embeddings'</span>][<span class="dv">0</span>]</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 过滤掉特殊标记和重复词</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        valid_indices <span class="op">=</span> []</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        valid_tokens <span class="op">=</span> []</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'[CLS]'</span>, <span class="st">'[SEP]'</span>, <span class="st">'[PAD]'</span>] <span class="kw">and</span> <span class="st">'##'</span> <span class="kw">not</span> <span class="kw">in</span> token:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> valid_tokens:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>                    valid_indices.append(i)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                    valid_tokens.append(token)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        all_tokens.extend([tokens[i] <span class="cf">for</span> i <span class="kw">in</span> valid_indices])</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        all_embeddings.extend([embeddings[i] <span class="cf">for</span> i <span class="kw">in</span> valid_indices])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> all_tokens:</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算每个词与文档中心的余弦相似度</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    doc_center <span class="op">=</span> np.mean(all_embeddings, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    token_scores <span class="op">=</span> []</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(all_tokens):</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> token <span class="kw">in</span> stop_words <span class="kw">or</span> <span class="bu">len</span>(token) <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> cosine_similarity([all_embeddings[i]], [doc_center])[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        token_scores.append((token, score))</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 按分数排序并返回前N个关键词</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    unique_tokens <span class="op">=</span> {}</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token, score <span class="kw">in</span> token_scores:</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> unique_tokens <span class="kw">or</span> score <span class="op">&gt;</span> unique_tokens[token]:</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            unique_tokens[token] <span class="op">=</span> score</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    sorted_tokens <span class="op">=</span> <span class="bu">sorted</span>(unique_tokens.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sorted_tokens[:top_n]</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="co"># 选择2023年报告进行分析</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>report_idx <span class="op">=</span> govreport[govreport[<span class="st">'Year'</span>] <span class="op">==</span> <span class="dv">2023</span>].index[<span class="dv">0</span>]</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>report_text <span class="op">=</span> govreport.loc[report_idx, <span class="st">'texts'</span>]</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>report_tokens <span class="op">=</span> corpus[report_idx]</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Word2Vec关键词（使用上一讲的中心性方法）</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_w2v_central_words(tokens, model, top_n<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 过滤不在模型中的词</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    valid_tokens <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> t <span class="kw">in</span> model.wv]</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> valid_tokens:</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算中心性</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    word_centrality <span class="op">=</span> {}</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> <span class="bu">set</span>(valid_tokens):</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(word) <span class="op">&lt;</span> <span class="dv">2</span>:  <span class="co"># 跳过单字词</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        similarities <span class="op">=</span> [model.wv.similarity(word, t) <span class="cf">for</span> t <span class="kw">in</span> valid_tokens <span class="cf">if</span> t <span class="op">!=</span> word]</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> similarities:</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>            word_centrality[word] <span class="op">=</span> <span class="bu">sum</span>(similarities) <span class="op">/</span> <span class="bu">len</span>(similarities)</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 返回中心性最高的词</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sorted</span>(word_centrality.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[:top_n]</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a><span class="co"># 提取关键词</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>w2v_keywords <span class="op">=</span> get_w2v_central_words(report_tokens, w2v_model)</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>bert_keywords <span class="op">=</span> extract_bert_keywords(report_text[:<span class="dv">2000</span>], tokenizer, model)  <span class="co"># 限制文本长度</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="co"># 打印结果比较</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Word2Vec提取的关键词:"</span>)</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> w2v_keywords:</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">BERT提取的关键词:"</span>)</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> bert_keywords:</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="使用预训练模型进行主题分析" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="使用预训练模型进行主题分析"><span class="header-section-number">19.2</span> 使用预训练模型进行主题分析</h2>
<p>接下来，我们探索如何使用BERT进行政府工作报告的主题分析。</p>
<section id="基于bert表示的文本聚类" class="level3" data-number="19.2.1">
<h3 data-number="19.2.1" class="anchored" data-anchor-id="基于bert表示的文本聚类"><span class="header-section-number">19.2.1</span> 基于BERT表示的文本聚类</h3>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 对BERT文档向量进行聚类</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>cluster_labels <span class="op">=</span> kmeans.fit_predict(bert_doc_vectors)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 将聚类结果与年份对应</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>cluster_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Year'</span>: years,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Cluster'</span>: cluster_labels</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 对结果进行可视化（使用PCA降维）</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>bert_2d <span class="op">=</span> pca.fit_transform(bert_doc_vectors)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 找出属于当前簇的点</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    cluster_points <span class="op">=</span> bert_2d[cluster_labels <span class="op">==</span> cluster]</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    cluster_years <span class="op">=</span> [years[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(years)) <span class="cf">if</span> cluster_labels[i] <span class="op">==</span> cluster]</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 绘制点</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>cluster<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 添加年份标签</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, point <span class="kw">in</span> <span class="bu">enumerate</span>(cluster_points):</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        plt.annotate(cluster_years[i], xy<span class="op">=</span>point, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'基于BERT表示的政府工作报告聚类'</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'PCA维度1'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'PCA维度2'</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 分析每个簇的主题特征</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    cluster_indices <span class="op">=</span> [i <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(cluster_labels) <span class="cf">if</span> label <span class="op">==</span> cluster]</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    cluster_years <span class="op">=</span> [years[i] <span class="cf">for</span> i <span class="kw">in</span> cluster_indices]</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">簇 </span><span class="sc">{</span>cluster<span class="sc">}</span><span class="ss"> 包含年份: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">map</span>(<span class="bu">str</span>, cluster_years))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 合并该簇的所有文档</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    cluster_text <span class="op">=</span> <span class="st">" "</span>.join([govreport.iloc[i][<span class="st">'texts'</span>][:<span class="dv">500</span>] <span class="cf">for</span> i <span class="kw">in</span> cluster_indices])</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取该簇的主题词</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    cluster_keywords <span class="op">=</span> extract_bert_keywords(cluster_text, tokenizer, model, top_n<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"主题词:"</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, score <span class="kw">in</span> cluster_keywords:</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="文本主题随时间的演变分析" class="level3" data-number="19.2.2">
<h3 data-number="19.2.2" class="anchored" data-anchor-id="文本主题随时间的演变分析"><span class="header-section-number">19.2.2</span> 文本主题随时间的演变分析</h3>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 对每年报告提取主题词，分析主题随时间的演变</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>years_list <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(years))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>yearly_topics <span class="op">=</span> {}</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year <span class="kw">in</span> years_list:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> govreport[govreport[<span class="st">'Year'</span>] <span class="op">==</span> year].index[<span class="dv">0</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> govreport.loc[idx, <span class="st">'texts'</span>][:<span class="dv">2000</span>]  <span class="co"># 限制长度</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取主题词</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    topics <span class="op">=</span> extract_bert_keywords(text, tokenizer, model, top_n<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    yearly_topics[year] <span class="op">=</span> topics</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 跟踪某些关键词随时间的变化</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>focus_words <span class="op">=</span> [<span class="st">'创新'</span>, <span class="st">'发展'</span>, <span class="st">'改革'</span>, <span class="st">'民生'</span>, <span class="st">'科技'</span>, <span class="st">'数字'</span>, <span class="st">'经济'</span>]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>word_trends <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> focus_words}</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year <span class="kw">in</span> years_list:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 创建当年主题词的字典</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    year_word_scores <span class="op">=</span> {word: score <span class="cf">for</span> word, score <span class="kw">in</span> yearly_topics[year]}</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 记录焦点词的出现情况</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> focus_words:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> year_word_scores:</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            word_trends[word].append((year, year_word_scores[word]))</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>            word_trends[word].append((year, <span class="dv">0</span>))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化关键词趋势</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">8</span>))</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, trend <span class="kw">in</span> word_trends.items():</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    years_data <span class="op">=</span> [t[<span class="dv">0</span>] <span class="cf">for</span> t <span class="kw">in</span> trend]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> [t[<span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> trend]</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    plt.plot(years_data, scores, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>word)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'政府工作报告中关键概念的重要性变化'</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'年份'</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'主题重要性得分'</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>plt.xticks(years_list)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="语义相似度异常点检测" class="level3" data-number="19.2.3">
<h3 data-number="19.2.3" class="anchored" data-anchor-id="语义相似度异常点检测"><span class="header-section-number">19.2.3</span> 语义相似度异常点检测</h3>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> LocalOutlierFactor</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用局部异常因子(LOF)算法检测异常点</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>lof <span class="op">=</span> LocalOutlierFactor(n_neighbors<span class="op">=</span><span class="dv">2</span>, contamination<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>outliers <span class="op">=</span> lof.fit_predict(bert_doc_vectors)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 异常点是-1，正常点是1</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>anomaly_indices <span class="op">=</span> [i <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(outliers) <span class="cf">if</span> label <span class="op">==</span> <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>anomaly_years <span class="op">=</span> [years[i] <span class="cf">for</span> i <span class="kw">in</span> anomaly_indices]</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"检测到的异常报告: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(<span class="bu">map</span>(<span class="bu">str</span>, anomaly_years))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化异常点</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(bert_2d[:, <span class="dv">0</span>], bert_2d[:, <span class="dv">1</span>], c<span class="op">=</span>[<span class="st">'red'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="op">-</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">'blue'</span> <span class="cf">for</span> x <span class="kw">in</span> outliers], </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span>[<span class="st">'Anomaly'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="op">-</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">'Normal'</span> <span class="cf">for</span> x <span class="kw">in</span> outliers])</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 添加年份标签</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, point <span class="kw">in</span> <span class="bu">enumerate</span>(bert_2d):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    plt.annotate(years[i], xy<span class="op">=</span>point, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'政府工作报告语义异常检测'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'PCA维度1'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'PCA维度2'</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 分析异常报告的特点</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> anomaly_indices:</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> anomaly_indices:</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        year <span class="op">=</span> years[idx]</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">年报告被检测为异常，分析其特点:"</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取该报告的特有词汇</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> govreport.iloc[idx][<span class="st">'texts'</span>][:<span class="dv">2000</span>]</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        keywords <span class="op">=</span> extract_bert_keywords(text, tokenizer, model, top_n<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"特征词:"</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, score <span class="kw">in</span> keywords:</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算与其他报告的平均相似度</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        similarities <span class="op">=</span> []</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bert_doc_vectors)):</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">!=</span> idx:</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>                sim <span class="op">=</span> cosine_similarity([bert_doc_vectors[idx]], [bert_doc_vectors[i]])[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>                similarities.append((years[i], sim))</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        avg_similarity <span class="op">=</span> <span class="bu">sum</span>(sim <span class="cf">for</span> _, sim <span class="kw">in</span> similarities) <span class="op">/</span> <span class="bu">len</span>(similarities)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"与其他报告的平均相似度: </span><span class="sc">{</span>avg_similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 找出最不相似的报告</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        least_similar <span class="op">=</span> <span class="bu">min</span>(similarities, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"最不相似的报告是</span><span class="sc">{</span>least_similar[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">年，相似度为</span><span class="sc">{</span>least_similar[<span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
</section>
<section id="使用大语言模型进行金融文本分析" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> 使用大语言模型进行金融文本分析</h1>
<p>在本节中，我们将探索如何利用强大的大语言模型（LLMs）进行金融文本分析。我们将介绍如何使用预训练的金融专业模型FinBERT以及通用大模型如DeepSeek等，实现无监督的文本理解任务。</p>
<section id="使用finbert进行金融文本分类" class="level2" data-number="20.1">
<h2 data-number="20.1" class="anchored" data-anchor-id="使用finbert进行金融文本分类"><span class="header-section-number">20.1</span> 使用FinBERT进行金融文本分类</h2>
<p>FinBERT是一种针对金融领域进行微调的BERT变体，专门为金融文本分析而设计。</p>
<section id="安装与加载finbert" class="level3" data-number="20.1.1">
<h3 data-number="20.1.1" class="anchored" data-anchor-id="安装与加载finbert"><span class="header-section-number">20.1.1</span> 安装与加载FinBERT</h3>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装所需库</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install transformers sentencepiece matplotlib</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install finbert</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, AutoTokenizer</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载FinBERT模型</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"ProsusAI/finbert"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(<span class="st">"ProsusAI/finbert"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># 设置为评估模式</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="金融情感分析" class="level3" data-number="20.1.2">
<h3 data-number="20.1.2" class="anchored" data-anchor-id="金融情感分析"><span class="header-section-number">20.1.2</span> 金融情感分析</h3>
<p>FinBERT的一个主要用途是金融文本的情感分析，可以将文本分类为正面、负面或中性：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_sentiment(text, model, tokenizer):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> torch.nn.functional.softmax(outputs.logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># FinBERT情感标签: 0=negative, 1=neutral, 2=positive</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    sentiment_labels <span class="op">=</span> [<span class="st">"负面"</span>, <span class="st">"中性"</span>, <span class="st">"正面"</span>]</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> predictions[<span class="dv">0</span>].numpy()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sentiment_labels)):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        results.append((sentiment_labels[i], <span class="bu">float</span>(scores[i])))</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sorted</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试几个金融相关的文本片段</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"今年以来，我国经济稳中向好，经济增长好于预期，通胀水平保持稳定。"</span>,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"受外部需求减弱影响，出口增长放缓，企业经营压力加大，就业形势更加严峻。"</span>,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"科技创新成为经济高质量发展的强大动力，数字经济蓬勃发展。"</span>,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"金融风险明显增加，部分企业债务违约，需要加强风险防控。"</span>,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"资本市场改革持续推进，投资者信心有所恢复，市场预期逐步改善。"</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 分析情感</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>sentiments <span class="op">=</span> []</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> test_texts:</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> analyze_sentiment(text, model, tokenizer)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    sentiments.append(sentiment)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"文本: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label, score <span class="kw">in</span> sentiment:</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化情感分析结果</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(test_texts))</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 提取各情感得分</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>negative_scores <span class="op">=</span> [sentiment[<span class="dv">2</span>][<span class="dv">1</span>] <span class="cf">for</span> sentiment <span class="kw">in</span> sentiments]</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>neutral_scores <span class="op">=</span> [sentiment[<span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">for</span> sentiment <span class="kw">in</span> sentiments]</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>positive_scores <span class="op">=</span> [sentiment[<span class="dv">0</span>][<span class="dv">1</span>] <span class="cf">for</span> sentiment <span class="kw">in</span> sentiments]</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制条形图</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">-</span> width, positive_scores, width, label<span class="op">=</span><span class="st">'正面'</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>ax.bar(x, neutral_scores, width, label<span class="op">=</span><span class="st">'中性'</span>)</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">+</span> width, negative_scores, width, label<span class="op">=</span><span class="st">'负面'</span>)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'情感得分'</span>)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'FinBERT金融文本情感分析'</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="ss">f'文本</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(test_texts))])</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="对政府工作报告进行金融情感分析" class="level3" data-number="20.1.3">
<h3 data-number="20.1.3" class="anchored" data-anchor-id="对政府工作报告进行金融情感分析"><span class="header-section-number">20.1.3</span> 对政府工作报告进行金融情感分析</h3>
<p>接下来，我们使用FinBERT分析政府工作报告中的金融相关段落：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 从政府工作报告中提取金融相关段落</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_finance_paragraphs(text, keywords<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keywords <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        keywords <span class="op">=</span> [<span class="st">'经济'</span>, <span class="st">'金融'</span>, <span class="st">'财政'</span>, <span class="st">'税收'</span>, <span class="st">'货币'</span>, <span class="st">'银行'</span>, <span class="st">'债务'</span>, <span class="st">'投资'</span>, </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                  <span class="st">'证券'</span>, <span class="st">'股市'</span>, <span class="st">'外汇'</span>, <span class="st">'通胀'</span>, <span class="st">'增长'</span>, <span class="st">'风险'</span>, <span class="st">'改革'</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将文本分成段落</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    paragraphs <span class="op">=</span> text.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 过滤出含有金融关键词的段落</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    finance_paragraphs <span class="op">=</span> []</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> para <span class="kw">in</span> paragraphs:</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(para) <span class="op">&lt;</span> <span class="dv">10</span>:  <span class="co"># 跳过短段落</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(keyword <span class="kw">in</span> para <span class="cf">for</span> keyword <span class="kw">in</span> keywords):</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>            finance_paragraphs.append(para)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> finance_paragraphs</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 按年份分析政府工作报告的金融情感</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>yearly_sentiments <span class="op">=</span> {}</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year <span class="kw">in</span> years_list:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> govreport[govreport[<span class="st">'Year'</span>] <span class="op">==</span> year].index[<span class="dv">0</span>]</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> govreport.loc[idx, <span class="st">'texts'</span>]</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取金融段落</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    finance_paras <span class="op">=</span> extract_finance_paragraphs(text)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 不分析段落过少的年份</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(finance_paras) <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 分析每个段落的情感</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    para_sentiments <span class="op">=</span> []</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> para <span class="kw">in</span> finance_paras[:<span class="dv">10</span>]:  <span class="co"># 限制段落数量</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        sentiment <span class="op">=</span> analyze_sentiment(para, model, tokenizer)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        para_sentiments.append(sentiment)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算平均情感分数</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    avg_positive <span class="op">=</span> np.mean([s[<span class="dv">0</span>][<span class="dv">1</span>] <span class="cf">for</span> s <span class="kw">in</span> para_sentiments])</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    avg_neutral <span class="op">=</span> np.mean([s[<span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">for</span> s <span class="kw">in</span> para_sentiments])</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    avg_negative <span class="op">=</span> np.mean([s[<span class="dv">2</span>][<span class="dv">1</span>] <span class="cf">for</span> s <span class="kw">in</span> para_sentiments])</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    yearly_sentiments[year] <span class="op">=</span> {</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"正面"</span>: avg_positive,</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"中性"</span>: avg_neutral,</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"负面"</span>: avg_negative</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化情感随时间的变化</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">8</span>))</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>years <span class="op">=</span> <span class="bu">sorted</span>(yearly_sentiments.keys())</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>positive_scores <span class="op">=</span> [yearly_sentiments[year][<span class="st">"正面"</span>] <span class="cf">for</span> year <span class="kw">in</span> years]</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>neutral_scores <span class="op">=</span> [yearly_sentiments[year][<span class="st">"中性"</span>] <span class="cf">for</span> year <span class="kw">in</span> years]</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>negative_scores <span class="op">=</span> [yearly_sentiments[year][<span class="st">"负面"</span>] <span class="cf">for</span> year <span class="kw">in</span> years]</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>plt.plot(years, positive_scores, <span class="st">'g-'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'正面'</span>)</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>plt.plot(years, neutral_scores, <span class="st">'b-'</span>, marker<span class="op">=</span><span class="st">'s'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'中性'</span>)</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>plt.plot(years, negative_scores, <span class="st">'r-'</span>, marker<span class="op">=</span><span class="st">'^'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'负面'</span>)</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'政府工作报告金融段落情感变化趋势'</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'年份'</span>)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'情感强度'</span>)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>plt.xticks(years)</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="使用通用大语言模型的零样本分类" class="level2" data-number="20.2">
<h2 data-number="20.2" class="anchored" data-anchor-id="使用通用大语言模型的零样本分类"><span class="header-section-number">20.2</span> 使用通用大语言模型的零样本分类</h2>
<p>除了专业领域模型，我们还可以利用通用大语言模型的强大能力进行零样本分类，无需额外训练。</p>
<section id="安装与设置大语言模型" class="level3" data-number="20.2.1">
<h3 data-number="20.2.1" class="anchored" data-anchor-id="安装与设置大语言模型"><span class="header-section-number">20.2.1</span> 安装与设置大语言模型</h3>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装所需库</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install openai</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install deepseek</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 导入必要的库</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> deepseek <span class="im">import</span> DeepSeekAPI</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置API密钥（请使用自己的API密钥）</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># openai.api_key = "your-api-key-here"  # GPT-4/3.5</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># deepseek_api = DeepSeekAPI("your-deepseek-api-key")  # DeepSeek</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="零样本文本分类" class="level3" data-number="20.2.2">
<h3 data-number="20.2.2" class="anchored" data-anchor-id="零样本文本分类"><span class="header-section-number">20.2.2</span> 零样本文本分类</h3>
<p>我们可以使用大语言模型进行零样本分类，无需提供训练数据：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify_text_with_gpt(text, categories, model<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用GPT模型进行零样本文本分类"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""请将以下文本分类到这些类别之一: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(categories)<span class="sc">}</span><span class="ss">。</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="ss">    只需回复类别名称，不要添加任何解释或标点符号。</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="ss">    文本: </span><span class="sc">{</span>text<span class="sc">}</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个精确的文本分类助手。"</span>},</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify_text_with_deepseek(text, categories, model<span class="op">=</span><span class="st">"deepseek-chat"</span>):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用DeepSeek模型进行零样本文本分类"""</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""请将以下文本分类到这些类别之一: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(categories)<span class="sc">}</span><span class="ss">。</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="ss">    只需回复类别名称，不要添加任何解释或标点符号。</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="ss">    文本: </span><span class="sc">{</span>text<span class="sc">}</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> deepseek_api.chat.completions.create(</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个精确的文本分类助手。"</span>},</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：将政府工作报告段落分类为不同政策领域</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>policy_categories <span class="op">=</span> [<span class="st">"经济发展"</span>, <span class="st">"科技创新"</span>, <span class="st">"民生改善"</span>, <span class="st">"环境保护"</span>, <span class="st">"改革开放"</span>, <span class="st">"风险防控"</span>]</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 从多个年份的报告中选取段落</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>sample_paragraphs <span class="op">=</span> []</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>sample_years <span class="op">=</span> [<span class="dv">2019</span>, <span class="dv">2020</span>, <span class="dv">2021</span>, <span class="dv">2022</span>, <span class="dv">2023</span>]</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year <span class="kw">in</span> sample_years:</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> govreport[govreport[<span class="st">'Year'</span>] <span class="op">==</span> year].index[<span class="dv">0</span>]</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> govreport.loc[idx, <span class="st">'texts'</span>]</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取段落</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    paragraphs <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> text.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) <span class="cf">if</span> <span class="bu">len</span>(p) <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> <span class="bu">len</span>(p) <span class="op">&lt;</span> <span class="dv">200</span>]</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> paragraphs:</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 随机选择一个段落</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> random</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        selected_para <span class="op">=</span> random.choice(paragraphs)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        sample_paragraphs.append((year, selected_para))</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用大语言模型进行分类</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：实际运行时取消注释以下代码，但需要API密钥</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="co">classification_results = []</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a><span class="co">for year, para in sample_paragraphs:</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="co">    # category = classify_text_with_gpt(para, policy_categories)</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a><span class="co">    category = classify_text_with_deepseek(para, policy_categories)</span></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co">    classification_results.append((year, para[:50] + "...", category))</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="co"># 打印分类结果</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="co">print("大语言模型零样本分类结果:")</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="co">for year, para_preview, category in classification_results:</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a><span class="co">    print(f"{year}年段落: {para_preview}")</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="co">    print(f"分类: {category}</span><span class="ch">\n</span><span class="co">")</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="使用大模型生成金融文本摘要" class="level3" data-number="20.2.3">
<h3 data-number="20.2.3" class="anchored" data-anchor-id="使用大模型生成金融文本摘要"><span class="header-section-number">20.2.3</span> 使用大模型生成金融文本摘要</h3>
<p>大语言模型在摘要生成方面表现出色，可以用于提取政府工作报告中的金融政策要点：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_summary_with_gpt(text, model<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用GPT模型生成摘要"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>text<span class="sc">}</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个专业的金融政策分析师。"</span>},</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">300</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_summary_with_deepseek(text, model<span class="op">=</span><span class="st">"deepseek-chat"</span>):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用DeepSeek模型生成摘要"""</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>text<span class="sc">}</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> deepseek_api.chat.completions.create(</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个专业的金融政策分析师。"</span>},</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">300</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 为近几年的报告生成金融政策摘要</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：实际运行时取消注释以下代码，但需要API密钥</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="co">policy_summaries = {}</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="co">for year in [2021, 2022, 2023]:</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="co">    idx = govreport[govreport['Year'] == year].index[0]</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="co">    text = govreport.loc[idx, 'texts']</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="co">    # 提取金融段落</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="co">    finance_paras = extract_finance_paragraphs(text)</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="co">    finance_text = "</span><span class="ch">\n</span><span class="co">".join(finance_paras[:10])  # 限制输入长度</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="co">    # 生成摘要</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a><span class="co">    # summary = generate_summary_with_gpt(finance_text)</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="co">    summary = generate_summary_with_deepseek(finance_text)</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="co">    policy_summaries[year] = summary</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="co"># 打印摘要结果</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="co">for year, summary in policy_summaries.items():</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="co">    print(f"{year}年政府工作报告金融政策要点:")</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="co">    print(summary)</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="co">    print("</span><span class="ch">\n</span><span class="co">" + "-"*50 + "</span><span class="ch">\n</span><span class="co">")</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="使用大语言模型进行高级文本分析" class="level2" data-number="20.3">
<h2 data-number="20.3" class="anchored" data-anchor-id="使用大语言模型进行高级文本分析"><span class="header-section-number">20.3</span> 使用大语言模型进行高级文本分析</h2>
<p>大语言模型还可以用于更复杂的无监督文本分析任务。</p>
<section id="提示式主题建模" class="level3" data-number="20.3.1">
<h3 data-number="20.3.1" class="anchored" data-anchor-id="提示式主题建模"><span class="header-section-number">20.3.1</span> 提示式主题建模</h3>
<p>使用提示工程（Prompt Engineering）引导大语言模型进行主题发现：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_topics_with_gpt(texts, n_topics<span class="op">=</span><span class="dv">5</span>, model<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用GPT模型进行提示式主题建模"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 合并文本，但限制长度以适应API限制</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    combined_text <span class="op">=</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(texts)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(combined_text) <span class="op">&gt;</span> <span class="dv">4000</span>:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        combined_text <span class="op">=</span> combined_text[:<span class="dv">4000</span>] <span class="op">+</span> <span class="st">"..."</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""作为一个文本挖掘专家，请分析以下多个文本片段，识别其中的</span><span class="sc">{</span>n_topics<span class="sc">}</span><span class="ss">个主要主题。</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="ss">    每个主题请提供一个简短标题和3-5个关键词。</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="ss">    请仅输出主题和关键词，不要有其他解释。</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="ss">    文本片段:</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>combined_text<span class="sc">}</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个专业的文本挖掘专家。"</span>},</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">500</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 从多个年份的报告中提取金融段落</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>all_finance_paras <span class="op">=</span> []</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year <span class="kw">in</span> years_list[<span class="op">-</span><span class="dv">5</span>:]:  <span class="co"># 只取最近5年</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> govreport[govreport[<span class="st">'Year'</span>] <span class="op">==</span> year].index[<span class="dv">0</span>]</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> govreport.loc[idx, <span class="st">'texts'</span>]</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    finance_paras <span class="op">=</span> extract_finance_paragraphs(text)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    all_finance_paras.extend(finance_paras[:<span class="dv">5</span>])  <span class="co"># 每年取5个段落</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用大语言模型进行主题建模</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：实际运行时取消注释以下代码，但需要API密钥</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="co">topics = extract_topics_with_gpt(all_finance_paras)</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="co">print("大语言模型识别的主题:")</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="co">print(topics)</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="思维链分析" class="level3" data-number="20.3.2">
<h3 data-number="20.3.2" class="anchored" data-anchor-id="思维链分析"><span class="header-section-number">20.3.2</span> 思维链分析</h3>
<p>使用思维链（Chain-of-Thought）技术引导大语言模型进行深度分析：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_with_cot(text, question, model<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""使用思维链技术进行深度分析"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""请分析以下政府工作报告文本，回答问题。</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="ss">    请先逐步思考，然后给出最终答案。</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="ss">    文本:</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>text<span class="sc">}</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="ss">    问题: </span><span class="sc">{</span>question<span class="sc">}</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="ss">    逐步思考:</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"你是一个专业的政策分析师。"</span>},</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">800</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：分析最新报告中的政策转变</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：实际运行时取消注释以下代码，但需要API密钥</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 获取2023年报告</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">latest_idx = govreport[govreport['Year'] == 2023].index[0]</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">latest_text = govreport.loc[latest_idx, 'texts']</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 限制文本长度</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">analysis_text = latest_text[:3000]</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义分析问题</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="co">analysis_question = "这份政府工作报告对金融风险防控政策有哪些新的调整？这些调整与前几年相比有何变化？"</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 进行思维链分析</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="co">analysis_result = analyze_with_cot(analysis_text, analysis_question)</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="co">print("思维链分析结果:")</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="co">print(analysis_result)</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="嵌入空间与向量检索" class="level3" data-number="20.3.3">
<h3 data-number="20.3.3" class="anchored" data-anchor-id="嵌入空间与向量检索"><span class="header-section-number">20.3.3</span> 嵌入空间与向量检索</h3>
<p>大语言模型的嵌入向量可用于高级语义检索：</p>
<div class="cell">
<details class="code-fold">
<summary>代码</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_embedding_with_openai(text, model<span class="op">=</span><span class="st">"text-embedding-ada-002"</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""获取OpenAI的文本嵌入向量"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> text.strip():</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.zeros(<span class="dv">1536</span>)  <span class="co"># OpenAI embeddings are 1536-dimensional</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 限制文本长度</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(text) <span class="op">&gt;</span> <span class="dv">8000</span>:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> text[:<span class="dv">8000</span>]</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.Embedding.create(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span><span class="op">=</span>text,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    embedding <span class="op">=</span> response[<span class="st">'data'</span>][<span class="dv">0</span>][<span class="st">'embedding'</span>]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(embedding)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 为政府工作报告段落生成嵌入向量</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：实际运行时取消注释以下代码，但需要API密钥</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备段落</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">all_paragraphs = []</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co">for year in years_list:</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co">    idx = govreport[govreport['Year'] == year].index[0]</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co">    text = govreport.loc[idx, 'texts']</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">    # 分段</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co">    paragraphs = [p for p in text.split('</span><span class="ch">\n</span><span class="co">') if len(p) &gt; 50]</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">    for para in paragraphs[:10]:  # 每年最多10个段落</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co">        all_paragraphs.append({</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co">            'year': year,</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co">            'content': para</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="co">        })</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成嵌入向量</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co">for i, para in enumerate(all_paragraphs):</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="co">    embedding = get_embedding_with_openai(para['content'])</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="co">    all_paragraphs[i]['embedding'] = embedding</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算相似度矩阵</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co">n_paras = len(all_paragraphs)</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co">similarity_matrix = np.zeros((n_paras, n_paras))</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="co">for i in range(n_paras):</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="co">    for j in range(n_paras):</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="co">        if i == j:</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a><span class="co">            similarity_matrix[i, j] = 1.0</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a><span class="co">        else:</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="co">            similarity_matrix[i, j] = cosine_similarity(</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="co">                [all_paragraphs[i]['embedding']], </span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="co">                [all_paragraphs[j]['embedding']]</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="co">            )[0, 0]</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a><span class="co"># 找出每个段落最相似的其他段落</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a><span class="co">for i, para in enumerate(all_paragraphs):</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="co">    similar_indices = np.argsort(similarity_matrix[i])[-3:-1][::-1]  # 排除自身，取前2个</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="co">    print(f"{para['year']}年段落:")</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a><span class="co">    print(para['content'][:100] + "...</span><span class="ch">\n</span><span class="co">")</span></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a><span class="co">    print("最相似的段落:")</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a><span class="co">    for idx in similar_indices:</span></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="co">        sim_para = all_paragraphs[idx]</span></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="co">        sim_score = similarity_matrix[i, idx]</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="co">        print(f"- {sim_para['year']}年 (相似度: {sim_score:.4f}):")</span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a><span class="co">        print(sim_para['content'][:100] + "...</span><span class="ch">\n</span><span class="co">")</span></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a><span class="co">    print("-"*80 + "</span><span class="ch">\n</span><span class="co">")</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="复制到剪贴板" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="应用大语言模型的最佳实践" class="level2" data-number="20.4">
<h2 data-number="20.4" class="anchored" data-anchor-id="应用大语言模型的最佳实践"><span class="header-section-number">20.4</span> 应用大语言模型的最佳实践</h2>
<p>在金融文本分析中应用大语言模型时，应注意以下几点：</p>
<section id="提示设计技巧" class="level3" data-number="20.4.1">
<h3 data-number="20.4.1" class="anchored" data-anchor-id="提示设计技巧"><span class="header-section-number">20.4.1</span> 提示设计技巧</h3>
<ol type="1">
<li><strong>明确任务界定</strong>：
<ul>
<li>清晰指定分析目标和期望输出格式</li>
<li>使用领域专业术语增强精确性</li>
</ul></li>
<li><strong>思维链设计</strong>：
<ul>
<li>引导模型分步思考复杂问题</li>
<li>要求模型先分析再总结</li>
</ul></li>
<li><strong>角色设定</strong>：
<ul>
<li>指定模型扮演”金融分析师”等专业角色</li>
<li>增强输出的专业性和针对性</li>
</ul></li>
</ol>
</section>
<section id="金融特定优化" class="level3" data-number="20.4.2">
<h3 data-number="20.4.2" class="anchored" data-anchor-id="金融特定优化"><span class="header-section-number">20.4.2</span> 金融特定优化</h3>
<ol type="1">
<li><strong>上下文补充</strong>：
<ul>
<li>提供行业背景信息增强理解</li>
<li>明确时间线帮助模型理解经济周期</li>
</ul></li>
<li><strong>多模型比较</strong>：
<ul>
<li>使用通用模型和金融专业模型对比结果</li>
<li>综合优势获得更全面分析</li>
</ul></li>
<li><strong>人机协作</strong>：
<ul>
<li>将模型输出作为专业分析的起点</li>
<li>关键决策仍需人类专家判断</li>
</ul></li>
</ol>
</section>
<section id="局限性与注意事项" class="level3" data-number="20.4.3">
<h3 data-number="20.4.3" class="anchored" data-anchor-id="局限性与注意事项"><span class="header-section-number">20.4.3</span> 局限性与注意事项</h3>
<ol type="1">
<li><strong>事实准确性</strong>：
<ul>
<li>大语言模型可能产生”幻觉”，输出虚构内容</li>
<li>关键数据和结论需要人工验证</li>
</ul></li>
<li><strong>偏见风险</strong>：
<ul>
<li>模型可能继承训练数据中的偏见</li>
<li>金融分析需要客观中立</li>
</ul></li>
<li><strong>时效性限制</strong>：
<ul>
<li>模型知识截止日期后的事件需要通过提示补充</li>
<li>定期更新分析以反映最新情况</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="小结与进阶方向" class="level1" data-number="21">
<h1 data-number="21"><span class="header-section-number">21</span> 小结与进阶方向</h1>
<section id="从静态向量到大语言模型的演进" class="level2" data-number="21.1">
<h2 data-number="21.1" class="anchored" data-anchor-id="从静态向量到大语言模型的演进"><span class="header-section-number">21.1</span> 从静态向量到大语言模型的演进</h2>
<p>本讲我们从Word2Vec的局限性出发，介绍了BERT等Transformer模型的原理，以及大语言模型的应用：</p>
<ol type="1">
<li><strong>表示方法演进</strong>：从静态词向量到上下文感知的动态表示</li>
<li><strong>架构演进</strong>：从浅层神经网络到深层Transformer架构</li>
<li><strong>规模演进</strong>：从百万参数到千亿参数</li>
<li><strong>应用演进</strong>：从特征提取到端到端文本理解与生成</li>
</ol>
</section>
<section id="无监督学习的新范式" class="level2" data-number="21.2">
<h2 data-number="21.2" class="anchored" data-anchor-id="无监督学习的新范式"><span class="header-section-number">21.2</span> 无监督学习的新范式</h2>
<p>大语言模型为无监督学习带来了新的范式：</p>
<ol type="1">
<li><strong>零样本学习</strong>：无需额外标注数据，直接分类新数据</li>
<li><strong>上下文学习</strong>：通过提示中的示例引导模型学习模式</li>
<li><strong>涌现能力</strong>：模型规模增长带来质的飞跃</li>
<li><strong>提示工程</strong>：通过设计提示引导模型行为</li>
</ol>
</section>
<section id="金融文本分析的未来方向" class="level2" data-number="21.3">
<h2 data-number="21.3" class="anchored" data-anchor-id="金融文本分析的未来方向"><span class="header-section-number">21.3</span> 金融文本分析的未来方向</h2>
<p>大语言模型在金融文本分析中的未来方向包括：</p>
<ol type="1">
<li><strong>多模态融合</strong>：结合文本、数值、图表等多种数据</li>
<li><strong>实时适应</strong>：持续学习最新市场信息和政策变化</li>
<li><strong>可解释性增强</strong>：提高模型决策的透明度</li>
<li><strong>领域知识增强</strong>：融入更多金融专业知识</li>
</ol>
</section>
<section id="进阶学习资源" class="level2" data-number="21.4">
<h2 data-number="21.4" class="anchored" data-anchor-id="进阶学习资源"><span class="header-section-number">21.4</span> 进阶学习资源</h2>
<ol type="1">
<li><strong>理论深入</strong>：
<ul>
<li>《Attention Is All You Need》 - Transformer原始论文</li>
<li>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</li>
</ul></li>
<li><strong>实践教程</strong>：
<ul>
<li>Hugging Face Transformers 库文档</li>
<li>OpenAI GPT API 文档</li>
</ul></li>
<li><strong>金融NLP资源</strong>：
<ul>
<li>FinBERT 和 FinGPT 项目</li>
<li>金融领域预训练模型集合</li>
</ul></li>
</ol>
</section>
<section id="本讲小结" class="level2" data-number="21.5">
<h2 data-number="21.5" class="anchored" data-anchor-id="本讲小结"><span class="header-section-number">21.5</span> 本讲小结</h2>
<p>本讲我们从Word2Vec的局限性出发，介绍了BERT和Transformer架构的原理，以及大语言模型在金融文本分析中的应用：</p>
<ol type="1">
<li>从静态词向量到动态上下文表示的演进</li>
<li>Transformer架构与自注意力机制的工作原理</li>
<li>BERT等预训练模型的内部结构和应用方法</li>
<li>大语言模型的关键创新与涌现能力</li>
<li>实践案例：使用BERT和大语言模型分析政府工作报告</li>
</ol>
<p>通过这些内容，我们理解了现代NLP技术在金融文本分析中的强大能力，以及如何将这些技术应用于实际金融分析任务。</p>
</section>
</section>
<section id="参考资料" class="level1" data-number="22">
<h1 data-number="22"><span class="header-section-number">22</span> 参考资料</h1>
<ol type="1">
<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</li>
<li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</li>
<li>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., … &amp; Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</li>
<li>Yang, Y., Uy, M. C. S., &amp; Huang, A. (2020). FinBERT: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097.</li>
<li>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., … &amp; Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./12_embedding.html" class="pagination-link" aria-label="文本分析(二)：词向量与深度学习基础">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析(二)：词向量与深度学习基础</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./project1_LC.html" class="pagination-link" aria-label="项目1：借贷违约风险评估">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>