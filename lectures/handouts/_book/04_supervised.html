<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; 监督学习（下） – 数据挖掘与机器学习课程讲义</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05_model_assess.html" rel="next">
<link href="./03_supervised.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ed6da6eef3892af8a4b5ed59bfb951f5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04_supervised.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">数据挖掘与机器学习课程讲义</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">前言</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_ml_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">机器学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab02_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_model_assess.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_ts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">时间序列监督学习</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab03_titanic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">泰坦尼克号生存预测实践</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_credit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">信用评分理论基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">非监督学习技术概览及其金融应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">非监督学习：聚类 (Clustering)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">非监督学习：降维 (Dimensionality Reduction)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">文本分析(一)：词频法与向量空间</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析(二)：词向量与深度学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析(三)：大语言模型及其应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project1_LC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2a_tspred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">项目2A：上证综指收益率时间序列预测</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2b_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">项目2B：财务报表文本分析与企业网络安全风险评估</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#集成学习-ensemble-learning" id="toc-集成学习-ensemble-learning" class="nav-link active" data-scroll-target="#集成学习-ensemble-learning"><span class="header-section-number">5.1</span> 集成学习 (Ensemble Learning)</a>
  <ul class="collapse">
  <li><a href="#bagging-bootstrap-aggregating" id="toc-bagging-bootstrap-aggregating" class="nav-link" data-scroll-target="#bagging-bootstrap-aggregating"><span class="header-section-number">5.1.1</span> Bagging (Bootstrap Aggregating)</a></li>
  <li><a href="#boosting-提升法" id="toc-boosting-提升法" class="nav-link" data-scroll-target="#boosting-提升法"><span class="header-section-number">5.1.2</span> Boosting (提升法)</a></li>
  </ul></li>
  <li><a href="#神经网络-neural-network" id="toc-神经网络-neural-network" class="nav-link" data-scroll-target="#神经网络-neural-network"><span class="header-section-number">5.2</span> 神经网络 (Neural Network)</a>
  <ul class="collapse">
  <li><a href="#神经网络模型及算法" id="toc-神经网络模型及算法" class="nav-link" data-scroll-target="#神经网络模型及算法"><span class="header-section-number">5.2.1</span> 神经网络模型及算法</a></li>
  <li><a href="#深度学习-deep-learning" id="toc-深度学习-deep-learning" class="nav-link" data-scroll-target="#深度学习-deep-learning"><span class="header-section-number">5.2.2</span> 深度学习 (Deep Learning)</a></li>
  </ul></li>
  <li><a href="#总结" id="toc-总结" class="nav-link" data-scroll-target="#总结"><span class="header-section-number">5.3</span> 总结</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="集成学习-ensemble-learning" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="集成学习-ensemble-learning"><span class="header-section-number">5.1</span> 集成学习 (Ensemble Learning)</h2>
<p>集成学习 (Ensemble Learning) 是一种<strong>将多个弱学习器 (Weak Learner) 组合成一个强学习器 (Strong Learner) 的技术</strong>。 弱学习器通常指<strong>预测性能略优于随机猜测的模型</strong>，例如<strong>单层决策树</strong>、<strong>简单的线性模型</strong>等。集成学习的核心思想是 <strong>“集思广益”</strong>，即通过<strong>组合多个弱学习器的预测结果</strong>，来获得比单个强学习器<strong>更全面、更鲁棒的预测能力</strong>。</p>
<p>集成学习模型通常具有比单个学习器<strong>更好的预测性能和泛化能力</strong>。 这是因为集成学习可以通过以下方式<strong>降低模型的误差</strong>：</p>
<ul>
<li><strong>降低方差 (Variance Reduction)</strong>：Bagging 等方法通过<strong>并行训练多个基学习器</strong>，并<strong>对它们的预测结果进行平均或投票</strong>，可以有效<strong>降低模型的方差</strong>，提高模型的稳定性。</li>
<li><strong>降低偏差 (Bias Reduction)</strong>：Boosting 等方法通过<strong>串行训练多个基学习器</strong>，<strong>每个基学习器都试图纠正前一个基学习器的错误</strong>，可以有效<strong>降低模型的偏差</strong>，提高模型的预测精度。</li>
<li><strong>提高鲁棒性 (Robustness Improvement)</strong>：集成学习模型通常<strong>不易过拟合</strong>，对<strong>异常值和噪声数据</strong>具有<strong>更强的鲁棒性</strong>。</li>
</ul>
<p>集成学习的核心思想是 <strong>“三个臭皮匠，顶个诸葛亮”</strong>，即通过<strong>集体智慧</strong>来提高模型的性能。常用的集成学习方法包括 <strong>Bagging (Bootstrap Aggregating)</strong>、<strong>Boosting (提升法)</strong> 和 <strong>Stacking (堆叠法)</strong>。</p>
<section id="bagging-bootstrap-aggregating" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="bagging-bootstrap-aggregating"><span class="header-section-number">5.1.1</span> Bagging (Bootstrap Aggregating)</h3>
<p>Bagging (Bootstrap Aggregating)，也称为<strong>自助采样聚合</strong>，是一种基于 <strong>自助采样 (Bootstrap Sampling)</strong> 的集成学习方法。Bagging 的核心思想是 <strong>并行集成</strong>，即<strong>同时训练多个独立的基学习器</strong>，然后通过<strong>投票 (Voting)</strong> 或 <strong>平均 (Averaging)</strong> 的方式将它们的预测结果集成起来。Bagging 可以有效<strong>降低模型的方差 (Variance)</strong>，提高模型的稳定性和泛化能力，尤其适用于<strong>容易过拟合</strong>的基学习器，如决策树、神经网络等。</p>
<p><strong>自助采样 (Bootstrap Sampling) 的直观解释:</strong></p>
<p>想象一下，你有一个装满弹珠的罐子（代表原始数据集），你想估计罐子里弹珠颜色的分布。自助采样就像是：</p>
<ol type="1">
<li><strong>有放回地从罐子里随机抽取一些弹珠</strong>，记录它们的颜色，然后<strong>放回罐子</strong>。</li>
<li><strong>重复步骤 1 多次</strong>，每次抽取都形成一个新的弹珠样本集（子数据集）。</li>
</ol>
<p>由于是<strong>有放回的抽样</strong>，每次抽取的子数据集都<strong>可能包含重复的弹珠</strong>，也<strong>可能缺少原始数据集中的某些弹珠</strong>。但从整体上看，<strong>每个子数据集都近似地代表了原始数据集的分布</strong>。</p>
<p><strong>Bagging 降低方差的原理:</strong></p>
<p>Bagging 通过<strong>自助采样</strong>创建多个略有不同的训练数据集，并在<strong>每个数据集上独立训练一个基学习器</strong>。由于<strong>每个基学习器都是在不同的数据集上训练的</strong>，它们之间具有一定的<strong>差异性</strong>。当<strong>对多个基学习器的预测结果进行平均或投票时</strong>，可以<strong>有效地平滑掉单个基学习器预测结果中的随机波动</strong>，从而<strong>降低整体模型的方差</strong>。</p>
<p><strong>算法流程:</strong></p>
<ol type="1">
<li><strong>自助采样 (Bootstrap Sampling)</strong>：从原始数据集 <span class="math inline">\(D\)</span> 中<strong>有放回地随机抽取</strong> <span class="math inline">\(N\)</span> 个样本，构建一个<strong>子数据集 <span class="math inline">\(D_t\)</span></strong>，也称为 <strong>自助样本集 (Bootstrap Sample)</strong>。重复 <span class="math inline">\(T\)</span> 次，得到 <span class="math inline">\(T\)</span> 个<strong>独立的子数据集</strong> <span class="math inline">\(\{D_1, D_2, ..., D_T\}\)</span>。每个子数据集的大小与原始数据集相同，但样本分布略有不同。由于是有放回抽样，因此每个子数据集中可能包含重复样本，也可能缺少原始数据集中的某些样本。一般来说，每个子数据集大约包含原始数据集 63.2% 的样本。</li>
<li><strong>训练基学习器 (Base Learner)</strong>：在<strong>每个子数据集 <span class="math inline">\(D_t\)</span> 上独立地训练一个基学习器 <span class="math inline">\(h_t\)</span></strong>。基学习器可以是同质的（例如，都使用决策树），也可以是异质的（例如，使用决策树、神经网络、SVM 等不同的模型）。常用的基学习器是决策树，此时的 Bagging 集成学习方法就是 <strong>随机森林 (Random Forest)</strong>。</li>
<li><strong>集成预测 (Ensemble Prediction)</strong>：对于<strong>分类问题</strong>，Bagging 通常使用 <strong>投票法 (Voting)</strong> 进行预测，即<strong>将 <span class="math inline">\(T\)</span> 个基学习器预测结果中出现次数最多的类别作为最终预测结果</strong>。对于<strong>回归问题</strong>，Bagging 通常使用 <strong>平均法 (Averaging)</strong> 进行预测，即<strong>将 <span class="math inline">\(T\)</span> 个基学习器预测结果的平均值作为最终预测结果</strong>。</li>
</ol>
<p><strong>随机森林 (Random Forest):</strong></p>
<p>随机森林 (Random Forest, RF) 是一种非常流行且强大的 <strong>基于 Bagging 思想的集成学习模型</strong>，以 <strong>决策树 (Decision Tree)</strong> 为基学习器。随机森林在 Bagging 的基础上，进一步引入了 <strong>特征随机选择 (Random Feature Selection)</strong>，使得基学习器之间具有<strong>更高的差异性 (Diversity)</strong>，从而进一步提高集成的性能。随机森林具有<strong>高精度</strong>、<strong>高效率</strong>、<strong>鲁棒性强</strong>、<strong>不易过拟合</strong>等优点，被广泛应用于分类、回归和特征重要性评估等任务。</p>
<p><strong>特征随机选择 (Random Feature Selection) 的直观解释:</strong></p>
<p>在构建决策树时，传统的决策树会在<strong>所有特征中选择最优特征</strong>进行节点分裂。而特征随机选择则是在<strong>每个节点分裂时</strong>，<strong>先随机选择一部分特征</strong>，然后<strong>只在这部分特征中选择最优特征</strong>。</p>
<p><strong>特征随机选择提高模型性能的原理:</strong></p>
<p>特征随机选择<strong>进一步增加了基学习器之间的差异性</strong>。由于<strong>每个决策树只使用一部分随机选择的特征进行训练</strong>，即使在相同的子数据集上训练，<strong>不同的决策树也会学习到不同的特征子集</strong>，从而<strong>降低基学习器之间的相关性</strong>，提高集成的效果。 此外，特征随机选择还有助于<strong>降低模型的过拟合风险</strong>，并<strong>提高模型的泛化能力</strong>。</p>
<p><strong>随机森林的构建过程:</strong></p>
<ol type="1">
<li><strong>自助采样 (Bootstrap Sampling)</strong>：与 Bagging 相同，从原始数据集 <span class="math inline">\(D\)</span> 中有放回地随机抽取 <span class="math inline">\(N\)</span> 个样本，构建 <span class="math inline">\(T\)</span> 个子数据集 <span class="math inline">\(\{D_1, D_2, ..., D_T\}\)</span>。</li>
<li><strong>训练基学习器 (决策树)</strong>：在<strong>每个子数据集 <span class="math inline">\(D_t\)</span> 上训练一个决策树 <span class="math inline">\(h_t\)</span></strong>。与传统的决策树不同，随机森林在训练决策树的过程中，<strong>引入了特征随机选择</strong>。具体来说，在决策树的<strong>每个节点分裂时</strong>，<strong>不是从所有特征中选择最优特征</strong>，而是先<strong>随机选择 <span class="math inline">\(m\)</span> 个特征构成一个特征子集</strong>（通常 <span class="math inline">\(m &lt;&lt; K\)</span>，例如 <span class="math inline">\(m = \sqrt{K}\)</span>），然后<strong>从这个特征子集中选择最优特征进行分裂</strong>。这里的 <span class="math inline">\(m\)</span> 是一个<strong>超参数</strong>，需要预先设定。特征随机选择进一步增加了基学习器之间的差异性，使得随机森林的集成效果更好。</li>
<li><strong>集成预测 (Ensemble Prediction)</strong>：与 Bagging 相同，对于分类问题使用投票法，对于回归问题使用平均法。</li>
</ol>
<p><strong>随机森林的特点:</strong></p>
<ul>
<li><strong>高精度</strong>：随机森林通常比单个决策树具有更高的预测精度。</li>
<li><strong>鲁棒性强</strong>：随机森林对异常值和噪声数据具有较好的鲁棒性。</li>
<li><strong>不易过拟合</strong>：随机森林通过 Bagging 和特征随机选择，有效降低了模型的方差，不易过拟合。</li>
<li><strong>泛化能力强</strong>：随机森林具有较强的泛化能力，在测试集上表现良好。</li>
<li><strong>可处理高维数据</strong>：随机森林可以处理高维数据，无需进行特征选择。</li>
<li><strong>可评估特征重要性</strong>：随机森林可以评估每个特征在模型中的重要性，用于特征选择和特征理解。</li>
<li><strong>实现简单，易于并行化</strong>：随机森林的构建过程简单高效，基学习器之间相互独立，易于并行化处理，训练速度快。</li>
</ul>
</section>
<section id="boosting-提升法" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="boosting-提升法"><span class="header-section-number">5.1.2</span> Boosting (提升法)</h3>
<p>Boosting (提升法) 是一种与 Bagging 不同的集成学习方法。Boosting 的核心思想是 <strong>串行集成</strong>，即<strong>迭代地训练一系列的基学习器</strong>，<strong>每个基学习器都试图纠正前一个基学习器的错误</strong>。Boosting 方法通过<strong>加权样本</strong>或<strong>调整预测结果</strong>的方式，<strong>逐步提升模型的性能</strong>。Boosting 方法可以有效<strong>降低模型的偏差 (Bias) 和方差</strong>，得到<strong>高精度</strong>的集成模型，尤其适用于<strong>弱学习器</strong>，如<strong>浅层决策树 (Decision Stump)</strong>。常用的 Boosting 算法包括 <strong>AdaBoost (Adaptive Boosting)</strong>、<strong>GBDT (Gradient Boosting Decision Tree)</strong>、<strong>XGBoost (Extreme Gradient Boosting)</strong>、<strong>LightGBM (Light Gradient Boosting Machine)</strong>、<strong>CatBoost (Categorical Boosting)</strong> 等。</p>
<p><strong>Boosting 的串行集成思想的直观解释:</strong></p>
<p>Boosting 就像是<strong>一个团队合作解决问题</strong>的过程，其中：</p>
<ol type="1">
<li><strong>第一个弱学习器先尝试解决问题</strong>，但可能做得不够好，会犯一些错误。</li>
<li><strong>后续的弱学习器会仔细研究第一个学习器犯的错误</strong>，并<strong>针对这些错误进行改进</strong>，<strong>尝试纠正之前的错误</strong>。</li>
<li><strong>每个弱学习器都在前一个学习器的基础上进行提升</strong>，<strong>逐步提高整体的预测能力</strong>。</li>
</ol>
<p><strong>Boosting 迭代提升模型性能的原理:</strong></p>
<p>Boosting 方法通过<strong>迭代训练</strong>，<strong>每一轮迭代都关注前一轮模型预测错误的样本</strong>，并<strong>调整样本权重或模型权重</strong>，使得<strong>后续的模型更加关注难以分类或预测的样本</strong>。 这样<strong>不断地迭代和调整</strong>，<strong>逐步将弱学习器提升为强学习器</strong>，最终得到一个<strong>高精度、高性能的集成模型</strong>。</p>
<p><strong>AdaBoost (Adaptive Boosting):</strong></p>
<p>AdaBoost (Adaptive Boosting, 自适应提升) 是一种经典的 <strong>Boosting 算法</strong>。AdaBoost 的核心思想是 <strong>“关注错误样本”</strong> 和 <strong>“加权基学习器”</strong>。AdaBoost 通过<strong>迭代地训练基学习器</strong>，<strong>每轮迭代都更加关注前一轮基学习器预测错误的样本</strong>，<strong>提高错误样本的权重</strong>，<strong>降低正确样本的权重</strong>，使得后续的基学习器更加关注难以分类的样本。同时，AdaBoost <strong>为每个基学习器赋予一个权重</strong>，<strong>预测性能好的基学习器权重较高</strong>，<strong>预测性能差的基学习器权重较低</strong>。最终模型是<strong>所有基学习器的加权线性组合</strong>。AdaBoost 算法主要用于<strong>二分类问题</strong>。</p>
<p><strong>“关注错误样本” 和 “加权基学习器” 的直观解释:</strong></p>
<ul>
<li><strong>关注错误样本</strong>：在每一轮迭代中，AdaBoost 会<strong>提高上一轮分类错误的样本的权重</strong>，使得<strong>后续的基学习器更加关注这些难分样本</strong>，努力将它们分类正确。 这就像老师在辅导学生时，会<strong>更加关注那些经常犯错的学生</strong>，帮助他们改正错误，提高学习成绩。</li>
<li><strong>加权基学习器</strong>：AdaBoost 会<strong>根据每个基学习器的预测性能</strong>，<strong>赋予不同的权重</strong>。<strong>预测性能好的基学习器</strong>，例如错误率低的基学习器，会被赋予<strong>更高的权重</strong>，在最终的预测中起<strong>更大的作用</strong>。 这就像专家团队中，<strong>更信任那些经验丰富、能力强的专家</strong>的意见。</li>
</ul>
<p><strong>AdaBoost 算法流程 (二分类):</strong></p>
<ol type="1">
<li><strong>初始化样本权重 (Initialize Sample Weights)</strong>：为<strong>每个样本赋予相同的初始权重</strong> <span class="math inline">\(w_{1i} = 1/N, i = 1, 2, ..., N\)</span>。初始时，所有样本的权重相同，表示所有样本同等重要。</li>
<li><strong>迭代训练基学习器 (Iterative Training of Base Learners)</strong>：进行 <span class="math inline">\(T\)</span> 轮迭代， <span class="math inline">\(t = 1, 2, ..., T\)</span>：
<ol type="a">
<li><strong>训练基学习器 (Train Base Learner)</strong>：使用<strong>带有样本权重的训练数据集</strong> <span class="math inline">\(\{( \mathbf{x}_i, y_i, w_{ti} )\}_{i=1}^{N}\)</span> 训练一个基学习器 <span class="math inline">\(h_t(\mathbf{x})\)</span>。在第 <span class="math inline">\(t\)</span> 轮迭代中，基学习器 <span class="math inline">\(h_t\)</span> 基于样本权重 <span class="math inline">\(w_{ti}\)</span> 进行训练，使得<strong>加权训练误差最小化</strong>。基学习器通常选择<strong>弱学习器</strong>，如<strong>决策树桩 (Decision Stump)</strong>，即<strong>单层决策树</strong>。</li>
<li><strong>计算基学习器权重 (Calculate Base Learner Weight)</strong>：计算基学习器 <span class="math inline">\(h_t\)</span> 在<strong>训练集上的加权错误率</strong> <span class="math inline">\(e_t = P(h_t(\mathbf{x}_i) \ne y_i) = \sum_{i=1}^{N} w_{ti} I(h_t(\mathbf{x}_i) \ne y_i)\)</span>。 <span class="math inline">\(e_t\)</span> 表示被基学习器 <span class="math inline">\(h_t\)</span> 误分类的样本的权重之和。如果 <span class="math inline">\(e_t &gt; 0.5\)</span>，则停止迭代，因为此时基学习器的性能甚至不如随机猜测。然后计算基学习器 <span class="math inline">\(h_t\)</span> 的<strong>权重</strong> <span class="math inline">\(\alpha_t = \frac{1}{2} \ln(\frac{1-e_t}{e_t})\)</span>。当 <span class="math inline">\(e_t\)</span> 越小时，<span class="math inline">\(\alpha_t\)</span> 越大，说明基学习器 <span class="math inline">\(h_t\)</span> 的预测性能越好，权重越高。</li>
<li><strong>更新样本权重 (Update Sample Weights)</strong>：根据基学习器 <span class="math inline">\(h_t\)</span> 的预测结果<strong>更新样本权重</strong>，<strong>提高误分类样本的权重</strong>，<strong>降低正确分类样本的权重</strong>，使得后续的基学习器更加关注难以分类的样本。样本权重更新公式为： <span class="math display">\[w_{t+1, i} = \frac{w_{ti}}{Z_t} \times \begin{cases} e^{-\alpha_t}, &amp; \text{if } h_t(\mathbf{x}_i) = y_i \\ e^{\alpha_t}, &amp; \text{if } h_t(\mathbf{x}_i) \ne y_i \end{cases} = \frac{w_{ti}}{Z_t} e^{-\alpha_t y_i h_t(\mathbf{x}_i)}\]</span> 其中 <span class="math inline">\(Z_t = \sum_{i=1}^{N} w_{ti} e^{-\alpha_t y_i h_t(\mathbf{x}_i)}\)</span> 是<strong>归一化因子 (Normalization Factor)</strong>，使得 <span class="math inline">\(\sum_{i=1}^{N} w_{t+1, i} = 1\)</span>，保证样本权重之和为 1。 <span class="math inline">\(y_i \in \{-1, +1\}\)</span> 是样本的真实标签， <span class="math inline">\(h_t(\mathbf{x}_i) \in \{-1, +1\}\)</span> 是基学习器的预测标签。如果样本被正确分类 (<span class="math inline">\(y_i h_t(\mathbf{x}_i) = +1\)</span>)，则样本权重乘以 <span class="math inline">\(e^{-\alpha_t} &lt; 1\)</span>，权重降低；如果样本被误分类 (<span class="math inline">\(y_i h_t(\mathbf{x}_i) = -1\)</span>)，则样本权重乘以 <span class="math inline">\(e^{\alpha_t} &gt; 1\)</span>，权重提高。</li>
</ol></li>
<li><strong>构建最终模型 (Build Final Model)</strong>：经过 <span class="math inline">\(T\)</span> 轮迭代后，得到 <span class="math inline">\(T\)</span> 个基学习器 <span class="math inline">\(\{h_1, h_2, ..., h_T\}\)</span> 及其对应的权重 <span class="math inline">\(\{\alpha_1, \alpha_2, ..., \alpha_T\}\)</span>。<strong>最终模型是基学习器的加权线性组合</strong>：对于新样本 <span class="math inline">\(\mathbf{x}\)</span>，最终模型的预测结果为： <span class="math inline">\(H(\mathbf{x}) = \text{sign}(\sum_{t=1}^{T} \alpha_t h_t(\mathbf{x}))\)</span>。 <span class="math inline">\(\text{sign}(z)\)</span> 是符号函数，当 <span class="math inline">\(z &gt; 0\)</span> 时，<span class="math inline">\(\text{sign}(z) = +1\)</span>；当 <span class="math inline">\(z &lt; 0\)</span> 时，<span class="math inline">\(\text{sign}(z) = -1\)</span>；当 <span class="math inline">\(z = 0\)</span> 时，<span class="math inline">\(\text{sign}(z) = 0\)</span> 或 <span class="math inline">\(+1\)</span> 或 <span class="math inline">\(-1\)</span>，通常取 <span class="math inline">\(+1\)</span> 或 <span class="math inline">\(-1\)</span>。</li>
</ol>
<p><strong>梯度提升决策树 (Gradient Boosting Decision Tree, GBDT):</strong></p>
<p>梯度提升决策树 (Gradient Boosting Decision Tree, GBDT) 是一种非常强大且广泛应用的 <strong>Boosting 算法</strong>，以 <strong>决策树 (Decision Tree)</strong> 为基学习器。GBDT 的核心思想是 <strong>梯度提升 (Gradient Boosting)</strong>，也称为 <strong>梯度下降提升 (Gradient Descent Boosting)</strong>。GBDT 使用 <strong>梯度提升算法</strong>，通过<strong>迭代地训练决策树</strong>来<strong>拟合负梯度残差 (Negative Gradient Residuals)</strong>，<strong>逐步逼近真实的目标函数</strong>。GBDT 可以用于<strong>回归和分类问题</strong>，尤其在<strong>非线性</strong>和<strong>复杂</strong>的预测问题中表现出色。GBDT 是许多高级 Boosting 算法（如 XGBoost、LightGBM、CatBoost）的基础。</p>
<p><strong>梯度提升 (Gradient Boosting) 的直观解释:</strong></p>
<p>梯度提升的思想可以理解为<strong>函数空间的梯度下降</strong>。在传统的梯度下降中，我们是在<strong>参数空间</strong>中沿着<strong>负梯度方向</strong>迭代优化参数，以<strong>最小化损失函数</strong>。而在梯度提升中，我们是在<strong>函数空间</strong>中沿着<strong>负梯度方向</strong>迭代优化<strong>模型函数</strong>，以<strong>逼近真实的目标函数</strong>。</p>
<p><strong>GBDT 拟合负梯度残差的原理:</strong></p>
<p>GBDT 的<strong>每一轮迭代</strong>都<strong>训练一个新的决策树</strong>，<strong>目标是拟合当前模型预测结果与真实值之间的残差</strong>。更精确地说，GBDT 拟合的是<strong>损失函数的负梯度</strong>，即<strong>残差的某种形式</strong>。通过<strong>不断地拟合残差</strong>，<strong>GBDT 逐步减小模型的预测误差</strong>，<strong>提高模型的预测精度</strong>。</p>
<p><strong>GBDT 算法流程 (以回归问题为例):</strong></p>
<p>GBDT 算法流程 (以回归问题为例):</p>
<ol type="1">
<li><strong>初始化模型 (Initialize Model)</strong>：<strong>初始化一个弱学习器</strong>（例如，<strong>常数模型 (Constant Model)</strong>，即所有样本的预测值都为一个常数） <span class="math inline">\(f_0(\mathbf{x}) = \text{average}(y_i)\)</span>。 <span class="math inline">\(f_0(\mathbf{x})\)</span> 是初始模型的预测函数，通常使用训练集样本目标变量的均值作为初始预测值。</li>
<li><strong>迭代训练基学习器 (Iterative Training of Base Learners)</strong>：进行 <span class="math inline">\(T\)</span> 轮迭代， <span class="math inline">\(t = 1, 2, ..., T\)</span>：
<ol type="a">
<li><strong>计算负梯度残差 (Compute Negative Gradient Residuals)</strong>：对于<strong>每个样本 <span class="math inline">\(i = 1, 2, ..., N\)</span></strong>，计算<strong>负梯度残差</strong> <span class="math inline">\(r_{ti} = y_i - f_{t-1}(\mathbf{x}_i)\)</span>。 <span class="math inline">\(f_{t-1}(\mathbf{x}_i)\)</span> 是<strong>前一轮迭代得到的模型 <span class="math inline">\(f_{t-1}\)</span> 对样本 <span class="math inline">\(\mathbf{x}_i\)</span> 的预测值</strong>。 <span class="math inline">\(r_{ti}\)</span> 表示<strong>真实值 <span class="math inline">\(y_i\)</span> 与当前模型预测值 <span class="math inline">\(f_{t-1}(\mathbf{x}_i)\)</span> 之间的差异</strong>，即<strong>模型在样本 <span class="math inline">\(\mathbf{x}_i\)</span> 上的预测误差</strong>。负梯度残差 <span class="math inline">\(r_{ti}\)</span> 可以看作是<strong>本轮迭代需要拟合的目标</strong>，即<strong>模型需要在本轮迭代中学习如何纠正前一轮的预测误差</strong>。</li>
<li><strong>训练决策树 (Train Decision Tree)</strong>：使用 <strong><span class="math inline">\((\mathbf{x}_i, r_{ti})_{i=1}^{N}\)</span> 作为训练数据</strong>，<strong>训练一个决策树 <span class="math inline">\(h_t(\mathbf{x})\)</span></strong>，<strong>拟合负梯度残差 <span class="math inline">\(r_{ti}\)</span></strong>。决策树 <span class="math inline">\(h_t(\mathbf{x})\)</span> 的<strong>叶节点区域</strong> <span class="math inline">\(\{R_{tj}\}_{j=1}^{J_t}\)</span> 将特征空间划分为 <span class="math inline">\(J_t\)</span> 个互不相交的区域， <span class="math inline">\(J_t\)</span> 是决策树 <span class="math inline">\(h_t\)</span> 的叶节点数量。</li>
<li><strong>确定叶节点区域的输出值 (Determine Leaf Node Output Values)</strong>：对于决策树 <span class="math inline">\(h_t(\mathbf{x})\)</span> 的<strong>每个叶节点区域 <span class="math inline">\(R_{tj}\)</span></strong>，<strong>计算该区域内样本负梯度残差的平均值</strong> <span class="math inline">\(c_{tj} = \text{average}_{\mathbf{x}_i \in R_{tj}} (r_{ti}) = \frac{1}{|R_{tj}|} \sum_{\mathbf{x}_i \in R_{tj}} r_{ti}\)</span>。 <span class="math inline">\(c_{tj}\)</span> 是<strong>决策树 <span class="math inline">\(h_t\)</span> 在叶节点区域 <span class="math inline">\(R_{tj}\)</span> 上的预测值</strong>，表示<strong>模型在本轮迭代中需要在区域 <span class="math inline">\(R_{tj}\)</span> 内进行的调整量</strong>。</li>
<li><strong>更新模型 (Update Model)</strong>：<strong>更新模型</strong> <span class="math inline">\(f_t(\mathbf{x}) = f_{t-1}(\mathbf{x}) + \alpha c_{tj} I(\mathbf{x} \in R_{tj})\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是 <strong>学习率 (Learning Rate)</strong>，也称为 <strong>shrinkage 参数</strong>，通常取值范围为 <span class="math inline">\((0, 1]\)</span>，例如 0.1、0.01 等。 <span class="math inline">\(\alpha\)</span> 控制<strong>每个基学习器的步长</strong>，<strong>减小学习率可以降低模型对后续基学习器的依赖程度</strong>，<strong>提高模型的泛化能力</strong>，但<strong>需要更多的迭代次数</strong>。 <span class="math inline">\(I(\mathbf{x} \in R_{tj})\)</span> 是<strong>指示函数</strong>，当样本 <span class="math inline">\(\mathbf{x}\)</span> 属于叶节点区域 <span class="math inline">\(R_{tj}\)</span> 时， <span class="math inline">\(I(\mathbf{x} \in R_{tj}) = 1\)</span>，否则 <span class="math inline">\(I(\mathbf{x} \in R_{tj}) = 0\)</span>。 <span class="math inline">\(f_t(\mathbf{x})\)</span> 是<strong>本轮迭代更新后的模型</strong>，它是在前一轮模型 <span class="math inline">\(f_{t-1}(\mathbf{x})\)</span> 的基础上，<strong>加上本轮训练的决策树 <span class="math inline">\(h_t(\mathbf{x})\)</span> 的加权结果</strong>，<strong>逐步逼近真实的目标函数</strong>。</li>
</ol></li>
<li><strong>得到最终模型 (Obtain Final Model)</strong>：经过 <span class="math inline">\(T\)</span> 轮迭代后，<strong>得到最终的 GBDT 模型</strong> <span class="math inline">\(f_T(\mathbf{x}) = f_0(\mathbf{x}) + \sum_{t=1}^{T} \sum_{j=1}^{J_t} \alpha c_{tj} I(\mathbf{x} \in R_{tj}) = f_0(\mathbf{x}) + \sum_{t=1}^{T} \alpha h_t(\mathbf{x})\)</span>。最终模型是<strong>所有基学习器的加权和</strong>。</li>
</ol>
<p><strong>GBDT 算法流程 (以分类问题为例):</strong></p>
<p>GBDT 用于分类问题时，算法流程与回归问题类似，主要区别在于： - <strong>损失函数不同</strong>：回归问题通常使用<strong>平方误差损失函数 (Squared Error Loss)</strong>，分类问题通常使用<strong>对数似然损失函数 (Log-Likelihood Loss)</strong> 或 <strong>指数损失函数 (Exponential Loss)</strong> 等。 - <strong>负梯度计算不同</strong>：不同损失函数的负梯度计算方式不同。例如，对于二分类问题，如果使用对数似然损失函数，则负梯度残差的计算公式与回归问题不同。 - <strong>叶节点区域的输出值确定方式不同</strong>：分类问题中，叶节点区域的输出值通常<strong>不是负梯度残差的平均值</strong>，而是<strong>根据具体的损失函数和优化目标确定</strong>。例如，对于二分类问题，可以使用<strong>对数几率 (Log Odds)</strong> 或 <strong>类别概率</strong> 作为叶节点输出值。</p>
</section>
</section>
<section id="神经网络-neural-network" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="神经网络-neural-network"><span class="header-section-number">5.2</span> 神经网络 (Neural Network)</h2>
<p>神经网络 (Neural Network, NN)，更精确地说是<strong>人工神经网络 (Artificial Neural Network, ANN)</strong>，是一种<strong>模拟生物神经系统结构</strong>的<strong>计算模型</strong>，由<strong>大量相互连接的神经元 (Neuron) 组成</strong>。神经网络可以学习<strong>复杂的非线性关系</strong>，具有强大的<strong>模式识别</strong>、<strong>函数逼近</strong>和<strong>自适应能力</strong>。神经网络在<strong>图像识别</strong>、<strong>自然语言处理</strong>、<strong>语音识别</strong>、<strong>金融预测</strong>等领域取得了巨大成功，是<strong>深度学习 (Deep Learning)</strong> 的基础。</p>
<p><strong>神经网络的核心思想:</strong></p>
<p>神经网络的核心思想是<strong>模拟生物神经系统的信息处理方式</strong>。生物神经系统由大量的神经元相互连接而成，神经元之间通过<strong>电信号和化学信号</strong>传递信息。神经网络试图<strong>抽象和模拟这种信息传递和处理机制</strong>，通过<strong>构建由大量神经元相互连接的网络结构</strong>，来实现<strong>复杂的信息处理和学习任务</strong>。</p>
<p><strong>神经网络的优势:</strong></p>
<ul>
<li><strong>强大的非线性建模能力</strong>：神经网络通过<strong>激活函数的非线性变换</strong>，可以<strong>学习和表示非常复杂的非线性关系</strong>，这是传统线性模型难以实现的。</li>
<li><strong>高度的并行分布式计算能力</strong>：神经网络由<strong>大量的神经元并行工作</strong>，可以<strong>高效地处理大规模数据</strong>和<strong>复杂计算任务</strong>。</li>
<li><strong>良好的泛化能力</strong>：通过<strong>合理的网络结构设计和训练方法</strong>，神经网络可以<strong>学习到数据中的本质规律</strong>，具有<strong>良好的泛化能力</strong>，在<strong>未见过的数据上也能表现良好</strong>。</li>
<li><strong>自适应学习能力</strong>：神经网络可以通过<strong>反向传播算法</strong>等方法，<strong>自动地从数据中学习</strong>，<strong>调整网络参数</strong>，<strong>适应不同的任务和数据</strong>。</li>
</ul>
<section id="神经网络模型及算法" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="神经网络模型及算法"><span class="header-section-number">5.2.1</span> 神经网络模型及算法</h3>
<p><strong>神经元模型 (Neuron Model):</strong></p>
<p>神经元是神经网络的<strong>基本单元</strong>，也称为 <strong>感知机 (Perceptron)</strong>。一个典型的神经元模型包括以下几个主要组成部分：</p>
<ul>
<li><strong>输入 (Input)</strong>：神经元接收来自<strong>其他神经元</strong>或<strong>外部环境</strong>的<strong>输入信号</strong>。输入信号可以是<strong>数值</strong>、<strong>向量</strong>或<strong>张量</strong>等形式。 例如，对于图像识别任务，输入可以是<strong>图像像素的灰度值</strong>；对于自然语言处理任务，输入可以是<strong>词向量</strong>。</li>
<li><strong>权重 (Weight)</strong>：<strong>每个输入连接</strong>都对应一个<strong>权重 <span class="math inline">\(w_{ij}\)</span></strong>，表示<strong>连接的强度</strong>或<strong>重要性</strong>。权重可以是<strong>正数</strong>（兴奋性连接）或<strong>负数</strong>（抑制性连接）。 <strong>权重是神经元学习的关键参数</strong>，通过<strong>调整权重</strong>，神经元可以<strong>选择性地接收和处理不同的输入信号</strong>。</li>
<li><strong>偏置 (Bias)</strong>：神经元还接收一个<strong>偏置 <span class="math inline">\(b_i\)</span></strong>，也称为 <strong>阈值 (Threshold)</strong>。偏置是一个<strong>常数</strong>，用于<strong>调整神经元的激活阈值</strong>，使得神经元更容易或更不容易被激活。 <strong>偏置可以看作是神经元的一个自由度</strong>，使得神经元可以<strong>更加灵活地进行激活</strong>。</li>
<li><strong>加权求和 (Weighted Summation)</strong>：神经元将<strong>所有输入信号</strong>与<strong>对应的权重</strong>进行<strong>加权求和</strong>，再加上<strong>偏置</strong>，得到<strong>神经元的净输入 (Net Input)</strong> <span class="math inline">\(z_i = \sum_{j} w_{ij} x_j + b_i\)</span>。 <strong>净输入 <span class="math inline">\(z_i\)</span> 表示神经元接收到的所有输入信号的综合强度</strong>。</li>
<li><strong>激活函数 (Activation Function)</strong>：神经元对<strong>净输入 <span class="math inline">\(z_i\)</span></strong> 进行<strong>非线性变换</strong>，通过<strong>激活函数 <span class="math inline">\(\sigma(\cdot)\)</span></strong> 产生<strong>神经元的输出 (Output)</strong> <span class="math inline">\(a_i = \sigma(z_i) = \sigma(\sum_{j} w_{ij} x_j + b_i)\)</span>。<strong>激活函数</strong>是神经网络<strong>实现非线性建模的关键</strong>。 <strong>激活函数引入了非线性因素</strong>，使得神经网络可以<strong>逼近任意复杂的非线性函数</strong>。常用的激活函数包括：
<ul>
<li><strong>Sigmoid 函数</strong>：<span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>，将输入值映射到 <span class="math inline">\((0, 1)\)</span> 区间，常用于<strong>二分类问题</strong>的输出层。 Sigmoid 函数的输出值可以<strong>解释为概率</strong>，例如样本属于正类的概率。</li>
<li><strong>Tanh 函数 (Hyperbolic Tangent Function)</strong>：<span class="math inline">\(\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1\)</span>，将输入值映射到 <span class="math inline">\((-1, 1)\)</span> 区间，与 Sigmoid 函数类似，但输出范围不同。 Tanh 函数的输出值<strong>以 0 为中心</strong>，可能在某些情况下<strong>比 Sigmoid 函数更易于训练</strong>。</li>
<li><strong>ReLU 函数 (Rectified Linear Unit)</strong>：<span class="math inline">\(\text{ReLU}(z) = \max(0, z)\)</span>，当输入值大于 0 时，输出值等于输入值；当输入值小于等于 0 时，输出值为 0。ReLU 函数<strong>计算简单</strong>，<strong>收敛速度快</strong>，是<strong>深度神经网络中最常用的激活函数之一</strong>。 ReLU 函数在<strong>正区间是线性函数</strong>，<strong>负区间是常数 0</strong>，这种<strong>简单的非线性</strong>使其在<strong>深度网络中表现出色</strong>。</li>
<li><strong>Leaky ReLU 函数 (Leaky Rectified Linear Unit)</strong>：<span class="math inline">\(\text{Leaky ReLU}(z) = \begin{cases} z, &amp; \text{if } z &gt; 0 \\ \alpha z, &amp; \text{if } z \le 0 \end{cases}\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是一个<strong>很小的常数</strong>，例如 0.01。Leaky ReLU 函数<strong>解决了 ReLU 函数在输入值为负数时输出值为 0 导致神经元 “死亡” 的问题</strong>。 Leaky ReLU 函数在<strong>负区间也保持一定的梯度</strong>，有助于<strong>信息在网络中更好地传播</strong>。</li>
<li><strong>ELU 函数 (Exponential Linear Unit)</strong>：<span class="math inline">\(\text{ELU}(z) = \begin{cases} z, &amp; \text{if } z &gt; 0 \\ \alpha (e^z - 1), &amp; \text{if } z \le 0 \end{cases}\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是一个<strong>正的常数</strong>。ELU 函数<strong>具有 ReLU 函数的优点</strong>，同时<strong>在输入值为负数时输出值也具有一定的梯度</strong>，可以<strong>加速神经网络的收敛</strong>。 ELU 函数在<strong>负区间使用指数函数</strong>，可以<strong>提供更平滑的输出</strong>，并<strong>有助于网络的鲁棒性</strong>。</li>
<li><strong>Softmax 函数</strong>：<span class="math inline">\(\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}\)</span>，用于<strong>多分类问题</strong>的输出层。Softmax 函数将<strong>一组输入值</strong>映射为<strong>一组概率值</strong>，<strong>每个概率值都在 <span class="math inline">\((0, 1)\)</span> 区间</strong>，<strong>且所有概率值之和为 1</strong>。 Softmax 函数可以将<strong>神经元的输出转换为类别概率分布</strong>，方便进行<strong>多分类任务</strong>。</li>
</ul></li>
<li><strong>输出 (Output)</strong>：神经元的<strong>输出信号 <span class="math inline">\(a_i\)</span></strong>，可以作为<strong>其他神经元的输入</strong>，或作为<strong>整个神经网络的输出</strong>。 <strong>神经元的输出是经过激活函数处理后的结果</strong>，可以<strong>传递给下一层神经元</strong>，或者<strong>作为最终的预测结果</strong>。</li>
</ul>
<p><strong>多层神经网络 (Multilayer Neural Network, MLNN) / 多层感知机 (Multilayer Perceptron, MLP):</strong></p>
<p>多层神经网络 (Multilayer Neural Network, MLNN) 或 多层感知机 (Multilayer Perceptron, MLP) 是由<strong>多个神经元层</strong>相互连接而成的神经网络。多层神经网络通常包括以下几种类型的层：</p>
<ul>
<li><strong>输入层 (Input Layer)</strong>：<strong>接收外部输入数据</strong>。输入层的神经元<strong>不进行任何计算</strong>，只是<strong>将输入数据传递给下一层</strong>。输入层神经元的数量<strong>等于输入特征的维度</strong>。 例如，如果输入是 28x28 的图像，则输入层通常有 784 个神经元（将图像展平成向量）。</li>
<li><strong>隐藏层 (Hidden Layer)</strong>：<strong>位于输入层和输出层之间</strong>，可以有<strong>一层或多层</strong>。隐藏层是神经网络的<strong>核心部分</strong>，负责<strong>提取输入数据的特征</strong>，<strong>学习输入数据中的复杂模式</strong>。隐藏层神经元的数量和层数是神经网络的<strong>超参数</strong>，需要根据具体问题进行设计和调整。 <strong>隐藏层越多，网络可以学习到的特征就越抽象、越复杂</strong>。</li>
<li><strong>输出层 (Output Layer)</strong>：<strong>产生最终的输出结果</strong>。输出层神经元的数量<strong>取决于具体的任务类型</strong>。例如，<strong>二分类问题</strong>的输出层通常<strong>只有一个神经元</strong>，使用 <strong>Sigmoid 激活函数</strong>，输出样本属于正类的概率；<strong>多分类问题</strong>的输出层通常<strong>有 <span class="math inline">\(C\)</span> 个神经元</strong>（<span class="math inline">\(C\)</span> 是类别数量），使用 <strong>Softmax 激活函数</strong>，输出样本属于每个类别的概率；<strong>回归问题</strong>的输出层通常<strong>只有一个神经元</strong>，<strong>不使用激活函数</strong>或使用 <strong>线性激活函数</strong>，直接输出预测值。</li>
</ul>
<p><strong>深度神经网络 (Deep Neural Network, DNN)</strong> 指的是<strong>具有多个隐藏层的神经网络</strong>。 “深度” 指的是<strong>网络结构的深度</strong>，即<strong>隐藏层的层数</strong>。 深度神经网络可以<strong>学习到更加抽象和层次化的特征表示</strong>，从而<strong>更好地处理复杂的数据和任务</strong>。</p>
<p><strong>全连接神经网络 (Fully Connected Neural Network, FCNN) / 稠密神经网络 (Dense Neural Network):</strong></p>
<p>在多层神经网络中，<strong>相邻层之间</strong>的神经元通常采用 <strong>全连接 (Fully Connected)</strong> 的方式进行连接，即<strong>前一层的每个神经元都与后一层的所有神经元连接</strong>。这种连接方式的神经网络称为 <strong>全连接神经网络 (Fully Connected Neural Network, FCNN)</strong> 或 <strong>稠密神经网络 (Dense Neural Network)</strong>。全连接神经网络是<strong>最基本的神经网络结构</strong>，也是许多复杂神经网络的基础。</p>
<p><strong>前向传播 (Forward Propagation):</strong></p>
<p>前向传播 (Forward Propagation) 是指<strong>输入信号从输入层经过隐藏层逐层传递到输出层的过程</strong>。在前向传播过程中，<strong>每一层的神经元接收来自前一层的输出</strong>，进行<strong>加权求和</strong>和<strong>激活函数处理</strong>，然后将<strong>输出传递给下一层</strong>。通过逐层传递和计算，最终得到<strong>输出层的输出</strong>，即<strong>神经网络的预测结果</strong>。</p>
<p><strong>前向传播的计算步骤:</strong></p>
<ol type="1">
<li><strong>输入层</strong>：将<strong>输入数据</strong>输入到输入层神经元。</li>
<li><strong>隐藏层</strong>：对于<strong>每个隐藏层</strong>，<strong>依次计算每个神经元的输出</strong>。计算过程包括：
<ul>
<li><strong>加权求和</strong>：将<strong>前一层所有神经元的输出</strong>与<strong>连接权重</strong>相乘，并<strong>加上偏置</strong>，得到<strong>净输入</strong>。</li>
<li><strong>激活函数</strong>：将<strong>净输入</strong>通过<strong>激活函数</strong>进行<strong>非线性变换</strong>，得到<strong>神经元的输出</strong>。</li>
</ul></li>
<li><strong>输出层</strong>：<strong>计算输出层每个神经元的输出</strong>，计算过程与隐藏层类似。<strong>输出层神经元的输出</strong>即为<strong>神经网络的最终预测结果</strong>。</li>
</ol>
<p><strong>反向传播算法 (Backpropagation, BP):</strong></p>
<p>反向传播算法 (Backpropagation, BP) 是<strong>训练多层神经网络最常用和最核心的算法</strong>。BP 算法是一种<strong>基于梯度下降法 (Gradient Descent)</strong> 的<strong>误差反向传播算法</strong>，通过<strong>计算损失函数 (Loss Function) 对网络参数（权重和偏置）的梯度</strong>，然后<strong>沿着梯度反方向更新参数</strong>，<strong>迭代地最小化损失函数</strong>，从而<strong>学习到最优的网络参数</strong>。BP 算法实现了<strong>误差信号从输出层反向传播到输入层</strong>，<strong>逐层调整网络参数</strong>，使得神经网络能够<strong>学习到输入数据中的复杂模式</strong>。</p>
<p><strong>梯度下降法 (Gradient Descent) 的直观解释:</strong></p>
<p>梯度下降法就像是<strong>在山坡上寻找山谷最低点</strong>的过程。</p>
<ol type="1">
<li><strong>确定当前位置</strong>（对应于<strong>当前的参数值</strong>）。</li>
<li><strong>沿着当前位置最陡峭的方向（负梯度方向）</strong>向下走一步（<strong>更新参数</strong>）。</li>
<li><strong>重复步骤 2</strong>，直到<strong>到达山谷最低点</strong>（<strong>损失函数达到最小值</strong>）。</li>
</ol>
<p><strong>反向传播算法的步骤:</strong></p>
<ol type="1">
<li><strong>前向传播 (Forward Propagation)</strong>：<strong>给定输入样本 <span class="math inline">\((\mathbf{x}_i, y_i)\)</span></strong>，<strong>计算每个样本的前向传播输出</strong>，<strong>得到神经网络的预测值 <span class="math inline">\(\hat{y}_i\)</span></strong>。前向传播过程是从输入层开始，逐层计算每个神经元的输出，直到输出层。</li>
<li><strong>计算损失 (Compute Loss)</strong>：<strong>根据预测值 <span class="math inline">\(\hat{y}_i\)</span> 和真实值 <span class="math inline">\(y_i\)</span></strong>，<strong>计算损失函数值 <span class="math inline">\(L_i = L(\hat{y}_i, y_i)\)</span></strong>。<strong>损失函数</strong>衡量了<strong>模型预测结果与真实值之间的差异</strong>。常用的损失函数包括：
<ul>
<li><strong>均方误差损失函数 (Mean Squared Error Loss, MSE)</strong>：<span class="math inline">\(L_{MSE}(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2\)</span>，常用于<strong>回归问题</strong>。</li>
<li><strong>交叉熵损失函数 (Cross-Entropy Loss Function)</strong>：<span class="math inline">\(L_{CE}(\hat{y}, y) = - [y \ln \hat{y} + (1-y) \ln (1-\hat{y})]\)</span>，常用于<strong>二分类问题</strong>。对于<strong>多分类问题</strong>，可以使用 <strong>多类交叉熵损失函数 (Categorical Cross-Entropy Loss)</strong>。</li>
</ul></li>
<li><strong>反向传播误差 (Backpropagate Error)</strong>：<strong>从输出层开始</strong>，<strong>反向计算每一层的误差项 (Error Term) <span class="math inline">\(\delta_l\)</span></strong>。<strong>误差项 <span class="math inline">\(\delta_l\)</span> 反映了第 <span class="math inline">\(l\)</span> 层神经元的输出对最终损失的影响程度</strong>，是<strong>损失函数关于第 <span class="math inline">\(l\)</span> 层神经元净输入 <span class="math inline">\(z_l\)</span> 的梯度</strong>。误差项的计算公式为：
<ul>
<li><strong>输出层误差项</strong>：<span class="math inline">\(\delta_{output} = \frac{\partial L}{\partial z_{output}} = \frac{\partial L}{\partial a_{output}} \odot \sigma'_{output}(z_{output})\)</span>，其中 <span class="math inline">\(\odot\)</span> 表示<strong>逐元素乘积 (Element-wise Product)</strong>，<span class="math inline">\(\sigma'_{output}(z_{output})\)</span> 是输出层激活函数 <span class="math inline">\(\sigma_{output}\)</span> 对净输入 <span class="math inline">\(z_{output}\)</span> 的导数。</li>
<li><strong>隐藏层误差项</strong>：<span class="math inline">\(\delta_l = \frac{\partial L}{\partial z_l} = (\mathbf{W}_{l+1}^T \delta_{l+1}) \odot \sigma'_{l}(z_{l})\)</span>，其中 <span class="math inline">\(\mathbf{W}_{l+1}\)</span> 是第 <span class="math inline">\(l\)</span> 层到第 <span class="math inline">\(l+1\)</span> 层的<strong>权重矩阵</strong>，<span class="math inline">\(\delta_{l+1}\)</span> 是第 <span class="math inline">\(l+1\)</span> 层的误差项，<span class="math inline">\(\sigma'_{l}(z_{l})\)</span> 是第 <span class="math inline">\(l\)</span> 层激活函数 <span class="math inline">\(\sigma_{l}\)</span> 对净输入 <span class="math inline">\(z_{l}\)</span> 的导数。误差项的计算是<strong>从输出层向输入层逐层反向传播</strong>的。</li>
</ul></li>
<li><strong>计算梯度 (Compute Gradients)</strong>：<strong>根据误差项 <span class="math inline">\(\delta_l\)</span></strong>，<strong>计算损失函数关于每一层权重 <span class="math inline">\(\mathbf{W}_l\)</span> 和偏置 <span class="math inline">\(\mathbf{b}_l\)</span> 的梯度</strong>。梯度的计算公式为：
<ul>
<li><strong>权重梯度</strong>：<span class="math inline">\(\frac{\partial L}{\partial \mathbf{W}_l} = \delta_l \mathbf{a}_{l-1}^T\)</span>，其中 <span class="math inline">\(\mathbf{a}_{l-1}\)</span> 是第 <span class="math inline">\(l-1\)</span> 层的输出（对于输入层，<span class="math inline">\(\mathbf{a}_0 = \mathbf{x}\)</span>）。</li>
<li><strong>偏置梯度</strong>：<span class="math inline">\(\frac{\partial L}{\partial \mathbf{b}_l} = \delta_l\)</span>。</li>
</ul></li>
<li><strong>更新参数 (Update Parameters)</strong>：<strong>沿着梯度反方向更新权重 <span class="math inline">\(\mathbf{W}_l\)</span> 和偏置 <span class="math inline">\(\mathbf{b}_l\)</span></strong>，<strong>最小化损失函数</strong>。参数更新公式为：
<ul>
<li><strong>权重更新</strong>：<span class="math inline">\(\mathbf{W}_l = \mathbf{W}_l - \alpha \frac{\partial L}{\partial \mathbf{W}_l} = \mathbf{W}_l - \alpha \delta_l \mathbf{a}_{l-1}^T\)</span></li>
<li><strong>偏置更新</strong>：<span class="math inline">\(\mathbf{b}_l = \mathbf{b}_l - \alpha \frac{\partial L}{\partial \mathbf{b}_l} = \mathbf{b}_l - \alpha \delta_l\)</span> 其中 <span class="math inline">\(\alpha\)</span> 是 <strong>学习率 (Learning Rate)</strong>，控制参数更新的步长。</li>
</ul></li>
<li><strong>迭代 (Iteration)</strong>：<strong>重复步骤 1-5</strong>，<strong>遍历所有训练样本</strong>（或<strong>一个批次的样本</strong>），<strong>进行多轮迭代 (Epoch)</strong>，直到<strong>损失函数收敛</strong>或<strong>达到预设的迭代次数</strong>。</li>
</ol>
</section>
<section id="深度学习-deep-learning" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="深度学习-deep-learning"><span class="header-section-number">5.2.2</span> 深度学习 (Deep Learning)</h3>
<p><strong>深度学习 (Deep Learning)</strong> 是<strong>机器学习的一个分支</strong>，<strong>本质上就是具有多层隐藏层的神经网络</strong>。 深度学习通过<strong>构建深层神经网络</strong>，<strong>学习数据中更加抽象和复杂的特征表示</strong>，从而<strong>解决更加复杂的任务</strong>。 深度学习在<strong>图像识别</strong>、<strong>自然语言处理</strong>、<strong>语音识别</strong>等领域取得了突破性进展，<strong>推动了人工智能的快速发展</strong>。</p>
<p><strong>深度学习的关键要素:</strong></p>
<ul>
<li><strong>深层神经网络结构</strong>：深度学习模型通常具有<strong>多个隐藏层</strong>，可以<strong>学习到多层次的特征表示</strong>。</li>
<li><strong>大规模数据集</strong>：深度学习模型通常需要<strong>大规模的数据集进行训练</strong>，才能<strong>充分发挥其性能</strong>。</li>
<li><strong>强大的计算能力</strong>：深度学习模型的训练通常需要<strong>大量的计算资源</strong>，例如 <strong>GPU (图形处理器)</strong>。</li>
<li><strong>高效的优化算法</strong>：深度学习模型的训练需要<strong>高效的优化算法</strong>，例如 <strong>Adam</strong>、<strong>SGD with Momentum</strong> 等。</li>
</ul>
<p><strong>神经网络的变体模型 (Variations of Neural Networks):</strong></p>
<p>随着深度学习的发展，研究者们提出了各种各样的神经网络变体模型，以适应不同的任务和数据类型。以下是一些常见的神经网络变体模型：</p>
<ul>
<li><strong>卷积神经网络 (Convolutional Neural Network, CNN)</strong>：
<ul>
<li><strong>特点</strong>：CNN 是一种<strong>专门用于处理图像数据</strong>的神经网络。CNN 的核心组件是<strong>卷积层 (Convolutional Layer)</strong> 和 <strong>池化层 (Pooling Layer)</strong>。<strong>卷积层</strong>可以<strong>自动学习图像中的局部特征</strong>，例如边缘、纹理等。<strong>池化层</strong>可以<strong>降低特征图的维度</strong>，<strong>减少计算量</strong>，并<strong>提高模型的鲁棒性</strong>。</li>
<li><strong>应用场景</strong>：图像分类、目标检测、图像分割、人脸识别等<strong>计算机视觉任务</strong>。</li>
</ul></li>
<li><strong>循环神经网络 (Recurrent Neural Network, RNN)</strong>：
<ul>
<li><strong>特点</strong>：RNN 是一种<strong>专门用于处理序列数据</strong>的神经网络。RNN 的特点是<strong>具有循环连接</strong>，使得网络可以<strong>记忆之前的输入信息</strong>，并<strong>应用于当前的输出</strong>。</li>
<li><strong>应用场景</strong>：自然语言处理 (如文本分类、机器翻译、文本生成)、语音识别、时间序列预测等<strong>序列数据处理任务</strong>。</li>
<li><strong>常见变体</strong>：<strong>长短期记忆网络 (Long Short-Term Memory Network, LSTM)</strong> 和 <strong>门控循环单元网络 (Gated Recurrent Unit Network, GRU)</strong>，它们<strong>解决了传统 RNN 在长序列数据中容易出现的梯度消失和梯度爆炸问题</strong>。</li>
</ul></li>
<li><strong>Transformer 网络</strong>:
<ul>
<li><strong>特点</strong>：Transformer 是一种<strong>基于自注意力机制 (Self-Attention Mechanism)</strong> 的神经网络结构。Transformer <strong>摒弃了传统的 RNN 结构</strong>，<strong>完全依赖自注意力机制来捕捉输入序列中不同位置之间的关系</strong>。Transformer 具有<strong>并行计算能力强</strong>、<strong>可以捕捉长距离依赖关系</strong>等优点。</li>
<li><strong>应用场景</strong>：自然语言处理 (如机器翻译、文本摘要、问答系统)、图像识别、语音识别等<strong>各种序列数据处理任务</strong>。</li>
<li><strong>重要模型</strong>：<strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>、<strong>GPT (Generative Pre-trained Transformer)</strong> 等<strong>预训练语言模型</strong>，在自然语言处理领域取得了革命性的进展。</li>
</ul></li>
<li><strong>图神经网络 (Graph Neural Network, GNN)</strong>：
<ul>
<li><strong>特点</strong>：GNN 是一种<strong>专门用于处理图结构数据</strong>的神经网络。GNN 可以<strong>学习图中节点和边的特征表示</strong>，并<strong>进行节点分类、链接预测、图分类等任务</strong>。</li>
<li><strong>应用场景</strong>：社交网络分析、知识图谱、推荐系统、生物信息学等<strong>图结构数据分析任务</strong>。</li>
</ul></li>
<li><strong>生成对抗网络 (Generative Adversarial Network, GAN)</strong>：
<ul>
<li><strong>特点</strong>：GAN 是一种<strong>生成模型</strong>，由<strong>生成器 (Generator)</strong> 和 <strong>判别器 (Discriminator)</strong> 两个神经网络组成。<strong>生成器</strong>负责<strong>生成假数据</strong>，<strong>判别器</strong>负责<strong>区分真假数据</strong>。<strong>两个网络相互对抗训练</strong>，<strong>最终生成器可以生成逼真的数据</strong>。</li>
<li><strong>应用场景</strong>：图像生成、图像编辑、数据增强、风格迁移等<strong>生成式任务</strong>。</li>
</ul></li>
<li><strong>自编码器 (Autoencoder, AE)</strong>：
<ul>
<li><strong>特点</strong>：自编码器 是一种<strong>无监督学习模型</strong>，用于<strong>学习数据的低维表示 (特征)</strong>。自编码器由<strong>编码器 (Encoder)</strong> 和 <strong>解码器 (Decoder)</strong> 两个神经网络组成。<strong>编码器</strong>将输入数据<strong>压缩到低维空间</strong>，<strong>解码器</strong>将低维表示<strong>重构回原始数据</strong>。</li>
<li><strong>应用场景</strong>：特征提取、降维、数据去噪、异常检测等<strong>无监督学习任务</strong>。</li>
<li><strong>常见变体</strong>：<strong>变分自编码器 (Variational Autoencoder, VAE)</strong>、<strong>稀疏自编码器 (Sparse Autoencoder)</strong>、<strong>降噪自编码器 (Denoising Autoencoder)</strong> 等。</li>
</ul></li>
</ul>
<p>总而言之，神经网络和深度学习是当前机器学习领域最热门和最重要的方向之一。 掌握神经网络的基本原理和常用模型，对于理解和应用人工智能技术至关重要。</p>
</section>
</section>
<section id="总结" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="总结"><span class="header-section-number">5.3</span> 总结</h2>
<p>本讲义深入探讨了集成学习和神经网络模型，主要内容包括：</p>
<ul>
<li><strong>集成学习</strong>: 详细介绍了 Bagging 和 Boosting 两种主要的集成学习方法，以及随机森林和 AdaBoost 算法的原理和特点。</li>
<li><strong>神经网络</strong>: 系统讲解了神经网络的基本结构、前向传播、反向传播算法以及深度学习的概念。</li>
<li><strong>深度学习变体模型</strong>: 简要介绍了卷积神经网络 (CNN)、循环神经网络 (RNN)、Transformer 网络、图神经网络 (GNN) 和生成对抗网络 (GAN) 等深度学习模型的特点和应用场景。</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03_supervised.html" class="pagination-link" aria-label="监督学习（上）">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05_model_assess.html" class="pagination-link" aria-label="模型评估与优化">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>