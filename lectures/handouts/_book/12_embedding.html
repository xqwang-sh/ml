<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; 文本分析2：词向量与深度学习基础 – 数据挖掘与机器学习课程讲义</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13_llm.html" rel="next">
<link href="./11_nlp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ed6da6eef3892af8a4b5ed59bfb951f5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./12_embedding.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析2：词向量与深度学习基础</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">数据挖掘与机器学习课程讲义</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">前言</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_ml_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">机器学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab02_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_model_assess.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_ts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">时间序列监督学习</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab03_titanic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">泰坦尼克号生存预测实践</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_credit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">信用评分理论基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">非监督学习技术概览及其金融应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">非监督学习：聚类 (Clustering)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">非监督学习：降维 (Dimensionality Reduction)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">文本分析1：词频法与向量空间</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_embedding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析2：词向量与深度学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析3：大语言模型及其应用</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project1_LC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2a_tspred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">项目2A：上证综指收益率时间序列预测</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2b_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">项目2B：财务报表文本分析与企业风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">《数据挖掘与机器学习》期末考试复习指南</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#从稀疏到密集表示" id="toc-从稀疏到密集表示" class="nav-link active" data-scroll-target="#从稀疏到密集表示"><span class="header-section-number">15</span> 从稀疏到密集表示</a>
  <ul class="collapse">
  <li><a href="#bag-of-words模型的局限性" id="toc-bag-of-words模型的局限性" class="nav-link" data-scroll-target="#bag-of-words模型的局限性"><span class="header-section-number">15.1</span> Bag of Words模型的局限性</a></li>
  <li><a href="#词向量的直觉理解" id="toc-词向量的直觉理解" class="nav-link" data-scroll-target="#词向量的直觉理解"><span class="header-section-number">15.2</span> 词向量的直觉理解</a></li>
  <li><a href="#分布式假设词向量的理论基础" id="toc-分布式假设词向量的理论基础" class="nav-link" data-scroll-target="#分布式假设词向量的理论基础"><span class="header-section-number">15.3</span> 分布式假设：词向量的理论基础</a></li>
  <li><a href="#密集表示的数学性质" id="toc-密集表示的数学性质" class="nav-link" data-scroll-target="#密集表示的数学性质"><span class="header-section-number">15.4</span> 密集表示的数学性质</a></li>
  </ul></li>
  <li><a href="#word2vec原理讲解" id="toc-word2vec原理讲解" class="nav-link" data-scroll-target="#word2vec原理讲解"><span class="header-section-number">16</span> Word2Vec原理讲解</a>
  <ul class="collapse">
  <li><a href="#word2vec简介" id="toc-word2vec简介" class="nav-link" data-scroll-target="#word2vec简介"><span class="header-section-number">16.1</span> Word2Vec简介</a></li>
  <li><a href="#skip-gram模型详解" id="toc-skip-gram模型详解" class="nav-link" data-scroll-target="#skip-gram模型详解"><span class="header-section-number">16.2</span> Skip-gram模型详解</a>
  <ul class="collapse">
  <li><a href="#模型结构" id="toc-模型结构" class="nav-link" data-scroll-target="#模型结构"><span class="header-section-number">16.2.1</span> 模型结构</a></li>
  <li><a href="#训练过程" id="toc-训练过程" class="nav-link" data-scroll-target="#训练过程"><span class="header-section-number">16.2.2</span> 训练过程</a></li>
  </ul></li>
  <li><a href="#cbow模型详解" id="toc-cbow模型详解" class="nav-link" data-scroll-target="#cbow模型详解"><span class="header-section-number">16.3</span> CBOW模型详解</a>
  <ul class="collapse">
  <li><a href="#模型结构-1" id="toc-模型结构-1" class="nav-link" data-scroll-target="#模型结构-1"><span class="header-section-number">16.3.1</span> 模型结构</a></li>
  <li><a href="#skip-gram与cbow对比" id="toc-skip-gram与cbow对比" class="nav-link" data-scroll-target="#skip-gram与cbow对比"><span class="header-section-number">16.3.2</span> Skip-gram与CBOW对比</a></li>
  </ul></li>
  <li><a href="#负采样negative-sampling技术" id="toc-负采样negative-sampling技术" class="nav-link" data-scroll-target="#负采样negative-sampling技术"><span class="header-section-number">16.4</span> 负采样（Negative Sampling）技术</a>
  <ul class="collapse">
  <li><a href="#负采样原理" id="toc-负采样原理" class="nav-link" data-scroll-target="#负采样原理"><span class="header-section-number">16.4.1</span> 负采样原理</a></li>
  <li><a href="#负采样的优势" id="toc-负采样的优势" class="nav-link" data-scroll-target="#负采样的优势"><span class="header-section-number">16.4.2</span> 负采样的优势</a></li>
  </ul></li>
  <li><a href="#词向量空间的语义特性" id="toc-词向量空间的语义特性" class="nav-link" data-scroll-target="#词向量空间的语义特性"><span class="header-section-number">16.5</span> 词向量空间的语义特性</a>
  <ul class="collapse">
  <li><a href="#语义相关性" id="toc-语义相关性" class="nav-link" data-scroll-target="#语义相关性"><span class="header-section-number">16.5.1</span> 语义相关性</a></li>
  <li><a href="#语义计算" id="toc-语义计算" class="nav-link" data-scroll-target="#语义计算"><span class="header-section-number">16.5.2</span> 语义计算</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#金融文本中应用word2vec" id="toc-金融文本中应用word2vec" class="nav-link" data-scroll-target="#金融文本中应用word2vec"><span class="header-section-number">17</span> 金融文本中应用Word2Vec</a>
  <ul class="collapse">
  <li><a href="#金融领域的词向量应用" id="toc-金融领域的词向量应用" class="nav-link" data-scroll-target="#金融领域的词向量应用"><span class="header-section-number">17.1</span> 金融领域的词向量应用</a></li>
  <li><a href="#训练金融领域词向量" id="toc-训练金融领域词向量" class="nav-link" data-scroll-target="#训练金融领域词向量"><span class="header-section-number">17.2</span> 训练金融领域词向量</a></li>
  </ul></li>
  <li><a href="#预训练词向量模型比较与应用" id="toc-预训练词向量模型比较与应用" class="nav-link" data-scroll-target="#预训练词向量模型比较与应用"><span class="header-section-number">18</span> 预训练词向量模型比较与应用</a>
  <ul class="collapse">
  <li><a href="#中文预训练词向量模型" id="toc-中文预训练词向量模型" class="nav-link" data-scroll-target="#中文预训练词向量模型"><span class="header-section-number">18.1</span> 中文预训练词向量模型</a>
  <ul class="collapse">
  <li><a href="#腾讯ai-lab词向量chinese-word-vectors" id="toc-腾讯ai-lab词向量chinese-word-vectors" class="nav-link" data-scroll-target="#腾讯ai-lab词向量chinese-word-vectors"><span class="header-section-number">18.1.1</span> 1. 腾讯AI Lab词向量（Chinese Word Vectors）</a></li>
  <li><a href="#哈工大讯飞联合实验室词向量hit-scir-chinese-word-vectors" id="toc-哈工大讯飞联合实验室词向量hit-scir-chinese-word-vectors" class="nav-link" data-scroll-target="#哈工大讯飞联合实验室词向量hit-scir-chinese-word-vectors"><span class="header-section-number">18.1.2</span> 2. 哈工大/讯飞联合实验室词向量（HIT-SCIR Chinese Word Vectors）</a></li>
  <li><a href="#北师大人大学者开发的词向量chinese-word-vectors" id="toc-北师大人大学者开发的词向量chinese-word-vectors" class="nav-link" data-scroll-target="#北师大人大学者开发的词向量chinese-word-vectors"><span class="header-section-number">18.1.3</span> 3. 北师大/人大学者开发的词向量（Chinese Word Vectors）</a></li>
  </ul></li>
  <li><a href="#英文预训练词向量模型" id="toc-英文预训练词向量模型" class="nav-link" data-scroll-target="#英文预训练词向量模型"><span class="header-section-number">18.2</span> 英文预训练词向量模型</a>
  <ul class="collapse">
  <li><a href="#word2vec词向量" id="toc-word2vec词向量" class="nav-link" data-scroll-target="#word2vec词向量"><span class="header-section-number">18.2.1</span> 1. Word2Vec词向量</a></li>
  <li><a href="#gloveglobal-vectors-for-word-representation" id="toc-gloveglobal-vectors-for-word-representation" class="nav-link" data-scroll-target="#gloveglobal-vectors-for-word-representation"><span class="header-section-number">18.2.2</span> 2. GloVe（Global Vectors for Word Representation）</a></li>
  <li><a href="#fasttext" id="toc-fasttext" class="nav-link" data-scroll-target="#fasttext"><span class="header-section-number">18.2.3</span> 3. FastText</a></li>
  </ul></li>
  <li><a href="#预训练模型的表现比较" id="toc-预训练模型的表现比较" class="nav-link" data-scroll-target="#预训练模型的表现比较"><span class="header-section-number">18.3</span> 预训练模型的表现比较</a>
  <ul class="collapse">
  <li><a href="#语义捕捉能力" id="toc-语义捕捉能力" class="nav-link" data-scroll-target="#语义捕捉能力"><span class="header-section-number">18.3.1</span> 1. 语义捕捉能力</a></li>
  <li><a href="#领域适应性" id="toc-领域适应性" class="nav-link" data-scroll-target="#领域适应性"><span class="header-section-number">18.3.2</span> 2. 领域适应性</a></li>
  <li><a href="#处理未登录词能力" id="toc-处理未登录词能力" class="nav-link" data-scroll-target="#处理未登录词能力"><span class="header-section-number">18.3.3</span> 3. 处理未登录词能力</a></li>
  <li><a href="#多语言词向量对齐" id="toc-多语言词向量对齐" class="nav-link" data-scroll-target="#多语言词向量对齐"><span class="header-section-number">18.3.4</span> 4. 多语言词向量对齐</a></li>
  </ul></li>
  <li><a href="#预训练模型在不同任务中的选择指南" id="toc-预训练模型在不同任务中的选择指南" class="nav-link" data-scroll-target="#预训练模型在不同任务中的选择指南"><span class="header-section-number">18.4</span> 预训练模型在不同任务中的选择指南</a></li>
  <li><a href="#词向量在实际应用中的评估" id="toc-词向量在实际应用中的评估" class="nav-link" data-scroll-target="#词向量在实际应用中的评估"><span class="header-section-number">18.5</span> 词向量在实际应用中的评估</a>
  <ul class="collapse">
  <li><a href="#内在评估intrinsic-evaluation" id="toc-内在评估intrinsic-evaluation" class="nav-link" data-scroll-target="#内在评估intrinsic-evaluation"><span class="header-section-number">18.5.1</span> 内在评估(Intrinsic Evaluation)</a></li>
  <li><a href="#外在评估extrinsic-evaluation" id="toc-外在评估extrinsic-evaluation" class="nav-link" data-scroll-target="#外在评估extrinsic-evaluation"><span class="header-section-number">18.5.2</span> 外在评估(Extrinsic Evaluation)</a></li>
  <li><a href="#金融领域案例分析" id="toc-金融领域案例分析" class="nav-link" data-scroll-target="#金融领域案例分析"><span class="header-section-number">18.5.3</span> 金融领域案例分析</a></li>
  </ul></li>
  <li><a href="#选择或训练自己的词向量模型" id="toc-选择或训练自己的词向量模型" class="nav-link" data-scroll-target="#选择或训练自己的词向量模型"><span class="header-section-number">18.6</span> 选择或训练自己的词向量模型</a></li>
  <li><a href="#结论" id="toc-结论" class="nav-link" data-scroll-target="#结论"><span class="header-section-number">18.7</span> 结论</a></li>
  </ul></li>
  <li><a href="#案例比较词频法与词向量在政府工作报告分析中的差异" id="toc-案例比较词频法与词向量在政府工作报告分析中的差异" class="nav-link" data-scroll-target="#案例比较词频法与词向量在政府工作报告分析中的差异"><span class="header-section-number">19</span> 案例比较：词频法与词向量在政府工作报告分析中的差异</a>
  <ul class="collapse">
  <li><a href="#相同文本的两种表示方法" id="toc-相同文本的两种表示方法" class="nav-link" data-scroll-target="#相同文本的两种表示方法"><span class="header-section-number">19.1</span> 相同文本的两种表示方法</a></li>
  <li><a href="#关键词提取比较" id="toc-关键词提取比较" class="nav-link" data-scroll-target="#关键词提取比较"><span class="header-section-number">19.2</span> 关键词提取比较</a></li>
  <li><a href="#语义关联发现比较" id="toc-语义关联发现比较" class="nav-link" data-scroll-target="#语义关联发现比较"><span class="header-section-number">19.3</span> 语义关联发现比较</a></li>
  <li><a href="#主题和情感分析的差异" id="toc-主题和情感分析的差异" class="nav-link" data-scroll-target="#主题和情感分析的差异"><span class="header-section-number">19.4</span> 主题和情感分析的差异</a></li>
  <li><a href="#综合比较与应用建议" id="toc-综合比较与应用建议" class="nav-link" data-scroll-target="#综合比较与应用建议"><span class="header-section-number">19.5</span> 综合比较与应用建议</a>
  <ul class="collapse">
  <li><a href="#应用建议" id="toc-应用建议" class="nav-link" data-scroll-target="#应用建议"><span class="header-section-number">19.5.1</span> 应用建议</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#小结与进阶方向" id="toc-小结与进阶方向" class="nav-link" data-scroll-target="#小结与进阶方向"><span class="header-section-number">20</span> 小结与进阶方向</a>
  <ul class="collapse">
  <li><a href="#词向量的优缺点" id="toc-词向量的优缺点" class="nav-link" data-scroll-target="#词向量的优缺点"><span class="header-section-number">20.1</span> 词向量的优缺点</a>
  <ul class="collapse">
  <li><a href="#优点" id="toc-优点" class="nav-link" data-scroll-target="#优点"><span class="header-section-number">20.1.1</span> 优点</a></li>
  <li><a href="#局限性" id="toc-局限性" class="nav-link" data-scroll-target="#局限性"><span class="header-section-number">20.1.2</span> 局限性</a></li>
  <li><a href="#多义词问题的解决方案" id="toc-多义词问题的解决方案" class="nav-link" data-scroll-target="#多义词问题的解决方案"><span class="header-section-number">20.1.3</span> 多义词问题的解决方案</a></li>
  </ul></li>
  <li><a href="#进阶方向" id="toc-进阶方向" class="nav-link" data-scroll-target="#进阶方向"><span class="header-section-number">20.2</span> 进阶方向</a></li>
  <li><a href="#本讲小结" id="toc-本讲小结" class="nav-link" data-scroll-target="#本讲小结"><span class="header-section-number">20.3</span> 本讲小结</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">文本分析2：词向量与深度学习基础</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="从稀疏到密集表示" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> 从稀疏到密集表示</h1>
<section id="bag-of-words模型的局限性" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="bag-of-words模型的局限性"><span class="header-section-number">15.1</span> Bag of Words模型的局限性</h2>
<p>上一讲中，我们学习了词袋模型（Bag of Words）和TF-IDF，这些是文本分析的基础方法。然而，这些方法存在明显局限性：</p>
<ol type="1">
<li><p><strong>丢失词序信息</strong>：词袋模型完全忽略词语出现的顺序。例如”政府调控房价”和”房价调控政府”在词袋表示中是完全相同的</p></li>
<li><p><strong>语义鸿沟问题</strong>：无法捕捉词与词之间的语义关系，如同义词、上下位词等</p></li>
<li><p><strong>维度灾难</strong>：高维稀疏向量（维度等于词汇量大小）导致计算效率低下</p></li>
<li><p><strong>未登录词问题</strong>：无法处理训练集中未出现过的词语</p></li>
</ol>
<p>词袋模型的这些局限性促使研究者寻找更先进的文本表示方法，词向量（Word Embedding）正是这一探索的重要成果。</p>
</section>
<section id="词向量的直觉理解" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="词向量的直觉理解"><span class="header-section-number">15.2</span> 词向量的直觉理解</h2>
<p><strong>词向量</strong>（Word Embedding）是将词语映射到一个低维稠密实数向量空间的技术，通常维度在50-300之间。与词袋模型不同，词向量具有以下特点：</p>
<ol type="1">
<li><strong>稠密表示</strong>：向量中的每个维度都有非零值</li>
<li><strong>语义编码</strong>：向量的不同维度隐含地编码了词语的语义特征</li>
<li><strong>相似性可计算</strong>：语义相近的词在向量空间中距离较近</li>
</ol>
<p>例如，“银行”和”金融”在向量空间中距离较近，而”银行”和”蔬菜”则距离较远。</p>
</section>
<section id="分布式假设词向量的理论基础" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="分布式假设词向量的理论基础"><span class="header-section-number">15.3</span> 分布式假设：词向量的理论基础</h2>
<p>词向量背后的核心理论是<strong>分布式假设</strong>（Distributional Hypothesis），这一理论由语言学家J.R. Firth在1957年提出：</p>
<blockquote class="blockquote">
<p>“You shall know a word by the company it keeps.”（一个词的含义取决于它的伙伴词）</p>
</blockquote>
<p>这一假设认为：<strong>上下文相似的词，其语义也相似</strong>。例如，“银行”和”金融机构”经常出现在相似的上下文中，因此它们可能具有相似的语义。</p>
<p>基于分布式假设，词向量学习的核心任务可以归纳为：学习一个映射函数，使得在语料库中上下文相似的词在向量空间中的位置也相近。</p>
</section>
<section id="密集表示的数学性质" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="密集表示的数学性质"><span class="header-section-number">15.4</span> 密集表示的数学性质</h2>
<p>从数学角度看，密集表示的优势在于：</p>
<ol type="1">
<li><strong>降维性</strong>：从稀疏高维空间（词汇量大小，如5万维）降至低维空间（如300维）</li>
<li><strong>连续性</strong>：连续向量空间允许进行向量代数运算，如类比推理</li>
<li><strong>泛化能力</strong>：能够更好地泛化到未见过的例子</li>
</ol>
<p>数学上，稀疏向量与密集向量的对比如下：</p>
<ul>
<li><strong>稀疏向量</strong>（Sparse Vector）：<span class="math inline">\(\mathbf{v} = [0, 0, 1, 0, ..., 0, 2, 0]\)</span>，大多数元素为0</li>
<li><strong>密集向量</strong>（Dense Vector）：<span class="math inline">\(\mathbf{v} = [0.2, -0.6, 0.5, 0.9, ..., -0.1, 0.3]\)</span>，大多数元素非0</li>
</ul>
<p>稠密向量表示的直观优势可以通过一个简单的类比来理解：假设我们要描述一个人，可以使用二元特征（是/否问题，对应稀疏表示）如”是否戴眼镜”、“是否有胡子”等，也可以使用连续特征（对应稠密表示）如身高、体重、年龄等。连续特征通常能更精确、更紧凑地描述对象。</p>
</section>
</section>
<section id="word2vec原理讲解" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> Word2Vec原理讲解</h1>
<section id="word2vec简介" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="word2vec简介"><span class="header-section-number">16.1</span> Word2Vec简介</h2>
<p><strong>Word2Vec</strong>是Mikolov等人于2013年提出的一种高效学习词向量的方法，它通过浅层神经网络从大规模语料库中学习词语的分布式表示。Word2Vec迅速成为NLP领域的里程碑技术，为后续深度学习在NLP中的应用奠定了基础。</p>
<p>Word2Vec的核心思想是：<strong>通过预测上下文中的词来学习词语的向量表示</strong>。基于这一思想，Word2Vec提出了两种模型：</p>
<ol type="1">
<li><strong>Skip-gram模型</strong>：预测上下文词</li>
<li><strong>CBOW（Continuous Bag of Words）模型</strong>：预测目标词</li>
</ol>
</section>
<section id="skip-gram模型详解" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="skip-gram模型详解"><span class="header-section-number">16.2</span> Skip-gram模型详解</h2>
<p>Skip-gram模型的目标是：<strong>给定中心词，预测其上下文词</strong>。</p>
<section id="模型结构" class="level3" data-number="16.2.1">
<h3 data-number="16.2.1" class="anchored" data-anchor-id="模型结构"><span class="header-section-number">16.2.1</span> 模型结构</h3>
<p>Skip-gram模型的网络结构如下：</p>
<ol type="1">
<li><strong>输入层</strong>：中心词的one-hot编码，维度为词汇量大小<span class="math inline">\(|V|\)</span></li>
<li><strong>隐藏层</strong>：不含激活函数的全连接层，维度为词向量维度<span class="math inline">\(d\)</span></li>
<li><strong>输出层</strong>：预测上下文词概率的softmax层，维度为词汇量大小<span class="math inline">\(|V|\)</span></li>
</ol>
<p>其数学表示为：</p>
<p><span class="math display">\[p(w_o|w_i) = \frac{\exp(v_{w_o}^{\prime T} \cdot v_{w_i})}{\sum_{w=1}^{|V|} \exp(v_w^{\prime T} \cdot v_{w_i})}\]</span></p>
<p>其中： - <span class="math inline">\(w_i\)</span>是中心词 - <span class="math inline">\(w_o\)</span>是上下文词 - <span class="math inline">\(v_{w_i}\)</span>是中心词的词向量（输入向量） - <span class="math inline">\(v_{w_o}^{\prime}\)</span>是上下文词的词向量（输出向量） - <span class="math inline">\(|V|\)</span>是词汇表大小</p>
</section>
<section id="训练过程" class="level3" data-number="16.2.2">
<h3 data-number="16.2.2" class="anchored" data-anchor-id="训练过程"><span class="header-section-number">16.2.2</span> 训练过程</h3>
<p>Skip-gram模型的训练过程如下：</p>
<ol type="1">
<li>从语料库中抽取中心词<span class="math inline">\(w_i\)</span>及其上下文窗口内的词<span class="math inline">\(w_o\)</span></li>
<li>最大化预测上下文词的条件概率<span class="math inline">\(p(w_o|w_i)\)</span></li>
<li>对所有词对<span class="math inline">\((w_i, w_o)\)</span>，优化目标函数：</li>
</ol>
<p><span class="math display">\[J(\theta) = \frac{1}{T}\sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)\]</span></p>
<p>其中： - <span class="math inline">\(T\)</span>是语料库中的词数 - <span class="math inline">\(c\)</span>是上下文窗口大小 - <span class="math inline">\(\theta\)</span>是模型参数</p>
</section>
</section>
<section id="cbow模型详解" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="cbow模型详解"><span class="header-section-number">16.3</span> CBOW模型详解</h2>
<p>CBOW（Continuous Bag of Words）模型与Skip-gram相反，其目标是：<strong>给定上下文词，预测中心词</strong>。</p>
<section id="模型结构-1" class="level3" data-number="16.3.1">
<h3 data-number="16.3.1" class="anchored" data-anchor-id="模型结构-1"><span class="header-section-number">16.3.1</span> 模型结构</h3>
<p>CBOW模型的网络结构如下：</p>
<ol type="1">
<li><strong>输入层</strong>：多个上下文词的one-hot编码，每个维度为词汇量大小<span class="math inline">\(|V|\)</span></li>
<li><strong>隐藏层</strong>：不含激活函数的全连接层，维度为词向量维度<span class="math inline">\(d\)</span></li>
<li><strong>输出层</strong>：预测中心词概率的softmax层，维度为词汇量大小<span class="math inline">\(|V|\)</span></li>
</ol>
<p>CBOW模型首先对上下文词的向量取平均：</p>
<p><span class="math display">\[\hat{v} = \frac{1}{2c} \sum_{-c \leq j \leq c, j \neq 0} v_{w_{t+j}}\]</span></p>
<p>然后预测中心词的概率：</p>
<p><span class="math display">\[p(w_t|\hat{v}) = \frac{\exp(v_{w_t}^{\prime T} \cdot \hat{v})}{\sum_{w=1}^{|V|} \exp(v_w^{\prime T} \cdot \hat{v})}\]</span></p>
</section>
<section id="skip-gram与cbow对比" class="level3" data-number="16.3.2">
<h3 data-number="16.3.2" class="anchored" data-anchor-id="skip-gram与cbow对比"><span class="header-section-number">16.3.2</span> Skip-gram与CBOW对比</h3>
<p>两种模型各有优缺点：</p>
<ol type="1">
<li><strong>Skip-gram</strong>:
<ul>
<li>更适合小型语料库</li>
<li>对低频词表现更好</li>
<li>计算复杂度较高</li>
</ul></li>
<li><strong>CBOW</strong>:
<ul>
<li>训练速度更快</li>
<li>对高频词表现更好</li>
<li>在大型语料库上更稳定</li>
</ul></li>
</ol>
</section>
</section>
<section id="负采样negative-sampling技术" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="负采样negative-sampling技术"><span class="header-section-number">16.4</span> 负采样（Negative Sampling）技术</h2>
<p>Word2Vec的一个主要计算瓶颈是softmax函数，其计算复杂度与词汇量成正比。为解决这一问题，Mikolov等人提出了<strong>负采样</strong>（Negative Sampling）技术。</p>
<section id="负采样原理" class="level3" data-number="16.4.1">
<h3 data-number="16.4.1" class="anchored" data-anchor-id="负采样原理"><span class="header-section-number">16.4.1</span> 负采样原理</h3>
<p>负采样将多分类问题转化为二分类问题：</p>
<ol type="1">
<li>对于真实的词对<span class="math inline">\((w_i, w_o)\)</span>，将其标记为正样本（标签为1）</li>
<li>对于每个正样本，随机采样<span class="math inline">\(k\)</span>个负样本<span class="math inline">\((w_i, w_n)\)</span>，其中<span class="math inline">\(w_n\)</span>是随机词（标签为0）</li>
<li>使用逻辑回归来判断词对是否真实共现</li>
</ol>
<p>优化目标变为：</p>
<p><span class="math display">\[J(\theta) = \log \sigma(v_{w_o}^{\prime T} \cdot v_{w_i}) + \sum_{j=1}^{k} \mathbb{E}_{w_j \sim P_n(w)} [\log \sigma(-v_{w_j}^{\prime T} \cdot v_{w_i})]\]</span></p>
<p>其中： - <span class="math inline">\(\sigma\)</span>是sigmoid函数 - <span class="math inline">\(P_n(w)\)</span>是负样本的噪声分布，通常为词频的3/4次方</p>
</section>
<section id="负采样的优势" class="level3" data-number="16.4.2">
<h3 data-number="16.4.2" class="anchored" data-anchor-id="负采样的优势"><span class="header-section-number">16.4.2</span> 负采样的优势</h3>
<p>负采样技术带来的主要优势包括：</p>
<ol type="1">
<li><strong>计算效率</strong>：将复杂度从<span class="math inline">\(O(|V|)\)</span>降至<span class="math inline">\(O(k)\)</span>，其中<span class="math inline">\(k \ll |V|\)</span>（通常<span class="math inline">\(k=5-20\)</span>）</li>
<li><strong>稀疏更新</strong>：每次只更新少量词向量，加速收敛</li>
<li><strong>控制学习难度</strong>：通过调整负样本数量控制任务难度</li>
</ol>
<p>负采样是Word2Vec能够在大规模语料库上高效训练的关键技术之一。</p>
</section>
</section>
<section id="词向量空间的语义特性" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="词向量空间的语义特性"><span class="header-section-number">16.5</span> 词向量空间的语义特性</h2>
<p>Word2Vec训练得到的词向量空间具有丰富的语义特性，这些特性使得词向量成为各种NLP任务的强大特征。</p>
<section id="语义相关性" class="level3" data-number="16.5.1">
<h3 data-number="16.5.1" class="anchored" data-anchor-id="语义相关性"><span class="header-section-number">16.5.1</span> 语义相关性</h3>
<p>相似概念在向量空间中距离较近。例如：</p>
<ul>
<li>“银行”和”金融”距离近</li>
<li>“苹果”(水果)和”橙子”距离近</li>
<li>“苹果”(公司)和”微软”距离近</li>
</ul>
<p>这种相似性可以通过余弦相似度定量衡量：</p>
<p><span class="math display">\[similarity(w_1, w_2) = \cos(\theta) = \frac{v_{w_1} \cdot v_{w_2}}{||v_{w_1}|| \cdot ||v_{w_2}||}\]</span></p>
</section>
<section id="语义计算" class="level3" data-number="16.5.2">
<h3 data-number="16.5.2" class="anchored" data-anchor-id="语义计算"><span class="header-section-number">16.5.2</span> 语义计算</h3>
<p>词向量空间中最令人惊讶的特性是支持向量代数运算，可以进行”语义计算”：</p>
<p><span class="math display">\[v(\text{"king"}) - v(\text{"man"}) + v(\text{"woman"}) \approx v(\text{"queen"})\]</span></p>
<p>这意味着我们可以通过向量运算回答类比问题：“man之于woman，相当于king之于什么？”</p>
<p>其他例子包括： - <span class="math inline">\(v(\text{"中国"}) - v(\text{"北京"}) + v(\text{"法国"}) \approx v(\text{"巴黎"})\)</span> - <span class="math inline">\(v(\text{"比特币"}) - v(\text{"数字"}) + v(\text{"实物"}) \approx v(\text{"黄金"})\)</span></p>
<p>这些语义运算的存在表明词向量确实捕获了复杂的语义关系，而非简单的共现统计。</p>
</section>
</section>
</section>
<section id="金融文本中应用word2vec" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> 金融文本中应用Word2Vec</h1>
<section id="金融领域的词向量应用" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="金融领域的词向量应用"><span class="header-section-number">17.1</span> 金融领域的词向量应用</h2>
<p>在金融领域，词向量技术已被广泛应用于多种任务：</p>
<ol type="1">
<li><strong>情感分析</strong>：分析金融新闻、社交媒体对市场情绪的影响</li>
<li><strong>风险评估</strong>：从文本数据中提取风险信号</li>
<li><strong>主题发现</strong>：自动识别财经报道中的热点话题</li>
<li><strong>市场预测</strong>：结合文本特征进行市场走势预测</li>
</ol>
<p>金融文本的特殊性（专业术语多、实体关系复杂）使得通用词向量模型可能表现不佳，因此针对金融领域训练的词向量至关重要。</p>
</section>
<section id="训练金融领域词向量" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="训练金融领域词向量"><span class="header-section-number">17.2</span> 训练金融领域词向量</h2>
<p>以下我们将使用政府工作报告和其他财经语料训练Word2Vec模型，展示其在金融领域的应用。</p>
</section>
</section>
<section id="预训练词向量模型比较与应用" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> 预训练词向量模型比较与应用</h1>
<p>在实际应用中，我们通常可以选择使用已有的预训练词向量模型，而不必从头开始训练。这些模型由大型组织或研究机构在海量文本上训练得到，具有更好的通用性和语义表示能力。下面我们将介绍几种常用的预训练词向量模型，并比较它们在不同应用场景下的表现。</p>
<section id="中文预训练词向量模型" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="中文预训练词向量模型"><span class="header-section-number">18.1</span> 中文预训练词向量模型</h2>
<section id="腾讯ai-lab词向量chinese-word-vectors" class="level3" data-number="18.1.1">
<h3 data-number="18.1.1" class="anchored" data-anchor-id="腾讯ai-lab词向量chinese-word-vectors"><span class="header-section-number">18.1.1</span> 1. 腾讯AI Lab词向量（Chinese Word Vectors）</h3>
<p>腾讯AI Lab发布的中文词向量是目前应用最广泛的中文预训练词向量之一。</p>
<ul>
<li><strong>训练语料</strong>：由8亿多条句子、超过200亿词汇组成</li>
<li><strong>词汇量</strong>：约800万个词、词组和实体</li>
<li><strong>向量维度</strong>：200维</li>
<li><strong>特点</strong>：覆盖面广，质量高，适用于多领域任务</li>
</ul>
</section>
<section id="哈工大讯飞联合实验室词向量hit-scir-chinese-word-vectors" class="level3" data-number="18.1.2">
<h3 data-number="18.1.2" class="anchored" data-anchor-id="哈工大讯飞联合实验室词向量hit-scir-chinese-word-vectors"><span class="header-section-number">18.1.2</span> 2. 哈工大/讯飞联合实验室词向量（HIT-SCIR Chinese Word Vectors）</h3>
<ul>
<li><strong>训练语料</strong>：人民日报语料库和其他新闻语料</li>
<li><strong>词汇量</strong>：约100万个词</li>
<li><strong>向量维度</strong>：300维</li>
<li><strong>特点</strong>：对专业术语和实体识别有较好的表现</li>
</ul>
</section>
<section id="北师大人大学者开发的词向量chinese-word-vectors" class="level3" data-number="18.1.3">
<h3 data-number="18.1.3" class="anchored" data-anchor-id="北师大人大学者开发的词向量chinese-word-vectors"><span class="header-section-number">18.1.3</span> 3. 北师大/人大学者开发的词向量（Chinese Word Vectors）</h3>
<ul>
<li><strong>训练语料</strong>：基于百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等语料</li>
<li><strong>词汇量</strong>：约200万个词</li>
<li><strong>向量维度</strong>：300维</li>
<li><strong>特点</strong>：对百科类内容和常识性知识表示较好</li>
</ul>
</section>
</section>
<section id="英文预训练词向量模型" class="level2" data-number="18.2">
<h2 data-number="18.2" class="anchored" data-anchor-id="英文预训练词向量模型"><span class="header-section-number">18.2</span> 英文预训练词向量模型</h2>
<section id="word2vec词向量" class="level3" data-number="18.2.1">
<h3 data-number="18.2.1" class="anchored" data-anchor-id="word2vec词向量"><span class="header-section-number">18.2.1</span> 1. Word2Vec词向量</h3>
<p>由Google在约1000亿单词的Google News数据集上训练的Word2Vec模型。</p>
<ul>
<li><strong>词汇量</strong>：约300万个词和短语</li>
<li><strong>向量维度</strong>：300维</li>
<li><strong>特点</strong>：通用性强，被广泛应用于英文NLP研究和应用</li>
</ul>
</section>
<section id="gloveglobal-vectors-for-word-representation" class="level3" data-number="18.2.2">
<h3 data-number="18.2.2" class="anchored" data-anchor-id="gloveglobal-vectors-for-word-representation"><span class="header-section-number">18.2.2</span> 2. GloVe（Global Vectors for Word Representation）</h3>
<p>由斯坦福NLP小组开发的词向量模型，训练自维基百科和网络文本。</p>
<ul>
<li><strong>训练语料</strong>：CommonCrawl（840B tokens）、Wikipedia（6B tokens）等</li>
<li><strong>词汇量</strong>：根据语料大小从40万到200万不等</li>
<li><strong>向量维度</strong>：50到300维不等</li>
<li><strong>特点</strong>：结合了全局矩阵分解和局部上下文窗口方法的优点</li>
</ul>
</section>
<section id="fasttext" class="level3" data-number="18.2.3">
<h3 data-number="18.2.3" class="anchored" data-anchor-id="fasttext"><span class="header-section-number">18.2.3</span> 3. FastText</h3>
<p>由Facebook AI Research开发，在维基百科语料上训练。</p>
<ul>
<li><strong>词汇量</strong>：约200万个词</li>
<li><strong>向量维度</strong>：300维</li>
<li><strong>特点</strong>：利用词的子词信息，能更好地处理罕见词和未登录词</li>
</ul>
</section>
</section>
<section id="预训练模型的表现比较" class="level2" data-number="18.3">
<h2 data-number="18.3" class="anchored" data-anchor-id="预训练模型的表现比较"><span class="header-section-number">18.3</span> 预训练模型的表现比较</h2>
<p>不同预训练词向量模型在各种任务上的表现各有优劣。下面我们将从多个维度对它们进行比较：</p>
<section id="语义捕捉能力" class="level3" data-number="18.3.1">
<h3 data-number="18.3.1" class="anchored" data-anchor-id="语义捕捉能力"><span class="header-section-number">18.3.1</span> 1. 语义捕捉能力</h3>
<p>通过测试几组典型的语义关系来比较不同模型：</p>
</section>
<section id="领域适应性" class="level3" data-number="18.3.2">
<h3 data-number="18.3.2" class="anchored" data-anchor-id="领域适应性"><span class="header-section-number">18.3.2</span> 2. 领域适应性</h3>
<p>不同模型在特定领域（如金融、医疗、法律等）的表现评估：</p>
</section>
<section id="处理未登录词能力" class="level3" data-number="18.3.3">
<h3 data-number="18.3.3" class="anchored" data-anchor-id="处理未登录词能力"><span class="header-section-number">18.3.3</span> 3. 处理未登录词能力</h3>
<p>FastText由于使用子词信息，对未登录词有独特优势：</p>
<p>三种预训练词向量模型在处理未登录词（Out-Of-Vocabulary，OOV）方面的能力存在显著差异：</p>
<ol type="1">
<li><strong>FastText</strong>：
<ul>
<li><strong>技术原理</strong>：将词表示为字符n-gram的集合，利用子词信息</li>
<li><strong>优势</strong>：即使词不在词表中，仍可通过子词组合生成向量</li>
<li><strong>实际效果</strong>：能有效处理形态变化（如复数、时态）和拼写变体</li>
<li><strong>适用场景</strong>：专业领域术语多、新词频繁出现的应用，如医疗、法律和技术文本</li>
</ul></li>
<li><strong>GloVe</strong>：
<ul>
<li><strong>技术原理</strong>：结合全局矩阵分解和局部上下文窗口方法</li>
<li><strong>优势</strong>：能较好地表示低频词，但不能直接处理未登录词</li>
<li><strong>处理方案</strong>：通常使用特殊的<unk>向量或基于字符级特征构建未知词向量</unk></li>
<li><strong>适用场景</strong>：词汇相对稳定但关注低频词的场景，如学术文本分析</li>
</ul></li>
<li><strong>Word2Vec</strong>：
<ul>
<li><strong>技术原理</strong>：纯粹基于词级别的分布式表示学习</li>
<li><strong>劣势</strong>：完全无法处理训练集中未出现的词</li>
<li><strong>处理方案</strong>：常用随机向量、零向量或所有词向量的平均值代替未登录词</li>
<li><strong>适用场景</strong>：封闭域应用，词汇变化少的场景，如特定主题的文档分类</li>
</ul></li>
</ol>
<p>在实际测试中，当处理”区块链”、“元宇宙”等新兴词汇时，FastText能生成语义相关的向量，而Word2Vec和GloVe则完全无法处理。这一点在金融科技等快速发展的领域尤为重要。</p>
</section>
<section id="多语言词向量对齐" class="level3" data-number="18.3.4">
<h3 data-number="18.3.4" class="anchored" data-anchor-id="多语言词向量对齐"><span class="header-section-number">18.3.4</span> 4. 多语言词向量对齐</h3>
<p>为了支持跨语言应用，可以将不同语言的词向量空间对齐：</p>
</section>
</section>
<section id="预训练模型在不同任务中的选择指南" class="level2" data-number="18.4">
<h2 data-number="18.4" class="anchored" data-anchor-id="预训练模型在不同任务中的选择指南"><span class="header-section-number">18.4</span> 预训练模型在不同任务中的选择指南</h2>
<p>根据不同应用场景，我们推荐选择的预训练词向量模型：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>应用场景</th>
<th>推荐中文模型</th>
<th>推荐英文模型</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>通用文本分类</td>
<td>腾讯AI Lab</td>
<td>GloVe 300d</td>
<td>覆盖面广，向量维度适中</td>
</tr>
<tr class="even">
<td>命名实体识别</td>
<td>哈工大词向量</td>
<td>FastText</td>
<td>对实体名称和罕见词有更好表现</td>
</tr>
<tr class="odd">
<td>情感分析</td>
<td>腾讯AI Lab</td>
<td>Word2Vec</td>
<td>对语义细微差别表现更好</td>
</tr>
<tr class="even">
<td>专业领域(如金融)</td>
<td>领域特定模型</td>
<td>领域特定模型</td>
<td>通用模型对专业术语表示不足</td>
</tr>
<tr class="odd">
<td>处理网络文本</td>
<td>搜狗新闻词向量</td>
<td>FastText</td>
<td>对网络流行语和新词表现更好</td>
</tr>
</tbody>
</table>
</section>
<section id="词向量在实际应用中的评估" class="level2" data-number="18.5">
<h2 data-number="18.5" class="anchored" data-anchor-id="词向量在实际应用中的评估"><span class="header-section-number">18.5</span> 词向量在实际应用中的评估</h2>
<p>评估词向量质量是选择或训练词向量模型的重要一环。词向量评估通常分为两类：内在评估和外在评估。</p>
<section id="内在评估intrinsic-evaluation" class="level3" data-number="18.5.1">
<h3 data-number="18.5.1" class="anchored" data-anchor-id="内在评估intrinsic-evaluation"><span class="header-section-number">18.5.1</span> 内在评估(Intrinsic Evaluation)</h3>
<p>内在评估直接测试词向量捕捉语言学特性的能力：</p>
<ol type="1">
<li><p><strong>词相似度任务</strong>：</p>
<ul>
<li><strong>WordSim-353</strong>：包含353对词语，每对都有人工标注的相似度评分</li>
<li><strong>SimLex-999</strong>：着重于真正的语义相似性（而非相关性）的数据集</li>
<li><strong>中文评估集</strong>：如哈工大汉语词相似度数据集（HowNet-500）</li>
</ul>
<p>评估方法：计算词向量余弦相似度与人工评分的相关系数（Spearman或Pearson）</p></li>
<li><p><strong>类比关系任务</strong>：</p>
<ul>
<li>测试模型是否捕捉语义关系，如”北京:中国::巴黎:法国”</li>
<li>常见关系类型：国家-首都、国家-货币、动词时态、形容词比较级等</li>
</ul>
<p>评估方法：计算向量运算（如v(“北京”)-v(“中国”)+v(“法国”)）的结果与目标词v(“巴黎”)的接近度</p></li>
<li><p><strong>多语言对齐质量</strong>：</p>
<ul>
<li>对于跨语言应用，评估词向量在不同语言间的语义一致性</li>
<li>使用平行词典或自动翻译进行评估</li>
</ul>
<p>评估方法：翻译精度或跨语言检索正确率</p></li>
</ol>
</section>
<section id="外在评估extrinsic-evaluation" class="level3" data-number="18.5.2">
<h3 data-number="18.5.2" class="anchored" data-anchor-id="外在评估extrinsic-evaluation"><span class="header-section-number">18.5.2</span> 外在评估(Extrinsic Evaluation)</h3>
<p>外在评估测量词向量在实际NLP任务中的表现：</p>
<ol type="1">
<li><strong>下游任务性能</strong>：
<ul>
<li>文本分类：准确率、F1分数</li>
<li>命名实体识别：精确率、召回率</li>
<li>机器翻译：BLEU分数</li>
<li>情感分析：准确率，特别是对微妙情感表达的捕捉能力</li>
</ul></li>
<li><strong>迁移能力</strong>：
<ul>
<li>从一个领域到另一个领域的泛化能力</li>
<li>从一种语言到另一种语言的迁移性能</li>
<li>低资源场景下的表现</li>
</ul></li>
<li><strong>资源效率</strong>：
<ul>
<li>训练时间和计算资源需求</li>
<li>模型大小和内存占用</li>
<li>推理速度</li>
</ul></li>
</ol>
</section>
<section id="金融领域案例分析" class="level3" data-number="18.5.3">
<h3 data-number="18.5.3" class="anchored" data-anchor-id="金融领域案例分析"><span class="header-section-number">18.5.3</span> 金融领域案例分析</h3>
<p>在金融文本分析中，领域特定词向量通常优于通用词向量：</p>
<ul>
<li>在市场情绪分析任务中，金融专用词向量比通用词向量提高8-15%的准确率</li>
<li>对专业术语和金融实体的识别准确率提升显著（&gt;20%）</li>
<li>在金融新闻聚类任务中，专业词向量能发现更细粒度的主题区分</li>
</ul>
<p>这种性能差异主要来自金融领域特有的语言使用模式和术语，如”牛市/熊市”、“做多/做空”等在通用语料中很少出现或具有不同含义的词。</p>
</section>
</section>
<section id="选择或训练自己的词向量模型" class="level2" data-number="18.6">
<h2 data-number="18.6" class="anchored" data-anchor-id="选择或训练自己的词向量模型"><span class="header-section-number">18.6</span> 选择或训练自己的词向量模型</h2>
<p>在实际应用中，我们需要根据具体任务和数据特点选择合适的预训练模型或决定是否需要训练自己的模型：</p>
<ol type="1">
<li><strong>使用预训练模型的情况</strong>：
<ul>
<li>数据量有限，无法支持有效训练</li>
<li>任务是通用领域，预训练模型已足够好</li>
<li>计算资源有限</li>
<li>需要快速开发原型系统</li>
</ul></li>
<li><strong>训练自己的模型的情况</strong>：
<ul>
<li>有大量特定领域的文本数据</li>
<li>应用领域有特殊术语或表达方式</li>
<li>现有预训练模型表现不佳</li>
<li>有足够的计算资源</li>
</ul></li>
<li><strong>微调预训练模型的折中方案</strong>：
<ul>
<li>从预训练模型开始，用领域数据继续训练</li>
<li>保留通用语言知识，同时学习领域特定表示</li>
<li>资源需求适中，效果通常不错</li>
</ul></li>
</ol>
</section>
<section id="结论" class="level2" data-number="18.7">
<h2 data-number="18.7" class="anchored" data-anchor-id="结论"><span class="header-section-number">18.7</span> 结论</h2>
<p>预训练词向量模型为NLP任务提供了便捷的起点，无需从头训练就能获得高质量的词语表示。根据我们的测试，腾讯AI Lab词向量和GloVe模型在通用任务中表现最佳，而FastText在处理未登录词方面具有明显优势。</p>
<p>对于金融文本分析，我们建议：如果数据量充足，可以在通用预训练模型基础上，使用金融领域文本进行进一步训练，以获得更符合领域特性的词向量表示；如果资源有限，可以选择腾讯AI Lab等高质量预训练模型作为基础，然后结合任务特点设计适当的特征工程。</p>
</section>
</section>
<section id="案例比较词频法与词向量在政府工作报告分析中的差异" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> 案例比较：词频法与词向量在政府工作报告分析中的差异</h1>
<p>基于上一讲中的政府工作报告分析案例，我们可以直观地比较词频法(Bag of Words/TF-IDF)和词向量(Word2Vec)在相同任务上的表现差异。以下我们在三个具体任务上进行比较分析：文本相似度计算、关键词提取和语义关联发现。</p>
<section id="相同文本的两种表示方法" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="相同文本的两种表示方法"><span class="header-section-number">19.1</span> 相同文本的两种表示方法</h2>
<p>首先，让我们回顾两种方法对同一文档的不同表示方式：</p>
<p><strong>相似度计算结果分析</strong>:</p>
<ol type="1">
<li><strong>TF-IDF相似度</strong>：
<ul>
<li>基于词汇重叠度，相同词汇出现越多，相似度越高</li>
<li>对关键词敏感，但没有语义理解</li>
<li>相似度变化跨度较大，更容易区分文档</li>
</ul></li>
<li><strong>Word2Vec相似度</strong>：
<ul>
<li>基于语义空间的接近程度，能捕捉同义词和相关概念</li>
<li>所有文档相似度普遍较高，因为它们都处于相似的语义空间</li>
<li>变化更平滑，体现语义连续性</li>
</ul></li>
</ol>
<p>例如，即使2020年疫情报告的词汇与其他年份有明显不同，但在Word2Vec中相似度依然较高，因为整体语义主题（政府工作）是相似的。</p>
</section>
<section id="关键词提取比较" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="关键词提取比较"><span class="header-section-number">19.2</span> 关键词提取比较</h2>
<p>两种方法在关键词提取上也有明显不同：</p>
<p><strong>关键词提取结果分析</strong>:</p>
<ol type="1">
<li><strong>TF-IDF关键词</strong>：
<ul>
<li>提取文档中特有的、区分度高的词</li>
<li>往往是该文档特有的专有名词或低频词</li>
<li>关注”独特性”而非”重要性”</li>
</ul></li>
<li><strong>Word2Vec关键词</strong>：
<ul>
<li>提取文档的语义中心词，与其他词语义联系最紧密的词</li>
<li>往往是文档主题的核心词，且在语义网络中起枢纽作用</li>
<li>关注”中心性”而非”频率”或”独特性”</li>
</ul></li>
</ol>
<p>例如，TF-IDF可能会提取”十四五”这样的特定术语，而Word2Vec可能会提取”发展”这样的核心概念词。</p>
</section>
<section id="语义关联发现比较" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="语义关联发现比较"><span class="header-section-number">19.3</span> 语义关联发现比较</h2>
<p>词频法和词向量在发现词语关联关系上有本质区别：</p>
<p><strong>语义关联分析结果比较</strong>:</p>
<ol type="1">
<li><strong>词频法（共现分析）</strong>：
<ul>
<li>基于词语在文本中出现的物理距离</li>
<li>只能发现直接共现的关系，不能泛化</li>
<li>需要大量文本才能得到可靠的统计结果</li>
<li>无法发现从未共现但语义相关的词</li>
</ul></li>
<li><strong>Word2Vec（语义网络）</strong>：
<ul>
<li>基于分布式表示学习的语义空间距离</li>
<li>能发现间接关联，即使两个词从未共现</li>
<li>能发现语义层次和类比关系</li>
<li>受预训练语料的影响大</li>
</ul></li>
</ol>
<p>例如，在政府工作报告中，“环保”和”低碳”可能很少直接共现，但在Word2Vec的语义空间中会很接近。</p>
</section>
<section id="主题和情感分析的差异" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="主题和情感分析的差异"><span class="header-section-number">19.4</span> 主题和情感分析的差异</h2>
<p>两种方法在主题建模和情感分析上也有明显区别：</p>
<p><strong>主题分析结果比较</strong>:</p>
<ol type="1">
<li><strong>基于词频的LDA</strong>：
<ul>
<li>基于词语在文档中的共现统计构建主题</li>
<li>主题是词语的概率分布，每个词有明确的主题概率</li>
<li>结果可解释性强，但受制于表面统计</li>
</ul></li>
<li><strong>基于Word2Vec的聚类</strong>：
<ul>
<li>基于文档在语义空间中的分布进行聚类</li>
<li>主题体现为语义空间中的区域，边界更加模糊</li>
<li>能发现更抽象的语义关联，但解释性较弱</li>
</ul></li>
</ol>
</section>
<section id="综合比较与应用建议" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="综合比较与应用建议"><span class="header-section-number">19.5</span> 综合比较与应用建议</h2>
<p>通过以上案例比较，我们总结词频法和词向量在政府工作报告分析中的优缺点：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>比较维度</th>
<th>词频法 (Bag of Words/TF-IDF)</th>
<th>词向量 (Word2Vec)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>数据表示</td>
<td>高维稀疏向量(~万维)</td>
<td>低维稠密向量(~百维)</td>
</tr>
<tr class="even">
<td>语义捕捉</td>
<td>基于表面词频统计，无语义</td>
<td>基于分布式假设，有语义</td>
</tr>
<tr class="odd">
<td>计算复杂度</td>
<td>低，适合大规模文档</td>
<td>中等，训练需要时间</td>
</tr>
<tr class="even">
<td>内存占用</td>
<td>大（稀疏矩阵）</td>
<td>小（稠密向量）</td>
</tr>
<tr class="odd">
<td>新词处理</td>
<td>无法处理未见词</td>
<td>也无法直接处理(FastText可以)</td>
</tr>
<tr class="even">
<td>相似度计算</td>
<td>仅基于词重叠</td>
<td>基于语义相似</td>
</tr>
<tr class="odd">
<td>关键词提取</td>
<td>偏向特有词</td>
<td>偏向中心词</td>
</tr>
<tr class="even">
<td>语义关联</td>
<td>仅能发现共现关系</td>
<td>能发现间接语义关联</td>
</tr>
<tr class="odd">
<td>应用场景</td>
<td>文档分类、信息检索</td>
<td>语义搜索、推荐系统</td>
</tr>
</tbody>
</table>
<section id="应用建议" class="level3" data-number="19.5.1">
<h3 data-number="19.5.1" class="anchored" data-anchor-id="应用建议"><span class="header-section-number">19.5.1</span> 应用建议</h3>
<p>基于我们对政府工作报告的分析经验，针对不同任务推荐的方法：</p>
<ol type="1">
<li><strong>文档去重或精确匹配</strong>：使用词频表示</li>
<li><strong>文档语义检索或推荐</strong>：使用词向量表示</li>
<li><strong>特有术语或政策提取</strong>：使用TF-IDF方法</li>
<li><strong>政策主题语义聚类</strong>：使用Word2Vec</li>
<li><strong>综合分析</strong>：可以同时使用两种方法并结合结果</li>
</ol>
<p>在实际应用中，选择合适的文本表示方法往往取决于具体任务需求、可用资源和期望的结果特性。词频法和词向量并非互斥，而是互补的分析视角。</p>
</section>
</section>
</section>
<section id="小结与进阶方向" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> 小结与进阶方向</h1>
<section id="词向量的优缺点" class="level2" data-number="20.1">
<h2 data-number="20.1" class="anchored" data-anchor-id="词向量的优缺点"><span class="header-section-number">20.1</span> 词向量的优缺点</h2>
<section id="优点" class="level3" data-number="20.1.1">
<h3 data-number="20.1.1" class="anchored" data-anchor-id="优点"><span class="header-section-number">20.1.1</span> 优点</h3>
<ol type="1">
<li><strong>语义丰富</strong>：捕获了词语间的语义关系</li>
<li><strong>维度可控</strong>：典型地为50-300维，远低于词汇量大小</li>
<li><strong>泛化能力</strong>：能处理未见过的词组合</li>
<li><strong>通用性</strong>：可用于各种NLP任务的特征提取</li>
</ol>
</section>
<section id="局限性" class="level3" data-number="20.1.2">
<h3 data-number="20.1.2" class="anchored" data-anchor-id="局限性"><span class="header-section-number">20.1.2</span> 局限性</h3>
<ol type="1">
<li><strong>多义词问题</strong>：无法区分同一个词的不同含义（如”苹果”可以是水果或公司）</li>
<li><strong>上下文依赖</strong>：固定的词向量无法根据上下文调整</li>
<li><strong>预训练依赖</strong>：需要大量语料预训练</li>
<li><strong>领域专一性</strong>：通用领域训练的词向量在专业领域效果可能不佳</li>
</ol>
</section>
<section id="多义词问题的解决方案" class="level3" data-number="20.1.3">
<h3 data-number="20.1.3" class="anchored" data-anchor-id="多义词问题的解决方案"><span class="header-section-number">20.1.3</span> 多义词问题的解决方案</h3>
<p>针对词向量无法处理多义词的局限性，研究者提出了多种解决方案：</p>
<ol type="1">
<li><strong>基于聚类的多义词表示</strong>：
<ul>
<li>收集词的所有上下文向量</li>
<li>对上下文向量进行聚类（如K-means）</li>
<li>为每个聚类中心分配一个词义向量</li>
<li>使用时根据上下文选择最相似的词义向量</li>
</ul></li>
<li><strong>动态词表示模型</strong>：
<ul>
<li><strong>ELMo (Embeddings from Language Models)</strong>：基于双向LSTM，根据句子上下文动态生成词表示</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>：基于Transformer架构，学习上下文敏感的词表示</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>：单向语言模型，生成考虑左侧上下文的表示</li>
</ul></li>
<li><strong>词义消歧与表示结合</strong>：
<ul>
<li>先使用词义消歧算法确定词的当前含义</li>
<li>再选择对应含义的词向量表示</li>
<li>例如MSSG (Multi-Sense Skip-Gram)模型</li>
</ul></li>
</ol>
<p>这些技术已经在很大程度上克服了传统Word2Vec模型处理多义词的局限，代表了词表示技术的发展方向。</p>
</section>
</section>
<section id="进阶方向" class="level2" data-number="20.2">
<h2 data-number="20.2" class="anchored" data-anchor-id="进阶方向"><span class="header-section-number">20.2</span> 进阶方向</h2>
<ol type="1">
<li><strong>上下文相关的词表示</strong>：如ELMo、BERT等模型能根据上下文动态生成词表示</li>
<li><strong>多语言词向量</strong>：跨语言的词向量对齐，支持多语言应用</li>
<li><strong>领域适应</strong>：将通用词向量迁移到特定领域</li>
<li><strong>可解释性研究</strong>：理解词向量空间的维度含义</li>
</ol>
</section>
<section id="本讲小结" class="level2" data-number="20.3">
<h2 data-number="20.3" class="anchored" data-anchor-id="本讲小结"><span class="header-section-number">20.3</span> 本讲小结</h2>
<p>本讲我们从Bag of Words模型的局限性出发，介绍了词向量的概念、原理和应用：</p>
<ol type="1">
<li>词向量通过低维稠密向量表示词语，克服了传统方法的局限</li>
<li>Word2Vec通过Skip-gram和CBOW两种模型高效学习词向量</li>
<li>负采样等技术大幅提高了训练效率</li>
<li>词向量空间具有丰富的语义特性，支持相似性计算和向量代数运算</li>
<li>在金融文本分析中，词向量可以发现政策热点、分析语义变化等</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11_nlp.html" class="pagination-link" aria-label="文本分析1：词频法与向量空间">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">文本分析1：词频法与向量空间</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13_llm.html" class="pagination-link" aria-label="文本分析3：大语言模型及其应用">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">文本分析3：大语言模型及其应用</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>