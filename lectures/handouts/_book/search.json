[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据挖掘与机器学习课程讲义",
    "section": "",
    "text": "本讲义将系统地介绍机器学习的基本概念、主要模型以及实践应用。\n引言:\n机器学习是人工智能的一个重要分支，它是指从数据中自动学习规律和模式，并利用这些规律和模式进行预测和决策的过程。机器学习在量化投资、金融科技等领域有广泛应用。由于课程时间有限，本讲义将重点介绍机器学习中的监督学习与无监督学习，以及其在金融预测中的应用。\n课程内容（理论）:\n\n机器学习基础\n监督学习理论(上)\n监督学习理论(下)\n模型评估与优化\n信用评分\n无监督学习\n聚类\n\n课程内容（实践）:\n\n金融数据获取与数据分析基础\n泰坦尼克号生存预测实践\n聚类算法实践\n\n课程项目:\n\n借贷违约风险评估\n\n下面两个项目二选一：\n2a. 股票价格预测 2b. 财务报表文本分析\n使用说明:\n\n本讲义使用 Quarto 创建，可以方便地生成 HTML, PDF, ePub 等多种格式。\n点击左侧导航栏可以浏览不同章节的内容。\n\n希望本讲义能帮助您更好地学习和理解机器学习！",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html",
    "href": "01_ml_basic.html",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1 机器学习简介",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习简介",
    "href": "01_ml_basic.html#机器学习简介",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1.1 什么是机器学习？\n机器学习是人工智能领域中一个非常热门且快速发展的分支。简单来说，机器学习就是让计算机通过学习数据，而不是依赖明确的编程指令，来完成特定的任务或解决问题。想象一下，我们教小孩子认识猫和狗，不是告诉他们猫和狗的具体特征（比如多少根胡须，耳朵的形状），而是给他们看大量的猫和狗的图片，告诉他们哪些是猫，哪些是狗。通过不断学习，孩子就能自己总结出猫和狗的区别，并且能够识别新的猫和狗。机器学习的原理与之类似，它使用算法来解析数据，从中学习，然后利用学到的知识对新数据做出预测或决策。\n\n\n2.1.2 机器学习的主要特点\n\n数据驱动: 机器学习模型的核心是数据。模型从数据中学习规律，数据越多、质量越高，模型通常就越强大。\n自动学习: 机器学习系统能够自动地从数据中发现模式和规律，无需人工明确指定规则。\n持续优化: 机器学习模型可以通过不断学习新的数据来提升性能，使其能够适应变化的环境。\n泛化能力: 训练好的模型不仅能处理训练数据，还能对未见过的新数据进行预测或决策。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习的主要类型",
    "href": "01_ml_basic.html#机器学习的主要类型",
    "title": "2  机器学习基础",
    "section": "2.2 机器学习的主要类型",
    "text": "2.2 机器学习的主要类型\n\n2.2.1 监督学习 (Supervised Learning)\n监督学习就像是有一位老师（监督者）指导计算机学习。我们提供给计算机带有”标签”的数据，标签就是我们希望模型预测的答案。例如，如果我们想让模型识别图片中的水果是苹果还是香蕉，我们就需要提供大量已经标记好（苹果或香蕉）的水果图片给模型学习。模型学习的目标就是找到输入数据（水果图片）和输出标签（苹果或香蕉）之间的关系。\n主要特点:\n\n需要使用带有标签的数据进行训练。\n目标是学习输入特征与输出标签之间的映射关系。\n主要解决分类和回归问题。分类问题是预测数据属于哪个类别（例如，垃圾邮件检测），回归问题是预测一个连续的数值（例如，房价预测）。\n\n现实生活案例:\n\n垃圾邮件检测: 通过分析邮件的内容（关键词、发件人等）来判断邮件是否为垃圾邮件。\n图像识别: 识别图片中的物体，例如人脸识别、交通标志识别等。\n语音识别: 将语音转换成文字。\n\n金融领域应用示例:\n\n信用评分: 根据用户的个人信息和交易记录预测其信用等级。\n股票价格预测: 预测股票未来价格的涨跌趋势。\n客户流失预测: 预测哪些客户可能在未来一段时间内停止使用某项金融服务。\n\n\n\n2.2.2 无监督学习 (Unsupervised Learning)\n无监督学习则像是让计算机在没有老师指导的情况下，自己去探索数据的内在结构和模式。我们提供给计算机的数据没有标签，模型需要自己去发现数据中的隐藏信息。例如，给计算机一大堆新闻报道，让它自己将这些新闻按照主题进行分类，这就是一个无监督学习的任务。\n主要特点:\n\n使用没有标签的数据进行学习。\n目标是发现数据中的内在结构、模式或关系。\n常用于聚类、降维和关联规则挖掘等任务。聚类是将相似的数据点 grouping 在一起，降维是在保留数据主要信息的同时减少数据的维度，关联规则挖掘是发现数据中不同项之间的关联关系。\n\n现实生活案例:\n\n客户分群: 根据用户的购买行为将用户分成不同的群体，以便进行个性化营销。\n社交网络分析: 分析社交网络中用户之间的关系，发现社区结构或影响力中心。\n异常检测: 在大量数据中找出异常或不正常的点，例如信用卡欺诈检测。\n\n金融领域应用示例:\n\n投资组合风险分析: 通过聚类分析将不同的投资资产进行分类，评估投资组合的风险。\n市场细分: 将市场上的客户按照不同的特征进行细分，以便更好地了解市场需求。\n交易异常检测: 检测金融市场中不正常的交易行为，例如内幕交易或市场操纵。\n\n\n\n2.2.3 强化学习 (Reinforcement Learning)\n强化学习更像是训练一只宠物。我们不直接告诉宠物应该做什么，而是通过奖励或惩罚来引导它学习。计算机作为一个”智能体”，在与环境的交互中不断尝试不同的动作。如果某个动作让它达到了目标（例如，在游戏中获得高分，或者在交易中获得盈利），我们就给予奖励；如果动作不好，就给予惩罚。通过不断地试错和学习，智能体最终学会如何在特定环境中做出最优的决策，以获得最大的累积奖励。\n主要特点:\n\n通过与环境的交互进行学习。\n通过奖励和惩罚来指导学习方向。\n目标是学习在特定环境中采取最优的行动策略，以最大化累积奖励。\n适合解决序贯决策问题，即一系列连续决策的问题。\n\n现实生活案例:\n\n游戏AI: 训练AI玩游戏，例如围棋、象棋、电子游戏等。\n机器人控制: 训练机器人完成各种任务，例如自动驾驶、物体抓取等。\n推荐系统优化: 通过用户与推荐系统的交互（点击、购买等）来优化推荐策略。\n\n金融领域应用示例:\n\n自动化交易: 开发自动交易程序，根据市场情况自动进行买卖操作。\n投资组合管理: 动态调整投资组合，以最大化收益并控制风险。\n订单执行优化: 优化股票交易的订单执行策略，以降低交易成本。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习项目流程",
    "href": "01_ml_basic.html#机器学习项目流程",
    "title": "2  机器学习基础",
    "section": "2.3 机器学习项目流程",
    "text": "2.3 机器学习项目流程\n一个完整的机器学习项目通常包含以下几个关键步骤：\n\n数据收集与预处理:\n\n数据获取: 收集项目所需的数据。数据来源可能包括数据库、文件、网络爬虫、传感器等等。\n数据清洗: 处理数据中的缺失值、异常值、重复值和错误数据，确保数据质量。\n特征工程: 从原始数据中提取有用的特征，或者创建新的特征，以便模型更好地学习。特征工程是机器学习项目中非常重要的一步，好的特征能够显著提升模型性能。\n\n模型选择与训练:\n\n选择合适的算法: 根据问题的类型（分类、回归、聚类等）和数据的特点，选择合适的机器学习算法。例如，对于分类问题可以选择逻辑回归、支持向量机、决策树、随机森林等算法。\n划分数据集: 将数据集划分为训练集、验证集和测试集。训练集用于训练模型，验证集用于调整模型参数，测试集用于评估模型的泛化能力。\n模型训练与调参: 使用训练集数据训练模型，并使用验证集调整模型参数，例如超参数优化。目标是找到在验证集上表现最好的模型参数。\n\n模型评估与优化:\n\n性能评估: 使用测试集评估模型的性能。根据问题的类型选择合适的评估指标，例如准确率、精确率、召回率、F1 值（分类问题），均方误差、平均绝对误差（回归问题）等。\n模型调优: 如果模型性能不理想，需要进一步分析原因，并进行模型调优。调优方法可能包括：调整模型参数、尝试不同的算法、改进特征工程、增加数据量等。\n结果分析: 分析模型的预测结果，理解模型的优点和不足，为后续的模型改进提供方向。\n\n模型部署与监控:\n\n模型部署: 将训练好的模型部署到实际应用环境中。部署方式可能包括将模型集成到应用程序中、部署为 Web 服务等。\n性能监控: 在模型上线运行后，需要持续监控模型的性能。因为实际应用环境中的数据分布可能会发生变化（即”概念漂移”），导致模型性能下降。\n定期更新: 根据监控结果，定期使用新的数据重新训练模型，或者调整模型参数，以保持模型的性能和适应性。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#总结",
    "href": "01_ml_basic.html#总结",
    "title": "2  机器学习基础",
    "section": "2.4 总结",
    "text": "2.4 总结\n\n机器学习是一种强大的数据分析和预测工具，能够从数据中自动学习模式和规律。\n监督学习、无监督学习和强化学习是机器学习的三种主要类型，它们适用于不同的问题场景。\n机器学习在金融领域有着广泛的应用前景，可以用于风险管理、投资决策、客户服务等多个方面。\n成功应用机器学习需要一个完整的项目流程，包括数据准备、模型构建、评估优化和部署监控等环节。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html",
    "href": "lab02_data.html",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "3.1 内容概要",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#内容概要",
    "href": "lab02_data.html#内容概要",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "金融数据获取\n\n股票、债券、期货市场数据\n数据接口 (Tushare, Yahoo Finance)\n上市公司财务报表数据\n金融文本数据\n\nPython数据分析基础\n\nNumPy, Pandas 常用功能\n数据预处理与清洗\n探索性数据分析 (EDA)\n\nAI辅助编程实践\n\n代码生成、解释、优化\n最佳实践案例",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#金融数据获取",
    "href": "lab02_data.html#金融数据获取",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.2 金融数据获取",
    "text": "3.2 金融数据获取\n\n3.2.1 股票市场数据\n\n数据类型：\n\n基本行情数据：股票代码、名称、交易所、行业\n交易数据：开盘价、收盘价、最高价、最低价、成交量、成交额\n财务数据：资产负债表、利润表、现金流量表、财务指标 (ROE, EPS, PE)\n股东信息：股东户数、十大股东\n分红送股：分红金额、送股比例\n\n常用数据源：\n\nCSMAR (csmar.com): 国泰安金融数据库，数据全面，学校IP范围内免费使用\nCNRDS (cnrds.com): 中国研究数据服务平台，另类数据全面，可向学校申请账号\nTushare (tushare.pro): 国内股票数据接口，数据全面，API友好 (稍后详细介绍)\nYahoo Finance (finance.yahoo.com): 全球股票数据，免费API (yfinance Python库)\n交易所官方API: 上海证券交易所 (sse.com.cn), 深圳证券交易所 (szse.cn) - 数据权威，但API要走申请流程\n券商API: 部分券商提供API接口，方便交易和数据获取 (例如：同花顺, 东方财富)\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商，数据质量高，但价格昂贵 (机构常用)\n\n\n\n\n3.2.2 债券市场数据\n\n数据类型：\n\n债券基本信息：债券代码、名称、发行人、债券类型、票面利率、到期日\n债券交易数据：成交价、收益率、成交量\n债券估值数据：中债估值、市场估值\n债券评级：评级机构、评级结果\n债券发行数据：发行规模、发行利率\n\n常用数据源：\n\nCSMAR (csmar.com): 国泰安金融数据库，学校IP范围内免费使用\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n中债网 (chinabond.com.cn): 中国债券信息网，官方数据\n交易所债券信息平台: 上海证券交易所债券信息平台, 深圳证券交易所债券信息平台\n\n\n\n\n3.2.3 期货市场数据\n\n数据类型：\n\n期货合约信息：合约代码、标的资产、交易单位、最小变动价位、交割月份\n期货交易数据：开盘价、收盘价、最高价、最低价、成交量、持仓量\n期货指数数据：商品期货指数、股指期货指数\n期货仓单数据：仓单数量、注册仓单、有效预报\n期货持仓排名：期货交易所公布的持仓排名数据\n\n常用数据源：\n\nCTP接口: 期货公司提供的交易接口，可以获取实时行情和历史数据 (专业交易者常用)\n同花顺, 文华财经: 金融软件，提供期货行情和数据\n期货交易所网站: 各期货交易所 (例如：上海期货交易所, 大连商品交易所, 郑州商品交易所) 网站通常提供数据下载\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n\n\n\n\n3.2.4 数据接口使用：Tushare\n\nTushare Pro (tushare.pro): 注册认证后可获取更丰富的数据和更高的API访问权限 (收费)\n安装: pip install tushare\n初始化: 需要token (注册Tushare Pro后获取)\nimport tushare as ts\n\n# 初始化 pro 接口\npro = ts.pro_api('YOUR_TOKEN') # 替换为你的token\n常用API示例：\n\n获取股票列表: pro.stock_basic()\n获取股票日线行情: ts.get_k_data('600519', start='2023-01-01', end='2023-01-31') (旧接口) 或 pro.daily(ts_code='600519.SH', start_date='20230101', end_date='20230131') (Pro接口)\n获取公司财务报表: pro.fina_indicator(ts_code='600519.SH', period='20221231')\n更多API: 参考 Tushare 官方文档 (https://tushare.pro/document/2)\n\n注意事项:\n\nAPI访问频率限制: 免费用户有访问频率限制，避免频繁调用\n数据权限: 不同级别用户权限不同，部分数据需要Pro会员\n数据质量: 注意核对数据质量，不同接口数据可能存在差异\n\n\n\n\n3.2.5 数据接口使用：Yahoo Finance\n\nyfinance 库主要用于获取海外股票数据，国内A股数据质量可能不如 Tushare 等国内接口，因此本课程示例主要使用 Tushare 获取A股数据。 Yahoo Finance 示例如下，如果需要分析海外股票，可以使用 yfinance。\nimport yfinance as yf\n\n# 下载 苹果 (AAPL) 股票数据\naapl = yf.Ticker(\"AAPL\")\n\n# 获取历史数据\nhist = aapl.history(period=\"5y\") # 5年历史数据\nprint(hist.head())\n\n# 获取公司信息\ninfo = aapl.info\nprint(info)\n\n# 获取分红信息\ndividends = aapl.dividends\nprint(dividends)\n\n# 更多功能参考 yfinance 文档\n优点: 免费，全球股票数据，使用简单 (如果分析海外股票)\n缺点: A 股数据质量可能不如国内专业数据源，API 稳定性可能不如官方接口，文档相对简单，A 股代码可能需要调整\n\n\n\n3.2.6 上市公司财务报表数据\n\n数据类型:\n\n资产负债表: 反映公司在特定时点的资产、负债和所有者权益状况\n利润表: 反映公司在特定期间的经营成果 (收入、成本、利润)\n现金流量表: 反映公司在特定期间的现金流入和流出\n财务指标: 根据财务报表计算的各种指标，例如：盈利能力指标 (ROE, ROA, 净利润率), 偿债能力指标 (资产负债率, 流动比率), 运营能力指标 (存货周转率, 应收账款周转率), 成长能力指标 (营业收入增长率, 净利润增长率)\n\n数据来源:\n\nCSMAR (csmar.com): 国泰安，国内权威的金融数据库，数据质量高，但收费，高校和研究机构常用\nCNRDS (cnrds.com): 中国研究数据服务平台，国内较全面的研究数据平台，数据覆盖范围广，部分数据收费，学术研究常用\nWind (wind.com.cn): 专业金融数据服务商，提供全面的财务报表和财务指标数据，收费昂贵，金融机构常用\n巨潮资讯网 (cninfo.com.cn): 免费的上市公司公告平台，包含上市公司定期报告 (年报、季报)，可以从中获取财务报表数据，但需要自行解析和整理\n\n\n\n\n3.2.7 金融文本数据\n\n数据类型:\n\n新闻: 上市公司新闻、行业新闻、宏观经济新闻\n公告: 上市公司公告、证监会公告、交易所公告\n研报: 券商研报、基金研报、保险研报\n社交媒体: 上市公司社交媒体动态、投资者互动平台言论\n\n数据来源:\n\n巨潮资讯网 (cninfo.com.cn): 官方指定信息披露平台，提供最权威的上市公司公告、证监会公告、交易所公告等数据，但需要自行解析和整理\n新浪财经 (finance.sina.com.cn): 综合性财经门户，以新闻资讯见长，提供及时的市场动态和深度分析，同时也有公告和研报数据\n东方财富网 (eastmoney.com): 专业财经平台，特色是提供全面的市场数据和投资工具，新闻、公告、研报等数据较为系统\n同花顺 (10jqka.com): 老牌股票软件，以实时行情和交易功能为主，同时提供新闻、公告、研报等数据，适合投资者使用\n雪球 (xueqiu.com): 投资者社交平台，特色是用户生成内容，提供新闻、公告、研报等数据的同时，还有丰富的投资者讨论和观点",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#python数据分析基础",
    "href": "lab02_data.html#python数据分析基础",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.3 Python数据分析基础",
    "text": "3.3 Python数据分析基础\n\n3.3.1 NumPy 基础\n\nNumPy: 基于Python的科学计算库，提供高效的多维数组对象和工具，用于数据分析和科学计算\n核心功能:\n\n数组操作: 创建、操作、转换数组\n数学运算: 线性代数、傅里叶变换、随机数生成\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n常用操作:\nimport numpy as np\nimport pandas as pd\n\n# 假设已下载茅台日线数据\n# CSV 文件包含 Date 和 Close 列\ndf = pd.read_csv('data/CSMAR/moutai_daily/TRD_Dalyr.csv')\nclose_prices = df['Clsprc'].values\n\n# 计算平均收盘价\navg_price = np.mean(close_prices)\nprint(f\"平均收盘价: {avg_price:.2f}\")\n\n# 计算收盘价的标准差\nstd_price = np.std(close_prices)\nprint(f\"收盘价标准差: {std_price:.2f}\")\n\n\n\n3.3.2 Pandas 基础\n\nPandas (Panel Data): 基于NumPy的数据分析库，提供 Series (一维带标签数组) 和 DataFrame (二维表格型数据) 数据结构\n核心功能:\n\n数据结构: Series 和 DataFrame，方便数据表示和操作\n数据清洗: 处理缺失值、重复值、异常值\n数据预处理: 数据转换、数据标准化、特征工程\n数据分析: 数据选择、过滤、排序、分组聚合、透视表\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n实践示例: 茅台股票数据分析:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 基础数据处理\ndf['trade_date'] = pd.to_datetime(df['trade_date'])  # 转换日期格式\ndf = df.sort_values('trade_date')  # 按日期排序\n\n# 计算基本指标\ndf['daily_return'] = df['close'].pct_change()  # 日收益率\ndf['MA5'] = df['close'].rolling(window=5).mean()  # 5日均线\ndf['MA20'] = df['close'].rolling(window=20).mean()  # 20日均线\ndf['volatility'] = df['daily_return'].rolling(window=20).std() * np.sqrt(252)  # 20日年化波动率\n\n# 数据分析示例\nprint(\"\\n基本统计信息:\")\nprint(df[['close', 'daily_return', 'volatility']].describe())\n\nprint(\"\\n交易量最大的5天:\")\nprint(df.nlargest(5, 'vol')[['trade_date', 'close', 'vol']])\n\n# 计算每周平均收盘价和成交量\nweekly_stats = df.set_index('trade_date').resample('W').agg({\n    'close': 'mean',\n    'vol': 'sum'\n})\nprint(\"\\n每周统计:\")\nprint(weekly_stats.head())\n\n# 可视化分析\nplt.figure(figsize=(15, 10))\n\n# 绘制K线图和均线\nplt.subplot(2, 1, 1)\nplt.plot(df['trade_date'], df['close'], label='收盘价')\nplt.plot(df['trade_date'], df['MA5'], label='5日均线')\nplt.plot(df['trade_date'], df['MA20'], label='20日均线')\nplt.title('贵州茅台股价走势')\nplt.legend()\nplt.grid(True)\n\n# 绘制成交量和波动率\nplt.subplot(2, 1, 2)\nplt.bar(df['trade_date'], df['vol'], alpha=0.5, label='成交量')\nplt.plot(df['trade_date'], df['volatility'] * 1000000, 'r', label='波动率(放大1000000倍)')\nplt.title('成交量和波动率')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n练习建议:\n\n尝试修改上述代码，计算不同时间窗口的均线（如10日、30日均线）\n添加其他技术指标的计算（如RSI、MACD）\n尝试对比茅台与其他白酒股的表现\n探索不同的可视化方式（如蜡烛图）\n\n\n\n\n3.3.3 数据预处理与清洗\n\n数据质量问题:\n\n缺失值 (Missing Values): 数据记录中某些字段为空 (例如：股票停牌日可能成交量为缺失值)\n异常值 (Outliers): 与其他数据明显偏离的值 (例如：交易数据中的错误记录)\n重复值 (Duplicates): 重复的数据记录\n数据不一致 (Inconsistent Data): 同一信息在不同数据源中表示不一致\n数据类型错误 (Data Type Errors): 例如：数值型字段存储为字符串\n\n数据预处理步骤:\n\n数据清洗 (Data Cleaning): 处理缺失值、异常值、重复值、数据不一致等\n数据转换 (Data Transformation): 数据类型转换、数据格式转换、数据编码 (例如：One-Hot Encoding)\n数据标准化/归一化 (Data Scaling/Normalization): 将数据缩放到特定范围，消除量纲影响 (例如：Min-Max Scaling, Standardization)\n特征选择/特征构建 (Feature Selection/Feature Engineering): 选择重要特征，构建新特征 (后续章节详细介绍)\n\n\n\n\n3.3.4 探索性数据分析 (EDA)\n\n目的: 初步了解数据特征、发现数据规律、为后续建模提供方向\n常用方法:\n\n描述性统计: 均值、中位数、标准差、分位数、最大值、最小值等，了解数据分布和集中趋势 (例如：分析股票收盘价的统计特征)\n数据可视化: 直方图、箱线图、散点图、折线图、热力图等，直观展示数据分布、关系和异常 (例如：绘制股票价格走势图、成交量直方图)\n相关性分析: 计算特征之间的相关性，了解特征之间的关系 (例如：分析股票收益率与成交量之间的相关性)\n分组分析: 按类别分组，比较不同组别的数据特征差异 (例如：按行业分组，比较不同行业股票的盈利能力)\n\n常用可视化工具:\n\nMatplotlib: Python 基础绘图库，功能强大，定制性强\nSeaborn: 基于Matplotlib的高级可视化库，更美观，更方便绘制统计图表\nPlotly: 交互式可视化库，可创建动态图表",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#ai-辅助编程实践",
    "href": "lab02_data.html#ai-辅助编程实践",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.4 AI 辅助编程实践",
    "text": "3.4 AI 辅助编程实践\n\n3.4.1 代码生成与解释\n\nAI 代码生成工具: GitHub Copilot\nInline代码生成: 根据自然语言描述或代码上下文，自动生成代码片段或完整函数。\nAsk Copilot: 使用Ask Copilot，AI 可以解释、修改选定代码的功能和逻辑。\nEdit with Copilot: 使用Edit with Copilot，AI 可以根据多个文件的上下文，生成、修改代码。\n\n\n\n3.4.2 关于数据分析的具体操作建议\n\n数据读入: 新建一个文件夹，将所有数据文件放在该文件夹下的 data 文件夹中，然后使用 Pandas 读入数据。\n数据清洗: 一次做一步，每一步都要检查。通过 print 输出中间结果，检查每一步的输出是否符合预期。\n\n检查观测值数量是否与原数据一致；\n检查变量名是否与原数据一致；\n检查数据类型是否符合预期；\n如果数据量很大，可以先读入一部分数据，检查数据是否符合预期，再决定是否读入全部数据。\n\n处理报错: 如果遇到报错，先定位报错信息位置，检查那一段代码，可以把错误发给AI，让它帮你修改代码。如果代码没有问题，再检查上一步代码中的数据输出。\n自然语言编程: 可以直接写注释，AI会根据你的注释生成代码。检查并运行代码。\n\n检查代码是否使用正确的数据与变量名；\n运行代码，检查代码的输出是否符合预期，如果符合预期，再进行下一步。\n如果代码涉及循环，可以先写一个，运行通过后，再改成循环。\n\n慢就是快：代码需要一步一步生成，不要一次性生成与运行多行代码。勤检查，勤运行，勤对比。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#总结",
    "href": "lab02_data.html#总结",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.5 总结",
    "text": "3.5 总结\n\n数据是机器学习的基石：高质量的数据是构建有效模型的关键。\n金融数据获取多样化：掌握不同数据源和API接口，灵活获取所需数据，包括股票、债券、期货、财务报表和金融文本数据。\nPython 数据分析是必备技能：熟练运用 NumPy 和 Pandas 进行金融数据处理和分析。\nEDA 帮助理解数据：通过探索性数据分析，发现金融数据规律，为建模提供方向。\nAI 辅助编程提升效率：善用 AI 工具，提高金融数据获取和分析效率。 熟练使用AI工具，可以显著提升开发效率。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "03_supervised.html",
    "href": "03_supervised.html",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "4.1 监督学习简介\n监督学习是机器学习的一个重要分支，它是指从带有标签的数据中自动学习规律和模式，并利用这些规律和模式对新数据进行预测和决策的过程。在监督学习中，我们拥有一个包含输入特征 \\(\\mathbf{x}\\) 和对应输出标签 \\(y\\) 的数据集，模型的目标是学习一个从输入特征到输出标签的映射关系。监督学习在量化投资、金融科技等领域有广泛应用，例如：\n由于课程时间有限，本讲义将重点介绍监督学习中的回归、分类和集成学习，以及它们在金融预测中的应用。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习简介",
    "href": "03_supervised.html#监督学习简介",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "风险评估：根据客户的历史信用数据（特征）预测其信用风险等级（标签）。\n欺诈检测：基于交易记录（特征）识别欺诈交易（标签）。\n量化交易：预测股票价格走势（标签）以辅助交易决策（特征）。\n客户细分：根据客户特征（特征）预测客户所属类别（标签），进行精准营销。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习模型详解",
    "href": "03_supervised.html#监督学习模型详解",
    "title": "4  监督学习（上）",
    "section": "4.2 监督学习模型详解",
    "text": "4.2 监督学习模型详解\n\n4.2.1 回归问题描述\n回归问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测连续数值型的结果 \\(y\\)。 给定训练数据 \\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(\\mathbf{x}_i\\) 是第 \\(i\\) 个样本的特征向量， \\(y_i\\) 是对应的真实值， \\(N\\) 是样本数量。我们的目标是找到一个函数 \\(f\\)，使得对于新的输入 \\(\\mathbf{x}\\)，模型预测值 \\(\\hat{y} = f(\\mathbf{x})\\) 尽可能接近真实值 \\(y\\)。假设真实值 \\(y_i\\) 与预测函数 \\(f(\\mathbf{x}_i)\\) 之间存在如下关系：\n\\[y_i = f(\\mathbf{x}_i) + \\epsilon_i\\]\n其中 \\(\\epsilon_i\\) 代表随机误差项，通常假设其服从均值为 0 的正态分布。在实际应用中，我们通常将观测值堆叠成矩阵和向量的形式，方便模型表达和计算：\n\n\\(N \\times 1\\) 维结果向量 \\(\\mathbf{y} = (y_1, y_2, ..., y_N)^T\\)\n\\(N \\times K\\) 维特征矩阵 \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N)^T\\)，每一行代表一个样本，每一列代表一个特征。\n\\(N \\times 1\\) 维误差向量 \\(\\mathbf{\\epsilon} = (\\epsilon_1, \\epsilon_2, ..., \\epsilon_N)^T\\)\n\n回归模型可以简洁地写为： \\(\\mathbf{y} = f(\\mathbf{X}) + \\mathbf{\\epsilon}\\)。我们的目标是通过训练数据学习到函数 \\(f\\) 的具体形式，从而能够对新的样本 \\(\\mathbf{x}\\) 进行预测。\n\n\n4.2.2 线性回归 (Linear Regression)\n线性回归模型是最简单且应用广泛的回归模型。它假设结果变量 \\(y\\) 与特征向量 \\(\\mathbf{x}\\) 之间存在线性关系。线性回归模型易于理解和实现，是许多复杂模型的基础。\n模型表达式:\n线性回归模型假设预测函数 \\(f(\\mathbf{x})\\) 是特征 \\(\\mathbf{x}\\) 的线性组合，模型表达式如下：\n\\(y = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\)\n或者对于单个样本 \\(i\\)，可以表示为：\n\\(y_i = \\mathbf{x}_i^T \\mathbf{\\beta} + \\epsilon_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_K x_{iK} + \\epsilon_i\\)\n其中： - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是 \\((K+1) \\times 1\\) 维回归系数向量，\\(\\beta_0\\) 是截距项（bias），\\(\\beta_1, ..., \\beta_K\\) 是特征的系数。为了方便表示，我们通常在特征矩阵 \\(\\mathbf{X}\\) 中添加一列全为 1 的列向量，对应于截距项 \\(\\beta_0\\)。 - \\(\\mathbf{x}_i = (1, x_{i1}, x_{i2}, ..., x_{iK})^T\\) 是 \\((K+1) \\times 1\\) 维增广特征向量，包含了常数项 1 和原始特征。 - \\(\\epsilon_i\\) 是误差项。\n最优化方法：最小二乘法 (OLS)\n线性回归的目标是找到最优的回归系数 \\(\\mathbf{\\beta}\\)，使得模型的预测值 \\(\\mathbf{X}\\mathbf{\\beta}\\) 与真实值 \\(\\mathbf{y}\\) 之间的误差平方和 (Sum of Squared Errors, SSE) 最小。最小二乘法 (Ordinary Least Squares, OLS) 是一种常用的求解线性回归模型参数的方法。其目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^T \\mathbf{\\beta})^2\\]\n为了求解最优的 \\(\\mathbf{\\beta}\\)，我们可以对目标函数 \\(L(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到正规方程 (Normal Equation)：\n\\[\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = \\mathbf{X}^T\\mathbf{y}\\]\n如果矩阵 \\(\\mathbf{X}^T\\mathbf{X}\\) 可逆（即满秩），则可以得到普通最小二乘 (OLS) 估计量的解析解：\n\\[\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n高维环境下的过拟合问题与正则化:\n在高维环境中，当特征数量 \\(K\\) 相对于观测数量 \\(N\\) 来说较大时（例如 \\(K &gt; N\\)，或 \\(K\\) 接近 \\(N\\)），OLS 估计可能会出现过拟合 (Overfitting) 问题。过拟合是指模型在训练集上表现得非常好（例如，训练误差很小），但在未见过的测试集上泛化能力很差，预测性能下降。这是因为在高维情况下，模型参数过多，容易捕捉到训练数据中的噪声和随机波动，而不是真实的 underlying pattern。\n为了解决过拟合问题，提高模型的泛化能力，可以引入正则化 (Regularization) 方法。正则化通过在损失函数中添加惩罚项，限制模型复杂度，从而避免模型过度拟合训练数据。常用的正则化方法包括 岭回归 (Ridge Regression) 和 Lasso 回归 (Lasso Regression)。\n\n\n4.2.3 岭回归 (Ridge Regression)\n岭回归是一种改进的线性回归方法，也称为 \\(L_2\\) 正则化线性回归。它通过在最小二乘法的损失函数中添加 \\(L_2\\) 范数惩罚项来对回归系数进行 shrinkage (收缩)，限制回归系数的大小，从而降低模型的复杂度和过拟合风险。岭回归特别适用于处理多重共线性问题，即特征之间存在高度相关性的情况。\n模型表达式:\n岭回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Ridge}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} \\right] \\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差 (Mean Squared Error, MSE) 项，衡量模型预测值与真实值之间的平均误差平方。 - \\(\\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} = \\lambda ||\\mathbf{\\beta}||_2^2 = \\lambda \\sum_{j=0}^{K} \\beta_j^2\\) 是 \\(L_2\\) 范数惩罚项，也称为 权重衰减项 (Weight Decay)。它惩罚回归系数 \\(\\mathbf{\\beta}\\) 的平方和，迫使系数趋向于较小的值。 - \\(\\lambda \\ge 0\\) 是正则化参数 (Regularization Parameter)，也称为 惩罚系数。它控制惩罚项的强度。\\(\\lambda\\) 越大，惩罚越强，回归系数越趋向于 0。当 \\(\\lambda = 0\\) 时，岭回归退化为普通线性回归。\n估计方法:\n类似于线性回归，我们可以对岭回归的目标函数 \\(L_{Ridge}(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到岭回归的估计结果：\n\\[\\hat{\\mathbf{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n其中 \\(\\mathbf{I}_{K+1}\\) 是 \\((K+1) \\times (K+1)\\) 单位矩阵。通过向 \\(\\mathbf{X}^T\\mathbf{X}\\) 添加对角矩阵 \\(\\lambda \\mathbf{I}_{K+1}\\)（即”岭”），可以使得在求逆运算时，即使 \\(\\mathbf{X}^T\\mathbf{X}\\) 接近奇异矩阵（例如，当存在多重共线性时），\\((\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})\\) 仍然具有较好的可逆性，保证了解的稳定性。并且 \\(\\lambda \\mathbf{I}_{K+1}\\) 的存在将导致回归系数 \\(\\hat{\\mathbf{\\beta}}_{Ridge}\\) 向零收缩。\n岭回归的特点:\n\n\\(L_2\\) 正则化：使用 \\(L_2\\) 范数惩罚项，将回归系数向零收缩，但不会精确地变为 0。\n缓解多重共线性：通过引入正则化项，降低了模型对特征之间相关性的敏感度，可以缓解多重共线性问题，提高模型稳定性。\n降低过拟合风险：通过限制模型复杂度，有效降低过拟合风险，提高模型的泛化能力。\n无法进行特征选择：岭回归会缩小所有特征的系数，但不会将任何系数精确地设置为 0，因此无法进行特征选择。\n\n\n\n4.2.4 Lasso 回归 (Lasso Regression)\nLasso (Least Absolute Shrinkage and Selection Operator) 回归是另一种常用的正则化线性回归方法，也称为 \\(L_1\\) 正则化线性回归。与岭回归不同，Lasso 回归使用 \\(L_1\\) 范数惩罚项进行正则化。 \\(L_1\\) 正则化不仅可以进行系数 shrinkage，更重要的是，它具有 特征选择 (Feature Selection) 的能力，可以将一些不重要特征的回归系数压缩为 精确的 0，从而得到 稀疏模型 (Sparse Model)。稀疏模型更易于解释，并且可以提高模型的泛化能力。\n模型表达式:\nLasso 回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Lasso}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma \\sum_{j=1}^{K} |\\beta_j| \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 仍然是均方误差项。 - \\(\\gamma \\sum_{j=1}^{K} |\\beta_j| = \\gamma ||\\mathbf{\\beta}_{1:K}||_1 = \\gamma (|\\beta_1| + |\\beta_2| + ... + |\\beta_K|)\\) 是 \\(L_1\\) 范数惩罚项，注意这里只惩罚了特征系数 \\(\\beta_1, ..., \\beta_K\\)，不惩罚截距项 \\(\\beta_0\\)。 \\(L_1\\) 范数惩罚项迫使一些回归系数变为 0。 - \\(\\gamma \\ge 0\\) 是 正则化参数，控制 \\(L_1\\) 惩罚项的强度。\\(\\gamma\\) 越大，惩罚越强，更多的回归系数会被压缩为 0。\n估计方法:\n与岭回归不同，Lasso 回归的目标函数由于包含 \\(L_1\\) 范数项，在 \\(\\beta_j = 0\\) 处不可导，因此 没有解析解。通常需要使用数值优化算法（如坐标轴下降法 (Coordinate Descent)、近端梯度下降法 (Proximal Gradient Descent)）进行求解。\nLasso 回归的特点:\n\n\\(L_1\\) 正则化：使用 \\(L_1\\) 范数惩罚项，不仅可以进行系数 shrinkage，还可以将一些不重要特征的回归系数压缩为精确的 0，实现特征选择。\n稀疏模型：Lasso 回归可以产生稀疏模型，即模型中只有少数特征的系数非零，这有助于模型解释和提高泛化能力。\n特征选择能力：在特征选择方面优于岭回归。Lasso 回归可以自动选择重要的特征，去除冗余和不相关的特征。\n适用于高维稀疏数据：Lasso 回归特别适用于处理高维稀疏数据，例如文本数据、基因数据等。\n\n\n\n4.2.5 弹性网 (Elastic Net)\n弹性网 (Elastic Net) 是一种结合了岭回归和 Lasso 回归的正则化方法，可以看作是岭回归和 Lasso 回归的折衷。弹性网同时使用 \\(L_1\\) 范数和 \\(L_2\\) 范数惩罚项进行正则化。弹性网同时使用 \\(L_1\\) 正则化和 \\(L_2\\) 正则化，综合利用 \\(L_1\\) 正则化的特征选择能力和 \\(L_2\\) 正则化的稳定性和 shrinkage 能力。在某些情况下，弹性网的性能优于单独的岭回归和 Lasso 回归，尤其是在特征之间高度相关时，弹性网表现更稳定。\n模型表达式:\n弹性网的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{ElasticNet}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma_1 \\sum_{j=1}^{K} |\\beta_j| + \\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta} \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差项。 - \\(\\gamma_1 \\sum_{j=1}^{K} |\\beta_j|\\) 是 \\(L_1\\) 范数惩罚项，用于特征选择和产生稀疏模型。 - \\(\\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta}\\) 是 \\(L_2\\) 范数惩罚项，用于系数 shrinkage 和缓解多重共线性。 - \\(\\gamma_1 \\ge 0\\) 和 \\(\\gamma_2 \\ge 0\\) 分别是 \\(L_1\\) 正则化参数 和 \\(L_2\\) 正则化参数，控制两种惩罚项的强度。通常需要通过交叉验证等方法来选择合适的 \\(\\gamma_1\\) 和 \\(\\gamma_2\\) 值。\n弹性网的特点:\n\n结合 \\(L_1\\) 和 \\(L_2\\) 正则化：弹性网同时使用 \\(L_1\\) 和 \\(L_2\\) 范数惩罚项，结合了两者的优点。\n既可以进行特征选择，又可以进行系数 shrinkage：弹性网既可以像 Lasso 回归一样进行特征选择，将一些不重要特征的系数压缩为 0，又可以像岭回归一样进行系数 shrinkage，缩小系数的整体大小，提高模型稳定性。\n性能更稳定：在某些情况下，弹性网的预测性能和鲁棒性优于岭回归和 Lasso 回归。\n处理特征高度相关性：当特征之间高度相关时，Lasso 回归可能随机选择其中一个特征，而弹性网倾向于选择一组相关的特征，表现更稳定。\n\n\n\n4.2.6 分类问题描述\n分类问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测离散类别型的结果 \\(y\\)。分类问题的目标是学习一个模型，将输入样本 \\(\\mathbf{x}\\) 划分到预定义的类别中。根据类别数量的不同，分类问题可以分为：\n\n二分类 (Binary Classification)：预测结果 \\(y\\) 只有两个类别，通常表示为 \\(y \\in \\{0, 1\\}\\) (或 \\(y \\in \\{-1, +1\\}\\))。例如，判断邮件是否为垃圾邮件（是/否），预测用户是否会点击广告（点击/不点击），识别交易是否为欺诈交易（欺诈/正常）。\n多分类 (Multiclass Classification)：预测结果 \\(y\\) 有两个以上的类别，表示为 \\(y \\in \\{C_1, C_2, ..., C_L\\}\\)，其中 \\(C_i\\) 是类别标签， \\(L \\ge 3\\) 是类别数量。例如，图像分类（猫、狗、鸟、鱼等），文本分类（政治、经济、体育、娱乐等），客户类型分类（高价值客户、中价值客户、低价值客户）。\n\n对于多分类问题，常用的处理策略是 “拆解法” (Decomposition)，即将多分类任务拆解为若干个二分类任务求解。常见的拆解策略包括 一对一 (One-vs-One, OvO)、一对多 (One-vs-Rest, OvR) 和 多对多 (Many-vs-Many, MvM) 等。\n\n\n4.2.7 类别不平衡问题\n在分类任务中，经常会遇到不同类别的训练样本数量差别很大的情况，即 类别不平衡 (Class Imbalance) 问题。例如，在欺诈检测、罕见病诊断、自然灾害预测等领域，少数类样本 (Minority Class)（如欺诈交易、患病样本、地震）的数量通常远远少于多数类样本 (Majority Class)（如正常交易、健康样本、非地震）。类别不平衡问题会严重影响模型的学习效果，使得模型更倾向于预测样本数量较多的类别，而对少数类别的识别率很低。\n类别不平衡的影响:\n\n模型偏向多数类：模型在训练过程中更容易学习到多数类样本的特征，而忽略少数类样本的特征，导致模型预测结果偏向多数类。\n整体分类精度虚高：由于多数类样本数量占优，即使模型将所有样本都预测为多数类，也可能获得较高的整体分类精度 (Accuracy)。但这种高精度是没有意义的，因为模型对少数类的识别能力很差。\n评估指标失效：常用的评估指标（如准确率 Accuracy）在类别不平衡数据集上可能失效，无法真实反映模型的性能。我们需要使用更合适的评估指标，例如 精确率 (Precision)、召回率 (Recall)、F1 值 (F1-score)、AUC 值 (Area Under ROC Curve) 等。\n\n类别不平衡的解决方案:\n为了解决类别不平衡问题，提高模型对少数类别的识别能力，常用的解决方案包括：\n\n再缩放 (Rescaling) / 阈值调整 (Threshold Adjustment)：不改变原始模型，而是调整分类阈值 (Classification Threshold)，使得模型在类别不平衡时也能做出合理的预测。例如，对于逻辑回归或 SVM 等输出概率的模型，默认的分类阈值通常为 0.5。当类别不平衡时，可以将预测为正例的阈值从 0.5 调整为更小的值，例如 \\(\\frac{m^{+}}{m^{-} + m^{+}}\\), 其中 \\(m^{+}\\) 和 \\(m^{-}\\) 分别是正类（少数类）和负类（多数类）样本的数量。降低阈值会使得模型更容易将样本预测为正类，从而提高少数类的召回率。\n重采样 (Resampling)：通过改变训练集中不同类别样本的比例来缓解类别不平衡问题。重采样方法包括 欠抽样 (Undersampling) 和 过抽样 (Oversampling)。\n\n欠抽样 (Undersampling)：减少多数类样本的数量，随机删除一部分多数类样本，使得正负类样本数量接近平衡。欠抽样方法简单易行，但可能会丢失一部分多数类样本的信息，适用于数据量较大的情况。\n过抽样 (Oversampling)：增加少数类样本的数量，例如通过复制少数类样本或生成合成样本（如 SMOTE (Synthetic Minority Over-sampling Technique)）。过抽样方法可以保留所有原始多数类样本的信息，但可能会导致过拟合，适用于数据量较小的情况。SMOTE 算法通过在少数类样本之间进行插值生成新的合成样本，可以有效缓解过拟合问题。\n\n阈值移动 (Threshold-moving)：这是一种代价敏感学习 (Cost-sensitive learning) 的思想。基于原始训练集进行学习，但在用训练好的分类器进行预测时，根据类别不平衡的程度调整决策阈值。例如，如果少数类样本的误分类代价更高，则可以将决策阈值向多数类方向移动，使得模型更倾向于将样本预测为少数类。\n代价敏感学习 (Cost-sensitive learning)：为不同类别的误分类设置不同的代价 (Cost)，使得模型在训练时更加关注少数类样本，最小化总的期望代价而不是最小化分类错误率。例如，可以使用代价矩阵 (Cost Matrix) 来定义不同误分类情况的代价，然后在训练过程中根据代价矩阵调整模型的学习策略。\n集成学习方法：一些集成学习方法，如 集成学习 (Ensemble Learning) 方法，例如 EasyEnsemble、BalanceCascade 等，通过将数据集划分为多个子集，在每个子集上训练基学习器，然后集成多个基学习器的预测结果，可以有效提高模型在类别不平衡数据集上的性能。\n\n\n\n4.2.8 逻辑回归 (Logistic Regression)\n逻辑回归 (Logistic Regression) 是一种广泛使用的二分类模型。虽然名字带有”回归”，但逻辑回归实际上是一种分类算法，主要用于解决二分类问题。逻辑回归模型简单高效，易于解释，是许多分类问题的 baseline 模型。\n模型表达式:\n逻辑回归模型基于线性回归的思想，但通过引入 Sigmoid 函数 (Sigmoid Function) 或 Logistic 函数，将线性回归的输出值映射到 \\((0, 1)\\) 区间，使其具有概率意义，用于表示样本属于正类的概率。\n逻辑回归模型的表达式如下：\n\\[P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}}\\]\n其中： - \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\) 表示给定特征向量 \\(\\mathbf{x}\\) 和模型参数 \\(\\mathbf{\\beta}\\) 的条件下，样本属于正类 (y=1) 的概率。 - \\(\\mathbf{x} = (1, x_1, x_2, ..., x_K)^T\\) 是增广特征向量。 - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是模型参数，与线性回归中的回归系数类似。 - \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) 是 Sigmoid 函数，也称为 Logistic 函数。Sigmoid 函数将任意实数 \\(z\\) 映射到 \\((0, 1)\\) 区间，函数图像呈 S 形。当 \\(z \\rightarrow +\\infty\\) 时，\\(\\sigma(z) \\rightarrow 1\\)；当 \\(z \\rightarrow -\\infty\\) 时，\\(\\sigma(z) \\rightarrow 0\\)；当 \\(z = 0\\) 时，\\(\\sigma(z) = 0.5\\)。\n对于二分类问题，逻辑回归模型预测样本属于正类的概率 \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\)，则样本属于负类的概率为 \\(P(y=0|\\mathbf{x}; \\mathbf{\\beta}) = 1 - P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = 1 - \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\sigma(-\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{e^{-\\mathbf{x}^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}} = \\frac{1}{1 + e^{\\mathbf{x}^T \\mathbf{\\beta}}}\\)。\n模型训练：最大似然估计 (Maximum Likelihood Estimation, MLE)\n逻辑回归模型的训练目标是最大化训练数据的似然函数 (Likelihood Function)，即找到一组模型参数 \\(\\mathbf{\\beta}\\)，使得在给定这组参数下，训练数据出现的概率最大。对于二分类问题，逻辑回归的似然函数可以表示为：\n\\[L(\\mathbf{\\beta}) = \\prod_{i=1}^{N} [P(y_i=1|\\mathbf{x}_i; \\mathbf{\\beta})]^{y_i} [P(y_i=0|\\mathbf{x}_i; \\mathbf{\\beta})]^{1-y_i} = \\prod_{i=1}^{N} [\\sigma(\\mathbf{x}_i^T \\mathbf{\\beta})]^{y_i} [\\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]^{1-y_i}\\]\n为了方便优化，通常将似然函数取对数，得到对数似然函数 (Log-Likelihood Function)：\n\\[\\ell(\\mathbf{\\beta}) = \\ln L(\\mathbf{\\beta}) = \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})] = \\sum_{i=1}^{N} [y_i \\ln \\frac{1}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}} + (1-y_i) \\ln \\frac{e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}]\\]\n我们的目标是最大化对数似然函数 \\(\\ell(\\mathbf{\\beta})\\)，等价于最小化负对数似然函数 (Negative Log-Likelihood Function)：\n\\[J(\\mathbf{\\beta}) = -\\ell(\\mathbf{\\beta}) = - \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]\\]\n负对数似然函数 \\(J(\\mathbf{\\beta})\\) 也称为 交叉熵损失函数 (Cross-Entropy Loss Function) 或 Logistic Loss Function。\n最优化方法：梯度下降法 (Gradient Descent)\n逻辑回归模型通常使用梯度下降法 (Gradient Descent) 或其变种（如 随机梯度下降 (SGD)、小批量梯度下降 (Mini-batch GD)、Adam 等）来求解最优参数 \\(\\mathbf{\\beta}\\)，最小化交叉熵损失函数 \\(J(\\mathbf{\\beta})\\)。梯度下降法是一种迭代优化算法，通过不断沿着损失函数梯度 负方向 更新参数，逐步逼近最优解。\n决策边界 (Decision Boundary):\n逻辑回归模型的决策边界是线性的。当 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 时，\\(\\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = 0.5\\)，模型预测样本属于正类和负类的概率均为 0.5。因此，线性方程 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 定义了逻辑回归模型的决策边界，将特征空间划分为正类区域和负类区域。\n\n\n4.2.9 支持向量机 (Support Vector Machine, SVM)\n支持向量机 (Support Vector Machine, SVM) 是一种强大且广泛应用于分类和回归问题的监督学习模型。SVM 的核心思想是找到一个最优超平面 (Optimal Hyperplane)，将不同类别的样本最大程度地分开，同时使得分类间隔 (Margin) 最大化。SVM 在高维空间和非线性分类问题中表现出色，通过核技巧 (Kernel Trick) 可以有效地处理非线性可分数据。\n线性可分支持向量机 (Linearly Separable SVM) / 硬间隔 SVM (Hard Margin SVM):\n对于线性可分 (Linearly Separable) 的数据集，即存在一个超平面可以将不同类别的样本完全分开的情况，我们可以构建线性可分支持向量机，也称为 硬间隔 SVM。硬间隔 SVM 旨在找到一个最大间隔超平面，将两类样本完全正确地分开，并且使得间隔最大化。间隔是指超平面到最近的样本点（称为 支持向量 (Support Vector)）的距离。\n模型表达式:\n给定线性可分的训练数据集 \\(D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(y_i \\in \\{-1, +1\\}\\)。线性可分 SVM 的目标是找到一个超平面 \\((\\mathbf{w}, b)\\)，使得：\n\n正确分类: 所有样本都被正确分类，即对于 \\(y_i = +1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\ge +1\\)；对于 \\(y_i = -1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\le -1\\)。可以将两个不等式统一为： \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\)。\n间隔最大化: 最大化分类间隔 \\(Margin = \\frac{2}{||\\mathbf{w}||}\\)，等价于最小化 \\(||\\mathbf{w}||^2 = \\mathbf{w}^T \\mathbf{w}\\)。\n\n因此，线性可分 SVM 的最优化问题可以表示为：\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\]\n这是一个凸二次规划 (Convex Quadratic Programming, QP) 问题，可以使用现成的 QP 求解器求解。\n线性不可分支持向量机 (Linearly Inseparable SVM) / 软间隔 SVM (Soft Margin SVM):\n在实际应用中，很多数据集不是线性可分的，即不存在一个超平面可以将不同类别的样本完全分开。为了处理线性不可分数据，我们需要引入软间隔 SVM，也称为 线性支持向量机。软间隔 SVM 允许模型在一些样本上分类错误，但希望尽可能减少分类错误，同时保持间隔最大化。\n模型表达式:\n软间隔 SVM 通过引入松弛变量 (Slack Variables) \\(\\xi_i \\ge 0\\)，允许一些样本不满足硬间隔约束 \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1\\)。松弛变量 \\(\\xi_i\\) 表示第 \\(i\\) 个样本违反约束的程度。软间隔 SVM 的最优化问题变为：\n\\[\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0, \\quad i = 1, 2, ..., N\\]\n其中： - \\(\\frac{1}{2} ||\\mathbf{w}||^2\\) 仍然是间隔最大化项。 - \\(C \\sum_{i=1}^{N} \\xi_i\\) 是惩罚项，表示对误分类的惩罚。 \\(\\xi_i\\) 越大，误分类程度越高，惩罚越大。 - \\(C &gt; 0\\) 是 惩罚参数 (Penalty Parameter)，也称为 正则化参数。 \\(C\\) 控制对误分类的惩罚程度。 \\(C\\) 越大，对误分类的惩罚越大，模型越倾向于减小误分类，但可能会导致间隔变小，容易过拟合； \\(C\\) 越小，对误分类的惩罚越小，模型更容忍误分类，间隔可能更大，泛化能力可能更好。 \\(C\\) 的选择需要通过交叉验证等方法进行调优。\n核函数 (Kernel Function):\n对于非线性可分 (Nonlinearly Separable) 的数据集，SVM 可以通过 核函数 (Kernel Function) 将数据映射到高维空间 (High-Dimensional Space)，使得在高维空间中数据变得线性可分，然后在高维空间中寻找最优超平面。核技巧 (Kernel Trick) 的强大之处在于，我们不需要显式地计算高维空间的特征向量，只需要定义一个核函数 \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)，它可以计算原始空间中两个向量 \\(\\mathbf{x}_i\\) 和 \\(\\mathbf{x}_j\\) 映射到高维空间后的内积。常用的核函数包括：\n\n线性核 (Linear Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\\)。线性核实际上没有进行特征映射，适用于线性可分数据。\n多项式核 (Polynomial Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d\\)。多项式核可以将数据映射到多项式特征空间，适用于多项式关系的数据。其中 \\(\\gamma &gt; 0, r \\ge 0, d \\ge 1\\) 是核参数。\n高斯核 / RBF 核 (Gaussian Kernel / Radial Basis Function Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)\\)。高斯核是最常用的核函数之一，可以将数据映射到无限维空间，适用于各种类型的数据，尤其是局部性模式的数据。其中 \\(\\gamma &gt; 0\\) 是核参数，控制核函数的宽度。\nSigmoid 核 (Sigmoid Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)\\)。Sigmoid 核类似于神经网络中的 Sigmoid 激活函数，SVM 使用 Sigmoid 核时，其行为类似于多层感知机神经网络。其中 \\(\\gamma &gt; 0, r &lt; 0\\) 是核参数。\n\n最优化方法：对偶问题与 SMO 算法\nSVM 的优化问题（无论是硬间隔还是软间隔）通常转化为 对偶问题 (Dual Problem) 进行求解。求解对偶问题的好处包括： 1. 更容易求解：对偶问题通常比原始问题更容易求解。 2. 引入核函数：在对偶问题中，目标函数和约束条件只涉及到样本之间的内积，可以方便地引入核函数，将线性 SVM 扩展到非线性 SVM。\n求解 SVM 对偶问题的高效算法是 SMO (Sequential Minimal Optimization) 算法。SMO 算法是一种启发式算法，它将大规模 QP 问题分解为一系列小规模 QP 子问题，通过迭代地优化两个变量，高效地求解 SVM 模型。\n\n\n4.2.10 决策树 (Decision Tree)\n决策树 (Decision Tree) 是一种树形结构的分类或回归模型。决策树模型直观易懂，易于解释，并且可以处理类别型和数值型特征，无需进行特征缩放。决策树模型的核心思想是基于特征对数据集进行递归划分，构建一棵树状的决策规则，用于对新样本进行分类或预测。\n决策树由节点 (Node) 和 有向边 (Directed Edge) 组成。节点分为两种类型： - 内部节点 (Internal Node)：表示一个特征或属性的测试条件，用于决定样本的划分方向。 - 叶节点 (Leaf Node / Terminal Node)：表示最终的决策结果，即类别标签（分类树）或预测值（回归树）。\n有向边代表划分规则，从父节点指向子节点。从根节点到每个叶节点的路径都对应着一条决策规则。\n决策树的学习过程主要包括三个步骤：特征选择、树的生成 和 树的剪枝。\n回归树 (Regression Tree):\n回归树 (Regression Tree) 用于预测连续数值型目标变量。例如，预测房价、股票价格等。\n模型构建:\n回归树的构建过程是一个递归的二叉树构建过程，也称为 CART (Classification and Regression Tree) 树。CART 树是一种二叉树，内部节点根据特征取值将数据集划分为两个子集，叶节点输出预测值。回归树的构建过程如下：\n\n选择划分特征和划分点：从所有特征和所有可能的划分点中，选择一个最优的特征 \\(j\\) 和切分点 \\(s\\)，将当前节点的数据集划分为两个区域 \\(R_1(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j \\le s\\}\\) 和 \\(R_2(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j &gt; s\\}\\)。\n最小化平方误差：选择最优划分属性 \\(j\\) 和划分点 \\(s\\) 的目标是最小化划分后的平方误差 (Squared Error)，即使得划分后的两个子区域内样本的目标变量值尽可能接近。对于给定的特征 \\(j\\) 和切分点 \\(s\\)，遍历所有可能的 \\((j, s)\\) 对，计算划分后的平方误差，选择使得平方误差最小的 \\((j, s)\\) 对作为最优划分。平方误差的计算公式为：\n\n\\[\\min_{j,s} \\left[ \\min_{c_1} \\sum_{\\mathbf{x}_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{\\mathbf{x}_i \\in R_2(j,s)} (y_i - c_2)^2\\right]\\]\n其中 \\(c_1\\) 和 \\(c_2\\) 分别是区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\) 的预测值。对于给定的区域 \\(R_m(j,s)\\)，最优的预测值 \\(\\hat{c}_m\\) 是该区域内样本目标变量的均值：\n\\[\\hat{c}_m = \\text{ave}(y_i|\\mathbf{x}_i \\in R_m(j,s)) = \\frac{1}{|R_m(j,s)|} \\sum_{\\mathbf{x}_i \\in R_m(j,s)} y_i\\]\n\n递归划分：对划分后的两个子区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\)，递归地重复步骤 1 和 2，继续选择最优特征和切分点进行划分，直到满足停止条件。停止条件通常包括：\n\n节点内样本数量小于某个预设阈值。\n节点内样本的目标变量方差或平方误差小于某个阈值。\n没有更多特征可用于划分，或所有特征都已用完。\n\n生成叶节点：当满足停止条件时，将当前节点作为叶节点，并计算叶节点的预测值，通常为叶节点内样本目标变量的均值。\n\n分类树 (Classification Tree):\n分类树 (Classification Tree) 用于预测离散类别型目标变量。例如，判断用户是否会流失、识别图像中的物体类别等。\n模型构建:\n分类树的构建过程与回归树类似，也是一个递归的二叉树构建过程。不同之处在于，分类树在选择最优特征和切分点时，使用的划分指标不同，以及叶节点的预测值类型不同。分类树常用的划分指标包括 信息增益 (Information Gain)、信息增益率 (Information Gain Ratio) 和 基尼指数 (Gini Index)。目标是使得划分后的子节点数据尽可能 “纯净” (Pure)，即属于同一类别的样本比例尽可能高。\n划分指标:\n\n信息增益 (Information Gain)：基于信息熵 (Entropy) 的划分指标。信息熵衡量了数据集的混乱程度或不确定性。信息增益表示使用特征 \\(A\\) 对数据集 \\(D\\) 进行划分后，数据集 \\(D\\) 的信息熵减少的程度。信息增益越大，说明使用特征 \\(A\\) 划分数据集的效果越好。常用的基于信息增益的决策树算法是 ID3 算法。\n信息增益率 (Information Gain Ratio)：为了克服信息增益对取值数目较多的特征的偏好，C4.5 算法引入了信息增益率。信息增益率在信息增益的基础上，除以特征 \\(A\\) 本身的熵，对特征取值数目较多的情况进行惩罚。常用的基于信息增益率的决策树算法是 C4.5 算法。\n基尼指数 (Gini Index)：基尼指数衡量了数据集的纯度。基尼指数越小，数据集纯度越高。CART 算法使用基尼指数作为分类树的划分指标。\n\n叶节点预测值:\n分类树的叶节点输出类别标签，通常是叶节点内样本数量最多的类别（多数表决）。\n决策树的特点:\n\n优点：\n\n易于理解和解释：决策树模型直观易懂，决策规则清晰可见，易于向业务人员解释。\n可以处理类别型和数值型特征：无需对特征进行预处理，如独热编码、标准化等。\n无需特征缩放：决策树模型对特征的尺度不敏感，无需进行特征缩放。\n可以处理缺失值：决策树模型可以处理包含缺失值的数据。\n可以进行特征选择：决策树模型在构建过程中会自动选择重要的特征进行划分。\n\n缺点：\n\n容易过拟合：决策树模型容易在训练集上过拟合，导致泛化能力差。可以通过剪枝 (Pruning) 等方法缓解过拟合问题。\n不稳定：决策树模型对训练数据敏感，训练数据的微小变化可能导致树结构发生很大变化。\n忽略特征之间的相关性：决策树模型在选择划分特征时，每次只考虑一个特征，忽略了特征之间的相关性。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#总结",
    "href": "03_supervised.html#总结",
    "title": "4  监督学习（上）",
    "section": "4.3 总结",
    "text": "4.3 总结\n本讲义主要介绍了监督学习的基本概念和常用模型，包括：\n\n监督学习概述: 介绍了监督学习的定义、应用场景以及与量化投资的结合。\n回归模型: 详细讲解了线性回归和岭回归模型，包括模型表达式、最小二乘法、正则化以及模型特点。\n分类模型: 深入探讨了支持向量机 (SVM) 和决策树模型，包括模型原理、核函数、优化方法以及模型优缺点。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html",
    "href": "04_supervised.html",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "5.1 集成学习 (Ensemble Learning)\n集成学习 (Ensemble Learning) 是一种将多个弱学习器 (Weak Learner) 组合成一个强学习器 (Strong Learner) 的技术。 弱学习器通常指预测性能略优于随机猜测的模型，例如单层决策树、简单的线性模型等。集成学习的核心思想是 “集思广益”，即通过组合多个弱学习器的预测结果，来获得比单个强学习器更全面、更鲁棒的预测能力。\n集成学习模型通常具有比单个学习器更好的预测性能和泛化能力。 这是因为集成学习可以通过以下方式降低模型的误差：\n集成学习的核心思想是 “三个臭皮匠，顶个诸葛亮”，即通过集体智慧来提高模型的性能。常用的集成学习方法包括 Bagging (Bootstrap Aggregating)、Boosting (提升法) 和 Stacking (堆叠法)。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#集成学习-ensemble-learning",
    "href": "04_supervised.html#集成学习-ensemble-learning",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "降低方差 (Variance Reduction)：Bagging 等方法通过并行训练多个基学习器，并对它们的预测结果进行平均或投票，可以有效降低模型的方差，提高模型的稳定性。\n降低偏差 (Bias Reduction)：Boosting 等方法通过串行训练多个基学习器，每个基学习器都试图纠正前一个基学习器的错误，可以有效降低模型的偏差，提高模型的预测精度。\n提高鲁棒性 (Robustness Improvement)：集成学习模型通常不易过拟合，对异常值和噪声数据具有更强的鲁棒性。\n\n\n\n5.1.1 Bagging (Bootstrap Aggregating)\nBagging (Bootstrap Aggregating)，也称为自助采样聚合，是一种基于 自助采样 (Bootstrap Sampling) 的集成学习方法。Bagging 的核心思想是 并行集成，即同时训练多个独立的基学习器，然后通过投票 (Voting) 或 平均 (Averaging) 的方式将它们的预测结果集成起来。Bagging 可以有效降低模型的方差 (Variance)，提高模型的稳定性和泛化能力，尤其适用于容易过拟合的基学习器，如决策树、神经网络等。\n自助采样 (Bootstrap Sampling) 的直观解释:\n想象一下，你有一个装满弹珠的罐子（代表原始数据集），你想估计罐子里弹珠颜色的分布。自助采样就像是：\n\n有放回地从罐子里随机抽取一些弹珠，记录它们的颜色，然后放回罐子。\n重复步骤 1 多次，每次抽取都形成一个新的弹珠样本集（子数据集）。\n\n由于是有放回的抽样，每次抽取的子数据集都可能包含重复的弹珠，也可能缺少原始数据集中的某些弹珠。但从整体上看，每个子数据集都近似地代表了原始数据集的分布。\nBagging 降低方差的原理:\nBagging 通过自助采样创建多个略有不同的训练数据集，并在每个数据集上独立训练一个基学习器。由于每个基学习器都是在不同的数据集上训练的，它们之间具有一定的差异性。当对多个基学习器的预测结果进行平均或投票时，可以有效地平滑掉单个基学习器预测结果中的随机波动，从而降低整体模型的方差。\n算法流程:\n\n自助采样 (Bootstrap Sampling)：从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建一个子数据集 \\(D_t\\)，也称为 自助样本集 (Bootstrap Sample)。重复 \\(T\\) 次，得到 \\(T\\) 个独立的子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。每个子数据集的大小与原始数据集相同，但样本分布略有不同。由于是有放回抽样，因此每个子数据集中可能包含重复样本，也可能缺少原始数据集中的某些样本。一般来说，每个子数据集大约包含原始数据集 63.2% 的样本。\n训练基学习器 (Base Learner)：在每个子数据集 \\(D_t\\) 上独立地训练一个基学习器 \\(h_t\\)。基学习器可以是同质的（例如，都使用决策树），也可以是异质的（例如，使用决策树、神经网络、SVM 等不同的模型）。常用的基学习器是决策树，此时的 Bagging 集成学习方法就是 随机森林 (Random Forest)。\n集成预测 (Ensemble Prediction)：对于分类问题，Bagging 通常使用 投票法 (Voting) 进行预测，即将 \\(T\\) 个基学习器预测结果中出现次数最多的类别作为最终预测结果。对于回归问题，Bagging 通常使用 平均法 (Averaging) 进行预测，即将 \\(T\\) 个基学习器预测结果的平均值作为最终预测结果。\n\n随机森林 (Random Forest):\n随机森林 (Random Forest, RF) 是一种非常流行且强大的 基于 Bagging 思想的集成学习模型，以 决策树 (Decision Tree) 为基学习器。随机森林在 Bagging 的基础上，进一步引入了 特征随机选择 (Random Feature Selection)，使得基学习器之间具有更高的差异性 (Diversity)，从而进一步提高集成的性能。随机森林具有高精度、高效率、鲁棒性强、不易过拟合等优点，被广泛应用于分类、回归和特征重要性评估等任务。\n特征随机选择 (Random Feature Selection) 的直观解释:\n在构建决策树时，传统的决策树会在所有特征中选择最优特征进行节点分裂。而特征随机选择则是在每个节点分裂时，先随机选择一部分特征，然后只在这部分特征中选择最优特征。\n特征随机选择提高模型性能的原理:\n特征随机选择进一步增加了基学习器之间的差异性。由于每个决策树只使用一部分随机选择的特征进行训练，即使在相同的子数据集上训练，不同的决策树也会学习到不同的特征子集，从而降低基学习器之间的相关性，提高集成的效果。 此外，特征随机选择还有助于降低模型的过拟合风险，并提高模型的泛化能力。\n随机森林的构建过程:\n\n自助采样 (Bootstrap Sampling)：与 Bagging 相同，从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建 \\(T\\) 个子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。\n训练基学习器 (决策树)：在每个子数据集 \\(D_t\\) 上训练一个决策树 \\(h_t\\)。与传统的决策树不同，随机森林在训练决策树的过程中，引入了特征随机选择。具体来说，在决策树的每个节点分裂时，不是从所有特征中选择最优特征，而是先随机选择 \\(m\\) 个特征构成一个特征子集（通常 \\(m &lt;&lt; K\\)，例如 \\(m = \\sqrt{K}\\)），然后从这个特征子集中选择最优特征进行分裂。这里的 \\(m\\) 是一个超参数，需要预先设定。特征随机选择进一步增加了基学习器之间的差异性，使得随机森林的集成效果更好。\n集成预测 (Ensemble Prediction)：与 Bagging 相同，对于分类问题使用投票法，对于回归问题使用平均法。\n\n随机森林的特点:\n\n高精度：随机森林通常比单个决策树具有更高的预测精度。\n鲁棒性强：随机森林对异常值和噪声数据具有较好的鲁棒性。\n不易过拟合：随机森林通过 Bagging 和特征随机选择，有效降低了模型的方差，不易过拟合。\n泛化能力强：随机森林具有较强的泛化能力，在测试集上表现良好。\n可处理高维数据：随机森林可以处理高维数据，无需进行特征选择。\n可评估特征重要性：随机森林可以评估每个特征在模型中的重要性，用于特征选择和特征理解。\n实现简单，易于并行化：随机森林的构建过程简单高效，基学习器之间相互独立，易于并行化处理，训练速度快。\n\n\n\n5.1.2 Boosting (提升法)\nBoosting (提升法) 是一种与 Bagging 不同的集成学习方法。Boosting 的核心思想是 串行集成，即迭代地训练一系列的基学习器，每个基学习器都试图纠正前一个基学习器的错误。Boosting 方法通过加权样本或调整预测结果的方式，逐步提升模型的性能。Boosting 方法可以有效降低模型的偏差 (Bias) 和方差，得到高精度的集成模型，尤其适用于弱学习器，如浅层决策树 (Decision Stump)。常用的 Boosting 算法包括 AdaBoost (Adaptive Boosting)、GBDT (Gradient Boosting Decision Tree)、XGBoost (Extreme Gradient Boosting)、LightGBM (Light Gradient Boosting Machine)、CatBoost (Categorical Boosting) 等。\nBoosting 的串行集成思想的直观解释:\nBoosting 就像是一个团队合作解决问题的过程，其中：\n\n第一个弱学习器先尝试解决问题，但可能做得不够好，会犯一些错误。\n后续的弱学习器会仔细研究第一个学习器犯的错误，并针对这些错误进行改进，尝试纠正之前的错误。\n每个弱学习器都在前一个学习器的基础上进行提升，逐步提高整体的预测能力。\n\nBoosting 迭代提升模型性能的原理:\nBoosting 方法通过迭代训练，每一轮迭代都关注前一轮模型预测错误的样本，并调整样本权重或模型权重，使得后续的模型更加关注难以分类或预测的样本。 这样不断地迭代和调整，逐步将弱学习器提升为强学习器，最终得到一个高精度、高性能的集成模型。\nAdaBoost (Adaptive Boosting):\nAdaBoost (Adaptive Boosting, 自适应提升) 是一种经典的 Boosting 算法。AdaBoost 的核心思想是 “关注错误样本” 和 “加权基学习器”。AdaBoost 通过迭代地训练基学习器，每轮迭代都更加关注前一轮基学习器预测错误的样本，提高错误样本的权重，降低正确样本的权重，使得后续的基学习器更加关注难以分类的样本。同时，AdaBoost 为每个基学习器赋予一个权重，预测性能好的基学习器权重较高，预测性能差的基学习器权重较低。最终模型是所有基学习器的加权线性组合。AdaBoost 算法主要用于二分类问题。\n“关注错误样本” 和 “加权基学习器” 的直观解释:\n\n关注错误样本：在每一轮迭代中，AdaBoost 会提高上一轮分类错误的样本的权重，使得后续的基学习器更加关注这些难分样本，努力将它们分类正确。 这就像老师在辅导学生时，会更加关注那些经常犯错的学生，帮助他们改正错误，提高学习成绩。\n加权基学习器：AdaBoost 会根据每个基学习器的预测性能，赋予不同的权重。预测性能好的基学习器，例如错误率低的基学习器，会被赋予更高的权重，在最终的预测中起更大的作用。 这就像专家团队中，更信任那些经验丰富、能力强的专家的意见。\n\nAdaBoost 算法流程 (二分类):\n\n初始化样本权重 (Initialize Sample Weights)：为每个样本赋予相同的初始权重 \\(w_{1i} = 1/N, i = 1, 2, ..., N\\)。初始时，所有样本的权重相同，表示所有样本同等重要。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n训练基学习器 (Train Base Learner)：使用带有样本权重的训练数据集 \\(\\{( \\mathbf{x}_i, y_i, w_{ti} )\\}_{i=1}^{N}\\) 训练一个基学习器 \\(h_t(\\mathbf{x})\\)。在第 \\(t\\) 轮迭代中，基学习器 \\(h_t\\) 基于样本权重 \\(w_{ti}\\) 进行训练，使得加权训练误差最小化。基学习器通常选择弱学习器，如决策树桩 (Decision Stump)，即单层决策树。\n计算基学习器权重 (Calculate Base Learner Weight)：计算基学习器 \\(h_t\\) 在训练集上的加权错误率 \\(e_t = P(h_t(\\mathbf{x}_i) \\ne y_i) = \\sum_{i=1}^{N} w_{ti} I(h_t(\\mathbf{x}_i) \\ne y_i)\\)。 \\(e_t\\) 表示被基学习器 \\(h_t\\) 误分类的样本的权重之和。如果 \\(e_t &gt; 0.5\\)，则停止迭代，因为此时基学习器的性能甚至不如随机猜测。然后计算基学习器 \\(h_t\\) 的权重 \\(\\alpha_t = \\frac{1}{2} \\ln(\\frac{1-e_t}{e_t})\\)。当 \\(e_t\\) 越小时，\\(\\alpha_t\\) 越大，说明基学习器 \\(h_t\\) 的预测性能越好，权重越高。\n更新样本权重 (Update Sample Weights)：根据基学习器 \\(h_t\\) 的预测结果更新样本权重，提高误分类样本的权重，降低正确分类样本的权重，使得后续的基学习器更加关注难以分类的样本。样本权重更新公式为： \\[w_{t+1, i} = \\frac{w_{ti}}{Z_t} \\times \\begin{cases} e^{-\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) = y_i \\\\ e^{\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) \\ne y_i \\end{cases} = \\frac{w_{ti}}{Z_t} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\] 其中 \\(Z_t = \\sum_{i=1}^{N} w_{ti} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\) 是归一化因子 (Normalization Factor)，使得 \\(\\sum_{i=1}^{N} w_{t+1, i} = 1\\)，保证样本权重之和为 1。 \\(y_i \\in \\{-1, +1\\}\\) 是样本的真实标签， \\(h_t(\\mathbf{x}_i) \\in \\{-1, +1\\}\\) 是基学习器的预测标签。如果样本被正确分类 (\\(y_i h_t(\\mathbf{x}_i) = +1\\))，则样本权重乘以 \\(e^{-\\alpha_t} &lt; 1\\)，权重降低；如果样本被误分类 (\\(y_i h_t(\\mathbf{x}_i) = -1\\))，则样本权重乘以 \\(e^{\\alpha_t} &gt; 1\\)，权重提高。\n\n构建最终模型 (Build Final Model)：经过 \\(T\\) 轮迭代后，得到 \\(T\\) 个基学习器 \\(\\{h_1, h_2, ..., h_T\\}\\) 及其对应的权重 \\(\\{\\alpha_1, \\alpha_2, ..., \\alpha_T\\}\\)。最终模型是基学习器的加权线性组合：对于新样本 \\(\\mathbf{x}\\)，最终模型的预测结果为： \\(H(\\mathbf{x}) = \\text{sign}(\\sum_{t=1}^{T} \\alpha_t h_t(\\mathbf{x}))\\)。 \\(\\text{sign}(z)\\) 是符号函数，当 \\(z &gt; 0\\) 时，\\(\\text{sign}(z) = +1\\)；当 \\(z &lt; 0\\) 时，\\(\\text{sign}(z) = -1\\)；当 \\(z = 0\\) 时，\\(\\text{sign}(z) = 0\\) 或 \\(+1\\) 或 \\(-1\\)，通常取 \\(+1\\) 或 \\(-1\\)。\n\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT):\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT) 是一种非常强大且广泛应用的 Boosting 算法，以 决策树 (Decision Tree) 为基学习器。GBDT 的核心思想是 梯度提升 (Gradient Boosting)，也称为 梯度下降提升 (Gradient Descent Boosting)。GBDT 使用 梯度提升算法，通过迭代地训练决策树来拟合负梯度残差 (Negative Gradient Residuals)，逐步逼近真实的目标函数。GBDT 可以用于回归和分类问题，尤其在非线性和复杂的预测问题中表现出色。GBDT 是许多高级 Boosting 算法（如 XGBoost、LightGBM、CatBoost）的基础。\n梯度提升 (Gradient Boosting) 的直观解释:\n梯度提升的思想可以理解为函数空间的梯度下降。在传统的梯度下降中，我们是在参数空间中沿着负梯度方向迭代优化参数，以最小化损失函数。而在梯度提升中，我们是在函数空间中沿着负梯度方向迭代优化模型函数，以逼近真实的目标函数。\nGBDT 拟合负梯度残差的原理:\nGBDT 的每一轮迭代都训练一个新的决策树，目标是拟合当前模型预测结果与真实值之间的残差。更精确地说，GBDT 拟合的是损失函数的负梯度，即残差的某种形式。通过不断地拟合残差，GBDT 逐步减小模型的预测误差，提高模型的预测精度。\nGBDT 算法流程 (以回归问题为例):\nGBDT 算法流程 (以回归问题为例):\n\n初始化模型 (Initialize Model)：初始化一个弱学习器（例如，常数模型 (Constant Model)，即所有样本的预测值都为一个常数） \\(f_0(\\mathbf{x}) = \\text{average}(y_i)\\)。 \\(f_0(\\mathbf{x})\\) 是初始模型的预测函数，通常使用训练集样本目标变量的均值作为初始预测值。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n计算负梯度残差 (Compute Negative Gradient Residuals)：对于每个样本 \\(i = 1, 2, ..., N\\)，计算负梯度残差 \\(r_{ti} = y_i - f_{t-1}(\\mathbf{x}_i)\\)。 \\(f_{t-1}(\\mathbf{x}_i)\\) 是前一轮迭代得到的模型 \\(f_{t-1}\\) 对样本 \\(\\mathbf{x}_i\\) 的预测值。 \\(r_{ti}\\) 表示真实值 \\(y_i\\) 与当前模型预测值 \\(f_{t-1}(\\mathbf{x}_i)\\) 之间的差异，即模型在样本 \\(\\mathbf{x}_i\\) 上的预测误差。负梯度残差 \\(r_{ti}\\) 可以看作是本轮迭代需要拟合的目标，即模型需要在本轮迭代中学习如何纠正前一轮的预测误差。\n训练决策树 (Train Decision Tree)：使用 \\((\\mathbf{x}_i, r_{ti})_{i=1}^{N}\\) 作为训练数据，训练一个决策树 \\(h_t(\\mathbf{x})\\)，拟合负梯度残差 \\(r_{ti}\\)。决策树 \\(h_t(\\mathbf{x})\\) 的叶节点区域 \\(\\{R_{tj}\\}_{j=1}^{J_t}\\) 将特征空间划分为 \\(J_t\\) 个互不相交的区域， \\(J_t\\) 是决策树 \\(h_t\\) 的叶节点数量。\n确定叶节点区域的输出值 (Determine Leaf Node Output Values)：对于决策树 \\(h_t(\\mathbf{x})\\) 的每个叶节点区域 \\(R_{tj}\\)，计算该区域内样本负梯度残差的平均值 \\(c_{tj} = \\text{average}_{\\mathbf{x}_i \\in R_{tj}} (r_{ti}) = \\frac{1}{|R_{tj}|} \\sum_{\\mathbf{x}_i \\in R_{tj}} r_{ti}\\)。 \\(c_{tj}\\) 是决策树 \\(h_t\\) 在叶节点区域 \\(R_{tj}\\) 上的预测值，表示模型在本轮迭代中需要在区域 \\(R_{tj}\\) 内进行的调整量。\n更新模型 (Update Model)：更新模型 \\(f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj})\\)，其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，也称为 shrinkage 参数，通常取值范围为 \\((0, 1]\\)，例如 0.1、0.01 等。 \\(\\alpha\\) 控制每个基学习器的步长，减小学习率可以降低模型对后续基学习器的依赖程度，提高模型的泛化能力，但需要更多的迭代次数。 \\(I(\\mathbf{x} \\in R_{tj})\\) 是指示函数，当样本 \\(\\mathbf{x}\\) 属于叶节点区域 \\(R_{tj}\\) 时， \\(I(\\mathbf{x} \\in R_{tj}) = 1\\)，否则 \\(I(\\mathbf{x} \\in R_{tj}) = 0\\)。 \\(f_t(\\mathbf{x})\\) 是本轮迭代更新后的模型，它是在前一轮模型 \\(f_{t-1}(\\mathbf{x})\\) 的基础上，加上本轮训练的决策树 \\(h_t(\\mathbf{x})\\) 的加权结果，逐步逼近真实的目标函数。\n\n得到最终模型 (Obtain Final Model)：经过 \\(T\\) 轮迭代后，得到最终的 GBDT 模型 \\(f_T(\\mathbf{x}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\sum_{j=1}^{J_t} \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\alpha h_t(\\mathbf{x})\\)。最终模型是所有基学习器的加权和。\n\nGBDT 算法流程 (以分类问题为例):\nGBDT 用于分类问题时，算法流程与回归问题类似，主要区别在于： - 损失函数不同：回归问题通常使用平方误差损失函数 (Squared Error Loss)，分类问题通常使用对数似然损失函数 (Log-Likelihood Loss) 或 指数损失函数 (Exponential Loss) 等。 - 负梯度计算不同：不同损失函数的负梯度计算方式不同。例如，对于二分类问题，如果使用对数似然损失函数，则负梯度残差的计算公式与回归问题不同。 - 叶节点区域的输出值确定方式不同：分类问题中，叶节点区域的输出值通常不是负梯度残差的平均值，而是根据具体的损失函数和优化目标确定。例如，对于二分类问题，可以使用对数几率 (Log Odds) 或 类别概率 作为叶节点输出值。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#神经网络-neural-network",
    "href": "04_supervised.html#神经网络-neural-network",
    "title": "5  监督学习（下）",
    "section": "5.2 神经网络 (Neural Network)",
    "text": "5.2 神经网络 (Neural Network)\n神经网络 (Neural Network, NN)，更精确地说是人工神经网络 (Artificial Neural Network, ANN)，是一种模拟生物神经系统结构的计算模型，由大量相互连接的神经元 (Neuron) 组成。神经网络可以学习复杂的非线性关系，具有强大的模式识别、函数逼近和自适应能力。神经网络在图像识别、自然语言处理、语音识别、金融预测等领域取得了巨大成功，是深度学习 (Deep Learning) 的基础。\n神经网络的核心思想:\n神经网络的核心思想是模拟生物神经系统的信息处理方式。生物神经系统由大量的神经元相互连接而成，神经元之间通过电信号和化学信号传递信息。神经网络试图抽象和模拟这种信息传递和处理机制，通过构建由大量神经元相互连接的网络结构，来实现复杂的信息处理和学习任务。\n神经网络的优势:\n\n强大的非线性建模能力：神经网络通过激活函数的非线性变换，可以学习和表示非常复杂的非线性关系，这是传统线性模型难以实现的。\n高度的并行分布式计算能力：神经网络由大量的神经元并行工作，可以高效地处理大规模数据和复杂计算任务。\n良好的泛化能力：通过合理的网络结构设计和训练方法，神经网络可以学习到数据中的本质规律，具有良好的泛化能力，在未见过的数据上也能表现良好。\n自适应学习能力：神经网络可以通过反向传播算法等方法，自动地从数据中学习，调整网络参数，适应不同的任务和数据。\n\n\n5.2.1 神经网络模型及算法\n神经元模型 (Neuron Model):\n神经元是神经网络的基本单元，也称为 感知机 (Perceptron)。一个典型的神经元模型包括以下几个主要组成部分：\n\n输入 (Input)：神经元接收来自其他神经元或外部环境的输入信号。输入信号可以是数值、向量或张量等形式。 例如，对于图像识别任务，输入可以是图像像素的灰度值；对于自然语言处理任务，输入可以是词向量。\n权重 (Weight)：每个输入连接都对应一个权重 \\(w_{ij}\\)，表示连接的强度或重要性。权重可以是正数（兴奋性连接）或负数（抑制性连接）。 权重是神经元学习的关键参数，通过调整权重，神经元可以选择性地接收和处理不同的输入信号。\n偏置 (Bias)：神经元还接收一个偏置 \\(b_i\\)，也称为 阈值 (Threshold)。偏置是一个常数，用于调整神经元的激活阈值，使得神经元更容易或更不容易被激活。 偏置可以看作是神经元的一个自由度，使得神经元可以更加灵活地进行激活。\n加权求和 (Weighted Summation)：神经元将所有输入信号与对应的权重进行加权求和，再加上偏置，得到神经元的净输入 (Net Input) \\(z_i = \\sum_{j} w_{ij} x_j + b_i\\)。 净输入 \\(z_i\\) 表示神经元接收到的所有输入信号的综合强度。\n激活函数 (Activation Function)：神经元对净输入 \\(z_i\\) 进行非线性变换，通过激活函数 \\(\\sigma(\\cdot)\\) 产生神经元的输出 (Output) \\(a_i = \\sigma(z_i) = \\sigma(\\sum_{j} w_{ij} x_j + b_i)\\)。激活函数是神经网络实现非线性建模的关键。 激活函数引入了非线性因素，使得神经网络可以逼近任意复杂的非线性函数。常用的激活函数包括：\n\nSigmoid 函数：\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)，将输入值映射到 \\((0, 1)\\) 区间，常用于二分类问题的输出层。 Sigmoid 函数的输出值可以解释为概率，例如样本属于正类的概率。\nTanh 函数 (Hyperbolic Tangent Function)：\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1\\)，将输入值映射到 \\((-1, 1)\\) 区间，与 Sigmoid 函数类似，但输出范围不同。 Tanh 函数的输出值以 0 为中心，可能在某些情况下比 Sigmoid 函数更易于训练。\nReLU 函数 (Rectified Linear Unit)：\\(\\text{ReLU}(z) = \\max(0, z)\\)，当输入值大于 0 时，输出值等于输入值；当输入值小于等于 0 时，输出值为 0。ReLU 函数计算简单，收敛速度快，是深度神经网络中最常用的激活函数之一。 ReLU 函数在正区间是线性函数，负区间是常数 0，这种简单的非线性使其在深度网络中表现出色。\nLeaky ReLU 函数 (Leaky Rectified Linear Unit)：\\(\\text{Leaky ReLU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha z, & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个很小的常数，例如 0.01。Leaky ReLU 函数解决了 ReLU 函数在输入值为负数时输出值为 0 导致神经元 “死亡” 的问题。 Leaky ReLU 函数在负区间也保持一定的梯度，有助于信息在网络中更好地传播。\nELU 函数 (Exponential Linear Unit)：\\(\\text{ELU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha (e^z - 1), & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个正的常数。ELU 函数具有 ReLU 函数的优点，同时在输入值为负数时输出值也具有一定的梯度，可以加速神经网络的收敛。 ELU 函数在负区间使用指数函数，可以提供更平滑的输出，并有助于网络的鲁棒性。\nSoftmax 函数：\\(\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\\)，用于多分类问题的输出层。Softmax 函数将一组输入值映射为一组概率值，每个概率值都在 \\((0, 1)\\) 区间，且所有概率值之和为 1。 Softmax 函数可以将神经元的输出转换为类别概率分布，方便进行多分类任务。\n\n输出 (Output)：神经元的输出信号 \\(a_i\\)，可以作为其他神经元的输入，或作为整个神经网络的输出。 神经元的输出是经过激活函数处理后的结果，可以传递给下一层神经元，或者作为最终的预测结果。\n\n多层神经网络 (Multilayer Neural Network, MLNN) / 多层感知机 (Multilayer Perceptron, MLP):\n多层神经网络 (Multilayer Neural Network, MLNN) 或 多层感知机 (Multilayer Perceptron, MLP) 是由多个神经元层相互连接而成的神经网络。多层神经网络通常包括以下几种类型的层：\n\n输入层 (Input Layer)：接收外部输入数据。输入层的神经元不进行任何计算，只是将输入数据传递给下一层。输入层神经元的数量等于输入特征的维度。 例如，如果输入是 28x28 的图像，则输入层通常有 784 个神经元（将图像展平成向量）。\n隐藏层 (Hidden Layer)：位于输入层和输出层之间，可以有一层或多层。隐藏层是神经网络的核心部分，负责提取输入数据的特征，学习输入数据中的复杂模式。隐藏层神经元的数量和层数是神经网络的超参数，需要根据具体问题进行设计和调整。 隐藏层越多，网络可以学习到的特征就越抽象、越复杂。\n输出层 (Output Layer)：产生最终的输出结果。输出层神经元的数量取决于具体的任务类型。例如，二分类问题的输出层通常只有一个神经元，使用 Sigmoid 激活函数，输出样本属于正类的概率；多分类问题的输出层通常有 \\(C\\) 个神经元（\\(C\\) 是类别数量），使用 Softmax 激活函数，输出样本属于每个类别的概率；回归问题的输出层通常只有一个神经元，不使用激活函数或使用 线性激活函数，直接输出预测值。\n\n深度神经网络 (Deep Neural Network, DNN) 指的是具有多个隐藏层的神经网络。 “深度” 指的是网络结构的深度，即隐藏层的层数。 深度神经网络可以学习到更加抽象和层次化的特征表示，从而更好地处理复杂的数据和任务。\n全连接神经网络 (Fully Connected Neural Network, FCNN) / 稠密神经网络 (Dense Neural Network):\n在多层神经网络中，相邻层之间的神经元通常采用 全连接 (Fully Connected) 的方式进行连接，即前一层的每个神经元都与后一层的所有神经元连接。这种连接方式的神经网络称为 全连接神经网络 (Fully Connected Neural Network, FCNN) 或 稠密神经网络 (Dense Neural Network)。全连接神经网络是最基本的神经网络结构，也是许多复杂神经网络的基础。\n前向传播 (Forward Propagation):\n前向传播 (Forward Propagation) 是指输入信号从输入层经过隐藏层逐层传递到输出层的过程。在前向传播过程中，每一层的神经元接收来自前一层的输出，进行加权求和和激活函数处理，然后将输出传递给下一层。通过逐层传递和计算，最终得到输出层的输出，即神经网络的预测结果。\n前向传播的计算步骤:\n\n输入层：将输入数据输入到输入层神经元。\n隐藏层：对于每个隐藏层，依次计算每个神经元的输出。计算过程包括：\n\n加权求和：将前一层所有神经元的输出与连接权重相乘，并加上偏置，得到净输入。\n激活函数：将净输入通过激活函数进行非线性变换，得到神经元的输出。\n\n输出层：计算输出层每个神经元的输出，计算过程与隐藏层类似。输出层神经元的输出即为神经网络的最终预测结果。\n\n反向传播算法 (Backpropagation, BP):\n反向传播算法 (Backpropagation, BP) 是训练多层神经网络最常用和最核心的算法。BP 算法是一种基于梯度下降法 (Gradient Descent) 的误差反向传播算法，通过计算损失函数 (Loss Function) 对网络参数（权重和偏置）的梯度，然后沿着梯度反方向更新参数，迭代地最小化损失函数，从而学习到最优的网络参数。BP 算法实现了误差信号从输出层反向传播到输入层，逐层调整网络参数，使得神经网络能够学习到输入数据中的复杂模式。\n梯度下降法 (Gradient Descent) 的直观解释:\n梯度下降法就像是在山坡上寻找山谷最低点的过程。\n\n确定当前位置（对应于当前的参数值）。\n沿着当前位置最陡峭的方向（负梯度方向）向下走一步（更新参数）。\n重复步骤 2，直到到达山谷最低点（损失函数达到最小值）。\n\n反向传播算法的步骤:\n\n前向传播 (Forward Propagation)：给定输入样本 \\((\\mathbf{x}_i, y_i)\\)，计算每个样本的前向传播输出，得到神经网络的预测值 \\(\\hat{y}_i\\)。前向传播过程是从输入层开始，逐层计算每个神经元的输出，直到输出层。\n计算损失 (Compute Loss)：根据预测值 \\(\\hat{y}_i\\) 和真实值 \\(y_i\\)，计算损失函数值 \\(L_i = L(\\hat{y}_i, y_i)\\)。损失函数衡量了模型预测结果与真实值之间的差异。常用的损失函数包括：\n\n均方误差损失函数 (Mean Squared Error Loss, MSE)：\\(L_{MSE}(\\hat{y}, y) = \\frac{1}{2} (\\hat{y} - y)^2\\)，常用于回归问题。\n交叉熵损失函数 (Cross-Entropy Loss Function)：\\(L_{CE}(\\hat{y}, y) = - [y \\ln \\hat{y} + (1-y) \\ln (1-\\hat{y})]\\)，常用于二分类问题。对于多分类问题，可以使用 多类交叉熵损失函数 (Categorical Cross-Entropy Loss)。\n\n反向传播误差 (Backpropagate Error)：从输出层开始，反向计算每一层的误差项 (Error Term) \\(\\delta_l\\)。误差项 \\(\\delta_l\\) 反映了第 \\(l\\) 层神经元的输出对最终损失的影响程度，是损失函数关于第 \\(l\\) 层神经元净输入 \\(z_l\\) 的梯度。误差项的计算公式为：\n\n输出层误差项：\\(\\delta_{output} = \\frac{\\partial L}{\\partial z_{output}} = \\frac{\\partial L}{\\partial a_{output}} \\odot \\sigma'_{output}(z_{output})\\)，其中 \\(\\odot\\) 表示逐元素乘积 (Element-wise Product)，\\(\\sigma'_{output}(z_{output})\\) 是输出层激活函数 \\(\\sigma_{output}\\) 对净输入 \\(z_{output}\\) 的导数。\n隐藏层误差项：\\(\\delta_l = \\frac{\\partial L}{\\partial z_l} = (\\mathbf{W}_{l+1}^T \\delta_{l+1}) \\odot \\sigma'_{l}(z_{l})\\)，其中 \\(\\mathbf{W}_{l+1}\\) 是第 \\(l\\) 层到第 \\(l+1\\) 层的权重矩阵，\\(\\delta_{l+1}\\) 是第 \\(l+1\\) 层的误差项，\\(\\sigma'_{l}(z_{l})\\) 是第 \\(l\\) 层激活函数 \\(\\sigma_{l}\\) 对净输入 \\(z_{l}\\) 的导数。误差项的计算是从输出层向输入层逐层反向传播的。\n\n计算梯度 (Compute Gradients)：根据误差项 \\(\\delta_l\\)，计算损失函数关于每一层权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\) 的梯度。梯度的计算公式为：\n\n权重梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\delta_l \\mathbf{a}_{l-1}^T\\)，其中 \\(\\mathbf{a}_{l-1}\\) 是第 \\(l-1\\) 层的输出（对于输入层，\\(\\mathbf{a}_0 = \\mathbf{x}\\)）。\n偏置梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\delta_l\\)。\n\n更新参数 (Update Parameters)：沿着梯度反方向更新权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\)，最小化损失函数。参数更新公式为：\n\n权重更新：\\(\\mathbf{W}_l = \\mathbf{W}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\mathbf{W}_l - \\alpha \\delta_l \\mathbf{a}_{l-1}^T\\)\n偏置更新：\\(\\mathbf{b}_l = \\mathbf{b}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\mathbf{b}_l - \\alpha \\delta_l\\) 其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，控制参数更新的步长。\n\n迭代 (Iteration)：重复步骤 1-5，遍历所有训练样本（或一个批次的样本），进行多轮迭代 (Epoch)，直到损失函数收敛或达到预设的迭代次数。\n\n\n\n5.2.2 深度学习 (Deep Learning)\n深度学习 (Deep Learning) 是机器学习的一个分支，本质上就是具有多层隐藏层的神经网络。 深度学习通过构建深层神经网络，学习数据中更加抽象和复杂的特征表示，从而解决更加复杂的任务。 深度学习在图像识别、自然语言处理、语音识别等领域取得了突破性进展，推动了人工智能的快速发展。\n深度学习的关键要素:\n\n深层神经网络结构：深度学习模型通常具有多个隐藏层，可以学习到多层次的特征表示。\n大规模数据集：深度学习模型通常需要大规模的数据集进行训练，才能充分发挥其性能。\n强大的计算能力：深度学习模型的训练通常需要大量的计算资源，例如 GPU (图形处理器)。\n高效的优化算法：深度学习模型的训练需要高效的优化算法，例如 Adam、SGD with Momentum 等。\n\n神经网络的变体模型 (Variations of Neural Networks):\n随着深度学习的发展，研究者们提出了各种各样的神经网络变体模型，以适应不同的任务和数据类型。以下是一些常见的神经网络变体模型：\n\n卷积神经网络 (Convolutional Neural Network, CNN)：\n\n特点：CNN 是一种专门用于处理图像数据的神经网络。CNN 的核心组件是卷积层 (Convolutional Layer) 和 池化层 (Pooling Layer)。卷积层可以自动学习图像中的局部特征，例如边缘、纹理等。池化层可以降低特征图的维度，减少计算量，并提高模型的鲁棒性。\n应用场景：图像分类、目标检测、图像分割、人脸识别等计算机视觉任务。\n\n循环神经网络 (Recurrent Neural Network, RNN)：\n\n特点：RNN 是一种专门用于处理序列数据的神经网络。RNN 的特点是具有循环连接，使得网络可以记忆之前的输入信息，并应用于当前的输出。\n应用场景：自然语言处理 (如文本分类、机器翻译、文本生成)、语音识别、时间序列预测等序列数据处理任务。\n常见变体：长短期记忆网络 (Long Short-Term Memory Network, LSTM) 和 门控循环单元网络 (Gated Recurrent Unit Network, GRU)，它们解决了传统 RNN 在长序列数据中容易出现的梯度消失和梯度爆炸问题。\n\nTransformer 网络:\n\n特点：Transformer 是一种基于自注意力机制 (Self-Attention Mechanism) 的神经网络结构。Transformer 摒弃了传统的 RNN 结构，完全依赖自注意力机制来捕捉输入序列中不同位置之间的关系。Transformer 具有并行计算能力强、可以捕捉长距离依赖关系等优点。\n应用场景：自然语言处理 (如机器翻译、文本摘要、问答系统)、图像识别、语音识别等各种序列数据处理任务。\n重要模型：BERT (Bidirectional Encoder Representations from Transformers)、GPT (Generative Pre-trained Transformer) 等预训练语言模型，在自然语言处理领域取得了革命性的进展。\n\n图神经网络 (Graph Neural Network, GNN)：\n\n特点：GNN 是一种专门用于处理图结构数据的神经网络。GNN 可以学习图中节点和边的特征表示，并进行节点分类、链接预测、图分类等任务。\n应用场景：社交网络分析、知识图谱、推荐系统、生物信息学等图结构数据分析任务。\n\n生成对抗网络 (Generative Adversarial Network, GAN)：\n\n特点：GAN 是一种生成模型，由生成器 (Generator) 和 判别器 (Discriminator) 两个神经网络组成。生成器负责生成假数据，判别器负责区分真假数据。两个网络相互对抗训练，最终生成器可以生成逼真的数据。\n应用场景：图像生成、图像编辑、数据增强、风格迁移等生成式任务。\n\n自编码器 (Autoencoder, AE)：\n\n特点：自编码器 是一种无监督学习模型，用于学习数据的低维表示 (特征)。自编码器由编码器 (Encoder) 和 解码器 (Decoder) 两个神经网络组成。编码器将输入数据压缩到低维空间，解码器将低维表示重构回原始数据。\n应用场景：特征提取、降维、数据去噪、异常检测等无监督学习任务。\n常见变体：变分自编码器 (Variational Autoencoder, VAE)、稀疏自编码器 (Sparse Autoencoder)、降噪自编码器 (Denoising Autoencoder) 等。\n\n\n总而言之，神经网络和深度学习是当前机器学习领域最热门和最重要的方向之一。 掌握神经网络的基本原理和常用模型，对于理解和应用人工智能技术至关重要。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#总结",
    "href": "04_supervised.html#总结",
    "title": "5  监督学习（下）",
    "section": "5.3 总结",
    "text": "5.3 总结\n本讲义深入探讨了集成学习和神经网络模型，主要内容包括：\n\n集成学习: 详细介绍了 Bagging 和 Boosting 两种主要的集成学习方法，以及随机森林和 AdaBoost 算法的原理和特点。\n神经网络: 系统讲解了神经网络的基本结构、前向传播、反向传播算法以及深度学习的概念。\n深度学习变体模型: 简要介绍了卷积神经网络 (CNN)、循环神经网络 (RNN)、Transformer 网络、图神经网络 (GNN) 和生成对抗网络 (GAN) 等深度学习模型的特点和应用场景。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html",
    "href": "05_model_assess.html",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1 模型评估与优化",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型评估与优化",
    "href": "05_model_assess.html#模型评估与优化",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1.1 为什么需要评估模型？\n\n避免过拟合与欠拟合: 模型可能在训练数据上表现很好，但在未见过的数据上表现很差 (过拟合)，或者模型可能无法捕捉到数据中的基本模式 (欠拟合)。\n选择最佳模型: 当我们尝试不同的模型或模型配置时，我们需要一种方法来比较它们的性能并选择最佳模型。\n了解模型性能: 评估可以帮助我们了解模型在不同情况下的表现，例如在不同的数据子集或不同的任务上。\n指导模型改进: 评估结果可以帮助我们识别模型的弱点，并指导我们如何改进模型。\n\n\n\n6.1.2 评估指标\n评估指标的选择取决于具体的机器学习任务类型。\n\n6.1.2.1 分类模型评估指标\n\n准确率 (Accuracy): 分类正确的样本数占总样本数的比例。\n\n适用于类别分布均衡的数据集。\n当类别不平衡时，准确率可能会产生误导。\n\n\\[\n  \\text{Accuracy} = \\frac{\\text{正确分类的样本数}}{\\text{总样本数}}\n  \\]\n精确率 (Precision): 预测为正例的样本中，真正例的比例。\n\n关注模型预测正例的准确性。\n\n\\[\n  \\text{Precision} = \\frac{TP}{TP + FP}\n  \\]\n\nTP (True Positive): 真正例，模型预测为正例，实际也是正例。\nFP (False Positive): 假正例，模型预测为正例，实际是负例。\n\n召回率 (Recall): 所有实际正例中，被模型正确预测为正例的比例。\n\n关注模型发现所有正例的能力。\n\n\\[\n  \\text{Recall} = \\frac{TP}{TP + FN}\n  \\]\n\nFN (False Negative): 假负例，模型预测为负例，实际是正例。\n\nF1 分数 (F1-Score): 精确率和召回率的调和平均值。\n\n综合考虑精确率和召回率。\n当精确率和召回率都很重要时，F1 分数是一个很好的指标。\n\n\\[\n  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n  \\]\nAUC-ROC 曲线: 受试者工作特征 (Receiver Operating Characteristic) 曲线下的面积。\n\nROC 曲线描述了在不同阈值下，真正例率 (TPR) 与假正例率 (FPR) 之间的关系。\nAUC 值越大，模型性能越好。\n适用于评估二分类模型的排序能力。\nTPR (True Positive Rate) 或 灵敏度 (Sensitivity): \\(\\frac{TP}{TP + FN}\\)， 等于召回率 (Recall)。\nFPR (False Positive Rate): \\(\\frac{FP}{FP + TN}\\)\nTN (True Negative): 真负例，模型预测为负例，实际也是负例。\n\n混淆矩阵 (Confusion Matrix): 总结分类模型预测结果的表格。\n\n可以直观地看到模型在每个类别上的预测情况。\n可以用于计算精确率、召回率、F1 分数等指标。\n\n\n\n\n\n\n\n\n\n\n\n预测为正例 (Positive Prediction)\n预测为负例 (Negative Prediction)\n\n\n\n\n实际正例 (Actual Positive)\nTP\nFN\n\n\n实际负例 (Actual Negative)\nFP\nTN\n\n\n\n\n\n6.1.2.2 回归模型评估指标\n\n均方误差 (Mean Squared Error, MSE): 预测值与真实值之差的平方的平均值。\n\n对误差进行平方，可以放大误差较大的样本的影响。\nMSE 越小，模型性能越好。\n\n\\[\n  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n\\(y_i\\): 第 \\(i\\) 个样本的真实值。\n\\(\\hat{y}_i\\): 第 \\(i\\) 个样本的预测值。\n\\(n\\): 样本总数。\n\n均绝对误差 (Mean Absolute Error, MAE): 预测值与真实值之差的绝对值的平均值。\n\n对误差取绝对值，可以避免正负误差相互抵消。\nMAE 越小，模型性能越好。\n对异常值 (outlier) 不敏感。\n\n\\[\n  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n  \\]\n均方根误差 (Root Mean Squared Error, RMSE): 均方误差的平方根。\n\n与 MSE 类似，但 RMSE 的量纲与原始数据一致，更易于解释。\nRMSE 越小，模型性能越好。\n\n\\[\n  \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n  \\]\nR 平方 (R-squared): 模型解释的方差比例。\n\n取值范围为 \\([0, 1]\\)，值越大，模型拟合程度越好。\n\\(R^2 = 1\\) 表示模型完美拟合数据。\n\\(R^2 = 0\\) 表示模型性能与使用均值作为预测值相当。\n可以用于评估模型对数据方差的解释能力。\n\n\\[\n  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n  \\]\n\n\\(SS_{res}\\): 残差平方和 (Sum of Squares of Residuals)，即 \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)。\n\\(SS_{tot}\\): 总平方和 (Total Sum of Squares)，即 \\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)。\n\\(\\bar{y}\\): 真实值的均值。\n\n\n\n\n\n6.1.3 交叉验证 (Cross-Validation)\n\n目的: 更可靠地评估模型的泛化能力，避免模型在特定数据集划分上的偶然性。\n基本思想: 将数据集分成若干份，轮流使用其中一份作为验证集，其余作为训练集进行模型训练和评估。\n常用方法:\n\nk 折交叉验证 (k-Fold Cross-Validation): 将数据集分成 k 份，每次使用其中 1 份作为验证集，其余 k-1 份作为训练集，重复 k 次。最终评估结果是 k 次评估结果的平均值。\n留一交叉验证 (Leave-One-Out Cross-Validation, LOOCV): k 折交叉验证的特殊情况，k 等于样本总数。每次只使用一个样本作为验证集，其余样本作为训练集。\n分层 k 折交叉验证 (Stratified k-Fold Cross-Validation): 在 k 折交叉验证的基础上，保证每个 fold 中各类别样本的比例与原始数据集中的比例大致相同。适用于类别不平衡的数据集。\n\n\n\n\n6.1.4 超参数调优 (Hyperparameter Tuning)\n\n超参数 (Hyperparameters): 模型训练前需要手动设置的参数，例如学习率、正则化系数、决策树的最大深度等。\n目的: 找到最佳的超参数组合，使模型在验证集上获得最佳性能。\n常用方法:\n\n网格搜索 (Grid Search): 预先定义超参数的候选值，然后穷举所有可能的超参数组合，并在验证集上评估每种组合的性能，选择最佳组合。\n随机搜索 (Random Search): 在预定义的超参数空间中随机采样超参数组合，并在验证集上评估性能，选择最佳组合。通常比网格搜索更高效，尤其是在超参数空间较大时。\n贝叶斯优化 (Bayesian Optimization): 使用贝叶斯方法建立超参数与模型性能之间的概率模型，然后根据该模型选择下一组超参数进行评估，以更有效地找到最佳超参数组合。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型优化策略",
    "href": "05_model_assess.html#模型优化策略",
    "title": "6  模型评估与优化",
    "section": "6.2 模型优化策略",
    "text": "6.2 模型优化策略\n\n特征工程 (Feature Engineering): 通过对原始特征进行转换、组合或选择，创建更有效的特征，以提高模型性能。\n\n特征转换: 例如，对数值特征进行标准化、归一化、对数变换、Box-Cox 变换等，使特征更符合模型假设或更易于模型学习。对类别特征进行独热编码、标签编码等。\n特征组合: 将多个特征进行组合，生成新的交叉特征，以捕捉特征之间的交互关系。例如，将年龄和收入进行组合，生成年龄*收入的特征。\n特征选择: 从原始特征中选择最相关的特征子集，去除冗余或不相关的特征，降低模型复杂度，提高模型泛化能力。常用的特征选择方法包括过滤式 (Filter)、包裹式 (Wrapper) 和嵌入式 (Embedded) 方法。\n\n模型选择 (Model Selection): 尝试不同的机器学习模型，例如线性回归、逻辑回归、决策树、支持向量机、神经网络等，选择最适合当前任务的模型。\n\n模型比较: 在同一个数据集上训练不同的模型，并使用交叉验证等方法评估它们的性能，选择性能最佳的模型。\n模型融合: 将多个不同模型的预测结果进行融合，以获得更好的预测性能。例如， stacking、blending 等集成方法。\n\n集成学习 (Ensemble Learning): 将多个弱学习器组合成一个强学习器，例如随机森林、梯度提升树 (GBDT)、XGBoost 等。集成学习通常可以提高模型的稳定性和泛化能力。\n\nBagging: 例如，随机森林 (Random Forest)。通过bootstrap 采样创建多个训练集，在每个训练集上训练一个弱学习器，然后将多个弱学习器的预测结果进行平均或投票。\nBoosting: 例如，梯度提升树 (Gradient Boosting Decision Tree, GBDT)、XGBoost、LightGBM、AdaBoost。 迭代地训练弱学习器，每个弱学习器都试图纠正前一个弱学习器的错误。最终将多个弱学习器加权组合成一个强学习器。\n\n数据增强 (Data Augmentation): 通过对训练数据进行变换 (例如旋转、平移、缩放、裁剪等)，增加训练数据的多样性，提高模型的泛化能力。\n\n图像数据增强: 例如，旋转、平移、缩放、裁剪、翻转、颜色变换、添加噪声等。\n文本数据增强: 例如，同义词替换、随机插入、随机删除、回译等。\n音频数据增强: 例如，添加噪声、时间拉伸、音调变换等。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#正则化-regularization",
    "href": "05_model_assess.html#正则化-regularization",
    "title": "6  模型评估与优化",
    "section": "6.3 正则化 (Regularization)",
    "text": "6.3 正则化 (Regularization)\n正则化是一种在机器学习中常用的技术，用于防止模型过拟合，提高模型的泛化能力。其基本思想是在损失函数中添加一个正则化项，以惩罚模型的复杂度。\n\nL1 正则化 (Lasso Regularization): 向损失函数添加模型权重向量的 L1 范数。\n\n作用: 使模型权重稀疏化，产生稀疏模型，有助于特征选择。\n特点: 可以将一部分权重压缩为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} |w_i|\n  \\]\n\n\\(\\lambda\\): 正则化强度超参数。\n\\(w_i\\): 模型权重。\n\nL2 正则化 (Ridge Regularization): 向损失函数添加模型权重向量的 L2 范数。\n\n作用: 减小模型权重，使模型更平滑，降低模型复杂度。\n特点: 权重趋向于变小，但不会变为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} w_i^2\n  \\]\nElastic Net: 结合 L1 和 L2 正则化的方法。\n\n作用: 结合 L1 和 L2 正则化的优点，既可以进行特征选择，又可以减小模型权重。\n特点: 通过调节 L1 和 L2 正则化项的比例，平衡特征选择和权重衰减的效果。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda_1 \\sum_{i} |w_i| + \\lambda_2 \\sum_{i} w_i^2\n  \\]\n\n\\(\\lambda_1, \\lambda_2\\): L1 和 L2 正则化强度超参数。\n\nDropout: 一种在神经网络中常用的正则化技术。\n\n原理: 在训练过程中，随机地将一部分神经元的输出置为 0。\n作用: 强制网络学习更鲁棒的特征表示，减少神经元之间的共适应性，提高泛化能力。\n特点: 简单有效，计算成本低，常用于深度神经网络。\n\nEarly Stopping (提前终止): 在模型训练过程中，监控验证集上的性能指标，当验证集性能不再提升或开始下降时，提前停止训练。\n\n原理: 随着训练的进行，模型在训练集上的性能会不断提升，但验证集上的性能可能会先提升后下降 (过拟合)。Early Stopping 旨在找到验证集性能最佳的训练迭代次数。\n优点: 简单易用，无需额外计算，可以有效防止过拟合。\n缺点: 可能会错过全局最优解，需要合适的验证集划分。\n\nBatch Normalization (批量归一化): 在神经网络的每一层输入或激活函数之前，对数据进行归一化处理。\n\n原理: 将每一批次 (batch) 的数据归一化到均值为 0，方差为 1 的分布。\n作用: 加速模型训练，提高训练稳定性，减轻内部协变量偏移 (Internal Covariate Shift) 问题，并具有一定的正则化效果。\n特点: 常用于深度神经网络，可以提高模型的泛化能力和鲁棒性。\n\n数据增强 (Data Augmentation): 虽然前面已经作为模型优化策略单独列出，但数据增强也可以被视为一种正则化技术。\n\n原理: 通过增加训练数据的多样性，使模型接触到更多不同的数据样本。\n作用: 提高模型的泛化能力，减少模型对特定训练样本的过拟合。\n特点: 特别适用于图像、文本和音频等数据，是一种有效的数据正则化方法。\n\n模型剪枝 (Pruning): 减小模型复杂度的技术，常用于决策树和神经网络。\n\n决策树剪枝: 通过剪去决策树中不必要的节点，简化决策树结构，防止过拟合。\n神经网络剪枝: 移除神经网络中不重要的连接或神经元，减小模型大小，提高模型效率，并具有一定的正则化效果。\n方法: 可以基于权重大小、梯度大小或其他指标进行剪枝。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#总结",
    "href": "05_model_assess.html#总结",
    "title": "6  模型评估与优化",
    "section": "6.4 总结",
    "text": "6.4 总结\n\n模型评估是机器学习流程中至关重要的一步，它可以帮助我们了解模型性能、选择最佳模型、并指导模型改进。\n选择合适的评估指标取决于具体的机器学习任务类型。\n交叉验证可以更可靠地评估模型的泛化能力。\n超参数调优可以找到最佳的模型配置。\n模型优化是一个迭代过程，需要不断尝试不同的策略来提高模型性能。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html",
    "href": "lab03_titanic.html",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "",
    "text": "8 导言",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#泰坦尼克号乘客数据",
    "href": "lab03_titanic.html#泰坦尼克号乘客数据",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "8.1 泰坦尼克号乘客数据",
    "text": "8.1 泰坦尼克号乘客数据\n泰坦尼克号是历史上著名的客轮，1912年首航时与冰山相撞沉没，造成了1500多人遇难。本实践课我们将使用泰坦尼克号乘客数据，尝试预测乘客在这场灾难中的生存情况。\n数据分为两个数据集： - 训练集 train.csv：有关键变量Survived显示是否生还 - 测试集 test.csv：没有Survived变量\n变量定义如下表所示：\n\n\n\n变量\n定义\n取值说明\n\n\n\n\nSurvived\n生存状态\n0 = 未生存, 1 = 生存\n\n\nPclass\n船票等级\n1 = 一等舱, 2 = 二等舱, 3 = 三等舱\n\n\nSex\n性别\n\n\n\nAge\n年龄\n\n\n\nSibSp\n船上兄弟姐妹/配偶数量\n\n\n\nParch\n船上父母/子女数量\n\n\n\nTicket\n船票号码\n\n\n\nFare\n票价\n\n\n\nCabin\n客舱号码\n\n\n\nEmbarked\n登船港口\nC = 瑟堡, Q = 皇后镇, S = 南安普顿",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#机器学习的一般步骤",
    "href": "lab03_titanic.html#机器学习的一般步骤",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "8.2 机器学习的一般步骤",
    "text": "8.2 机器学习的一般步骤\n\n8.2.1 探索性分析\n\n对变量进行初步描述性统计分析，用以检测空值、不合法值、异常值等\n数据可视化展示，发现变量之间的关系\n\n\n\n8.2.2 数据清理与特征工程\n\n特征工程：从现有特征中提取更有价值的信息\n填补缺失值：对缺失数据进行合理的估计和填补\n\n\n\n8.2.3 数据建模与模型选择\n\n建立分类模型（sklearn包）\n\n在训练集上拟合模型\n根据交叉验证的模型评估指标选择超参数\n\n备选模型包括：\n\n逻辑回归模型\n决策树模型\n随机森林\n提升树（XGBoost）\n神经网络\n\n模型评估指标包括：\n\n精度accuracy\n查全率recall与F1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据基本信息与统计描述",
    "href": "lab03_titanic.html#数据基本信息与统计描述",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "9.1 数据基本信息与统计描述",
    "text": "9.1 数据基本信息与统计描述\n我们需要了解数据的基本信息，包括各变量的数据类型和缺失情况：\n\n\n代码\n# 查看数据基本信息\ntrain_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \n 12  train        891 non-null    int64  \ndtypes: float64(2), int64(6), object(5)\nmemory usage: 90.6+ KB\n\n\n对数值型变量进行统计描述，了解其分布特征：\n\n\n代码\n# 统计描述\ntrain_data.describe()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\ntrain\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n891.0\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n1.0\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n0.0\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n1.0\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n1.0\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n1.0\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n1.0\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n1.0\n\n\n\n\n\n\n\n检查各变量的缺失值情况：\n\n\n代码\n# 检查缺失值\nprint(\"缺失值统计:\")\nall_data.isnull().sum()\n\n\n缺失值统计:\n\n\nPassengerId       0\nSurvived        418\nPclass            0\nName              0\nSex               0\nAge             263\nSibSp             0\nParch             0\nTicket            0\nFare              1\nCabin          1014\nEmbarked          2\ntrain             0\ndtype: int64\n\n\n从上面的分析可以看出： - Age（年龄）有一部分缺失 - Cabin（船舱）缺失严重 - Embarked（登船港口）有少量缺失",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#生存情况分析",
    "href": "lab03_titanic.html#生存情况分析",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "9.2 生存情况分析",
    "text": "9.2 生存情况分析\n首先我们看一下整体的生存率：\n\n\n代码\n# 分析生存率\nsurvival_rate = train_data['Survived'].mean() * 100\nprint(f\"总体生存率: {survival_rate:.2f}%\")\n\n\n总体生存率: 38.38%",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据可视化分析",
    "href": "lab03_titanic.html#数据可视化分析",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "9.3 数据可视化分析",
    "text": "9.3 数据可视化分析\n通过可视化，我们可以更直观地了解不同特征与生存率之间的关系：\n\n\n代码\n# 设置图形大小\nplt.figure(figsize=(15, 12))\n\n# 1. 性别与生存率\nplt.subplot(2, 2, 1)\nsns.countplot(x='Sex', hue='Survived', data=train_data)\nplt.title('性别与生存状况')\nplt.xlabel('性别')\nplt.ylabel('人数')\n\n# 2. 船票等级与生存率\nplt.subplot(2, 2, 2)\nsns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.title('船票等级与生存状况')\nplt.xlabel('船票等级')\nplt.ylabel('人数')\n\n# 3. 年龄分布与生存情况\nplt.subplot(2, 2, 3)\nsns.histplot(data=train_data, x='Age', hue='Survived', multiple='stack', bins=20)\nplt.title('年龄分布与生存状况')\nplt.xlabel('年龄')\nplt.ylabel('人数')\n\n# 4. 家庭规模与生存情况\nplt.subplot(2, 2, 4)\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\nsns.countplot(x='FamilySize', hue='Survived', data=train_data)\nplt.title('家庭规模与生存状况')\nplt.xlabel('家庭规模')\nplt.ylabel('人数')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n从以上可视化分析可以看出：\n\n性别与生存率：女性的生存率明显高于男性，这可能与”妇女和儿童优先”的救生原则有关。\n船票等级与生存率：一等舱乘客生存率最高，三等舱乘客生存率最低，表明社会经济地位可能影响了获救机会。\n年龄分布与生存率：儿童的生存率相对较高，而中年人的生存率较低。\n家庭规模与生存率：家庭规模中等(2-4人)的乘客生存率较高，而独自一人或家庭规模过大的乘客生存率较低。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#特征工程",
    "href": "lab03_titanic.html#特征工程",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "10.1 特征工程",
    "text": "10.1 特征工程\n\n10.1.1 提取姓名中的头衔信息\n乘客的姓名中包含头衔信息（如Mr., Mrs., Miss等），这可能与社会地位和性别相关，进而影响生存率：\n\n\n代码\n# 提取姓名中的头衔\nall_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# 统计头衔分布\nprint(\"头衔分布:\")\nall_data['Title'].value_counts()\n\n\n头衔分布:\n\n\nTitle\nMr          757\nMiss        260\nMrs         197\nMaster       61\nRev           8\nDr            8\nCol           4\nMlle          2\nMajor         2\nMs            2\nLady          1\nSir           1\nMme           1\nDon           1\nCapt          1\nCountess      1\nJonkheer      1\nDona          1\nName: count, dtype: int64\n\n\n有些头衔出现次数很少，我们将它们合并为”Rare”类别：\n\n\n代码\n# 合并稀有头衔\nrare_titles = ['Capt', 'Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir', 'Countess']\nall_data['Title'] = all_data['Title'].replace(rare_titles, 'Rare')\nall_data['Title'] = all_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n\n# 合并后的头衔分布\nprint(\"合并后的头衔分布:\")\nall_data['Title'].value_counts()\n\n\n合并后的头衔分布:\n\n\nTitle\nMr        757\nMiss      264\nMrs       198\nMaster     61\nRare       29\nName: count, dtype: int64\n\n\n\n\n10.1.2 创建家庭规模特征\n我们将SibSp（兄弟姐妹/配偶数量）和Parch（父母/子女数量）相加，再加1（乘客自己），创建家庭规模特征：\n\n\n代码\n# 创建家庭规模特征\nall_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n\n# 创建是否独自一人特征\nall_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)\n\n# 查看是否独自一人的分布\nprint(\"是否独自一人的分布:\")\nall_data['IsAlone'].value_counts()\n\n\n是否独自一人的分布:\n\n\nIsAlone\n1    790\n0    519\nName: count, dtype: int64\n\n\n\n\n10.1.3 从Cabin提取信息\n虽然Cabin变量缺失严重，但我们仍可以提取是否有记录Cabin信息作为一个特征：\n\n\n代码\n# 从Cabin提取信息：是否有Cabin记录\nall_data['HasCabin'] = (~all_data['Cabin'].isnull()).astype(int)\n\nprint(\"Cabin记录情况分布:\")\nall_data['HasCabin'].value_counts()\n\n\nCabin记录情况分布:\n\n\nHasCabin\n0    1014\n1     295\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#缺失值处理",
    "href": "lab03_titanic.html#缺失值处理",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "10.2 缺失值处理",
    "text": "10.2 缺失值处理\n\n10.2.1 处理Age缺失值\n对年龄的缺失值，我们使用按Pclass和Sex分组的中位数进行填充：\n\n\n代码\n# Age缺失值填充（按Pclass和Sex分组的中位数）\nage_imputer = all_data.groupby(['Pclass', 'Sex'])['Age'].transform('median')\nall_data['Age'] = all_data['Age'].fillna(age_imputer)\n\n\n\n\n10.2.2 处理Embarked缺失值\n对登船港口缺失值，使用众数填充：\n\n\n代码\n# Embarked缺失值用众数填充\nmost_common_embarked = all_data['Embarked'].mode()[0]\nall_data['Embarked'] = all_data['Embarked'].fillna(most_common_embarked)\n\n\n\n\n10.2.3 处理Fare缺失值\n对票价缺失值，使用相同船票等级的中位数填充：\n\n\n代码\n# Fare缺失值用Pclass中位数填充\nfare_imputer = all_data.groupby('Pclass')['Fare'].transform('median')\nall_data['Fare'] = all_data['Fare'].fillna(fare_imputer)\n\n# 检查缺失值是否都已处理\nprint(\"缺失值处理后的统计:\")\nall_data[['Age', 'Embarked', 'Fare']].isnull().sum()\n\n\n缺失值处理后的统计:\n\n\nAge         0\nEmbarked    0\nFare        0\ndtype: int64",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据转换",
    "href": "lab03_titanic.html#数据转换",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "10.3 数据转换",
    "text": "10.3 数据转换\n将分类变量转换为数值型变量，便于模型处理：\n\n\n代码\n# 类别特征转换为数值\nall_data['Sex'] = all_data['Sex'].map({'male': 0, 'female': 1})\nall_data['Embarked'] = all_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\nall_data['Title'] = all_data['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\n\n\n删除不需要的特征：\n\n\n代码\n# 删除不需要的特征\nall_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n\n# 查看处理后的数据结构\nall_data.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\ntrain\nTitle\nFamilySize\nIsAlone\nHasCabin\n\n\n\n\n0\n0.0\n3\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n2\n0\n0\n\n\n1\n1.0\n1\n1\n38.0\n1\n0\n71.2833\n1\n1\n2\n2\n0\n1\n\n\n2\n1.0\n3\n1\n26.0\n0\n0\n7.9250\n0\n1\n1\n1\n1\n0\n\n\n3\n1.0\n1\n1\n35.0\n1\n0\n53.1000\n0\n1\n2\n2\n0\n1\n\n\n4\n0.0\n3\n0\n35.0\n0\n0\n8.0500\n0\n1\n0\n1\n1\n0",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#准备训练和测试数据",
    "href": "lab03_titanic.html#准备训练和测试数据",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.1 准备训练和测试数据",
    "text": "11.1 准备训练和测试数据\n\n\n代码\n# 准备训练和测试数据\ntrain_data = all_data[all_data['train'] == 1].drop('train', axis=1)\ntest_data = all_data[all_data['train'] == 0].drop(['train', 'Survived'], axis=1)\n\nX_train = train_data.drop('Survived', axis=1)\ny_train = train_data['Survived'].astype(int)\n\nprint(f\"训练特征形状: {X_train.shape}\")\nprint(f\"训练标签形状: {y_train.shape}\")\nprint(f\"测试特征形状: {test_data.shape}\")\n\n\n训练特征形状: (891, 11)\n训练标签形状: (891,)\n测试特征形状: (418, 11)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#模型训练与评估函数",
    "href": "lab03_titanic.html#模型训练与评估函数",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.2 模型训练与评估函数",
    "text": "11.2 模型训练与评估函数\n定义一个通用函数用于训练模型并评估性能：\n\n\n代码\n# 模型训练与评估函数\ndef train_and_evaluate(model, X, y, cv=5, model_name=\"模型\"):\n    \"\"\"训练模型并评估性能\"\"\"\n    # 交叉验证评估\n    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n    precision = cross_val_score(model, X, y, cv=cv, scoring='precision')\n    recall = cross_val_score(model, X, y, cv=cv, scoring='recall')\n    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')\n    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n    \n    # 输出评估结果\n    print(f\"{model_name}交叉验证结果：\")\n    print(f\"准确率: {accuracy.mean():.4f} (±{accuracy.std():.4f})\")\n    print(f\"精确率: {precision.mean():.4f} (±{precision.std():.4f})\")\n    print(f\"召回率: {recall.mean():.4f} (±{recall.std():.4f})\")\n    print(f\"F1分数: {f1.mean():.4f} (±{f1.std():.4f})\")\n    print(f\"ROC AUC: {roc_auc.mean():.4f} (±{roc_auc.std():.4f})\")\n    \n    # 在全部训练数据上拟合模型\n    model.fit(X, y)\n    return model",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#逻辑回归模型",
    "href": "lab03_titanic.html#逻辑回归模型",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.3 逻辑回归模型",
    "text": "11.3 逻辑回归模型\n\n\n代码\n# 逻辑回归模型\nlogreg_model = LogisticRegression(max_iter=1000, random_state=42)\nlogreg_fitted = train_and_evaluate(logreg_model, X_train, y_train, model_name=\"逻辑回归\")\n\n# 查看特征系数\nlogreg_coef = pd.DataFrame(\n    logreg_fitted.coef_[0],\n    index=X_train.columns,\n    columns=['系数']\n).sort_values('系数', ascending=False)\n\nprint(\"\\n逻辑回归系数（特征重要性）:\")\nlogreg_coef\n\n\n逻辑回归交叉验证结果：\n准确率: 0.8159 (±0.0139)\n精确率: 0.7708 (±0.0385)\n召回率: 0.7454 (±0.0315)\nF1分数: 0.7566 (±0.0159)\nROC AUC: 0.8613 (±0.0194)\n\n逻辑回归系数（特征重要性）:\n\n\n\n\n\n\n\n\n\n系数\n\n\n\n\nSex\n2.149786\n\n\nHasCabin\n0.690087\n\n\nTitle\n0.489232\n\n\nEmbarked\n0.155761\n\n\nFare\n0.002529\n\n\nParch\n-0.025733\n\n\nAge\n-0.042430\n\n\nSibSp\n-0.268012\n\n\nFamilySize\n-0.282378\n\n\nIsAlone\n-0.461523\n\n\nPclass\n-0.827330",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#决策树模型",
    "href": "lab03_titanic.html#决策树模型",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.4 决策树模型",
    "text": "11.4 决策树模型\n\n\n代码\n# 决策树模型\n# 超参数网格搜索\nparam_grid = {\n    'max_depth': range(1, 20),\n    'min_samples_split': range(2, 20)\n}\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_grid = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\nprint(f\"决策树最佳参数: {dt_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {dt_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = range(1, 20)\ntest_scores = []\n\nfor depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=depth, \n                              min_samples_split=dt_grid.best_params_['min_samples_split'],\n                              random_state=42)\n    dt.fit(X_train, y_train)\n    # 使用交叉验证来评估\n    score = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('决策树性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\ndt_fitted = train_and_evaluate(dt_grid.best_estimator_, X_train, y_train, model_name=\"决策树\")\n\n# 特征重要性\ndt_importance = pd.DataFrame(\n    dt_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n决策树特征重要性:\")\ndt_importance\n\n\n决策树最佳参数: {'max_depth': 4, 'min_samples_split': 17}\n最佳交叉验证分数: 0.8260\n决策树交叉验证结果：\n准确率: 0.8260 (±0.0237)\n精确率: 0.8136 (±0.0331)\n召回率: 0.7159 (±0.1060)\nF1分数: 0.7549 (±0.0577)\nROC AUC: 0.8510 (±0.0399)\n\n决策树特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.661197\n\n\nFamilySize\n0.141799\n\n\nFare\n0.131015\n\n\nPclass\n0.041912\n\n\nAge\n0.012085\n\n\nHasCabin\n0.011992\n\n\nSex\n0.000000\n\n\nSibSp\n0.000000\n\n\nParch\n0.000000\n\n\nEmbarked\n0.000000\n\n\nIsAlone\n0.000000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#随机森林模型",
    "href": "lab03_titanic.html#随机森林模型",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.5 随机森林模型",
    "text": "11.5 随机森林模型\n\n\n代码\n# 随机森林模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [10, 30, 50, 70, 100, 200],\n    'max_depth': [3, 5, 7, 10, 15, 20, None]\n}\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_grid = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nrf_grid.fit(X_train, y_train)\n\nprint(f\"随机森林最佳参数: {rf_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {rf_grid.best_score_:.4f}\")\n\n# 可视化n_estimators对模型性能的影响\nplt.figure(figsize=(10, 6))\nn_estimators = [10, 30, 50, 70, 100, 200]\ntest_scores = []\n\nfor n in n_estimators:\n    rf = RandomForestClassifier(n_estimators=n,\n                              max_depth=rf_grid.best_params_['max_depth'],\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(n_estimators, test_scores)\nplt.xlabel('n_estimators (树的数量)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随树数量的变化')\nplt.grid(True)\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 10, 15, 20]\ntest_scores = []\n\nfor depth in max_depths:\n    rf = RandomForestClassifier(n_estimators=rf_grid.best_params_['n_estimators'],\n                              max_depth=depth,\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nrf_fitted = train_and_evaluate(rf_grid.best_estimator_, X_train, y_train, model_name=\"随机森林\")\n\n# 特征重要性\nrf_importance = pd.DataFrame(\n    rf_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n随机森林特征重要性:\")\nrf_importance\n\n\n随机森林最佳参数: {'max_depth': 5, 'n_estimators': 10}\n最佳交叉验证分数: 0.8294\n随机森林交叉验证结果：\n准确率: 0.8294 (±0.0124)\n精确率: 0.7989 (±0.0240)\n召回率: 0.7453 (±0.0610)\nF1分数: 0.7690 (±0.0281)\nROC AUC: 0.8710 (±0.0378)\n\n随机森林特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.367692\n\n\nSex\n0.146653\n\n\nAge\n0.097934\n\n\nFare\n0.093234\n\n\nHasCabin\n0.089965\n\n\nPclass\n0.085205\n\n\nFamilySize\n0.052974\n\n\nSibSp\n0.032125\n\n\nEmbarked\n0.018703\n\n\nParch\n0.011941\n\n\nIsAlone\n0.003575",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#xgboost模型",
    "href": "lab03_titanic.html#xgboost模型",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.6 XGBoost模型",
    "text": "11.6 XGBoost模型\n\n\n代码\n# XGBoost模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, 9],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n}\n\nxgb_model = xgb.XGBClassifier(random_state=42)\nxgb_grid = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\n\nprint(f\"XGBoost最佳参数: {xgb_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {xgb_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 9, 11]\ntest_scores = []\n\nfor depth in max_depths:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=depth,\n        learning_rate=xgb_grid.best_params_['learning_rate'],\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随最大深度的变化')\nplt.grid(True)\n\n# 可视化learning_rate对模型性能的影响\nplt.figure(figsize=(10, 6))\nlearning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\ntest_scores = []\n\nfor lr in learning_rates:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=xgb_grid.best_params_['max_depth'],\n        learning_rate=lr,\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(learning_rates, test_scores)\nplt.xlabel('learning_rate (学习率)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随学习率的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nxgb_fitted = train_and_evaluate(xgb_grid.best_estimator_, X_train, y_train, model_name=\"XGBoost\")\n\n# 特征重要性\nxgb_importance = pd.DataFrame(\n    xgb_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\nXGBoost特征重要性:\")\nxgb_importance\n\n\nXGBoost最佳参数: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}\n最佳交叉验证分数: 0.8440\nXGBoost交叉验证结果：\n准确率: 0.8440 (±0.0229)\n精确率: 0.8236 (±0.0284)\n召回率: 0.7571 (±0.0667)\nF1分数: 0.7870 (±0.0394)\nROC AUC: 0.8686 (±0.0345)\n\nXGBoost特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.545888\n\n\nPclass\n0.111701\n\n\nFamilySize\n0.095393\n\n\nHasCabin\n0.082403\n\n\nSex\n0.035206\n\n\nFare\n0.034001\n\n\nAge\n0.027042\n\n\nParch\n0.025975\n\n\nSibSp\n0.022535\n\n\nEmbarked\n0.019856\n\n\nIsAlone\n0.000000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#神经网络模型",
    "href": "lab03_titanic.html#神经网络模型",
    "title": "7  泰坦尼克号生存预测实践",
    "section": "11.7 神经网络模型",
    "text": "11.7 神经网络模型\n在本节中，我们将从简单的全连接神经网络开始，逐步增加网络复杂度，并展示模型从欠拟合到过拟合的演变过程，最后通过引入正则化技术来解决过拟合问题。\n首先，我们需要对特征进行标准化，这对神经网络的训练非常重要：\n\n\n代码\n# 对特征进行标准化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ntest_data_scaled = scaler.transform(test_data)\n\n# 划分训练集和验证集\nX_train_nn, X_val, y_train_nn, y_val = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42\n)\n\nprint(f\"神经网络训练集形状: {X_train_nn.shape}\")\nprint(f\"神经网络验证集形状: {X_val.shape}\")\n\n\n神经网络训练集形状: (712, 11)\n神经网络验证集形状: (179, 11)\n\n\n\n11.7.1 简单全连接网络 - 可能欠拟合\n我们先从一个非常简单的全连接网络开始，看看其性能如何：\n\n\n代码\n# 定义简单的全连接网络\ndef create_simple_nn():\n    model = keras.Sequential([\n        layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练简单神经网络\nsimple_nn = create_simple_nn()\nsimple_nn.summary()  # 显示模型结构\n\n# 训练模型\nsimple_history = simple_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nsimple_train_loss, simple_train_acc = simple_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nsimple_val_loss, simple_val_acc = simple_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"简单网络 - 训练集准确率: {simple_train_acc:.4f}\")\nprint(f\"简单网络 - 验证集准确率: {simple_val_acc:.4f}\")\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 8)                 96        \n                                                                 \n dense_1 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 105 (420.00 Byte)\nTrainable params: 105 (420.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n简单网络 - 训练集准确率: 0.8287\n简单网络 - 验证集准确率: 0.7989\n\n\n绘制简单网络的学习曲线：\n\n\n代码\n# 绘制简单网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(simple_history.history['accuracy'])\nplt.plot(simple_history.history['val_accuracy'])\nplt.title('简单网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(simple_history.history['loss'])\nplt.plot(simple_history.history['val_loss'])\nplt.title('简单网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n11.7.2 中等复杂度网络 - 适度拟合\n现在，让我们增加网络的复杂度，添加更多层和更多神经元：\n\n\n代码\n# 定义中等复杂度的网络\ndef create_medium_nn():\n    model = keras.Sequential([\n        layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练中等复杂度神经网络\nmedium_nn = create_medium_nn()\nmedium_nn.summary()  # 显示模型结构\n\n# 训练模型\nmedium_history = medium_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nmedium_train_loss, medium_train_acc = medium_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nmedium_val_loss, medium_val_acc = medium_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"中等网络 - 训练集准确率: {medium_train_acc:.4f}\")\nprint(f\"中等网络 - 验证集准确率: {medium_val_acc:.4f}\")\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 16)                192       \n                                                                 \n dense_3 (Dense)             (None, 8)                 136       \n                                                                 \n dense_4 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 337 (1.32 KB)\nTrainable params: 337 (1.32 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n中等网络 - 训练集准确率: 0.8511\n中等网络 - 验证集准确率: 0.8045\n\n\n绘制中等复杂度网络的学习曲线：\n\n\n代码\n# 绘制中等网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(medium_history.history['accuracy'])\nplt.plot(medium_history.history['val_accuracy'])\nplt.title('中等网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(medium_history.history['loss'])\nplt.plot(medium_history.history['val_loss'])\nplt.title('中等网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n11.7.3 复杂网络 - 可能过拟合\n现在，我们进一步增加网络复杂度，使其具有更多层和更多神经元，观察是否会出现过拟合：\n\n\n代码\n# 定义复杂网络\ndef create_complex_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练复杂神经网络\ncomplex_nn = create_complex_nn()\ncomplex_nn.summary()  # 显示模型结构\n\n# 训练模型\ncomplex_history = complex_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,  # 增加训练轮次以观察过拟合\n    batch_size=16,  # 减小批量大小\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\ncomplex_train_loss, complex_train_acc = complex_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\ncomplex_val_loss, complex_val_acc = complex_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"复杂网络 - 训练集准确率: {complex_train_acc:.4f}\")\nprint(f\"复杂网络 - 验证集准确率: {complex_val_acc:.4f}\")\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 64)                768       \n                                                                 \n dense_6 (Dense)             (None, 32)                2080      \n                                                                 \n dense_7 (Dense)             (None, 16)                528       \n                                                                 \n dense_8 (Dense)             (None, 8)                 136       \n                                                                 \n dense_9 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 3521 (13.75 KB)\nTrainable params: 3521 (13.75 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n复杂网络 - 训练集准确率: 0.8975\n复杂网络 - 验证集准确率: 0.7989\n\n\n绘制复杂网络的学习曲线：\n\n\n代码\n# 绘制复杂网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(complex_history.history['accuracy'])\nplt.plot(complex_history.history['val_accuracy'])\nplt.title('复杂网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(complex_history.history['loss'])\nplt.plot(complex_history.history['val_loss'])\nplt.title('复杂网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n11.7.4 正则化网络 - 解决过拟合\n现在，我们将为复杂网络添加正则化技术，包括 Dropout 和 BatchNormalization，以解决过拟合问题：\n\n\n代码\n# 定义带有正则化的复杂网络\ndef create_regularized_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(32, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(16, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(8, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练带有正则化的神经网络\nregularized_nn = create_regularized_nn()\nregularized_nn.summary()  # 显示模型结构\n\n# 添加早停策略\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)\n\n# 训练模型\nregularized_history = regularized_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,\n    batch_size=16,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n# 评估性能\nregularized_train_loss, regularized_train_acc = regularized_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nregularized_val_loss, regularized_val_acc = regularized_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"正则化网络 - 训练集准确率: {regularized_train_acc:.4f}\")\nprint(f\"正则化网络 - 验证集准确率: {regularized_val_acc:.4f}\")\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_10 (Dense)            (None, 64)                768       \n                                                                 \n batch_normalization (Batch  (None, 64)                256       \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_11 (Dense)            (None, 32)                2080      \n                                                                 \n batch_normalization_1 (Bat  (None, 32)                128       \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_12 (Dense)            (None, 16)                528       \n                                                                 \n batch_normalization_2 (Bat  (None, 16)                64        \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_13 (Dense)            (None, 8)                 136       \n                                                                 \n batch_normalization_3 (Bat  (None, 8)                 32        \n chNormalization)                                                \n                                                                 \n dropout_3 (Dropout)         (None, 8)                 0         \n                                                                 \n dense_14 (Dense)            (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 4001 (15.63 KB)\nTrainable params: 3761 (14.69 KB)\nNon-trainable params: 240 (960.00 Byte)\n_________________________________________________________________\n正则化网络 - 训练集准确率: 0.8427\n正则化网络 - 验证集准确率: 0.8101\n\n\n绘制正则化网络的学习曲线：\n\n\n代码\n# 绘制正则化网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(regularized_history.history['accuracy'])\nplt.plot(regularized_history.history['val_accuracy'])\nplt.title('正则化网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(regularized_history.history['loss'])\nplt.plot(regularized_history.history['val_loss'])\nplt.title('正则化网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n11.7.5 模型比较\n我们来比较不同复杂度和正则化策略下神经网络的性能：\n\n\n代码\n# 比较不同网络的性能\nnn_models = ['简单网络', '中等网络', '复杂网络', '正则化网络']\ntrain_accuracy = [simple_train_acc, medium_train_acc, complex_train_acc, regularized_train_acc]\nval_accuracy = [simple_val_acc, medium_val_acc, complex_val_acc, regularized_val_acc]\n\nplt.figure(figsize=(12, 6))\nx = np.arange(len(nn_models))\nwidth = 0.35\n\nplt.bar(x - width/2, train_accuracy, width, label='训练集准确率')\nplt.bar(x + width/2, val_accuracy, width, label='验证集准确率')\n\nplt.ylabel('准确率')\nplt.title('不同神经网络模型的性能比较')\nplt.xticks(x, nn_models)\nplt.legend()\n\n# 显示差距\nfor i in range(len(nn_models)):\n    gap = train_accuracy[i] - val_accuracy[i]\n    plt.text(i, 0.5, f'差距: {gap:.4f}', ha='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n11.7.6 最终神经网络模型\n基于上述实验，我们选择性能最好的正则化网络作为最终的神经网络模型：\n\n\n代码\n# 使用正则化网络作为最终模型\nnn_model = regularized_nn\n\n# 在完整训练集上重新训练\nX_full_scaled = scaler.transform(X_train)\nnn_model.fit(X_full_scaled, y_train, epochs=50, batch_size=16, verbose=0)\n\n# 生成最终预测\nnn_pred = (nn_model.predict(X_full_scaled) &gt; 0.5).astype(int).flatten()\nnn_accuracy = accuracy_score(y_train, nn_pred)\nprint(f\"最终神经网络在训练集上的准确率: {nn_accuracy:.4f}\")\n\n# 将正则化网络用于测试集预测\ntest_pred_nn = (nn_model.predict(test_data_scaled) &gt; 0.5).astype(int).flatten()\n\n\n 1/28 [&gt;.............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/28 [==============================] - 0s 313us/step\n最终神经网络在训练集上的准确率: 0.8563\n 1/14 [=&gt;............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/14 [==============================] - 0s 323us/step\n\n\n\n\n11.7.7 神经网络模型小结\n通过这一系列实验，我们观察到：\n\n简单网络：容易欠拟合，训练集和验证集性能都不够理想\n中等网络：提高了模型复杂度，性能有所改善\n复杂网络：进一步增加复杂度，在训练集表现良好但可能在验证集上表现下降，出现过拟合\n正则化网络：通过添加Dropout和BatchNormalization等正则化技术，在保持模型复杂度的同时有效减轻了过拟合，使训练集和验证集的性能差距减小\n\n这个过程展示了神经网络建模中的一个关键问题：如何在模型复杂度和泛化能力之间取得平衡。正则化技术是解决这一问题的有效工具。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "06_credit.html",
    "href": "06_credit.html",
    "title": "8  信用评分理论基础",
    "section": "",
    "text": "8.1 信用评分理论基础",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "06_credit.html#信用评分理论基础",
    "href": "06_credit.html#信用评分理论基础",
    "title": "8  信用评分理论基础",
    "section": "",
    "text": "8.1.1 信用评分的概念与意义\n信用评分是一种利用统计模型和机器学习技术，对借款人或交易对手的信用风险进行量化评估的方法。它通过分析借款人的各种信息，如个人特征、财务状况、还款历史等，预测其未来违约的可能性，并以分数的形式呈现。\n信用评分的意义：\n\n风险管理： 帮助金融机构识别和评估信用风险，作为信贷决策的重要依据，降低坏账率。\n效率提升： 自动化审批流程，提高信贷审批效率，降低人工审核成本。\n差异化定价： 根据信用评分对客户进行风险分层，实现差异化定价，优化资源配置。\n金融普惠： 使得更多信用记录不足但信用良好的人群获得金融服务。\n\n\n\n8.1.2 信用风险度量\n信用风险是指借款人未能按时足额偿还债务的可能性。常用的信用风险度量指标包括：\n\n违约概率 (Probability of Default, PD)：指在一定时期内，借款人发生违约的可能性。信用评分模型的核心目标就是预测 PD。\n违约损失率 (Loss Given Default, LGD)：指发生违约时，债权人可能遭受的损失占总风险暴露的比例。\n违约风险暴露 (Exposure at Default, EAD)：指发生违约时，债权人面临的风险敞口总额。\n\n信用风险的期望损失 (Expected Loss, EL) 可以表示为：\n[ EL = PD LGD EAD ]\n信用评分模型主要关注违约概率 PD 的预测。\n\n\n8.1.3 评分卡模型原理\n评分卡模型是一种常用的信用评分工具，它将借款人的各种特征转化为分数，然后将这些分数加总得到一个总分，总分越高表示信用风险越低。\n评分卡模型的基本原理：\n\n特征选择与处理： 选择与信用风险相关的特征，并进行数据清洗、缺失值处理、异常值处理、特征转换等。\n模型选择与训练： 常用的模型包括逻辑回归、决策树、支持向量机、神经网络等。逻辑回归因其可解释性和稳定性在评分卡中应用最为广泛。\n评分转换： 将模型预测的违约概率转换为评分值。常用的转换方法包括对数 odds 转换等。\n模型验证与评估： 评估模型的区分能力、校准性、稳定性和业务有效性。常用的评估指标包括 AUC、KS 统计量、Lift 图、PSI 指数等。\n\n常用评估指标详解：\n\nAUC (Area Under the ROC Curve): 衡量模型区分好坏客户的能力。AUC 值越接近 1，表示模型区分能力越强。ROC 曲线是以假正例率 (FPR) 为横轴，真正例率 (TPR) 为纵轴绘制的曲线。\nKS (Kolmogorov-Smirnov) 统计量: 衡量模型区分度的指标，表示好坏样本累积分布差异的最大值。KS 值越大，表示模型区分能力越强，通常用于选择最佳阈值。\n提升图 (Lift Chart): 展示模型相比随机选择，在识别好/坏客户方面的提升效果。\nPSI (Population Stability Index): 衡量模型在不同时间段或不同数据集上评分分布的稳定性。PSI 过高可能表示模型需要重新训练或调整。\n\n\n\n\n\n8.1.4 经典信用评分理论\n经典的信用评分理论，例如 5C 信用评估体系，为我们推测可能影响信用评分的指标提供了理论基础。5C 体系从五个方面评估借款人的信用状况：\n\n品格 (Character)：借款人的还款意愿和信用历史。\n\n可能指标：\n\n历史还款记录： 是否有逾期记录、逾期次数、逾期时长等。\n稳定性： 居住时长、工作时长、工作稳定性等。\n社交媒体行为： （在某些情况下）社交媒体上的信用相关行为。\n\n\n偿还能力 (Capacity)：借款人的收入水平和偿债能力。\n\n可能指标：\n\n收入水平： 月收入、年收入、工资流水等。\n职业类型： 职业稳定性、行业前景等。\n资产状况： 房产、车辆、存款、投资等。\n负债水平： 负债收入比、信用卡负债、其他贷款等。\n\n\n资本 (Capital)：借款人的净资产和财务实力。\n\n可能指标：\n\n净资产： 总资产减去总负债。\n自有资金比例： 例如，在购房贷款中，首付比例。\n紧急备用金： 可随时动用的资金储备。\n\n\n抵押品 (Collateral)：借款人提供的抵押物或担保物。\n\n可能指标：\n\n抵押物类型： 房产、车辆、设备等。\n抵押物价值： 抵押物的评估价值。\n抵押率： 贷款金额与抵押物价值的比例。\n\n\n环境 (Condition)：外部经济环境和行业状况。\n\n可能指标：\n\n宏观经济指标： GDP 增长率、失业率、通货膨胀率、利率水平等。\n行业景气度： 借款人所在行业的整体发展状况。\n政策法规： 相关的信贷政策、监管法规等。\n\n\n\n\n\n8.1.5 FICO 评分\nFICO 评分是由美国 Fair Isaac Corporation (FICO) 公司开发的一种个人信用评分系统，是目前在美国最广泛应用的信用评分模型。它被广泛应用于信贷审批、贷款定价、风险管理等领域。\nFICO 评分范围：\nFICO 评分通常在 300 到 850 之间。分数越高，表示信用风险越低。\nFICO 评分的主要影响因素 (权重由高到低):\n\n还款历史 (Payment History) (约占 35%): 这是最重要的因素。包括：\n\n是否按时还款 (信用卡、贷款等)。\n逾期记录 (逾期次数、逾期时长、逾期金额)。\n不良记录 (例如，催收、破产等)。\n\n欠款金额 (Amounts Owed) (约占 30%): 包括：\n\n总欠款金额。\n已用信用额度比例 (Utilization Ratio)：已用信用额度 / 总信用额度。 比例越低越好。\n欠款账户数量。\n\n信用历史长度 (Length of Credit History) (约占 15%): 包括：\n\n最早信用账户的开户时间。\n平均信用账户的开户时间。\n信用历史越长，通常评分越高。\n\n新开立信用账户 (New Credit) (约占 10%): 包括：\n\n新开立信用账户的数量。\n短期内申请信用账户的频率。\n频繁申请新的信用账户可能会降低评分。\n\n信用类型 (Credit Mix) (约占 10%): 包括：\n\n拥有的不同类型的信用账户，例如：\n\n循环信用 (Revolving Credit)：信用卡、信用额度贷款等。\n分期付款信用 (Installment Credit)：房屋贷款、汽车贷款、个人贷款等。\n\n拥有多种类型的信用账户，并良好管理，通常对评分有正面影响。\n\n\nFICO 评分的意义：\n\n信贷决策： 银行和其他金融机构使用 FICO 评分来评估借款人的信用风险，决定是否批准贷款、信用卡申请，以及确定贷款利率和额度。\n消费者信用管理： 消费者可以通过了解 FICO 评分的影响因素，更好地管理自己的信用，提高信用评分，从而获得更好的金融服务。\n\n总结：\n信用评分理论基础是构建有效信用评分模型的基石。理解信用评分的概念、意义、风险度量方法和评分卡模型原理，并结合经典信用评估理论，可以帮助我们更好地选择和构建信用评分模型所需的特征指标，从而提升模型的预测能力和业务价值。\n\n\n8.1.6 信用评分面临的挑战与伦理考量\n尽管信用评分在金融领域发挥着重要作用，但也面临着一些挑战和伦理问题：\n\n数据稀疏性与冷启动问题 (Data Sparsity & Cold Start): 对于缺乏信用历史记录的“白户”或“薄档”人群，难以建立准确的评分模型。\n数据质量与准确性 (Data Quality & Accuracy): 评分模型的准确性高度依赖于输入数据的质量。错误或过时的数据可能导致评分偏差。\n模型偏见与公平性 (Model Bias & Fairness): 模型可能无意中学习到数据中存在的社会偏见（如地域、种族、性别歧视），导致对特定群体的评分不公平。确保算法公平性是一个重要的研究方向。\n模型可解释性与透明度 (Interpretability & Transparency): 复杂的机器学习模型（如深度学习）往往缺乏可解释性，使得用户和监管机构难以理解评分决策的原因，这与“负责任的人工智能”原则相悖。\n模型漂移与稳定性 (Model Drift & Stability): 随着时间的推移，经济环境和用户行为会发生变化，导致模型性能下降（模型漂移）。需要定期监控和更新模型。\n数据隐私与安全 (Data Privacy & Security): 信用评分涉及大量敏感个人信息，必须严格遵守相关法律法规（如 GDPR、个人信息保护法），确保数据安全和用户隐私。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "06_credit.html#互联网时代信用评分的新发展",
    "href": "06_credit.html#互联网时代信用评分的新发展",
    "title": "8  信用评分理论基础",
    "section": "8.2 互联网时代信用评分的新发展",
    "text": "8.2 互联网时代信用评分的新发展\n随着互联网技术的发展和普及，越来越多的互联网企业开始构建自己的信用评分模型，以服务于其业务场景，例如消费金融、电商信贷、共享经济等。这些模型通常具有以下特点：\n\n数据来源多样化： 除了传统的金融数据外，还包括大量的互联网行为数据，例如：\n\n电商数据： 购物历史、消费偏好、收货地址、退货记录等。\n社交数据： 社交关系、社交行为、社交媒体信息等（需注意隐私合规）。\n行为数据： App 使用行为、网站浏览行为、地理位置信息等。\n运营商数据： 通话记录、短信记录、流量使用情况等（需注意隐私合规）。\n\n模型算法更复杂： 更多地采用机器学习和人工智能技术，例如：\n\n集成学习模型： GBDT、XGBoost、LightGBM 等。\n深度学习模型： 神经网络、循环神经网络等。\n图神经网络： 用于分析社交网络和关系数据。\n\n评分维度更丰富： 除了传统的信用风险评估，还可能包括：\n\n消费能力评估： 预测用户的消费潜力，用于授信额度确定。\n欺诈风险评估： 识别欺诈行为，保障交易安全。\n用户质量评估： 评估用户的活跃度、忠诚度、价值贡献等。\n\n\n一些典型的互联网企业信用评分模型案例：\n\n蚂蚁金服 - 芝麻信用: 中国最早、也是最知名的互联网信用评分体系之一。\n\n数据来源： 支付宝、淘宝、天猫等阿里系电商和金融平台数据，以及合作机构数据。\n评分维度： 身份特质、行为偏好、履约能力、信用历史、人脉关系五个维度。\n应用场景： 消费金融（花呗、借呗）、免押服务、信用租房、信用出行等。\n\n腾讯 - 腾讯信用分 (已停止更新): 腾讯也曾推出过信用评分产品，但目前已停止更新。\n\n数据来源： 微信、QQ 等社交平台数据，以及腾讯支付、游戏等业务数据。\n评分维度： 财富、安全、守约、消费、社交五个维度。\n应用场景： 微信支付分、部分消费金融场景。\n\n京东 - 京东小白信用: 京东推出的信用评分产品。\n\n数据来源： 京东电商平台数据、京东金融数据等。\n评分维度： 身份、资产、偏好、履约能力、关系五个维度。\n应用场景： 京东白条、购物优惠、会员权益等。\n\nFoursquare - Pilgrim SDK: 美国地理位置社交网络 Foursquare 提供的 Pilgrim SDK，可以基于用户的位置数据进行风险评估。\n\n数据来源： 用户的位置轨迹、签到数据等。\n评分维度： 用户行为的真实性、稳定性、风险偏好等。\n应用场景： 反欺诈、风险控制、个性化推荐等。\n\n\n总结：\n互联网企业信用评分模型是传统信用评分在互联网时代的创新和发展。它们利用更丰富的数据来源、更复杂的算法模型和更多样的评分维度，为互联网业务场景提供了更精准、更全面的信用风险评估和用户画像能力。但同时也面临着数据隐私、算法公平性、模型可解释性等新的挑战。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "07_unsupervised.html",
    "href": "07_unsupervised.html",
    "title": "9  非监督学习技术概览及其金融应用",
    "section": "",
    "text": "9.1 非监督学习简介\n非监督学习 (Unsupervised Learning) 是机器学习的一个分支，其主要特点是训练数据没有预先标记的输出标签。与监督学习不同，非监督学习的目标是从数据本身发现隐藏的结构、模式或关系。它试图理解数据的内在分布和特征，而不是预测一个特定的目标变量。\n非监督学习在探索性数据分析、数据预处理和发现未知模式方面非常有用。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>非监督学习技术概览及其金融应用</span>"
    ]
  },
  {
    "objectID": "07_unsupervised.html#主要非监督学习技术",
    "href": "07_unsupervised.html#主要非监督学习技术",
    "title": "9  非监督学习技术概览及其金融应用",
    "section": "9.2 主要非监督学习技术",
    "text": "9.2 主要非监督学习技术\n以下是一些主要的非监督学习技术及其在金融领域的应用：\n\n9.2.1 1. 聚类 (Clustering)\n聚类是将数据集中的样本划分为若干个相似组（簇）的过程，使得同一簇内的样本彼此相似，而不同簇之间的样本差异较大。\n常用算法：\n\nK-Means (K均值聚类): 基于距离的划分聚类算法，简单高效，适用于大数据集。需要预先指定簇的数量 K。\n层次聚类 (Hierarchical Clustering): 通过构建样本间的层次结构来进行聚类，可以是凝聚式（自底向上）或分裂式（自顶向下）。不需要预先指定簇数，但计算复杂度较高。\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): 基于密度的聚类算法，可以发现任意形状的簇，并能有效处理噪声点。不需要预先指定簇数。\n高斯混合模型 (Gaussian Mixture Model, GMM): 基于概率模型的聚类方法，假设数据由多个高斯分布混合而成。\n\n金融应用场景：\n\n客户细分 (Customer Segmentation): 根据客户的交易行为、人口统计特征、风险偏好等将客户分组，以便进行精准营销、个性化推荐和风险管理。\n异常检测 (Anomaly Detection): 识别与正常模式显著不同的交易或行为，用于欺诈检测、洗钱识别等。\n投资组合构建 (Portfolio Construction): 将具有相似风险收益特征的资产聚类，辅助构建多元化的投资组合。\n\n\n\n9.2.2 2. 降维 (Dimensionality Reduction)\n降维是指在保留数据主要信息的前提下，减少数据特征数量的过程。高维数据往往存在冗余信息、计算复杂度高、可视化困难等问题（“维度灾难”）。\n常用算法：\n\n主成分分析 (Principal Component Analysis, PCA): 最常用的线性降维方法，通过寻找数据方差最大的方向（主成分）来投影数据，实现降维。\n线性判别分析 (Linear Discriminant Analysis, LDA): 一种有监督的降维方法（但也常与非监督场景对比），旨在最大化类间距离和最小化类内距离，常用于分类任务的预处理。(注意：严格来说 LDA 是有监督的，但常与 PCA 一起讨论)\nt-分布随机邻域嵌入 (t-Distributed Stochastic Neighbor Embedding, t-SNE): 非线性降维方法，特别擅长高维数据的可视化，能较好地保留数据的局部结构。\n自编码器 (Autoencoders): 基于神经网络的非线性降维方法，通过编码器将数据压缩到低维表示，再通过解码器重构原始数据。\n\n金融应用场景：\n\n风险因子识别 (Risk Factor Identification): 从大量市场指标中提取主要的风险因子，简化风险模型。\n数据可视化 (Data Visualization): 将高维金融数据（如客户特征、资产特征）降维到二维或三维空间进行可视化，便于理解和分析。\n特征工程 (Feature Engineering): 减少模型输入的特征数量，提高模型训练效率和泛化能力，避免过拟合。\n资产定价 (Asset Pricing): 减少影响资产价格的因素数量，构建更简洁的定价模型。\n\n\n\n9.2.3 3. 文本分析 (Text Analysis) / 自然语言处理 (NLP) - 非监督部分\n虽然 NLP 包含监督和非监督方法，但许多非监督技术在处理金融文本数据中至关重要。\n常用非监督算法/技术：\n\n词嵌入 (Word Embeddings): 如 Word2Vec, GloVe, FastText。将词语表示为低维稠密向量，捕捉词语间的语义关系。\n主题建模 (Topic Modeling): 如潜在狄利克雷分配 (Latent Dirichlet Allocation, LDA - 注意与线性判别分析区分)。从大量文档中自动发现隐藏的主题结构。\n文本聚类 (Text Clustering): 将相似的文本（如新闻、报告、评论）分组。\n\n金融应用场景：\n\n舆情分析 (Sentiment Analysis): 分析新闻、社交媒体、研报等文本数据中的市场情绪或对特定公司/资产的情感倾向。（通常需要结合监督学习进行情感分类，但词嵌入和主题模型是基础）\n信息提取 (Information Extraction): 从财报、公告、合同等非结构化文本中自动提取关键信息（如公司名称、财务数据、关键条款）。\n文档摘要与分类 (Document Summarization & Classification): 自动生成研报摘要，或将大量金融文档按主题分类。\n风险信号挖掘 (Risk Signal Mining): 从新闻或监管文件中识别潜在的风险事件或趋势。\n\n非监督学习为理解和利用没有明确标签的大量金融数据提供了强大的工具集，有助于发现隐藏的模式、简化复杂性并做出更明智的决策。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>非监督学习技术概览及其金融应用</span>"
    ]
  },
  {
    "objectID": "07_cluster.html",
    "href": "07_cluster.html",
    "title": "10  非监督学习：聚类 (Clustering)",
    "section": "",
    "text": "10.1 聚类 (Clustering)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "07_cluster.html#聚类-clustering",
    "href": "07_cluster.html#聚类-clustering",
    "title": "10  非监督学习：聚类 (Clustering)",
    "section": "",
    "text": "10.1.1 核心概念\n聚类是一种非监督学习技术，其目标是将数据集中的样本根据它们的相似性划分为若干个组，这些组被称为”簇” (Cluster)。聚类的核心思想是：\n\n簇内相似性最大化 (Maximize Intra-cluster Similarity): 同一个簇内的数据点（样本）应该尽可能相似。\n簇间差异性最大化 (Maximize Inter-cluster Dissimilarity): 不同簇之间的数据点应该尽可能不相似。\n\n在金融领域，我们可以对各种对象进行聚类，例如：\n\n资产 (Assets): 如股票、债券，根据它们的风险收益特征、价格波动模式等进行聚类。\n客户 (Customers): 根据客户的交易行为、风险偏好、人口统计学特征等进行聚类。\n市场时期 (Market Regimes): 根据市场波动性、相关性等指标对不同的市场阶段进行聚类。\n\n\n\n10.1.2 常用算法简介\n\n10.1.2.1 K-Means (K均值聚类)\n\n原理: K-Means 是一种迭代算法，旨在将数据划分为预先指定的 K 个簇。它通过不断更新簇的中心点（质心）并将每个数据点分配给最近的质心来实现。\n\n随机选择 K 个初始质心。\n分配步骤: 将每个数据点分配给距离其最近的质心，形成 K 个簇。\n更新步骤: 重新计算每个簇的质心（通常是簇内所有点的均值）。\n重复步骤 2 和 3，直到质心不再发生显著变化或达到最大迭代次数。\n\n优点:\n\n算法简单，容易理解和实现。\n计算效率高，处理大数据集速度较快。\n\n缺点:\n\n需要预先指定簇的数量 K，而 K 值的选择往往比较困难。\n对初始质心的选择敏感，可能陷入局部最优解。\n对非球状簇、不同大小和密度的簇效果不佳。\n对异常值（Outliers）比较敏感。\n\n\n\n\n10.1.2.2 层次聚类 (Hierarchical Clustering)\n\n原理: 层次聚类通过构建数据点之间的层次结构（通常表示为树状图，Dendrogram）来进行聚类。它有两种主要方式：\n\n凝聚式 (Agglomerative): 自底向上，开始时每个点自成一簇，然后逐步合并最相似的簇，直到所有点合并为一个簇。\n分裂式 (Divisive): 自顶向下，开始时所有点在一个簇，然后逐步分裂最不相似的簇，直到每个点自成一簇或满足停止条件。\n\n优点:\n\n不需要预先指定簇的数量 K，可以根据树状图决定合适的簇数。\n可以揭示数据的层次结构关系。\n对簇的形状没有严格假设。\n\n缺点:\n\n计算复杂度较高，特别是对于大数据集（通常为 O(n^2) 或 O(n^3)）。\n一旦合并或分裂完成，就无法撤销，可能导致次优结果。\n算法选择（如距离度量、合并策略）对结果影响较大。\n\n\n\n\n10.1.2.3 DBSCAN (基于密度的聚类)\n\n原理: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一种基于密度的聚类算法，它将密度相连的点划分为一个簇，并且能够发现任意形状的簇，同时自动识别噪声点。\n\n核心概念:\n\nε (Epsilon): 邻域半径，定义点之间的”近”的概念。\nMinPts: 邻域内最少点数，用于判断密度。\n核心点 (Core Point): 在其 ε 邻域内至少有 MinPts 个点的点。\n边界点 (Border Point): 不是核心点，但在某个核心点的 ε 邻域内的点。\n噪声点 (Noise Point): 既不是核心点也不是边界点的点。\n\n算法步骤:\n\n任选一个未被访问的点 p。\n标记 p 为已访问。\n如果 p 是核心点，创建一个新簇，并将 p 的所有密度可达点加入该簇。\n如果 p 不是核心点，标记为噪声点并继续。\n重复以上步骤，直到所有点都被访问。\n\n\n优点:\n\n不需要预先指定簇的数量。\n能发现任意形状的簇，不限于球形簇。\n能自动识别和处理噪声点。\n对离群点不敏感。\n\n缺点:\n\n参数选择（ε 和 MinPts）有时较为困难。\n对数据集中密度差异较大的簇效果不佳。\n计算复杂度较高（约为 O(n²)），但通常可以通过空间索引优化。\n不能处理高维空间中的”维度灾难”问题。\n\n\n\n\n\n10.1.3 具体金融市场应用实例：股票板块轮动分析\n目标: 利用聚类技术识别股票市场中可能存在的板块轮动现象或隐藏的股票群体特征。\n步骤:\n\n数据收集:\n\n选择一个股票池，例如沪深300指数的成分股。\n收集这些股票在过去一段时间（如一年）的每日收益率数据。\n\n数据预处理:\n\n处理缺失值（如有）。\n可以考虑对收益率数据进行标准化，以消除量纲影响。\n\n聚类分析 (使用 K-Means):\n\n选择 K-Means 算法对股票进行聚类，输入数据为每只股票的日收益率序列（可以看作一个高维向量）。\n尝试不同的 K 值（例如，从 K=5 到 K=10）。可以使用肘部法则或轮廓系数等方法辅助选择最优的 K 值（见下文评估方法）。\n\n结果解读与分析:\n\n簇成员分析: 查看每个形成的簇中包含哪些股票。\n\n传统行业板块验证: 这些簇是否大致对应于已知的传统行业板块（如银行、地产、科技、消费、医药等）？\n跨行业/因子聚类发现: 是否出现了跨越传统行业的聚类？例如，一个簇可能同时包含高分红的银行股、公用事业股和部分稳定消费股，这可能揭示了一个基于”价值”或”低波动”因子的股票群体。另一个簇可能包含新能源、半导体和部分高端制造股票，揭示了基于”高成长性”或”科技主题”的群体。\n\n簇表现分析: 计算每个簇内股票在不同时间段（如最近一个月、一个季度）的平均收益率、整体波动性（如平均标准差）。\n\n识别强势/弱势群体: 比较各簇的表现，识别出近期表现显著优于或劣于市场平均水平的股票群体。例如，如果发现某个包含新能源和高端制造的簇近期平均收益率远超大盘指数，这可能指示了当前的市场热点和资金流向。\n板块轮动证据: 观察不同时间段内各簇表现的相对变化，可以为板块轮动的投资策略提供数据支持。例如，上个季度表现强势的”价值”簇本季度表现平平，而”成长”簇开始领涨。\n\n\n\n意义: 通过聚类，可以超越传统的行业划分，从数据驱动的角度发现股票之间更深层次的关联性（如同涨同跌模式），为投资组合构建和择时策略提供新的视角。\n\n\n10.1.4 评估方法 (详细介绍)\n如何评估聚类效果的好坏，以及如何选择合适的 K 值？\n\n10.1.4.1 1. 轮廓系数 (Silhouette Coefficient)\n轮廓系数是评估聚类质量的一种方法，它综合考虑了簇内的紧密度和簇间的分离度。\n\n计算方法:\n\n对于数据集中的每个样本点 i：\n\n计算 a(i): 点 i 与同簇中其他所有点的平均距离（衡量簇内紧密度）。\n计算 b(i): 点 i 与距离最近的其他簇中所有点的平均距离（衡量簇间分离度）。\n轮廓系数 s(i) = (b(i) - a(i)) / max(a(i), b(i))。\n\n数据集的整体轮廓系数是所有样本点轮廓系数的平均值。\n\n轮廓系数解读:\n\n值域范围：[-1, 1]\n接近 +1：表示样本与自己的簇很匹配，与相邻簇的差距很大，聚类效果好。\n接近 0：表示样本处于两个簇的边界附近。\n接近 -1：表示样本可能被分配到了错误的簇。\n通常认为，平均轮廓系数大于 0.5 是较好的聚类结果，大于 0.7 是优秀的聚类结果。\n\n使用方法:\n\n对不同的簇数量 (K) 计算平均轮廓系数。\n选择平均轮廓系数最大的 K 值。\n可以额外检查每个簇的平均轮廓系数，以识别问题簇（轮廓系数较低的簇）。\n\nPython 实现:\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\n# 计算整体轮廓系数\nsil_score = silhouette_score(X, labels)\n\n# 计算每个样本的轮廓系数\nsil_samples = silhouette_samples(X, labels)\n\n# 计算每个簇的平均轮廓系数\nfor i in range(n_clusters):\n    cluster_sil = sil_samples[labels == i].mean()\n    print(f\"Cluster {i} silhouette coefficient: {cluster_sil:.3f}\")\n\n\n\n10.1.4.2 2. 肘部法则 (Elbow Method)\n肘部法则是一种启发式方法，用于确定 K-Means 中的最佳聚类数量 K。\n\n原理:\n\n对不同的 K 值运行 K-Means 算法，计算每个 K 值对应的簇内平方和 (WCSS, Within-Cluster Sum of Squares)。WCSS 是每个点与其所属簇的中心点之间距离的平方和。\n绘制 K 值与 WCSS 的关系图。\n随着 K 值的增加，WCSS 总体呈下降趋势，但下降速率会逐渐变缓。\n寻找图中的”肘部”，即曲线下降速率发生明显变化的点。该点对应的 K 值被视为较佳选择。\n\n数学表示:\n\nWCSS = Σ(xi - μj)²，其中 xi 是簇 j 中的点，μj 是簇 j 的中心点。\n在 scikit-learn 中，WCSS 可以通过 K-Means 的 inertia_ 属性获取。\n\n选择肘部的依据:\n\n当 K 值小于真实簇数时，增加 K 会显著减少 WCSS（曲线陡峭部分）。\n当 K 值超过真实簇数时，再增加 K 对 WCSS 的减少效果不再明显（曲线平缓部分）。\n肘部通常是曲线从陡峭变为平缓的转折点。\n\nPython 实现:\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, marker='o')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('WCSS (Inertia)')\nplt.grid(True)\nplt.show()\n注意事项:\n\n肘部法则是一种视觉方法，有时肘部可能不够明显，需要结合业务理解和其他评估指标（如轮廓系数）来确定最终的 K 值。\n如果数据中的簇数量不明显，曲线可能会很平滑，没有明显的肘部。\n该方法主要适用于球形簇，对于复杂形状的簇可能效果不佳。\n\n\n\n\n10.1.4.3 3. 戴维斯-布尔丁指数 (Davies-Bouldin Index, DBI)\n戴维斯-布尔丁指数是另一种评估聚类质量的度量，它关注簇内相似度和簇间差异性的比率。\n\n计算方法:\n\n对于每对簇 i 和 j，计算两个簇的”相似度” Rij = (Si + Sj) / Dij，其中：\n\nSi 和 Sj 分别是簇 i 和簇 j 的平均点到中心的距离（簇内散布度）。\nDij 是簇 i 和簇 j 中心之间的距离（簇间距离）。\n\n对于每个簇 i，找到它与其他所有簇 j 的最大相似度 max(Rij)。\nDBI 是所有簇最大相似度的平均值。\n\nDBI 解读:\n\n值越小越好，表示簇内紧密且簇间分离。\n理想的聚类会产生低的 DBI 值。\n\n使用方法:\n\n对不同的簇数量 (K) 计算 DBI。\n选择 DBI 值最小的 K 值。\n\nPython 实现:\nfrom sklearn.metrics import davies_bouldin_score\n\ndbi = davies_bouldin_score(X, labels)\nprint(f\"Davies-Bouldin Index: {dbi:.3f}\")\n\n\n\n10.1.4.4 4. 为 DBSCAN 选择参数\n与 K-Means 不同，DBSCAN 不需要预先指定簇的数量，但需要选择两个关键参数：邻域半径 ε 和最小点数 MinPts。\n\n选择 MinPts:\n\n一般经验法则是设置 MinPts ≥ 维度 + 1，例如对于二维数据，MinPts ≥ 3。\n更大的 MinPts 值可以减少噪声点的误判，但也可能导致小簇被视为噪声。\n常用值范围：3-10，但对于大型数据集可能需要更大的值。\n\n选择 ε (Epsilon):\n\nK-距离图:\n\n对每个点，计算到其第 k 个最近邻点 (k = MinPts) 的距离。\n对这些距离进行升序排序，并绘制成图。\n寻找曲线上的”拐点”，该点对应的距离值可作为 ε 的值。\n\nPython 实现 K-距离图:\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 选择 MinPts 值，例如 4\nMinPts = 4\n\n# 找到每个点的 k 个最近邻\nnbrs = NearestNeighbors(n_neighbors=MinPts).fit(X)\ndistances, indices = nbrs.kneighbors(X)\n\n# 距离排序\ndistances = np.sort(distances[:, MinPts-1])\n\n# 绘制 K-距离图\nplt.figure(figsize=(10, 6))\nplt.plot(range(len(distances)), distances)\nplt.title('K-Distance Plot (k={})'.format(MinPts))\nplt.xlabel('Points sorted by distance')\nplt.ylabel('Distance to k-th nearest neighbor')\nplt.grid(True)\nplt.show()\n\n\n通过综合使用上述评估方法，我们可以更客观、定量地判断聚类算法的效果，选择合适的参数，从而获得更有意义的聚类结果。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "lab07_cluster.html",
    "href": "lab07_cluster.html",
    "title": "11  聚类算法实践",
    "section": "",
    "text": "11.1 1. 环境设置\n首先导入必要的库和设置绘图参数：\n代码\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.datasets import make_blobs, make_moons\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.mixture import GaussianMixture\nfrom scipy import stats\n\n# 设置随机种子\nnp.random.seed(42)\n\n# 设置matplotlib参数\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# 创建保存图像的函数\nfrom pathlib import Path\nIMAGES_PATH = Path() / \"images\" / \"clustering\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>聚类算法实践</span>"
    ]
  },
  {
    "objectID": "lab07_cluster.html#k均值聚类",
    "href": "lab07_cluster.html#k均值聚类",
    "title": "11  聚类算法实践",
    "section": "11.2 2. K均值聚类",
    "text": "11.2 2. K均值聚类\n\n11.2.1 2.1 分类 vs 聚类\n我们先来看一个简单的例子，展示分类和聚类的区别：\n\n\n代码\n# 加载鸢尾花数据集\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target\n\nplt.figure(figsize=(9, 3.5))\n\n# 左侧：带有类别标签的数据（分类问题）\nplt.subplot(121)\nplt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\nplt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\nplt.xlabel(\"花瓣长度\")\nplt.ylabel(\"花瓣宽度\")\nplt.grid()\nplt.legend()\n\n# 右侧：不带标签的数据（聚类问题）\nplt.subplot(122)\nplt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\nplt.xlabel(\"花瓣长度\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\nplt.grid()\n\nsave_fig(\"classification_vs_clustering_plot\")\nplt.show()\n\n\n\n\n11.2.2 2.2 创建和拟合K均值模型\n让我们生成一些数据点并应用K均值聚类：\n\n\n代码\n# 生成模拟数据\nblob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n                         [-2.8,  2.8], [-2.8,  1.3]])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nX, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\n\n# 可视化数据\nplt.figure(figsize=(8, 4))\nplt.scatter(X[:, 0], X[:, 1], c='k', s=1)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\nsave_fig(\"blobs_plot\")\nplt.show()\n\n# 使用K-Means聚类\nk = 5\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\nprint(\"预测的簇标签（前10个）:\", y_pred[:10])\nprint(\"簇中心:\", kmeans.cluster_centers_)\n\n\n\n\n11.2.3 2.3 绘制决策边界（沃罗诺伊图）\n\n\n代码\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                           show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans, X)\nsave_fig(\"voronoi_plot\")\nplt.show()\n\n\n\n\n11.2.4 2.4 软聚类 vs 硬聚类\nK-means通常执行硬聚类（每个点属于一个簇），但我们也可以查看每个点到各个簇中心的距离：\n\n\n代码\n# 创建新的测试点\nX_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n\n# 计算到每个簇中心的距离\ndistances = kmeans.transform(X_new)\nprint(\"每个测试点到各个簇中心的距离:\")\nprint(distances.round(2))\n\n# 预测簇标签\ny_pred_new = kmeans.predict(X_new)\nprint(\"预测的簇标签:\", y_pred_new)\n\n\n\n\n11.2.5 2.5 K均值算法的步骤演示\n我们来看看K均值算法的迭代过程：\n\n\n代码\nkmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n                      random_state=5)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n                      random_state=5)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=3,\n                      random_state=5)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(321)\nplot_data(X)\nplot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.tick_params(labelbottom=False)\nplt.title(\"更新质心（初始随机）\")\n\nplt.subplot(322)\nplot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n                         show_ylabels=False)\nplt.title(\"标记实例\")\n\nplt.subplot(323)\nplot_decision_boundaries(kmeans_iter1, X, show_centroids=False,\n                         show_xlabels=False)\nplot_centroids(kmeans_iter2.cluster_centers_)\n\nplt.subplot(324)\nplot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,\n                         show_ylabels=False)\n\nplt.subplot(325)\nplot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\nplot_centroids(kmeans_iter3.cluster_centers_)\n\nplt.subplot(326)\nplot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n\nsave_fig(\"kmeans_algorithm_plot\")\nplt.show()\n\n\n\n\n11.2.6 2.6 K均值的变异性问题\nK均值对初始质心的位置很敏感，不同的初始点可能导致不同的结果：\n\n\n代码\ndef plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(10, 3.2))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2)\n\nkmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\nkmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n\nplot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n                          \"解决方案 1\",\n                          \"解决方案 2（使用不同的随机初始化）\")\n\nsave_fig(\"kmeans_variability_plot\")\nplt.show()\n\n# 查看惯性值\nprint(\"解决方案1的惯性:\", kmeans_rnd_init1.inertia_)\nprint(\"解决方案2的惯性:\", kmeans_rnd_init2.inertia_)\n\n\n\n\n11.2.7 2.7 确定最佳K值\n\n11.2.7.1 肘部法则\n\n\n代码\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"惯性\")\nplt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n             arrowprops=dict(facecolor='black', shrink=0.1))\nplt.text(4.5, 650, \"肘部\", horizontalalignment=\"center\")\nplt.axis([1, 8.5, 0, 1300])\nplt.grid()\nsave_fig(\"inertia_vs_k_plot\")\nplt.show()\n\n\n\n\n11.2.7.2 轮廓系数\n\n\n代码\nsilhouette_scores = [silhouette_score(X, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"轮廓分数\")\nplt.axis([1.8, 8.5, 0.55, 0.7])\nplt.grid()\nsave_fig(\"silhouette_score_vs_k_plot\")\nplt.show()\n\n\n\n\n11.2.7.3 轮廓图分析\n\n\n代码\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"簇\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"轮廓系数\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n\nsave_fig(\"silhouette_analysis_plot\")\nplt.show()\n\n\n\n\n\n11.2.8 2.8 K均值的局限性\nK均值在处理非球形或不同密度的簇时效果不佳：\n\n\n代码\n# 生成一个更难聚类的数据集\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX_difficult = np.r_[X1, X2]\n\n# 两种不同初始化方式的K-means\nkmeans_good = KMeans(n_clusters=3,\n                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n                     n_init=1, random_state=42)\nkmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmeans_good.fit(X_difficult)\nkmeans_bad.fit(X_difficult)\n\nplt.figure(figsize=(10, 3.2))\n\nplt.subplot(121)\nplot_decision_boundaries(kmeans_good, X_difficult)\nplt.title(f\"惯性 = {kmeans_good.inertia_:.1f}\")\n\nplt.subplot(122)\nplot_decision_boundaries(kmeans_bad, X_difficult, show_ylabels=False)\nplt.title(f\"惯性 = {kmeans_bad.inertia_:.1f}\")\n\nsave_fig(\"bad_kmeans_plot\")\nplt.show()\n\n\n\n\n11.2.9 2.9 图像分割应用\n让我们用K均值进行图像分割：\n\n\n代码\n# 加载图像\nimport PIL\n\n# 若没有图像，可直接使用代码中的示例图像链接下载\nimport urllib.request\nhoml3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\nfilename = \"ladybug.png\"\nfilepath = IMAGES_PATH / filename\nif not filepath.is_file():\n    print(\"下载\", filename)\n    url = f\"{homl3_root}/images/unsupervised_learning/{filename}\"\n    urllib.request.urlretrieve(url, filepath)\n\n# 读取图像并应用K-means\nimage = np.asarray(PIL.Image.open(filepath))\nprint(\"图像形状:\", image.shape)\n\nX_img = image.reshape(-1, 3)\nsegmented_imgs = []\nn_colors = (10, 8, 6, 4, 2)\n\nfor n_clusters in n_colors:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X_img)\n    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_imgs.append(segmented_img.reshape(image.shape))\n\n# 显示原始图像和分割后的图像\nplt.figure(figsize=(10, 5))\nplt.subplots_adjust(wspace=0.05, hspace=0.1)\n\nplt.subplot(2, 3, 1)\nplt.imshow(image)\nplt.title(\"原始图像\")\nplt.axis('off')\n\nfor idx, n_clusters in enumerate(n_colors):\n    plt.subplot(2, 3, 2 + idx)\n    plt.imshow(segmented_imgs[idx] / 255)\n    plt.title(f\"{n_clusters} 种颜色\")\n    plt.axis('off')\n\nsave_fig('image_segmentation_plot', tight_layout=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>聚类算法实践</span>"
    ]
  },
  {
    "objectID": "lab07_cluster.html#dbscan聚类",
    "href": "lab07_cluster.html#dbscan聚类",
    "title": "11  聚类算法实践",
    "section": "11.3 3. DBSCAN聚类",
    "text": "11.3 3. DBSCAN聚类\nDBSCAN是一种基于密度的聚类算法，特别适合处理形状不规则的簇。\n\n11.3.1 3.1 应用DBSCAN算法\n\n\n代码\n# 创建一个新月形数据集\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\n# 应用DBSCAN\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\n# 查看结果\nprint(\"标签（前10个）:\", dbscan.labels_[:10])\nprint(\"核心样本索引（前10个）:\", dbscan.core_sample_indices_[:10])\n\n\n\n\n11.3.2 3.2 可视化DBSCAN结果\n\n\n代码\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\n# 试两个不同的epsilon值\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nsave_fig(\"dbscan_plot\")\nplt.show()\n\n\n\n\n11.3.3 3.3 基于DBSCAN的分类\n可以使用DBSCAN找到的核心点构建分类器：\n\n\n代码\n# 使用核心点构建KNN分类器\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan2.components_, dbscan2.labels_[dbscan2.core_sample_indices_])\n\n# 预测新点的类别\nX_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\nprint(\"预测标签:\", knn.predict(X_new))\nprint(\"预测概率:\", knn.predict_proba(X_new).round(2))\n\n# 可视化分类边界\nplt.figure(figsize=(6, 3))\nplot_decision_boundaries(knn, X, show_centroids=False)\nplt.scatter(X_new[:, 0], X_new[:, 1], c=\"b\", marker=\"+\", s=200, zorder=10)\nsave_fig(\"cluster_classification_plot\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>聚类算法实践</span>"
    ]
  },
  {
    "objectID": "lab07_cluster.html#使用聚类进行半监督学习",
    "href": "lab07_cluster.html#使用聚类进行半监督学习",
    "title": "11  聚类算法实践",
    "section": "11.4 4. 使用聚类进行半监督学习",
    "text": "11.4 4. 使用聚类进行半监督学习\n聚类算法可以应用于半监督学习场景：\n\n\n代码\n# 加载digits数据集\nfrom sklearn.datasets import load_digits\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test = X_digits[:1400], X_digits[1400:]\ny_train, y_test = y_digits[:1400], y_digits[1400:]\n\n# 仅使用少量标记数据训练\nn_labeled = 50\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nprint(f\"使用{n_labeled}个标记样本的准确率:\", log_reg.score(X_test, y_test))\n\n# 使用K-means找到代表性数字\nk = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = X_digits_dist.argmin(axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\n# 绘制代表性数字\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n               interpolation=\"bilinear\")\n    plt.axis('off')\n\nsave_fig(\"representative_images_plot\", tight_layout=False)\nplt.show()\n\n# 手动标记这些代表性数字（这里使用预定义的标签）\ny_representative_digits = np.array([\n    1, 3, 6, 0, 7, 9, 2, 4, 8, 9,\n    5, 4, 7, 1, 2, 6, 1, 2, 5, 1,\n    4, 1, 3, 3, 8, 8, 2, 5, 6, 9,\n    1, 4, 0, 6, 8, 3, 4, 6, 7, 2,\n    4, 1, 0, 7, 5, 1, 9, 9, 3, 7\n])\n\n# 使用代表性数字训练模型\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nprint(\"使用代表性数字的准确率:\", log_reg.score(X_test, y_test))\n\n# 将标签传播到同一簇中的所有实例\ny_train_propagated = np.empty(len(X_train), dtype=np.int64)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n\n# 使用扩展的训练集\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train, y_train_propagated)\nprint(\"使用标签传播后的准确率:\", log_reg.score(X_test, y_test))\n\n# 移除异常值\npercentile_closest = 99\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\nprint(\"移除异常值后的准确率:\", log_reg.score(X_test, y_test))\n\n# 评估传播标签的质量\nprint(\"传播标签的准确率:\", \n      (y_train_partially_propagated == y_train[partially_propagated]).mean())",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>聚类算法实践</span>"
    ]
  },
  {
    "objectID": "lab07_cluster.html#总结与比较",
    "href": "lab07_cluster.html#总结与比较",
    "title": "11  聚类算法实践",
    "section": "11.5 5. 总结与比较",
    "text": "11.5 5. 总结与比较\n\n11.5.1 5.1 K均值优缺点：\n\n优点：\n\n算法简单，易于实现\n计算效率高，适合大数据集\n当簇呈球形且大小相近时效果好\n\n缺点：\n\n需要预先指定K值\n对初始质心选择敏感\n无法处理非球形簇\n对异常值敏感\n\n\n\n\n11.5.2 5.2 DBSCAN优缺点：\n\n优点：\n\n不需要预先指定簇的数量\n能识别任意形状的簇\n能自动识别噪声点\n对异常值不敏感\n\n缺点：\n\n参数选择（ε和MinPts）较为困难\n对密度差异大的簇效果不佳\n计算复杂度较高\n\n\n\n\n11.5.3 5.3 选择合适的聚类算法：\n\n如果簇形状规则、大小相似，且需要明确的簇数量，选择K均值\n如果簇形状不规则、有噪声点，且不知道簇的确切数量，选择DBSCAN\n对于复杂数据集，可能需要尝试多种算法并比较结果",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>聚类算法实践</span>"
    ]
  },
  {
    "objectID": "project1_LC.html",
    "href": "project1_LC.html",
    "title": "12  项目1：借贷违约风险评估",
    "section": "",
    "text": "12.1 项目背景\nLending Club（NYSE：LC）创立于2006年，是一家在线撮合借款和出借的P2P平台，公司位于美国旧金山。公司上线运营初期仅提供个人贷款（personal loans）服务，后增加了医疗信贷（patient loans）、车贷分期（auto refinancing loans）。2014年12月12日开始在纽交所挂牌交易，成为当年最大的科技股IPO，2014年前后公司增加小微企业贷（small business loans）服务。该公司报告称，截至2015年12月31日，已通过其平台发放了159.8亿美元的贷款。\n借款人可以在LC平台上申请1,000美元到40,000美元之间的无担保个人贷款。标准贷款期限为三年。投资者可以在LC网站上搜索和浏览贷款清单，并根据提供的有关借款人、贷款金额、贷款等级和贷款目的的信息选择他们想要投资的贷款。投资者从这些贷款的利息中获利。\nLC负责贷款的审批和定价，贷款对应票据凭证的发行，以及贷后月度收款付款以及逾期后催收等服务。贷款的实际发放者是一家注册在犹他州的商业银行Web Bank。贷款产生的违约风险、提前还款和再投资风险，都由投资者自行承担。\nLC自行开发了风险评估和定价模型。公司会采用来自两个以上信用局的FICO评分（由美国Fair Isaac公司开发出的个人信用评级法），有时候借款人满足以上所有要求，他们也可能被拒绝。LC可能会要求验证借款人的其他信息。虽然LC的贷款审批只需7天-14天，但目前只有10%的贷款申请被批准，约90%的贷款申请被拒绝。\nLending Club的收入来源为交易手续费、服务费和管理费。交易手续费是向借款人收取的费用；服务费是向投资者收取的费用；管理费是管理投资基金和其他管理账户的费用。交易手续费是Lending Club收入的主要来源。\n尽管被视为金融科技行业的先驱和最大的此类公司之一，LC在2016年初遇到了问题，难以吸引投资者，公司的一些贷款丑闻以及董事会对首席执行官雷诺拉普朗什披露信息的担忧导致其股价大幅下跌和拉普朗什辞职。\n2020年，LC收购了Radius Bank，并宣布将关闭其P2P借贷平台。现有账户持有人将继续对现有票据收取利息，直到每笔贷款还清或违约，但没有新贷款可用于个人投资。也不再可能像以前那样通过二级市场出售现有贷款。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目目标",
    "href": "project1_LC.html#项目目标",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.2 项目目标",
    "text": "12.2 项目目标\n本项目旨在利用Lending Club提供的历史贷款数据，构建机器学习模型以预测贷款是否会违约。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#数据简介",
    "href": "project1_LC.html#数据简介",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.3 数据简介",
    "text": "12.3 数据简介\nLending Club贷款数据，覆盖2007.6-2018.12时间段，包含以下几类信息：\n\n贷款基本信息：\n\nid：贷款唯一标识符\nissue_d：贷款发布时间\nloan_amnt：贷款金额\nterm：贷款期限（36或60个月）\nint_rate：贷款利率\ninstallment：每月还款额\ngrade & sub_grade：LC给出的信用评级\nloan_status：贷款状态（是否违约）\npurpose：贷款目的\n\n借款人信息：\n\nemp_title：工作职位\nemp_length：工作年限\nannual_inc：年收入\ndti：债务收入比(DTI)\nhome_ownership：房产拥有状态\n\n信用数据：\n\nfico_range_low & fico_range_high：FICO分数范围\nopen_acc：开放信用账户数\nrevol_bal：循环信用余额\nrevol_util：循环额度利用率\n\n\n原始数据集包含145个变量和约200万条记录。本项目将使用其中的子集进行分析。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#样本与变量选择",
    "href": "project1_LC.html#样本与变量选择",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.4 样本与变量选择",
    "text": "12.4 样本与变量选择\n\n时间范围：选择2013-2014年发放的、期限为3年的贷款数据。这些贷款在2018年底已全部结束，因此有完整的还款结果。\n特征选择原则：\n\n剔除所有贷后信息，因为这些信息在贷款发放时并不可得，包括：\n\n包含recover字段的变量（与回收相关）\n包含settlement字段的变量（与结算相关）\n包含pymnt字段的变量（与付款相关）\n以total_rec开始的变量（与收款总额相关）\n以out_prncp开始的变量（与未偿本金相关）\n\n只保留那些在贷款申请和审批过程中可获得的信息，以构建具有实际预测价值的模型",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目步骤建议仅供参考",
    "href": "project1_LC.html#项目步骤建议仅供参考",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.5 项目步骤建议（仅供参考）",
    "text": "12.5 项目步骤建议（仅供参考）\n\n12.5.1 数据清理与特征工程\n\n数据探索分析：\n\n计算各变量的基本统计量（均值、中位数、标准差等）\n检测并处理缺失值、异常值和不合法值\n分析目标变量的分布情况，评估类别不平衡程度\n\n数据预处理：\n\n缺失值处理：根据变量类型选择适当的填补方法（均值、中位数、众数或特殊值）\n异常值处理：识别并处理离群点（可使用箱线图、Z-score等方法）\n特征变换：将分类变量转换为哑变量，对数值变量进行标准化或归一化\n\n特征工程：\n\n特征选择：去除低方差特征、高度相关特征或具有较多缺失值的特征\n特征创建：根据业务理解创建新的特征（如各类比率、差值等）\n特征重要性评估：使用统计方法或模型预测能力评估特征重要性\n\n数据可视化：\n\n绘制变量分布图，分析变量与目标的关系\n使用散点图、热力图等展示变量间相关性\n生成变量重要性图表\n\n\n\n\n12.5.2 数据建模与模型评估\n\n数据集划分：\n\n训练集（60%）：用于模型训练\n验证集（20%）：用于超参数调优\n测试集（20%）：用于最终模型评估，模拟真实应用场景\n\n处理类别不平衡：\n\n尝试欠抽样（减少多数类样本）或过抽样（增加少数类样本）技术\n考虑SMOTE等合成样本生成方法\n调整类别权重或使用集成学习方法\n\n模型构建与选择： 尝试以下几种分类模型并进行比较：\n\nLogistic回归：基准模型，易于解释\n决策树：能够捕捉非线性关系，提供决策规则\n随机森林：降低过拟合风险，提高预测稳定性\n梯度提升树（如XGBoost、LightGBM）：通常具有较高的预测准确率\n\n模型调优：\n\n使用网格搜索或随机搜索方法确定最优超参数\n利用交叉验证评估模型稳定性\n根据验证集表现选择最佳模型配置\n\n模型评估：\n\n计算多种评估指标：\n\n混淆矩阵：TP、TN、FP、FN\n精度（Accuracy）：整体分类正确率\n查准率（Precision）：预测为违约中实际违约的比例\n查全率（Recall）：实际违约中被成功预测的比例\nF1分数：Precision和Recall的调和平均\nROC曲线与AUC值：评估模型在不同阈值下的性能\nKS统计量：衡量模型区分好坏客户的能力\n\n分析模型的业务价值：计算不同决策阈值下的潜在收益和损失\n\n模型解释：\n\n分析特征重要性\n部分依赖图或SHAP值分析，理解特征对预测的影响\n提出基于模型的业务洞见和建议",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#提交要求",
    "href": "project1_LC.html#提交要求",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.6 提交要求",
    "text": "12.6 提交要求\n\n项目报告：\n\n项目背景和目标的理解\n数据探索分析结果及发现\n特征工程和数据预处理的详细步骤\n模型构建、评估和比较的过程与结果\n最终模型的性能分析和业务意义解读\n项目总结与进一步改进建议\n报告长度建议不超过10页\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n说明对违约预测有显著影响的变量、特征工程、模型选择\n包含关键可视化图表\n演示时间控制在8分钟以内\n展示时间：2025年4月21日上课时\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据导入、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目报告和项目代码打包为一个ZIP文件\n报告提交截止日期：2025年5月5日23:59",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#评分标准",
    "href": "project1_LC.html#评分标准",
    "title": "12  项目1：借贷违约风险评估",
    "section": "12.7 评分标准",
    "text": "12.7 评分标准\n\n项目报告（40分）\n\n背景与目标理解：准确阐述项目背景、业务逻辑和预测目标\n数据探索分析：缺失值/异常值处理得当，可视化分析深入，数据分布描述清晰\n特征工程：特征选择合理，创造有效新特征，编码转换方法正确\n模型构建与评估：模型选择恰当，评估指标完整，对比分析深入\n总结建议：结论有数据支撑，改进建议具有实操性\n\n代码质量（30分）\n\n完整性：包含数据清洗、特征工程、建模评估全流程\n可重复性：代码可直接运行并复现结果\n规范性：代码结构清晰，有详细注释说明\n数据处理：缺失值/异常值处理逻辑正确\n模型实现：正确使用机器学习库，参数设置合理\n\n模型性能（20分）\n\n基准模型：实现合理的基准模型（如逻辑回归）\n优化模型：通过特征工程/参数调优显著提升性能\n模型对比：尝试3种以上模型并进行横向比较\n\n课堂展示（10分）\n\n内容组织：逻辑清晰，重点突出，时间控制得当\n可视化呈现：图表专业，信息传达有效\n问答环节：准确回答评委提问",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html",
    "href": "project2a_tspred.html",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "",
    "text": "13.1 项目背景\n上海证券交易所(上交所)成立于1990年11月26日，是中国大陆第一家证券交易所，与深圳证券交易所、北京证券交易所并称为中国大陆三大证券交易所。上证综指(Shanghai Composite Index)是以上海证券交易所挂牌上市的所有股票为样本，以发行量为权数计算的加权综合股价指数，是反映上海证券市场整体走势的重要指标。\n上证综指自1991年7月15日正式发布，基日为1990年12月19日，基点为100点。作为中国股市最具代表性的指数之一，上证综指的走势不仅反映了中国资本市场的整体状况，也在很大程度上反映了中国宏观经济的发展态势。\n金融时间序列预测一直是金融数据分析和量化投资领域的重要研究方向。相比传统的截面数据分析，时间序列数据具有明显的时序依赖性，这使得时间序列预测面临更多的挑战。特别是金融市场数据，其高波动性、非线性特征以及受多种复杂因素影响的特点，进一步增加了预测的难度。\n随着机器学习技术的发展，各种先进的预测方法被应用于金融时间序列分析，从传统的ARIMA、GARCH模型，到现代的支持向量机(SVM)、随机森林、深度学习网络等，为金融时间序列预测提供了更多可能性。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目目标",
    "href": "project2a_tspred.html#项目目标",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.2 项目目标",
    "text": "13.2 项目目标\n本项目旨在利用机器学习方法对上证综指的收益率进行时间序列预测。具体目标包括：\n\n理解并处理金融时间序列数据的特性，如平稳性、季节性、趋势等\n构建和评估不同的时间序列预测模型\n比较传统统计方法与现代机器学习方法在金融时间序列预测中的表现\n探索能够提高预测准确性的特征工程方法\n分析预测结果的经济意义和实际应用价值",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据简介",
    "href": "project2a_tspred.html#数据简介",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.3 数据简介",
    "text": "13.3 数据简介\n本项目将使用上证综指的历史数据，时间范围为2000年1月至2023年12月，包含以下几类信息：\n\n基础价格数据（CSMAR、WIND）：\n\nDate：交易日期\nOpen：开盘价\nHigh：最高价\nLow：最低价\nClose：收盘价\nAdj Close：调整后收盘价\nVolume：成交量\n\n派生指标（CSMAR、WIND）：\n\nReturns：日收益率（当日收盘价相对前一日收盘价的百分比变化）\nVolatility：基于历史窗口计算的波动率\n\n技术指标（需自行构建）：\n\n移动平均线(MA)：不同时间窗口的简单移动平均和指数移动平均\n相对强弱指数(RSI)\n布林带(Bollinger Bands)\nMACD(Moving Average Convergence Divergence)\n\n宏观经济数据（CSMAR、WIND）：\n\n中国GDP增长率\nCPI（消费者价格指数）\n利率\n汇率（美元/人民币）",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据特点与挑战",
    "href": "project2a_tspred.html#数据特点与挑战",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.4 数据特点与挑战",
    "text": "13.4 数据特点与挑战\n金融时间序列数据具有以下特点，这些都给预测带来了挑战：\n\n非平稳性：金融市场数据通常表现出非平稳特性，即统计特性随时间变化\n高噪声：市场价格受多种随机因素影响，含有大量噪声\n异方差性：金融数据的波动性往往会随时间聚集，表现为波动聚类现象\n胖尾分布：收益率分布常常表现出比正态分布更胖的尾部，意味着极端事件发生的概率更高\n长期记忆和短期记忆：金融时间序列可能同时表现出短期和长期的相关性特征",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目步骤建议仅供参考",
    "href": "project2a_tspred.html#项目步骤建议仅供参考",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.5 项目步骤建议（仅供参考）",
    "text": "13.5 项目步骤建议（仅供参考）\n\n13.5.1 数据预处理与探索性分析\n\n数据获取与清洗：\n\n下载上证综指历史数据\n处理缺失值（如节假日）\n检测并处理异常值\n确保时间索引的连续性和正确性\n\n探索性数据分析：\n\n绘制时间序列图，观察长期趋势和季节性模式\n计算并分析基本统计量（均值、标准差、偏度、峰度等）\n检验序列的平稳性（ADF检验、KPSS检验等）\n分析自相关性（ACF和PACF图）\n检测异方差性（ARCH效应检验）\n\n特征工程：\n\n构建基于不同滞后期的特征\n计算技术分析指标作为特征（如移动平均、相对强弱指数等）\n引入宏观经济变量作为外生变量\n特征选择与降维（如主成分分析）\n考虑时间结构特征（如一周中的某天、月份效应等）\n\n数据转换：\n\n差分变换以实现平稳性\n标准化或归一化处理\n考虑非线性变换（如对数变换）\n处理数据频率不匹配问题（如将宏观经济月度数据转换为日度数据）\n\n\n\n\n13.5.2 模型构建与评估\n\n传统时间序列模型：\n\n自回归模型(AR)\n移动平均模型(MA)\n自回归移动平均模型(ARMA)\n自回归积分移动平均模型(ARIMA)\n广义自回归条件异方差模型(GARCH)及其变种\n\n机器学习模型：\n\n支持向量回归(SVR)\n随机森林(RF)\n梯度提升树(如XGBoost、LightGBM)\nK近邻回归(KNN)\n神经网络模型(如MLP、RNN、LSTM)\n\n集成方法与混合模型：\n\n组合不同模型的预测结果\n构建混合模型融合时间序列模型和机器学习模型的优势\n\n交叉验证策略：\n\n使用时间序列交叉验证方法（如时间滚动窗口法）\n避免数据泄露\n考虑不同长度的训练窗口和预测窗口\n\n评估指标：\n\n均方误差(MSE)\n平均绝对误差(MAE)\n平均绝对百分比误差(MAPE)\n方向准确率（预测涨跌方向的准确度）\n信息系数(IC)和累积信息系数(CIC)\n\n\n\n\n13.5.3 预测结果分析与应用\n\n模型解释性分析：\n\n分析特征重要性\n理解模型学习到的模式\n分析不同市场条件下的预测表现\n\n交易策略构建：\n\n基于预测结果设计简单交易策略\n回测策略表现\n考虑交易成本和滑点\n计算策略风险调整后收益（如夏普比率）\n\n鲁棒性分析：\n\n在不同市场环境下测试模型性能\n分析模型对极端事件的适应能力\n考虑参数变化对预测结果的影响",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#提交要求",
    "href": "project2a_tspred.html#提交要求",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.6 提交要求",
    "text": "13.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据获取、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#评分标准",
    "href": "project2a_tspred.html#评分标准",
    "title": "13  项目2A：上证综指收益率时间序列预测",
    "section": "13.7 评分标准",
    "text": "13.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示数据分析、特征工程、模型构建和评估的完整流程\n技术理解：准确阐述金融时间序列特性及采用模型的优缺点\n结果分析：深入解读预测结果的经济意义和实际应用价值\n可视化呈现：包含时间序列分解图、ACF/PACF图、模型预测对比图等专业图表\n创新性：展示对模型局限性的思考或改进尝试\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含数据预处理、特征生成、模型训练、回测评估全流程\n时序处理：正确实现时间序列分割、滚动窗口验证，避免数据泄露\n可重复性：设置随机种子，保证结果可复现\n规范性：代码模块化设计，有清晰的函数注释和文档说明\n可视化：实现关键时序分析和预测结果的可视化功能\n\n模型性能（30分）\n\n基准模型：实现ARIMA/GARCH等传统时序模型作为基准\n优化模型：通过特征工程/模型融合提升预测精度\n模型多样性：至少包含3种不同类别模型（如ARIMA、LSTM、XGBoost）\n评估全面性：同时报告点预测精度（MSE）和方向准确性指标\n策略应用：设计基于预测结果的简单交易策略并评估其表现",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2b_text.html",
    "href": "project2b_text.html",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "",
    "text": "14.1 项目背景\n随着数字化转型的加速，网络安全已成为现代企业面临的最重要风险之一。数据泄露、系统入侵、勒索软件攻击等安全事件不仅会导致直接的经济损失，还会损害企业声誉、客户信任，甚至引发监管处罚和法律诉讼。据相关统计，中国企业平均数据泄露成本持续上升，重大网络安全事件对企业造成的负面影响也日益加剧。\n中国证监会及相关监管机构近年来逐步加强了对上市公司信息披露的要求，特别是在企业风险管理方面。《公开发行证券的公司信息披露内容与格式准则》要求上市公司在年度报告中披露可能面临的各类风险，包括网络安全风险。2021年，《数据安全法》和《个人信息保护法》的实施进一步提高了企业对网络安全和数据保护的合规要求，使得上市公司在财务报表中更加详细地披露相关风险信息。\n这些文本披露为研究者提供了丰富的数据源，使我们能够通过自然语言处理和机器学习技术，从财务报表文本中提取、量化和分析企业的网络安全风险。这种基于文本的风险分析方法，不仅可以帮助投资者识别潜在的高风险企业，还可以帮助监管机构发现网络安全披露不足的公司，并为企业自身提供行业基准和改进方向。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目目标",
    "href": "project2b_text.html#项目目标",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.2 项目目标",
    "text": "14.2 项目目标\n本项目旨在通过对中国上市公司财务报表的文本分析，构建模型以评估和预测企业的网络安全风险水平。具体目标包括：\n\n掌握中文财务文本数据的获取和预处理方法\n学习和应用中文文本分析的核心技术，包括文本特征提取、情感分析和主题建模\n构建能够评估企业网络安全风险水平的预测模型\n分析不同行业、不同规模企业的网络安全风险特征和趋势\n探索网络安全风险与企业财务表现、股价波动等因素之间的关系",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据简介",
    "href": "project2b_text.html#数据简介",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.3 数据简介",
    "text": "14.3 数据简介\n本项目将使用中国A股上市公司的财务报表数据，时间范围为2018年至2023年，主要包含以下数据来源：\n\n上市公司公告文本（巨潮网）：\n\n年度报告文本\n半年度报告文本\n季度报告文本\n临时公告和重大事件公告\n\n网络安全事件数据（需自行获取）：\n\n国家互联网应急中心(CNCERT)发布的安全事件报告\n公开报道的重大网络安全事件\n行业安全报告中记录的数据泄露和系统入侵事件\n\n财务和市场数据（CSMAR、WIND）：\n\n股价和股票回报数据\n财务比率和业绩指标\n市值和行业分类\n\n文本特征数据（需自行构建）：\n\n中文网络安全相关词汇表和术语库\n行业特定风险指标\n中文情感和语调词典",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据特点与挑战",
    "href": "project2b_text.html#数据特点与挑战",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.4 数据特点与挑战",
    "text": "14.4 数据特点与挑战\n中文财务报表文本分析面临以下特点和挑战：\n\n中文文本特性：中文没有空格分词，需要特殊的分词技术；同时存在词义多样、同义词丰富等特点\n非结构化数据：财务报表文本是典型的非结构化数据，需要特殊的处理和特征提取方法\n专业术语丰富：财务和网络安全领域均有大量专业术语，普通NLP模型可能难以准确理解\n披露规范差异：与欧美市场相比，中国上市公司在风险披露方面的规范和实践存在差异\n行业差异：不同行业面临的网络安全风险类型和程度各不相同\n时间演变：网络安全威胁和披露要求随时间不断变化\n因果关系复杂：网络安全风险与企业表现之间的因果关系难以确定",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目步骤建议仅供参考",
    "href": "project2b_text.html#项目步骤建议仅供参考",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.5 项目步骤建议（仅供参考）",
    "text": "14.5 项目步骤建议（仅供参考）\n\n14.5.1 数据收集与预处理\n\n文本数据获取：\n\n从巨潮网下载上市公司年报、半年报、季报文件\n提取文件中与风险相关的章节（尤其是”风险因素”和”管理层讨论与分析”部分）\n识别并提取与网络安全相关的段落\n\n中文文本清洗与标准化：\n\n对PDF文件进行清洗\n中文分词处理\n去除停用词\n简繁体转换（如有必要）\n标点符号和特殊字符处理\n\n创建标记数据集：\n\n标记历史上发生过重大网络安全事件的企业\n根据事件严重程度和影响创建风险等级标签\n构建时间序列标签，反映风险随时间变化\n\n\n\n\n14.5.2 特征工程与文本分析\n\n网络安全关键词提取：\n\n基于行业专家知识和相关法规构建中文网络安全术语词典（如”数据泄露”、“勒索软件”、“网络攻击”等）\n计算网络安全相关术语的出现频率、密度和分布位置\n对比不同时期同一企业的网络安全关键词变化\n分析风险披露部分中网络安全词汇占比\n\n风险披露的复杂性与模糊性：\n\n计算风险描述的语言复杂度（如句长、复合句比例）\n分析模糊表述词汇使用（如”可能”、“或许”、“不确定”等）\n对比明确风险描述与模糊风险描述比例\n构建模糊性指数，衡量风险披露的具体程度\n\n财务报表特定结构特征：\n\n提取风险因素章节位置和长度\n分析网络安全风险在整体风险披露中的相对位置\n计算网络安全风险披露的篇幅占比\n构建章节重要性特征（如风险因素被放在报告前部的企业可能对风险更重视）\n\n\n\n\n14.5.3 模型构建与评估\n\n基础文本聚类分析：\n\n使用简单K-means聚类（sklearn库）将企业按风险披露特征分为几个组\n尝试不同的聚类数量（如3-5类），观察企业风险分组情况\n计算并可视化各聚类中心，理解不同企业组的风险特点\n使用词云图直观展示各类企业风险表述的关键词差异\n\n简单风险指标构建：\n\n统计每份报告中网络安全关键词出现的数量作为风险指标\n计算风险词汇占总词数的百分比，作为风险关注度指标\n设计简单的风险评分公式：结合关键词频率、模糊词使用程度等\n对比不同企业的风险指标，建立相对排名\n\n基础主题分析：\n\n使用Python的gensim库实现简单的LDA主题模型\n设置3-5个主题，识别网络安全风险中的主要话题\n分析不同企业的主题分布情况\n观察主题随时间的变化趋势\n\n简易验证方法：\n\n随机抽取样本进行人工检查，验证模型发现的模式是否合理\n与公开报道的网络安全事件进行对比（作为间接验证）\n绘制散点图，观察企业在风险指标上的分布是否符合预期\n使用统计检验（如t检验）比较不同行业企业的风险指标差异\n\n\n\n\n14.5.4 分析与应用\n\n跨行业比较分析：\n\n比较中国不同行业上市公司的网络安全风险特征\n识别高风险行业特征\n分析中国特色行业（如互联网、金融科技等）的风险特点\n\n风险与财务表现关系：\n\n分析网络安全风险与股价波动的关系\n研究风险披露与企业估值的关联\n测量网络安全事件对企业长期财务影响\n\n披露质量评估：\n\n评估中国企业网络安全风险披露的完整性\n分析披露内容与实际风险的一致性\n比较不同市场板块（如主板、科创板、创业板）企业的披露差异\n\n案例研究：\n\n选择典型企业进行深入分析\n研究重大网络安全事件前后的披露变化\n分析最佳实践和常见缺陷",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#提交要求",
    "href": "project2b_text.html#提交要求",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.6 提交要求",
    "text": "14.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表和案例分析\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码\n代码应包含从数据获取、中文文本处理、特征工程到后续应用的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#评分标准",
    "href": "project2b_text.html#评分标准",
    "title": "14  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "14.7 评分标准",
    "text": "14.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示中文文本数据获取、预处理、特征提取和风险评估的完整流程\n技术理解：准确阐述中文财务文本分析的特点和所采用NLP技术的优缺点\n结果分析：深入解读企业网络安全风险评估结果及其行业差异\n可视化呈现：包含关键词频统计图、主题分布图、风险评分对比图等专业可视化图表\n创新性：展示特色词典构建或针对中文财务文本的特殊处理方法\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含文本获取、中文分词、特征提取、模型构建和结果评估全流程\n中文处理：正确实现中文文本的分词、去停用词和特征提取流程\n可重复性：设置随机种子，保证结果可复现，包含必要的词典文件\n规范性：代码模块化设计，有清晰的函数注释和文档说明（特别是中文注释）\n可视化：实现文本分析关键结果的可视化功能，包括词云、主题分布等\n\n模型性能（30分）\n\n特征工程：构建全面的中文网络安全相关特征，包括关键词频率、语义特征等\n风险指标：成功构建有效的网络安全风险评估指标体系\n模型多样性：至少实现3种不同文本分析方法（如词频统计、主题模型、情感分析）\n评估全面性：从多角度（行业对比、时间趋势、案例分析）评估分析结果\n实际应用：将文本分析结果与企业财务表现或市场反应进行关联分析",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  }
]