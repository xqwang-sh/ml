[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据挖掘与机器学习课程讲义",
    "section": "",
    "text": "本讲义将系统地介绍机器学习的基本概念、主要模型以及实践应用。\n引言:\n机器学习是人工智能的一个重要分支，它是指从数据中自动学习规律和模式，并利用这些规律和模式进行预测和决策的过程。机器学习在量化投资、金融科技等领域有广泛应用。由于课程时间有限，本讲义将重点介绍机器学习中的监督学习与无监督学习，以及其在金融预测中的应用。\n课程内容（理论）:\n\n机器学习基础\n监督学习理论(上)\n监督学习理论(下)\n模型评估与优化\n时间序列监督学习\n信用评分\n无监督学习\n聚类\n降维\n文本分析1:词频法与向量空间\n文本分析2:词向量与深度学习基础\n文本分析3:大语言模型及其应用\n\n课程内容（实践）:\n\n金融数据获取与数据分析基础\n泰坦尼克号生存预测实践\n\n课程项目:\n\n借贷违约风险评估\n\n下面两个项目二选一：\n2a. 股票价格预测 2b. 财务报表文本分析\n期末考试:\n\n期末考试复习指南\n\n使用说明:\n\n本讲义使用 Quarto 创建，可以方便地生成 HTML, PDF, ePub 等多种格式。\n点击左侧导航栏可以浏览不同章节的内容。\n\n希望本讲义能帮助您更好地学习和理解机器学习！",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html",
    "href": "01_ml_basic.html",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1 机器学习简介",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习简介",
    "href": "01_ml_basic.html#机器学习简介",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1.1 什么是机器学习？\n机器学习是人工智能领域中一个非常热门且快速发展的分支。简单来说，机器学习就是让计算机通过学习数据，而不是依赖明确的编程指令，来完成特定的任务或解决问题。想象一下，我们教小孩子认识猫和狗，不是告诉他们猫和狗的具体特征（比如多少根胡须，耳朵的形状），而是给他们看大量的猫和狗的图片，告诉他们哪些是猫，哪些是狗。通过不断学习，孩子就能自己总结出猫和狗的区别，并且能够识别新的猫和狗。机器学习的原理与之类似，它使用算法来解析数据，从中学习，然后利用学到的知识对新数据做出预测或决策。\n\n\n2.1.2 机器学习的主要特点\n\n数据驱动: 机器学习模型的核心是数据。模型从数据中学习规律，数据越多、质量越高，模型通常就越强大。\n自动学习: 机器学习系统能够自动地从数据中发现模式和规律，无需人工明确指定规则。\n持续优化: 机器学习模型可以通过不断学习新的数据来提升性能，使其能够适应变化的环境。\n泛化能力: 训练好的模型不仅能处理训练数据，还能对未见过的新数据进行预测或决策。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习的主要类型",
    "href": "01_ml_basic.html#机器学习的主要类型",
    "title": "2  机器学习基础",
    "section": "2.2 机器学习的主要类型",
    "text": "2.2 机器学习的主要类型\n\n2.2.1 监督学习 (Supervised Learning)\n监督学习就像是有一位老师（监督者）指导计算机学习。我们提供给计算机带有”标签”的数据，标签就是我们希望模型预测的答案。例如，如果我们想让模型识别图片中的水果是苹果还是香蕉，我们就需要提供大量已经标记好（苹果或香蕉）的水果图片给模型学习。模型学习的目标就是找到输入数据（水果图片）和输出标签（苹果或香蕉）之间的关系。\n主要特点:\n\n需要使用带有标签的数据进行训练。\n目标是学习输入特征与输出标签之间的映射关系。\n主要解决分类和回归问题。分类问题是预测数据属于哪个类别（例如，垃圾邮件检测），回归问题是预测一个连续的数值（例如，房价预测）。\n\n现实生活案例:\n\n垃圾邮件检测: 通过分析邮件的内容（关键词、发件人等）来判断邮件是否为垃圾邮件。\n图像识别: 识别图片中的物体，例如人脸识别、交通标志识别等。\n语音识别: 将语音转换成文字。\n\n金融领域应用示例:\n\n信用评分: 根据用户的个人信息和交易记录预测其信用等级。\n股票价格预测: 预测股票未来价格的涨跌趋势。\n客户流失预测: 预测哪些客户可能在未来一段时间内停止使用某项金融服务。\n\n\n\n2.2.2 无监督学习 (Unsupervised Learning)\n无监督学习则像是让计算机在没有老师指导的情况下，自己去探索数据的内在结构和模式。我们提供给计算机的数据没有标签，模型需要自己去发现数据中的隐藏信息。例如，给计算机一大堆新闻报道，让它自己将这些新闻按照主题进行分类，这就是一个无监督学习的任务。\n主要特点:\n\n使用没有标签的数据进行学习。\n目标是发现数据中的内在结构、模式或关系。\n常用于聚类、降维和关联规则挖掘等任务。聚类是将相似的数据点 grouping 在一起，降维是在保留数据主要信息的同时减少数据的维度，关联规则挖掘是发现数据中不同项之间的关联关系。\n\n现实生活案例:\n\n客户分群: 根据用户的购买行为将用户分成不同的群体，以便进行个性化营销。\n社交网络分析: 分析社交网络中用户之间的关系，发现社区结构或影响力中心。\n异常检测: 在大量数据中找出异常或不正常的点，例如信用卡欺诈检测。\n\n金融领域应用示例:\n\n投资组合风险分析: 通过聚类分析将不同的投资资产进行分类，评估投资组合的风险。\n市场细分: 将市场上的客户按照不同的特征进行细分，以便更好地了解市场需求。\n交易异常检测: 检测金融市场中不正常的交易行为，例如内幕交易或市场操纵。\n\n\n\n2.2.3 强化学习 (Reinforcement Learning)\n强化学习更像是训练一只宠物。我们不直接告诉宠物应该做什么，而是通过奖励或惩罚来引导它学习。计算机作为一个”智能体”，在与环境的交互中不断尝试不同的动作。如果某个动作让它达到了目标（例如，在游戏中获得高分，或者在交易中获得盈利），我们就给予奖励；如果动作不好，就给予惩罚。通过不断地试错和学习，智能体最终学会如何在特定环境中做出最优的决策，以获得最大的累积奖励。\n主要特点:\n\n通过与环境的交互进行学习。\n通过奖励和惩罚来指导学习方向。\n目标是学习在特定环境中采取最优的行动策略，以最大化累积奖励。\n适合解决序贯决策问题，即一系列连续决策的问题。\n\n现实生活案例:\n\n游戏AI: 训练AI玩游戏，例如围棋、象棋、电子游戏等。\n机器人控制: 训练机器人完成各种任务，例如自动驾驶、物体抓取等。\n推荐系统优化: 通过用户与推荐系统的交互（点击、购买等）来优化推荐策略。\n\n金融领域应用示例:\n\n自动化交易: 开发自动交易程序，根据市场情况自动进行买卖操作。\n投资组合管理: 动态调整投资组合，以最大化收益并控制风险。\n订单执行优化: 优化股票交易的订单执行策略，以降低交易成本。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习项目流程",
    "href": "01_ml_basic.html#机器学习项目流程",
    "title": "2  机器学习基础",
    "section": "2.3 机器学习项目流程",
    "text": "2.3 机器学习项目流程\n一个完整的机器学习项目通常包含以下几个关键步骤：\n\n数据收集与预处理:\n\n数据获取: 收集项目所需的数据。数据来源可能包括数据库、文件、网络爬虫、传感器等等。\n数据清洗: 处理数据中的缺失值、异常值、重复值和错误数据，确保数据质量。\n特征工程: 从原始数据中提取有用的特征，或者创建新的特征，以便模型更好地学习。特征工程是机器学习项目中非常重要的一步，好的特征能够显著提升模型性能。\n\n模型选择与训练:\n\n选择合适的算法: 根据问题的类型（分类、回归、聚类等）和数据的特点，选择合适的机器学习算法。例如，对于分类问题可以选择逻辑回归、支持向量机、决策树、随机森林等算法。\n划分数据集: 将数据集划分为训练集、验证集和测试集。训练集用于训练模型，验证集用于调整模型参数，测试集用于评估模型的泛化能力。\n模型训练与调参: 使用训练集数据训练模型，并使用验证集调整模型参数，例如超参数优化。目标是找到在验证集上表现最好的模型参数。\n\n模型评估与优化:\n\n性能评估: 使用测试集评估模型的性能。根据问题的类型选择合适的评估指标，例如准确率、精确率、召回率、F1 值（分类问题），均方误差、平均绝对误差（回归问题）等。\n模型调优: 如果模型性能不理想，需要进一步分析原因，并进行模型调优。调优方法可能包括：调整模型参数、尝试不同的算法、改进特征工程、增加数据量等。\n结果分析: 分析模型的预测结果，理解模型的优点和不足，为后续的模型改进提供方向。\n\n模型部署与监控:\n\n模型部署: 将训练好的模型部署到实际应用环境中。部署方式可能包括将模型集成到应用程序中、部署为 Web 服务等。\n性能监控: 在模型上线运行后，需要持续监控模型的性能。因为实际应用环境中的数据分布可能会发生变化（即”概念漂移”），导致模型性能下降。\n定期更新: 根据监控结果，定期使用新的数据重新训练模型，或者调整模型参数，以保持模型的性能和适应性。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#总结",
    "href": "01_ml_basic.html#总结",
    "title": "2  机器学习基础",
    "section": "2.4 总结",
    "text": "2.4 总结\n\n机器学习是一种强大的数据分析和预测工具，能够从数据中自动学习模式和规律。\n监督学习、无监督学习和强化学习是机器学习的三种主要类型，它们适用于不同的问题场景。\n机器学习在金融领域有着广泛的应用前景，可以用于风险管理、投资决策、客户服务等多个方面。\n成功应用机器学习需要一个完整的项目流程，包括数据准备、模型构建、评估优化和部署监控等环节。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html",
    "href": "lab02_data.html",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "3.1 内容概要",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#内容概要",
    "href": "lab02_data.html#内容概要",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "金融数据获取\n\n股票、债券、期货市场数据\n数据接口 (Tushare, Yahoo Finance)\n上市公司财务报表数据\n金融文本数据\n\nPython数据分析基础\n\nNumPy, Pandas 常用功能\n数据预处理与清洗\n探索性数据分析 (EDA)\n\nAI辅助编程实践\n\n代码生成、解释、优化\n最佳实践案例",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#金融数据获取",
    "href": "lab02_data.html#金融数据获取",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.2 金融数据获取",
    "text": "3.2 金融数据获取\n\n3.2.1 股票市场数据\n\n数据类型：\n\n基本行情数据：股票代码、名称、交易所、行业\n交易数据：开盘价、收盘价、最高价、最低价、成交量、成交额\n财务数据：资产负债表、利润表、现金流量表、财务指标 (ROE, EPS, PE)\n股东信息：股东户数、十大股东\n分红送股：分红金额、送股比例\n\n常用数据源：\n\nCSMAR (csmar.com): 国泰安金融数据库，数据全面，学校IP范围内免费使用\nCNRDS (cnrds.com): 中国研究数据服务平台，另类数据全面，可向学校申请账号\nTushare (tushare.pro): 国内股票数据接口，数据全面，API友好 (稍后详细介绍)\nYahoo Finance (finance.yahoo.com): 全球股票数据，免费API (yfinance Python库)\n交易所官方API: 上海证券交易所 (sse.com.cn), 深圳证券交易所 (szse.cn) - 数据权威，但API要走申请流程\n券商API: 部分券商提供API接口，方便交易和数据获取 (例如：同花顺, 东方财富)\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商，数据质量高，但价格昂贵 (机构常用)\n\n\n\n\n3.2.2 债券市场数据\n\n数据类型：\n\n债券基本信息：债券代码、名称、发行人、债券类型、票面利率、到期日\n债券交易数据：成交价、收益率、成交量\n债券估值数据：中债估值、市场估值\n债券评级：评级机构、评级结果\n债券发行数据：发行规模、发行利率\n\n常用数据源：\n\nCSMAR (csmar.com): 国泰安金融数据库，学校IP范围内免费使用\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n中债网 (chinabond.com.cn): 中国债券信息网，官方数据\n交易所债券信息平台: 上海证券交易所债券信息平台, 深圳证券交易所债券信息平台\n\n\n\n\n3.2.3 期货市场数据\n\n数据类型：\n\n期货合约信息：合约代码、标的资产、交易单位、最小变动价位、交割月份\n期货交易数据：开盘价、收盘价、最高价、最低价、成交量、持仓量\n期货指数数据：商品期货指数、股指期货指数\n期货仓单数据：仓单数量、注册仓单、有效预报\n期货持仓排名：期货交易所公布的持仓排名数据\n\n常用数据源：\n\nCTP接口: 期货公司提供的交易接口，可以获取实时行情和历史数据 (专业交易者常用)\n同花顺, 文华财经: 金融软件，提供期货行情和数据\n期货交易所网站: 各期货交易所 (例如：上海期货交易所, 大连商品交易所, 郑州商品交易所) 网站通常提供数据下载\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n\n\n\n\n3.2.4 数据接口使用：Tushare\n\nTushare Pro (tushare.pro): 注册认证后可获取更丰富的数据和更高的API访问权限 (收费)\n安装: pip install tushare\n初始化: 需要token (注册Tushare Pro后获取)\nimport tushare as ts\n\n# 初始化 pro 接口\npro = ts.pro_api('YOUR_TOKEN') # 替换为你的token\n常用API示例：\n\n获取股票列表: pro.stock_basic()\n获取股票日线行情: ts.get_k_data('600519', start='2023-01-01', end='2023-01-31') (旧接口) 或 pro.daily(ts_code='600519.SH', start_date='20230101', end_date='20230131') (Pro接口)\n获取公司财务报表: pro.fina_indicator(ts_code='600519.SH', period='20221231')\n更多API: 参考 Tushare 官方文档 (https://tushare.pro/document/2)\n\n注意事项:\n\nAPI访问频率限制: 免费用户有访问频率限制，避免频繁调用\n数据权限: 不同级别用户权限不同，部分数据需要Pro会员\n数据质量: 注意核对数据质量，不同接口数据可能存在差异\n\n\n\n\n3.2.5 数据接口使用：Yahoo Finance\n\nyfinance 库主要用于获取海外股票数据，国内A股数据质量可能不如 Tushare 等国内接口，因此本课程示例主要使用 Tushare 获取A股数据。 Yahoo Finance 示例如下，如果需要分析海外股票，可以使用 yfinance。\nimport yfinance as yf\n\n# 下载 苹果 (AAPL) 股票数据\naapl = yf.Ticker(\"AAPL\")\n\n# 获取历史数据\nhist = aapl.history(period=\"5y\") # 5年历史数据\nprint(hist.head())\n\n# 获取公司信息\ninfo = aapl.info\nprint(info)\n\n# 获取分红信息\ndividends = aapl.dividends\nprint(dividends)\n\n# 更多功能参考 yfinance 文档\n优点: 免费，全球股票数据，使用简单 (如果分析海外股票)\n缺点: A 股数据质量可能不如国内专业数据源，API 稳定性可能不如官方接口，文档相对简单，A 股代码可能需要调整\n\n\n\n3.2.6 上市公司财务报表数据\n\n数据类型:\n\n资产负债表: 反映公司在特定时点的资产、负债和所有者权益状况\n利润表: 反映公司在特定期间的经营成果 (收入、成本、利润)\n现金流量表: 反映公司在特定期间的现金流入和流出\n财务指标: 根据财务报表计算的各种指标，例如：盈利能力指标 (ROE, ROA, 净利润率), 偿债能力指标 (资产负债率, 流动比率), 运营能力指标 (存货周转率, 应收账款周转率), 成长能力指标 (营业收入增长率, 净利润增长率)\n\n数据来源:\n\nCSMAR (csmar.com): 国泰安，国内权威的金融数据库，数据质量高，但收费，高校和研究机构常用\nCNRDS (cnrds.com): 中国研究数据服务平台，国内较全面的研究数据平台，数据覆盖范围广，部分数据收费，学术研究常用\nWind (wind.com.cn): 专业金融数据服务商，提供全面的财务报表和财务指标数据，收费昂贵，金融机构常用\n巨潮资讯网 (cninfo.com.cn): 免费的上市公司公告平台，包含上市公司定期报告 (年报、季报)，可以从中获取财务报表数据，但需要自行解析和整理\n\n\n\n\n3.2.7 金融文本数据\n\n数据类型:\n\n新闻: 上市公司新闻、行业新闻、宏观经济新闻\n公告: 上市公司公告、证监会公告、交易所公告\n研报: 券商研报、基金研报、保险研报\n社交媒体: 上市公司社交媒体动态、投资者互动平台言论\n\n数据来源:\n\n巨潮资讯网 (cninfo.com.cn): 官方指定信息披露平台，提供最权威的上市公司公告、证监会公告、交易所公告等数据，但需要自行解析和整理\n新浪财经 (finance.sina.com.cn): 综合性财经门户，以新闻资讯见长，提供及时的市场动态和深度分析，同时也有公告和研报数据\n东方财富网 (eastmoney.com): 专业财经平台，特色是提供全面的市场数据和投资工具，新闻、公告、研报等数据较为系统\n同花顺 (10jqka.com): 老牌股票软件，以实时行情和交易功能为主，同时提供新闻、公告、研报等数据，适合投资者使用\n雪球 (xueqiu.com): 投资者社交平台，特色是用户生成内容，提供新闻、公告、研报等数据的同时，还有丰富的投资者讨论和观点",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#python数据分析基础",
    "href": "lab02_data.html#python数据分析基础",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.3 Python数据分析基础",
    "text": "3.3 Python数据分析基础\n\n3.3.1 NumPy 基础\n\nNumPy: 基于Python的科学计算库，提供高效的多维数组对象和工具，用于数据分析和科学计算\n核心功能:\n\n数组操作: 创建、操作、转换数组\n数学运算: 线性代数、傅里叶变换、随机数生成\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n常用操作:\nimport numpy as np\nimport pandas as pd\n\n# 假设已下载茅台日线数据\n# CSV 文件包含 Date 和 Close 列\ndf = pd.read_csv('data/CSMAR/moutai_daily/TRD_Dalyr.csv')\nclose_prices = df['Clsprc'].values\n\n# 计算平均收盘价\navg_price = np.mean(close_prices)\nprint(f\"平均收盘价: {avg_price:.2f}\")\n\n# 计算收盘价的标准差\nstd_price = np.std(close_prices)\nprint(f\"收盘价标准差: {std_price:.2f}\")\n\n\n\n3.3.2 Pandas 基础\n\nPandas (Panel Data): 基于NumPy的数据分析库，提供 Series (一维带标签数组) 和 DataFrame (二维表格型数据) 数据结构\n核心功能:\n\n数据结构: Series 和 DataFrame，方便数据表示和操作\n数据清洗: 处理缺失值、重复值、异常值\n数据预处理: 数据转换、数据标准化、特征工程\n数据分析: 数据选择、过滤、排序、分组聚合、透视表\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n实践示例: 茅台股票数据分析:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 基础数据处理\ndf['trade_date'] = pd.to_datetime(df['trade_date'])  # 转换日期格式\ndf = df.sort_values('trade_date')  # 按日期排序\n\n# 计算基本指标\ndf['daily_return'] = df['close'].pct_change()  # 日收益率\ndf['MA5'] = df['close'].rolling(window=5).mean()  # 5日均线\ndf['MA20'] = df['close'].rolling(window=20).mean()  # 20日均线\ndf['volatility'] = df['daily_return'].rolling(window=20).std() * np.sqrt(252)  # 20日年化波动率\n\n# 数据分析示例\nprint(\"\\n基本统计信息:\")\nprint(df[['close', 'daily_return', 'volatility']].describe())\n\nprint(\"\\n交易量最大的5天:\")\nprint(df.nlargest(5, 'vol')[['trade_date', 'close', 'vol']])\n\n# 计算每周平均收盘价和成交量\nweekly_stats = df.set_index('trade_date').resample('W').agg({\n    'close': 'mean',\n    'vol': 'sum'\n})\nprint(\"\\n每周统计:\")\nprint(weekly_stats.head())\n\n# 可视化分析\nplt.figure(figsize=(15, 10))\n\n# 绘制K线图和均线\nplt.subplot(2, 1, 1)\nplt.plot(df['trade_date'], df['close'], label='收盘价')\nplt.plot(df['trade_date'], df['MA5'], label='5日均线')\nplt.plot(df['trade_date'], df['MA20'], label='20日均线')\nplt.title('贵州茅台股价走势')\nplt.legend()\nplt.grid(True)\n\n# 绘制成交量和波动率\nplt.subplot(2, 1, 2)\nplt.bar(df['trade_date'], df['vol'], alpha=0.5, label='成交量')\nplt.plot(df['trade_date'], df['volatility'] * 1000000, 'r', label='波动率(放大1000000倍)')\nplt.title('成交量和波动率')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n练习建议:\n\n尝试修改上述代码，计算不同时间窗口的均线（如10日、30日均线）\n添加其他技术指标的计算（如RSI、MACD）\n尝试对比茅台与其他白酒股的表现\n探索不同的可视化方式（如蜡烛图）\n\n\n\n\n3.3.3 数据预处理与清洗\n\n数据质量问题:\n\n缺失值 (Missing Values): 数据记录中某些字段为空 (例如：股票停牌日可能成交量为缺失值)\n异常值 (Outliers): 与其他数据明显偏离的值 (例如：交易数据中的错误记录)\n重复值 (Duplicates): 重复的数据记录\n数据不一致 (Inconsistent Data): 同一信息在不同数据源中表示不一致\n数据类型错误 (Data Type Errors): 例如：数值型字段存储为字符串\n\n数据预处理步骤:\n\n数据清洗 (Data Cleaning): 处理缺失值、异常值、重复值、数据不一致等\n数据转换 (Data Transformation): 数据类型转换、数据格式转换、数据编码 (例如：One-Hot Encoding)\n数据标准化/归一化 (Data Scaling/Normalization): 将数据缩放到特定范围，消除量纲影响 (例如：Min-Max Scaling, Standardization)\n特征选择/特征构建 (Feature Selection/Feature Engineering): 选择重要特征，构建新特征 (后续章节详细介绍)\n\n\n\n\n3.3.4 探索性数据分析 (EDA)\n\n目的: 初步了解数据特征、发现数据规律、为后续建模提供方向\n常用方法:\n\n描述性统计: 均值、中位数、标准差、分位数、最大值、最小值等，了解数据分布和集中趋势 (例如：分析股票收盘价的统计特征)\n数据可视化: 直方图、箱线图、散点图、折线图、热力图等，直观展示数据分布、关系和异常 (例如：绘制股票价格走势图、成交量直方图)\n相关性分析: 计算特征之间的相关性，了解特征之间的关系 (例如：分析股票收益率与成交量之间的相关性)\n分组分析: 按类别分组，比较不同组别的数据特征差异 (例如：按行业分组，比较不同行业股票的盈利能力)\n\n常用可视化工具:\n\nMatplotlib: Python 基础绘图库，功能强大，定制性强\nSeaborn: 基于Matplotlib的高级可视化库，更美观，更方便绘制统计图表\nPlotly: 交互式可视化库，可创建动态图表",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#ai-辅助编程实践",
    "href": "lab02_data.html#ai-辅助编程实践",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.4 AI 辅助编程实践",
    "text": "3.4 AI 辅助编程实践\n\n3.4.1 代码生成与解释\n\nAI 代码生成工具: GitHub Copilot\nInline代码生成: 根据自然语言描述或代码上下文，自动生成代码片段或完整函数。\nAsk Copilot: 使用Ask Copilot，AI 可以解释、修改选定代码的功能和逻辑。\nEdit with Copilot: 使用Edit with Copilot，AI 可以根据多个文件的上下文，生成、修改代码。\n\n\n\n3.4.2 关于数据分析的具体操作建议\n\n数据读入: 新建一个文件夹，将所有数据文件放在该文件夹下的 data 文件夹中，然后使用 Pandas 读入数据。\n数据清洗: 一次做一步，每一步都要检查。通过 print 输出中间结果，检查每一步的输出是否符合预期。\n\n检查观测值数量是否与原数据一致；\n检查变量名是否与原数据一致；\n检查数据类型是否符合预期；\n如果数据量很大，可以先读入一部分数据，检查数据是否符合预期，再决定是否读入全部数据。\n\n处理报错: 如果遇到报错，先定位报错信息位置，检查那一段代码，可以把错误发给AI，让它帮你修改代码。如果代码没有问题，再检查上一步代码中的数据输出。\n自然语言编程: 可以直接写注释，AI会根据你的注释生成代码。检查并运行代码。\n\n检查代码是否使用正确的数据与变量名；\n运行代码，检查代码的输出是否符合预期，如果符合预期，再进行下一步。\n如果代码涉及循环，可以先写一个，运行通过后，再改成循环。\n\n慢就是快：代码需要一步一步生成，不要一次性生成与运行多行代码。勤检查，勤运行，勤对比。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#总结",
    "href": "lab02_data.html#总结",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.5 总结",
    "text": "3.5 总结\n\n数据是机器学习的基石：高质量的数据是构建有效模型的关键。\n金融数据获取多样化：掌握不同数据源和API接口，灵活获取所需数据，包括股票、债券、期货、财务报表和金融文本数据。\nPython 数据分析是必备技能：熟练运用 NumPy 和 Pandas 进行金融数据处理和分析。\nEDA 帮助理解数据：通过探索性数据分析，发现金融数据规律，为建模提供方向。\nAI 辅助编程提升效率：善用 AI 工具，提高金融数据获取和分析效率。 熟练使用AI工具，可以显著提升开发效率。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "03_supervised.html",
    "href": "03_supervised.html",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "4.1 监督学习简介\n监督学习是机器学习的一个重要分支，它是指从带有标签的数据中自动学习规律和模式，并利用这些规律和模式对新数据进行预测和决策的过程。在监督学习中，我们拥有一个包含输入特征 \\(\\mathbf{x}\\) 和对应输出标签 \\(y\\) 的数据集，模型的目标是学习一个从输入特征到输出标签的映射关系。监督学习在量化投资、金融科技等领域有广泛应用，例如：\n由于课程时间有限，本讲义将重点介绍监督学习中的回归、分类和集成学习，以及它们在金融预测中的应用。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习简介",
    "href": "03_supervised.html#监督学习简介",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "风险评估：根据客户的历史信用数据（特征）预测其信用风险等级（标签）。\n欺诈检测：基于交易记录（特征）识别欺诈交易（标签）。\n量化交易：预测股票价格走势（标签）以辅助交易决策（特征）。\n客户细分：根据客户特征（特征）预测客户所属类别（标签），进行精准营销。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习模型详解",
    "href": "03_supervised.html#监督学习模型详解",
    "title": "4  监督学习（上）",
    "section": "4.2 监督学习模型详解",
    "text": "4.2 监督学习模型详解\n\n4.2.1 回归问题描述\n回归问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测连续数值型的结果 \\(y\\)。 给定训练数据 \\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(\\mathbf{x}_i\\) 是第 \\(i\\) 个样本的特征向量， \\(y_i\\) 是对应的真实值， \\(N\\) 是样本数量。我们的目标是找到一个函数 \\(f\\)，使得对于新的输入 \\(\\mathbf{x}\\)，模型预测值 \\(\\hat{y} = f(\\mathbf{x})\\) 尽可能接近真实值 \\(y\\)。假设真实值 \\(y_i\\) 与预测函数 \\(f(\\mathbf{x}_i)\\) 之间存在如下关系：\n\\[y_i = f(\\mathbf{x}_i) + \\epsilon_i\\]\n其中 \\(\\epsilon_i\\) 代表随机误差项，通常假设其服从均值为 0 的正态分布。在实际应用中，我们通常将观测值堆叠成矩阵和向量的形式，方便模型表达和计算：\n\n\\(N \\times 1\\) 维结果向量 \\(\\mathbf{y} = (y_1, y_2, ..., y_N)^T\\)\n\\(N \\times K\\) 维特征矩阵 \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N)^T\\)，每一行代表一个样本，每一列代表一个特征。\n\\(N \\times 1\\) 维误差向量 \\(\\mathbf{\\epsilon} = (\\epsilon_1, \\epsilon_2, ..., \\epsilon_N)^T\\)\n\n回归模型可以简洁地写为： \\(\\mathbf{y} = f(\\mathbf{X}) + \\mathbf{\\epsilon}\\)。我们的目标是通过训练数据学习到函数 \\(f\\) 的具体形式，从而能够对新的样本 \\(\\mathbf{x}\\) 进行预测。\n\n\n4.2.2 线性回归 (Linear Regression)\n线性回归模型是最简单且应用广泛的回归模型。它假设结果变量 \\(y\\) 与特征向量 \\(\\mathbf{x}\\) 之间存在线性关系。线性回归模型易于理解和实现，是许多复杂模型的基础。\n模型表达式:\n线性回归模型假设预测函数 \\(f(\\mathbf{x})\\) 是特征 \\(\\mathbf{x}\\) 的线性组合，模型表达式如下：\n\\(y = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\)\n或者对于单个样本 \\(i\\)，可以表示为：\n\\(y_i = \\mathbf{x}_i^T \\mathbf{\\beta} + \\epsilon_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_K x_{iK} + \\epsilon_i\\)\n其中： - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是 \\((K+1) \\times 1\\) 维回归系数向量，\\(\\beta_0\\) 是截距项（bias），\\(\\beta_1, ..., \\beta_K\\) 是特征的系数。为了方便表示，我们通常在特征矩阵 \\(\\mathbf{X}\\) 中添加一列全为 1 的列向量，对应于截距项 \\(\\beta_0\\)。 - \\(\\mathbf{x}_i = (1, x_{i1}, x_{i2}, ..., x_{iK})^T\\) 是 \\((K+1) \\times 1\\) 维增广特征向量，包含了常数项 1 和原始特征。 - \\(\\epsilon_i\\) 是误差项。\n最优化方法：最小二乘法 (OLS)\n线性回归的目标是找到最优的回归系数 \\(\\mathbf{\\beta}\\)，使得模型的预测值 \\(\\mathbf{X}\\mathbf{\\beta}\\) 与真实值 \\(\\mathbf{y}\\) 之间的误差平方和 (Sum of Squared Errors, SSE) 最小。最小二乘法 (Ordinary Least Squares, OLS) 是一种常用的求解线性回归模型参数的方法。其目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^T \\mathbf{\\beta})^2\\]\n为了求解最优的 \\(\\mathbf{\\beta}\\)，我们可以对目标函数 \\(L(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到正规方程 (Normal Equation)：\n\\[\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = \\mathbf{X}^T\\mathbf{y}\\]\n如果矩阵 \\(\\mathbf{X}^T\\mathbf{X}\\) 可逆（即满秩），则可以得到普通最小二乘 (OLS) 估计量的解析解：\n\\[\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n高维环境下的过拟合问题与正则化:\n在高维环境中，当特征数量 \\(K\\) 相对于观测数量 \\(N\\) 来说较大时（例如 \\(K &gt; N\\)，或 \\(K\\) 接近 \\(N\\)），OLS 估计可能会出现过拟合 (Overfitting) 问题。过拟合是指模型在训练集上表现得非常好（例如，训练误差很小），但在未见过的测试集上泛化能力很差，预测性能下降。这是因为在高维情况下，模型参数过多，容易捕捉到训练数据中的噪声和随机波动，而不是真实的 underlying pattern。\n为了解决过拟合问题，提高模型的泛化能力，可以引入正则化 (Regularization) 方法。正则化通过在损失函数中添加惩罚项，限制模型复杂度，从而避免模型过度拟合训练数据。常用的正则化方法包括 岭回归 (Ridge Regression) 和 Lasso 回归 (Lasso Regression)。\n\n\n4.2.3 岭回归 (Ridge Regression)\n岭回归是一种改进的线性回归方法，也称为 \\(L_2\\) 正则化线性回归。它通过在最小二乘法的损失函数中添加 \\(L_2\\) 范数惩罚项来对回归系数进行 shrinkage (收缩)，限制回归系数的大小，从而降低模型的复杂度和过拟合风险。岭回归特别适用于处理多重共线性问题，即特征之间存在高度相关性的情况。\n模型表达式:\n岭回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Ridge}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} \\right] \\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差 (Mean Squared Error, MSE) 项，衡量模型预测值与真实值之间的平均误差平方。 - \\(\\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} = \\lambda ||\\mathbf{\\beta}||_2^2 = \\lambda \\sum_{j=0}^{K} \\beta_j^2\\) 是 \\(L_2\\) 范数惩罚项，也称为 权重衰减项 (Weight Decay)。它惩罚回归系数 \\(\\mathbf{\\beta}\\) 的平方和，迫使系数趋向于较小的值。 - \\(\\lambda \\ge 0\\) 是正则化参数 (Regularization Parameter)，也称为 惩罚系数。它控制惩罚项的强度。\\(\\lambda\\) 越大，惩罚越强，回归系数越趋向于 0。当 \\(\\lambda = 0\\) 时，岭回归退化为普通线性回归。\n估计方法:\n类似于线性回归，我们可以对岭回归的目标函数 \\(L_{Ridge}(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到岭回归的估计结果：\n\\[\\hat{\\mathbf{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n其中 \\(\\mathbf{I}_{K+1}\\) 是 \\((K+1) \\times (K+1)\\) 单位矩阵。通过向 \\(\\mathbf{X}^T\\mathbf{X}\\) 添加对角矩阵 \\(\\lambda \\mathbf{I}_{K+1}\\)（即”岭”），可以使得在求逆运算时，即使 \\(\\mathbf{X}^T\\mathbf{X}\\) 接近奇异矩阵（例如，当存在多重共线性时），\\((\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})\\) 仍然具有较好的可逆性，保证了解的稳定性。并且 \\(\\lambda \\mathbf{I}_{K+1}\\) 的存在将导致回归系数 \\(\\hat{\\mathbf{\\beta}}_{Ridge}\\) 向零收缩。\n岭回归的特点:\n\n\\(L_2\\) 正则化：使用 \\(L_2\\) 范数惩罚项，将回归系数向零收缩，但不会精确地变为 0。\n缓解多重共线性：通过引入正则化项，降低了模型对特征之间相关性的敏感度，可以缓解多重共线性问题，提高模型稳定性。\n降低过拟合风险：通过限制模型复杂度，有效降低过拟合风险，提高模型的泛化能力。\n无法进行特征选择：岭回归会缩小所有特征的系数，但不会将任何系数精确地设置为 0，因此无法进行特征选择。\n\n\n\n4.2.4 Lasso 回归 (Lasso Regression)\nLasso (Least Absolute Shrinkage and Selection Operator) 回归是另一种常用的正则化线性回归方法，也称为 \\(L_1\\) 正则化线性回归。与岭回归不同，Lasso 回归使用 \\(L_1\\) 范数惩罚项进行正则化。 \\(L_1\\) 正则化不仅可以进行系数 shrinkage，更重要的是，它具有 特征选择 (Feature Selection) 的能力，可以将一些不重要特征的回归系数压缩为 精确的 0，从而得到 稀疏模型 (Sparse Model)。稀疏模型更易于解释，并且可以提高模型的泛化能力。\n模型表达式:\nLasso 回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Lasso}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma \\sum_{j=1}^{K} |\\beta_j| \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 仍然是均方误差项。 - \\(\\gamma \\sum_{j=1}^{K} |\\beta_j| = \\gamma ||\\mathbf{\\beta}_{1:K}||_1 = \\gamma (|\\beta_1| + |\\beta_2| + ... + |\\beta_K|)\\) 是 \\(L_1\\) 范数惩罚项，注意这里只惩罚了特征系数 \\(\\beta_1, ..., \\beta_K\\)，不惩罚截距项 \\(\\beta_0\\)。 \\(L_1\\) 范数惩罚项迫使一些回归系数变为 0。 - \\(\\gamma \\ge 0\\) 是 正则化参数，控制 \\(L_1\\) 惩罚项的强度。\\(\\gamma\\) 越大，惩罚越强，更多的回归系数会被压缩为 0。\n估计方法:\n与岭回归不同，Lasso 回归的目标函数由于包含 \\(L_1\\) 范数项，在 \\(\\beta_j = 0\\) 处不可导，因此 没有解析解。通常需要使用数值优化算法（如坐标轴下降法 (Coordinate Descent)、近端梯度下降法 (Proximal Gradient Descent)）进行求解。\nLasso 回归的特点:\n\n\\(L_1\\) 正则化：使用 \\(L_1\\) 范数惩罚项，不仅可以进行系数 shrinkage，还可以将一些不重要特征的回归系数压缩为精确的 0，实现特征选择。\n稀疏模型：Lasso 回归可以产生稀疏模型，即模型中只有少数特征的系数非零，这有助于模型解释和提高泛化能力。\n特征选择能力：在特征选择方面优于岭回归。Lasso 回归可以自动选择重要的特征，去除冗余和不相关的特征。\n适用于高维稀疏数据：Lasso 回归特别适用于处理高维稀疏数据，例如文本数据、基因数据等。\n\n\n\n4.2.5 弹性网 (Elastic Net)\n弹性网 (Elastic Net) 是一种结合了岭回归和 Lasso 回归的正则化方法，可以看作是岭回归和 Lasso 回归的折衷。弹性网同时使用 \\(L_1\\) 范数和 \\(L_2\\) 范数惩罚项进行正则化。弹性网同时使用 \\(L_1\\) 正则化和 \\(L_2\\) 正则化，综合利用 \\(L_1\\) 正则化的特征选择能力和 \\(L_2\\) 正则化的稳定性和 shrinkage 能力。在某些情况下，弹性网的性能优于单独的岭回归和 Lasso 回归，尤其是在特征之间高度相关时，弹性网表现更稳定。\n模型表达式:\n弹性网的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{ElasticNet}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma_1 \\sum_{j=1}^{K} |\\beta_j| + \\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta} \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差项。 - \\(\\gamma_1 \\sum_{j=1}^{K} |\\beta_j|\\) 是 \\(L_1\\) 范数惩罚项，用于特征选择和产生稀疏模型。 - \\(\\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta}\\) 是 \\(L_2\\) 范数惩罚项，用于系数 shrinkage 和缓解多重共线性。 - \\(\\gamma_1 \\ge 0\\) 和 \\(\\gamma_2 \\ge 0\\) 分别是 \\(L_1\\) 正则化参数 和 \\(L_2\\) 正则化参数，控制两种惩罚项的强度。通常需要通过交叉验证等方法来选择合适的 \\(\\gamma_1\\) 和 \\(\\gamma_2\\) 值。\n弹性网的特点:\n\n结合 \\(L_1\\) 和 \\(L_2\\) 正则化：弹性网同时使用 \\(L_1\\) 和 \\(L_2\\) 范数惩罚项，结合了两者的优点。\n既可以进行特征选择，又可以进行系数 shrinkage：弹性网既可以像 Lasso 回归一样进行特征选择，将一些不重要特征的系数压缩为 0，又可以像岭回归一样进行系数 shrinkage，缩小系数的整体大小，提高模型稳定性。\n性能更稳定：在某些情况下，弹性网的预测性能和鲁棒性优于岭回归和 Lasso 回归。\n处理特征高度相关性：当特征之间高度相关时，Lasso 回归可能随机选择其中一个特征，而弹性网倾向于选择一组相关的特征，表现更稳定。\n\n\n\n4.2.6 分类问题描述\n分类问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测离散类别型的结果 \\(y\\)。分类问题的目标是学习一个模型，将输入样本 \\(\\mathbf{x}\\) 划分到预定义的类别中。根据类别数量的不同，分类问题可以分为：\n\n二分类 (Binary Classification)：预测结果 \\(y\\) 只有两个类别，通常表示为 \\(y \\in \\{0, 1\\}\\) (或 \\(y \\in \\{-1, +1\\}\\))。例如，判断邮件是否为垃圾邮件（是/否），预测用户是否会点击广告（点击/不点击），识别交易是否为欺诈交易（欺诈/正常）。\n多分类 (Multiclass Classification)：预测结果 \\(y\\) 有两个以上的类别，表示为 \\(y \\in \\{C_1, C_2, ..., C_L\\}\\)，其中 \\(C_i\\) 是类别标签， \\(L \\ge 3\\) 是类别数量。例如，图像分类（猫、狗、鸟、鱼等），文本分类（政治、经济、体育、娱乐等），客户类型分类（高价值客户、中价值客户、低价值客户）。\n\n对于多分类问题，常用的处理策略是 “拆解法” (Decomposition)，即将多分类任务拆解为若干个二分类任务求解。常见的拆解策略包括 一对一 (One-vs-One, OvO)、一对多 (One-vs-Rest, OvR) 和 多对多 (Many-vs-Many, MvM) 等。\n\n\n4.2.7 类别不平衡问题\n在分类任务中，经常会遇到不同类别的训练样本数量差别很大的情况，即 类别不平衡 (Class Imbalance) 问题。例如，在欺诈检测、罕见病诊断、自然灾害预测等领域，少数类样本 (Minority Class)（如欺诈交易、患病样本、地震）的数量通常远远少于多数类样本 (Majority Class)（如正常交易、健康样本、非地震）。类别不平衡问题会严重影响模型的学习效果，使得模型更倾向于预测样本数量较多的类别，而对少数类别的识别率很低。\n类别不平衡的影响:\n\n模型偏向多数类：模型在训练过程中更容易学习到多数类样本的特征，而忽略少数类样本的特征，导致模型预测结果偏向多数类。\n整体分类精度虚高：由于多数类样本数量占优，即使模型将所有样本都预测为多数类，也可能获得较高的整体分类精度 (Accuracy)。但这种高精度是没有意义的，因为模型对少数类的识别能力很差。\n评估指标失效：常用的评估指标（如准确率 Accuracy）在类别不平衡数据集上可能失效，无法真实反映模型的性能。我们需要使用更合适的评估指标，例如 精确率 (Precision)、召回率 (Recall)、F1 值 (F1-score)、AUC 值 (Area Under ROC Curve) 等。\n\n类别不平衡的解决方案:\n为了解决类别不平衡问题，提高模型对少数类别的识别能力，常用的解决方案包括：\n\n再缩放 (Rescaling) / 阈值调整 (Threshold Adjustment)：不改变原始模型，而是调整分类阈值 (Classification Threshold)，使得模型在类别不平衡时也能做出合理的预测。例如，对于逻辑回归或 SVM 等输出概率的模型，默认的分类阈值通常为 0.5。当类别不平衡时，可以将预测为正例的阈值从 0.5 调整为更小的值，例如 \\(\\frac{m^{+}}{m^{-} + m^{+}}\\), 其中 \\(m^{+}\\) 和 \\(m^{-}\\) 分别是正类（少数类）和负类（多数类）样本的数量。降低阈值会使得模型更容易将样本预测为正类，从而提高少数类的召回率。\n重采样 (Resampling)：通过改变训练集中不同类别样本的比例来缓解类别不平衡问题。重采样方法包括 欠抽样 (Undersampling) 和 过抽样 (Oversampling)。\n\n欠抽样 (Undersampling)：减少多数类样本的数量，随机删除一部分多数类样本，使得正负类样本数量接近平衡。欠抽样方法简单易行，但可能会丢失一部分多数类样本的信息，适用于数据量较大的情况。\n过抽样 (Oversampling)：增加少数类样本的数量，例如通过复制少数类样本或生成合成样本（如 SMOTE (Synthetic Minority Over-sampling Technique)）。过抽样方法可以保留所有原始多数类样本的信息，但可能会导致过拟合，适用于数据量较小的情况。SMOTE 算法通过在少数类样本之间进行插值生成新的合成样本，可以有效缓解过拟合问题。\n\n阈值移动 (Threshold-moving)：这是一种代价敏感学习 (Cost-sensitive learning) 的思想。基于原始训练集进行学习，但在用训练好的分类器进行预测时，根据类别不平衡的程度调整决策阈值。例如，如果少数类样本的误分类代价更高，则可以将决策阈值向多数类方向移动，使得模型更倾向于将样本预测为少数类。\n代价敏感学习 (Cost-sensitive learning)：为不同类别的误分类设置不同的代价 (Cost)，使得模型在训练时更加关注少数类样本，最小化总的期望代价而不是最小化分类错误率。例如，可以使用代价矩阵 (Cost Matrix) 来定义不同误分类情况的代价，然后在训练过程中根据代价矩阵调整模型的学习策略。\n集成学习方法：一些集成学习方法，如 集成学习 (Ensemble Learning) 方法，例如 EasyEnsemble、BalanceCascade 等，通过将数据集划分为多个子集，在每个子集上训练基学习器，然后集成多个基学习器的预测结果，可以有效提高模型在类别不平衡数据集上的性能。\n\n\n\n4.2.8 逻辑回归 (Logistic Regression)\n逻辑回归 (Logistic Regression) 是一种广泛使用的二分类模型。虽然名字带有”回归”，但逻辑回归实际上是一种分类算法，主要用于解决二分类问题。逻辑回归模型简单高效，易于解释，是许多分类问题的 baseline 模型。\n模型表达式:\n逻辑回归模型基于线性回归的思想，但通过引入 Sigmoid 函数 (Sigmoid Function) 或 Logistic 函数，将线性回归的输出值映射到 \\((0, 1)\\) 区间，使其具有概率意义，用于表示样本属于正类的概率。\n逻辑回归模型的表达式如下：\n\\[P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}}\\]\n其中： - \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\) 表示给定特征向量 \\(\\mathbf{x}\\) 和模型参数 \\(\\mathbf{\\beta}\\) 的条件下，样本属于正类 (y=1) 的概率。 - \\(\\mathbf{x} = (1, x_1, x_2, ..., x_K)^T\\) 是增广特征向量。 - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是模型参数，与线性回归中的回归系数类似。 - \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) 是 Sigmoid 函数，也称为 Logistic 函数。Sigmoid 函数将任意实数 \\(z\\) 映射到 \\((0, 1)\\) 区间，函数图像呈 S 形。当 \\(z \\rightarrow +\\infty\\) 时，\\(\\sigma(z) \\rightarrow 1\\)；当 \\(z \\rightarrow -\\infty\\) 时，\\(\\sigma(z) \\rightarrow 0\\)；当 \\(z = 0\\) 时，\\(\\sigma(z) = 0.5\\)。\n对于二分类问题，逻辑回归模型预测样本属于正类的概率 \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\)，则样本属于负类的概率为 \\(P(y=0|\\mathbf{x}; \\mathbf{\\beta}) = 1 - P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = 1 - \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\sigma(-\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{e^{-\\mathbf{x}^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}} = \\frac{1}{1 + e^{\\mathbf{x}^T \\mathbf{\\beta}}}\\)。\n模型训练：最大似然估计 (Maximum Likelihood Estimation, MLE)\n逻辑回归模型的训练目标是最大化训练数据的似然函数 (Likelihood Function)，即找到一组模型参数 \\(\\mathbf{\\beta}\\)，使得在给定这组参数下，训练数据出现的概率最大。对于二分类问题，逻辑回归的似然函数可以表示为：\n\\[L(\\mathbf{\\beta}) = \\prod_{i=1}^{N} [P(y_i=1|\\mathbf{x}_i; \\mathbf{\\beta})]^{y_i} [P(y_i=0|\\mathbf{x}_i; \\mathbf{\\beta})]^{1-y_i} = \\prod_{i=1}^{N} [\\sigma(\\mathbf{x}_i^T \\mathbf{\\beta})]^{y_i} [\\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]^{1-y_i}\\]\n为了方便优化，通常将似然函数取对数，得到对数似然函数 (Log-Likelihood Function)：\n\\[\\ell(\\mathbf{\\beta}) = \\ln L(\\mathbf{\\beta}) = \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})] = \\sum_{i=1}^{N} [y_i \\ln \\frac{1}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}} + (1-y_i) \\ln \\frac{e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}]\\]\n我们的目标是最大化对数似然函数 \\(\\ell(\\mathbf{\\beta})\\)，等价于最小化负对数似然函数 (Negative Log-Likelihood Function)：\n\\[J(\\mathbf{\\beta}) = -\\ell(\\mathbf{\\beta}) = - \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]\\]\n负对数似然函数 \\(J(\\mathbf{\\beta})\\) 也称为 交叉熵损失函数 (Cross-Entropy Loss Function) 或 Logistic Loss Function。\n最优化方法：梯度下降法 (Gradient Descent)\n逻辑回归模型通常使用梯度下降法 (Gradient Descent) 或其变种（如 随机梯度下降 (SGD)、小批量梯度下降 (Mini-batch GD)、Adam 等）来求解最优参数 \\(\\mathbf{\\beta}\\)，最小化交叉熵损失函数 \\(J(\\mathbf{\\beta})\\)。梯度下降法是一种迭代优化算法，通过不断沿着损失函数梯度 负方向 更新参数，逐步逼近最优解。\n决策边界 (Decision Boundary):\n逻辑回归模型的决策边界是线性的。当 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 时，\\(\\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = 0.5\\)，模型预测样本属于正类和负类的概率均为 0.5。因此，线性方程 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 定义了逻辑回归模型的决策边界，将特征空间划分为正类区域和负类区域。\n\n\n4.2.9 支持向量机 (Support Vector Machine, SVM)\n支持向量机 (Support Vector Machine, SVM) 是一种强大且广泛应用于分类和回归问题的监督学习模型。SVM 的核心思想是找到一个最优超平面 (Optimal Hyperplane)，将不同类别的样本最大程度地分开，同时使得分类间隔 (Margin) 最大化。SVM 在高维空间和非线性分类问题中表现出色，通过核技巧 (Kernel Trick) 可以有效地处理非线性可分数据。\n线性可分支持向量机 (Linearly Separable SVM) / 硬间隔 SVM (Hard Margin SVM):\n对于线性可分 (Linearly Separable) 的数据集，即存在一个超平面可以将不同类别的样本完全分开的情况，我们可以构建线性可分支持向量机，也称为 硬间隔 SVM。硬间隔 SVM 旨在找到一个最大间隔超平面，将两类样本完全正确地分开，并且使得间隔最大化。间隔是指超平面到最近的样本点（称为 支持向量 (Support Vector)）的距离。\n模型表达式:\n给定线性可分的训练数据集 \\(D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(y_i \\in \\{-1, +1\\}\\)。线性可分 SVM 的目标是找到一个超平面 \\((\\mathbf{w}, b)\\)，使得：\n\n正确分类: 所有样本都被正确分类，即对于 \\(y_i = +1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\ge +1\\)；对于 \\(y_i = -1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\le -1\\)。可以将两个不等式统一为： \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\)。\n间隔最大化: 最大化分类间隔 \\(Margin = \\frac{2}{||\\mathbf{w}||}\\)，等价于最小化 \\(||\\mathbf{w}||^2 = \\mathbf{w}^T \\mathbf{w}\\)。\n\n因此，线性可分 SVM 的最优化问题可以表示为：\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\]\n这是一个凸二次规划 (Convex Quadratic Programming, QP) 问题，可以使用现成的 QP 求解器求解。\n线性不可分支持向量机 (Linearly Inseparable SVM) / 软间隔 SVM (Soft Margin SVM):\n在实际应用中，很多数据集不是线性可分的，即不存在一个超平面可以将不同类别的样本完全分开。为了处理线性不可分数据，我们需要引入软间隔 SVM，也称为 线性支持向量机。软间隔 SVM 允许模型在一些样本上分类错误，但希望尽可能减少分类错误，同时保持间隔最大化。\n模型表达式:\n软间隔 SVM 通过引入松弛变量 (Slack Variables) \\(\\xi_i \\ge 0\\)，允许一些样本不满足硬间隔约束 \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1\\)。松弛变量 \\(\\xi_i\\) 表示第 \\(i\\) 个样本违反约束的程度。软间隔 SVM 的最优化问题变为：\n\\[\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0, \\quad i = 1, 2, ..., N\\]\n其中： - \\(\\frac{1}{2} ||\\mathbf{w}||^2\\) 仍然是间隔最大化项。 - \\(C \\sum_{i=1}^{N} \\xi_i\\) 是惩罚项，表示对误分类的惩罚。 \\(\\xi_i\\) 越大，误分类程度越高，惩罚越大。 - \\(C &gt; 0\\) 是 惩罚参数 (Penalty Parameter)，也称为 正则化参数。 \\(C\\) 控制对误分类的惩罚程度。 \\(C\\) 越大，对误分类的惩罚越大，模型越倾向于减小误分类，但可能会导致间隔变小，容易过拟合； \\(C\\) 越小，对误分类的惩罚越小，模型更容忍误分类，间隔可能更大，泛化能力可能更好。 \\(C\\) 的选择需要通过交叉验证等方法进行调优。\n核函数 (Kernel Function):\n对于非线性可分 (Nonlinearly Separable) 的数据集，SVM 可以通过 核函数 (Kernel Function) 将数据映射到高维空间 (High-Dimensional Space)，使得在高维空间中数据变得线性可分，然后在高维空间中寻找最优超平面。核技巧 (Kernel Trick) 的强大之处在于，我们不需要显式地计算高维空间的特征向量，只需要定义一个核函数 \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)，它可以计算原始空间中两个向量 \\(\\mathbf{x}_i\\) 和 \\(\\mathbf{x}_j\\) 映射到高维空间后的内积。常用的核函数包括：\n\n线性核 (Linear Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\\)。线性核实际上没有进行特征映射，适用于线性可分数据。\n多项式核 (Polynomial Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d\\)。多项式核可以将数据映射到多项式特征空间，适用于多项式关系的数据。其中 \\(\\gamma &gt; 0, r \\ge 0, d \\ge 1\\) 是核参数。\n高斯核 / RBF 核 (Gaussian Kernel / Radial Basis Function Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)\\)。高斯核是最常用的核函数之一，可以将数据映射到无限维空间，适用于各种类型的数据，尤其是局部性模式的数据。其中 \\(\\gamma &gt; 0\\) 是核参数，控制核函数的宽度。\nSigmoid 核 (Sigmoid Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)\\)。Sigmoid 核类似于神经网络中的 Sigmoid 激活函数，SVM 使用 Sigmoid 核时，其行为类似于多层感知机神经网络。其中 \\(\\gamma &gt; 0, r &lt; 0\\) 是核参数。\n\n最优化方法：对偶问题与 SMO 算法\nSVM 的优化问题（无论是硬间隔还是软间隔）通常转化为 对偶问题 (Dual Problem) 进行求解。求解对偶问题的好处包括： 1. 更容易求解：对偶问题通常比原始问题更容易求解。 2. 引入核函数：在对偶问题中，目标函数和约束条件只涉及到样本之间的内积，可以方便地引入核函数，将线性 SVM 扩展到非线性 SVM。\n求解 SVM 对偶问题的高效算法是 SMO (Sequential Minimal Optimization) 算法。SMO 算法是一种启发式算法，它将大规模 QP 问题分解为一系列小规模 QP 子问题，通过迭代地优化两个变量，高效地求解 SVM 模型。\n\n\n4.2.10 决策树 (Decision Tree)\n决策树 (Decision Tree) 是一种树形结构的分类或回归模型。决策树模型直观易懂，易于解释，并且可以处理类别型和数值型特征，无需进行特征缩放。决策树模型的核心思想是基于特征对数据集进行递归划分，构建一棵树状的决策规则，用于对新样本进行分类或预测。\n决策树由节点 (Node) 和 有向边 (Directed Edge) 组成。节点分为两种类型： - 内部节点 (Internal Node)：表示一个特征或属性的测试条件，用于决定样本的划分方向。 - 叶节点 (Leaf Node / Terminal Node)：表示最终的决策结果，即类别标签（分类树）或预测值（回归树）。\n有向边代表划分规则，从父节点指向子节点。从根节点到每个叶节点的路径都对应着一条决策规则。\n决策树的学习过程主要包括三个步骤：特征选择、树的生成 和 树的剪枝。\n回归树 (Regression Tree):\n回归树 (Regression Tree) 用于预测连续数值型目标变量。例如，预测房价、股票价格等。\n模型构建:\n回归树的构建过程是一个递归的二叉树构建过程，也称为 CART (Classification and Regression Tree) 树。CART 树是一种二叉树，内部节点根据特征取值将数据集划分为两个子集，叶节点输出预测值。回归树的构建过程如下：\n\n选择划分特征和划分点：从所有特征和所有可能的划分点中，选择一个最优的特征 \\(j\\) 和切分点 \\(s\\)，将当前节点的数据集划分为两个区域 \\(R_1(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j \\le s\\}\\) 和 \\(R_2(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j &gt; s\\}\\)。\n最小化平方误差：选择最优划分属性 \\(j\\) 和划分点 \\(s\\) 的目标是最小化划分后的平方误差 (Squared Error)，即使得划分后的两个子区域内样本的目标变量值尽可能接近。对于给定的特征 \\(j\\) 和切分点 \\(s\\)，遍历所有可能的 \\((j, s)\\) 对，计算划分后的平方误差，选择使得平方误差最小的 \\((j, s)\\) 对作为最优划分。平方误差的计算公式为：\n\n\\[\\min_{j,s} \\left[ \\min_{c_1} \\sum_{\\mathbf{x}_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{\\mathbf{x}_i \\in R_2(j,s)} (y_i - c_2)^2\\right]\\]\n其中 \\(c_1\\) 和 \\(c_2\\) 分别是区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\) 的预测值。对于给定的区域 \\(R_m(j,s)\\)，最优的预测值 \\(\\hat{c}_m\\) 是该区域内样本目标变量的均值：\n\\[\\hat{c}_m = \\text{ave}(y_i|\\mathbf{x}_i \\in R_m(j,s)) = \\frac{1}{|R_m(j,s)|} \\sum_{\\mathbf{x}_i \\in R_m(j,s)} y_i\\]\n\n递归划分：对划分后的两个子区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\)，递归地重复步骤 1 和 2，继续选择最优特征和切分点进行划分，直到满足停止条件。停止条件通常包括：\n\n节点内样本数量小于某个预设阈值。\n节点内样本的目标变量方差或平方误差小于某个阈值。\n没有更多特征可用于划分，或所有特征都已用完。\n\n生成叶节点：当满足停止条件时，将当前节点作为叶节点，并计算叶节点的预测值，通常为叶节点内样本目标变量的均值。\n\n分类树 (Classification Tree):\n分类树 (Classification Tree) 用于预测离散类别型目标变量。例如，判断用户是否会流失、识别图像中的物体类别等。\n模型构建:\n分类树的构建过程与回归树类似，也是一个递归的二叉树构建过程。不同之处在于，分类树在选择最优特征和切分点时，使用的划分指标不同，以及叶节点的预测值类型不同。分类树常用的划分指标包括 信息增益 (Information Gain)、信息增益率 (Information Gain Ratio) 和 基尼指数 (Gini Index)。目标是使得划分后的子节点数据尽可能 “纯净” (Pure)，即属于同一类别的样本比例尽可能高。\n划分指标:\n\n信息增益 (Information Gain)：基于信息熵 (Entropy) 的划分指标。信息熵衡量了数据集的混乱程度或不确定性。信息增益表示使用特征 \\(A\\) 对数据集 \\(D\\) 进行划分后，数据集 \\(D\\) 的信息熵减少的程度。信息增益越大，说明使用特征 \\(A\\) 划分数据集的效果越好。常用的基于信息增益的决策树算法是 ID3 算法。\n信息增益率 (Information Gain Ratio)：为了克服信息增益对取值数目较多的特征的偏好，C4.5 算法引入了信息增益率。信息增益率在信息增益的基础上，除以特征 \\(A\\) 本身的熵，对特征取值数目较多的情况进行惩罚。常用的基于信息增益率的决策树算法是 C4.5 算法。\n基尼指数 (Gini Index)：基尼指数衡量了数据集的纯度。基尼指数越小，数据集纯度越高。CART 算法使用基尼指数作为分类树的划分指标。\n\n叶节点预测值:\n分类树的叶节点输出类别标签，通常是叶节点内样本数量最多的类别（多数表决）。\n决策树的特点:\n\n优点：\n\n易于理解和解释：决策树模型直观易懂，决策规则清晰可见，易于向业务人员解释。\n可以处理类别型和数值型特征：无需对特征进行预处理，如独热编码、标准化等。\n无需特征缩放：决策树模型对特征的尺度不敏感，无需进行特征缩放。\n可以处理缺失值：决策树模型可以处理包含缺失值的数据。\n可以进行特征选择：决策树模型在构建过程中会自动选择重要的特征进行划分。\n\n缺点：\n\n容易过拟合：决策树模型容易在训练集上过拟合，导致泛化能力差。可以通过剪枝 (Pruning) 等方法缓解过拟合问题。\n不稳定：决策树模型对训练数据敏感，训练数据的微小变化可能导致树结构发生很大变化。\n忽略特征之间的相关性：决策树模型在选择划分特征时，每次只考虑一个特征，忽略了特征之间的相关性。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#总结",
    "href": "03_supervised.html#总结",
    "title": "4  监督学习（上）",
    "section": "4.3 总结",
    "text": "4.3 总结\n本讲义主要介绍了监督学习的基本概念和常用模型，包括：\n\n监督学习概述: 介绍了监督学习的定义、应用场景以及与量化投资的结合。\n回归模型: 详细讲解了线性回归和岭回归模型，包括模型表达式、最小二乘法、正则化以及模型特点。\n分类模型: 深入探讨了支持向量机 (SVM) 和决策树模型，包括模型原理、核函数、优化方法以及模型优缺点。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html",
    "href": "04_supervised.html",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "5.1 集成学习 (Ensemble Learning)\n集成学习 (Ensemble Learning) 是一种将多个弱学习器 (Weak Learner) 组合成一个强学习器 (Strong Learner) 的技术。 弱学习器通常指预测性能略优于随机猜测的模型，例如单层决策树、简单的线性模型等。集成学习的核心思想是 “集思广益”，即通过组合多个弱学习器的预测结果，来获得比单个强学习器更全面、更鲁棒的预测能力。\n集成学习模型通常具有比单个学习器更好的预测性能和泛化能力。 这是因为集成学习可以通过以下方式降低模型的误差：\n集成学习的核心思想是 “三个臭皮匠，顶个诸葛亮”，即通过集体智慧来提高模型的性能。常用的集成学习方法包括 Bagging (Bootstrap Aggregating)、Boosting (提升法) 和 Stacking (堆叠法)。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#集成学习-ensemble-learning",
    "href": "04_supervised.html#集成学习-ensemble-learning",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "降低方差 (Variance Reduction)：Bagging 等方法通过并行训练多个基学习器，并对它们的预测结果进行平均或投票，可以有效降低模型的方差，提高模型的稳定性。\n降低偏差 (Bias Reduction)：Boosting 等方法通过串行训练多个基学习器，每个基学习器都试图纠正前一个基学习器的错误，可以有效降低模型的偏差，提高模型的预测精度。\n提高鲁棒性 (Robustness Improvement)：集成学习模型通常不易过拟合，对异常值和噪声数据具有更强的鲁棒性。\n\n\n\n5.1.1 Bagging (Bootstrap Aggregating)\nBagging (Bootstrap Aggregating)，也称为自助采样聚合，是一种基于 自助采样 (Bootstrap Sampling) 的集成学习方法。Bagging 的核心思想是 并行集成，即同时训练多个独立的基学习器，然后通过投票 (Voting) 或 平均 (Averaging) 的方式将它们的预测结果集成起来。Bagging 可以有效降低模型的方差 (Variance)，提高模型的稳定性和泛化能力，尤其适用于容易过拟合的基学习器，如决策树、神经网络等。\n自助采样 (Bootstrap Sampling) 的直观解释:\n想象一下，你有一个装满弹珠的罐子（代表原始数据集），你想估计罐子里弹珠颜色的分布。自助采样就像是：\n\n有放回地从罐子里随机抽取一些弹珠，记录它们的颜色，然后放回罐子。\n重复步骤 1 多次，每次抽取都形成一个新的弹珠样本集（子数据集）。\n\n由于是有放回的抽样，每次抽取的子数据集都可能包含重复的弹珠，也可能缺少原始数据集中的某些弹珠。但从整体上看，每个子数据集都近似地代表了原始数据集的分布。\nBagging 降低方差的原理:\nBagging 通过自助采样创建多个略有不同的训练数据集，并在每个数据集上独立训练一个基学习器。由于每个基学习器都是在不同的数据集上训练的，它们之间具有一定的差异性。当对多个基学习器的预测结果进行平均或投票时，可以有效地平滑掉单个基学习器预测结果中的随机波动，从而降低整体模型的方差。\n算法流程:\n\n自助采样 (Bootstrap Sampling)：从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建一个子数据集 \\(D_t\\)，也称为 自助样本集 (Bootstrap Sample)。重复 \\(T\\) 次，得到 \\(T\\) 个独立的子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。每个子数据集的大小与原始数据集相同，但样本分布略有不同。由于是有放回抽样，因此每个子数据集中可能包含重复样本，也可能缺少原始数据集中的某些样本。一般来说，每个子数据集大约包含原始数据集 63.2% 的样本。\n训练基学习器 (Base Learner)：在每个子数据集 \\(D_t\\) 上独立地训练一个基学习器 \\(h_t\\)。基学习器可以是同质的（例如，都使用决策树），也可以是异质的（例如，使用决策树、神经网络、SVM 等不同的模型）。常用的基学习器是决策树，此时的 Bagging 集成学习方法就是 随机森林 (Random Forest)。\n集成预测 (Ensemble Prediction)：对于分类问题，Bagging 通常使用 投票法 (Voting) 进行预测，即将 \\(T\\) 个基学习器预测结果中出现次数最多的类别作为最终预测结果。对于回归问题，Bagging 通常使用 平均法 (Averaging) 进行预测，即将 \\(T\\) 个基学习器预测结果的平均值作为最终预测结果。\n\n随机森林 (Random Forest):\n随机森林 (Random Forest, RF) 是一种非常流行且强大的 基于 Bagging 思想的集成学习模型，以 决策树 (Decision Tree) 为基学习器。随机森林在 Bagging 的基础上，进一步引入了 特征随机选择 (Random Feature Selection)，使得基学习器之间具有更高的差异性 (Diversity)，从而进一步提高集成的性能。随机森林具有高精度、高效率、鲁棒性强、不易过拟合等优点，被广泛应用于分类、回归和特征重要性评估等任务。\n特征随机选择 (Random Feature Selection) 的直观解释:\n在构建决策树时，传统的决策树会在所有特征中选择最优特征进行节点分裂。而特征随机选择则是在每个节点分裂时，先随机选择一部分特征，然后只在这部分特征中选择最优特征。\n特征随机选择提高模型性能的原理:\n特征随机选择进一步增加了基学习器之间的差异性。由于每个决策树只使用一部分随机选择的特征进行训练，即使在相同的子数据集上训练，不同的决策树也会学习到不同的特征子集，从而降低基学习器之间的相关性，提高集成的效果。 此外，特征随机选择还有助于降低模型的过拟合风险，并提高模型的泛化能力。\n随机森林的构建过程:\n\n自助采样 (Bootstrap Sampling)：与 Bagging 相同，从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建 \\(T\\) 个子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。\n训练基学习器 (决策树)：在每个子数据集 \\(D_t\\) 上训练一个决策树 \\(h_t\\)。与传统的决策树不同，随机森林在训练决策树的过程中，引入了特征随机选择。具体来说，在决策树的每个节点分裂时，不是从所有特征中选择最优特征，而是先随机选择 \\(m\\) 个特征构成一个特征子集（通常 \\(m &lt;&lt; K\\)，例如 \\(m = \\sqrt{K}\\)），然后从这个特征子集中选择最优特征进行分裂。这里的 \\(m\\) 是一个超参数，需要预先设定。特征随机选择进一步增加了基学习器之间的差异性，使得随机森林的集成效果更好。\n集成预测 (Ensemble Prediction)：与 Bagging 相同，对于分类问题使用投票法，对于回归问题使用平均法。\n\n随机森林的特点:\n\n高精度：随机森林通常比单个决策树具有更高的预测精度。\n鲁棒性强：随机森林对异常值和噪声数据具有较好的鲁棒性。\n不易过拟合：随机森林通过 Bagging 和特征随机选择，有效降低了模型的方差，不易过拟合。\n泛化能力强：随机森林具有较强的泛化能力，在测试集上表现良好。\n可处理高维数据：随机森林可以处理高维数据，无需进行特征选择。\n可评估特征重要性：随机森林可以评估每个特征在模型中的重要性，用于特征选择和特征理解。\n实现简单，易于并行化：随机森林的构建过程简单高效，基学习器之间相互独立，易于并行化处理，训练速度快。\n\n\n\n5.1.2 Boosting (提升法)\nBoosting (提升法) 是一种与 Bagging 不同的集成学习方法。Boosting 的核心思想是 串行集成，即迭代地训练一系列的基学习器，每个基学习器都试图纠正前一个基学习器的错误。Boosting 方法通过加权样本或调整预测结果的方式，逐步提升模型的性能。Boosting 方法可以有效降低模型的偏差 (Bias) 和方差，得到高精度的集成模型，尤其适用于弱学习器，如浅层决策树 (Decision Stump)。常用的 Boosting 算法包括 AdaBoost (Adaptive Boosting)、GBDT (Gradient Boosting Decision Tree)、XGBoost (Extreme Gradient Boosting)、LightGBM (Light Gradient Boosting Machine)、CatBoost (Categorical Boosting) 等。\nBoosting 的串行集成思想的直观解释:\nBoosting 就像是一个团队合作解决问题的过程，其中：\n\n第一个弱学习器先尝试解决问题，但可能做得不够好，会犯一些错误。\n后续的弱学习器会仔细研究第一个学习器犯的错误，并针对这些错误进行改进，尝试纠正之前的错误。\n每个弱学习器都在前一个学习器的基础上进行提升，逐步提高整体的预测能力。\n\nBoosting 迭代提升模型性能的原理:\nBoosting 方法通过迭代训练，每一轮迭代都关注前一轮模型预测错误的样本，并调整样本权重或模型权重，使得后续的模型更加关注难以分类或预测的样本。 这样不断地迭代和调整，逐步将弱学习器提升为强学习器，最终得到一个高精度、高性能的集成模型。\nAdaBoost (Adaptive Boosting):\nAdaBoost (Adaptive Boosting, 自适应提升) 是一种经典的 Boosting 算法。AdaBoost 的核心思想是 “关注错误样本” 和 “加权基学习器”。AdaBoost 通过迭代地训练基学习器，每轮迭代都更加关注前一轮基学习器预测错误的样本，提高错误样本的权重，降低正确样本的权重，使得后续的基学习器更加关注难以分类的样本。同时，AdaBoost 为每个基学习器赋予一个权重，预测性能好的基学习器权重较高，预测性能差的基学习器权重较低。最终模型是所有基学习器的加权线性组合。AdaBoost 算法主要用于二分类问题。\n“关注错误样本” 和 “加权基学习器” 的直观解释:\n\n关注错误样本：在每一轮迭代中，AdaBoost 会提高上一轮分类错误的样本的权重，使得后续的基学习器更加关注这些难分样本，努力将它们分类正确。 这就像老师在辅导学生时，会更加关注那些经常犯错的学生，帮助他们改正错误，提高学习成绩。\n加权基学习器：AdaBoost 会根据每个基学习器的预测性能，赋予不同的权重。预测性能好的基学习器，例如错误率低的基学习器，会被赋予更高的权重，在最终的预测中起更大的作用。 这就像专家团队中，更信任那些经验丰富、能力强的专家的意见。\n\nAdaBoost 算法流程 (二分类):\n\n初始化样本权重 (Initialize Sample Weights)：为每个样本赋予相同的初始权重 \\(w_{1i} = 1/N, i = 1, 2, ..., N\\)。初始时，所有样本的权重相同，表示所有样本同等重要。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n训练基学习器 (Train Base Learner)：使用带有样本权重的训练数据集 \\(\\{( \\mathbf{x}_i, y_i, w_{ti} )\\}_{i=1}^{N}\\) 训练一个基学习器 \\(h_t(\\mathbf{x})\\)。在第 \\(t\\) 轮迭代中，基学习器 \\(h_t\\) 基于样本权重 \\(w_{ti}\\) 进行训练，使得加权训练误差最小化。基学习器通常选择弱学习器，如决策树桩 (Decision Stump)，即单层决策树。\n计算基学习器权重 (Calculate Base Learner Weight)：计算基学习器 \\(h_t\\) 在训练集上的加权错误率 \\(e_t = P(h_t(\\mathbf{x}_i) \\ne y_i) = \\sum_{i=1}^{N} w_{ti} I(h_t(\\mathbf{x}_i) \\ne y_i)\\)。 \\(e_t\\) 表示被基学习器 \\(h_t\\) 误分类的样本的权重之和。如果 \\(e_t &gt; 0.5\\)，则停止迭代，因为此时基学习器的性能甚至不如随机猜测。然后计算基学习器 \\(h_t\\) 的权重 \\(\\alpha_t = \\frac{1}{2} \\ln(\\frac{1-e_t}{e_t})\\)。当 \\(e_t\\) 越小时，\\(\\alpha_t\\) 越大，说明基学习器 \\(h_t\\) 的预测性能越好，权重越高。\n更新样本权重 (Update Sample Weights)：根据基学习器 \\(h_t\\) 的预测结果更新样本权重，提高误分类样本的权重，降低正确分类样本的权重，使得后续的基学习器更加关注难以分类的样本。样本权重更新公式为： \\[w_{t+1, i} = \\frac{w_{ti}}{Z_t} \\times \\begin{cases} e^{-\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) = y_i \\\\ e^{\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) \\ne y_i \\end{cases} = \\frac{w_{ti}}{Z_t} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\] 其中 \\(Z_t = \\sum_{i=1}^{N} w_{ti} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\) 是归一化因子 (Normalization Factor)，使得 \\(\\sum_{i=1}^{N} w_{t+1, i} = 1\\)，保证样本权重之和为 1。 \\(y_i \\in \\{-1, +1\\}\\) 是样本的真实标签， \\(h_t(\\mathbf{x}_i) \\in \\{-1, +1\\}\\) 是基学习器的预测标签。如果样本被正确分类 (\\(y_i h_t(\\mathbf{x}_i) = +1\\))，则样本权重乘以 \\(e^{-\\alpha_t} &lt; 1\\)，权重降低；如果样本被误分类 (\\(y_i h_t(\\mathbf{x}_i) = -1\\))，则样本权重乘以 \\(e^{\\alpha_t} &gt; 1\\)，权重提高。\n\n构建最终模型 (Build Final Model)：经过 \\(T\\) 轮迭代后，得到 \\(T\\) 个基学习器 \\(\\{h_1, h_2, ..., h_T\\}\\) 及其对应的权重 \\(\\{\\alpha_1, \\alpha_2, ..., \\alpha_T\\}\\)。最终模型是基学习器的加权线性组合：对于新样本 \\(\\mathbf{x}\\)，最终模型的预测结果为： \\(H(\\mathbf{x}) = \\text{sign}(\\sum_{t=1}^{T} \\alpha_t h_t(\\mathbf{x}))\\)。 \\(\\text{sign}(z)\\) 是符号函数，当 \\(z &gt; 0\\) 时，\\(\\text{sign}(z) = +1\\)；当 \\(z &lt; 0\\) 时，\\(\\text{sign}(z) = -1\\)；当 \\(z = 0\\) 时，\\(\\text{sign}(z) = 0\\) 或 \\(+1\\) 或 \\(-1\\)，通常取 \\(+1\\) 或 \\(-1\\)。\n\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT):\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT) 是一种非常强大且广泛应用的 Boosting 算法，以 决策树 (Decision Tree) 为基学习器。GBDT 的核心思想是 梯度提升 (Gradient Boosting)，也称为 梯度下降提升 (Gradient Descent Boosting)。GBDT 使用 梯度提升算法，通过迭代地训练决策树来拟合负梯度残差 (Negative Gradient Residuals)，逐步逼近真实的目标函数。GBDT 可以用于回归和分类问题，尤其在非线性和复杂的预测问题中表现出色。GBDT 是许多高级 Boosting 算法（如 XGBoost、LightGBM、CatBoost）的基础。\n梯度提升 (Gradient Boosting) 的直观解释:\n梯度提升的思想可以理解为函数空间的梯度下降。在传统的梯度下降中，我们是在参数空间中沿着负梯度方向迭代优化参数，以最小化损失函数。而在梯度提升中，我们是在函数空间中沿着负梯度方向迭代优化模型函数，以逼近真实的目标函数。\nGBDT 拟合负梯度残差的原理:\nGBDT 的每一轮迭代都训练一个新的决策树，目标是拟合当前模型预测结果与真实值之间的残差。更精确地说，GBDT 拟合的是损失函数的负梯度，即残差的某种形式。通过不断地拟合残差，GBDT 逐步减小模型的预测误差，提高模型的预测精度。\nGBDT 算法流程 (以回归问题为例):\nGBDT 算法流程 (以回归问题为例):\n\n初始化模型 (Initialize Model)：初始化一个弱学习器（例如，常数模型 (Constant Model)，即所有样本的预测值都为一个常数） \\(f_0(\\mathbf{x}) = \\text{average}(y_i)\\)。 \\(f_0(\\mathbf{x})\\) 是初始模型的预测函数，通常使用训练集样本目标变量的均值作为初始预测值。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n计算负梯度残差 (Compute Negative Gradient Residuals)：对于每个样本 \\(i = 1, 2, ..., N\\)，计算负梯度残差 \\(r_{ti} = y_i - f_{t-1}(\\mathbf{x}_i)\\)。 \\(f_{t-1}(\\mathbf{x}_i)\\) 是前一轮迭代得到的模型 \\(f_{t-1}\\) 对样本 \\(\\mathbf{x}_i\\) 的预测值。 \\(r_{ti}\\) 表示真实值 \\(y_i\\) 与当前模型预测值 \\(f_{t-1}(\\mathbf{x}_i)\\) 之间的差异，即模型在样本 \\(\\mathbf{x}_i\\) 上的预测误差。负梯度残差 \\(r_{ti}\\) 可以看作是本轮迭代需要拟合的目标，即模型需要在本轮迭代中学习如何纠正前一轮的预测误差。\n训练决策树 (Train Decision Tree)：使用 \\((\\mathbf{x}_i, r_{ti})_{i=1}^{N}\\) 作为训练数据，训练一个决策树 \\(h_t(\\mathbf{x})\\)，拟合负梯度残差 \\(r_{ti}\\)。决策树 \\(h_t(\\mathbf{x})\\) 的叶节点区域 \\(\\{R_{tj}\\}_{j=1}^{J_t}\\) 将特征空间划分为 \\(J_t\\) 个互不相交的区域， \\(J_t\\) 是决策树 \\(h_t\\) 的叶节点数量。\n确定叶节点区域的输出值 (Determine Leaf Node Output Values)：对于决策树 \\(h_t(\\mathbf{x})\\) 的每个叶节点区域 \\(R_{tj}\\)，计算该区域内样本负梯度残差的平均值 \\(c_{tj} = \\text{average}_{\\mathbf{x}_i \\in R_{tj}} (r_{ti}) = \\frac{1}{|R_{tj}|} \\sum_{\\mathbf{x}_i \\in R_{tj}} r_{ti}\\)。 \\(c_{tj}\\) 是决策树 \\(h_t\\) 在叶节点区域 \\(R_{tj}\\) 上的预测值，表示模型在本轮迭代中需要在区域 \\(R_{tj}\\) 内进行的调整量。\n更新模型 (Update Model)：更新模型 \\(f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj})\\)，其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，也称为 shrinkage 参数，通常取值范围为 \\((0, 1]\\)，例如 0.1、0.01 等。 \\(\\alpha\\) 控制每个基学习器的步长，减小学习率可以降低模型对后续基学习器的依赖程度，提高模型的泛化能力，但需要更多的迭代次数。 \\(I(\\mathbf{x} \\in R_{tj})\\) 是指示函数，当样本 \\(\\mathbf{x}\\) 属于叶节点区域 \\(R_{tj}\\) 时， \\(I(\\mathbf{x} \\in R_{tj}) = 1\\)，否则 \\(I(\\mathbf{x} \\in R_{tj}) = 0\\)。 \\(f_t(\\mathbf{x})\\) 是本轮迭代更新后的模型，它是在前一轮模型 \\(f_{t-1}(\\mathbf{x})\\) 的基础上，加上本轮训练的决策树 \\(h_t(\\mathbf{x})\\) 的加权结果，逐步逼近真实的目标函数。\n\n得到最终模型 (Obtain Final Model)：经过 \\(T\\) 轮迭代后，得到最终的 GBDT 模型 \\(f_T(\\mathbf{x}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\sum_{j=1}^{J_t} \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\alpha h_t(\\mathbf{x})\\)。最终模型是所有基学习器的加权和。\n\nGBDT 算法流程 (以分类问题为例):\nGBDT 用于分类问题时，算法流程与回归问题类似，主要区别在于： - 损失函数不同：回归问题通常使用平方误差损失函数 (Squared Error Loss)，分类问题通常使用对数似然损失函数 (Log-Likelihood Loss) 或 指数损失函数 (Exponential Loss) 等。 - 负梯度计算不同：不同损失函数的负梯度计算方式不同。例如，对于二分类问题，如果使用对数似然损失函数，则负梯度残差的计算公式与回归问题不同。 - 叶节点区域的输出值确定方式不同：分类问题中，叶节点区域的输出值通常不是负梯度残差的平均值，而是根据具体的损失函数和优化目标确定。例如，对于二分类问题，可以使用对数几率 (Log Odds) 或 类别概率 作为叶节点输出值。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#神经网络-neural-network",
    "href": "04_supervised.html#神经网络-neural-network",
    "title": "5  监督学习（下）",
    "section": "5.2 神经网络 (Neural Network)",
    "text": "5.2 神经网络 (Neural Network)\n神经网络 (Neural Network, NN)，更精确地说是人工神经网络 (Artificial Neural Network, ANN)，是一种模拟生物神经系统结构的计算模型，由大量相互连接的神经元 (Neuron) 组成。神经网络可以学习复杂的非线性关系，具有强大的模式识别、函数逼近和自适应能力。神经网络在图像识别、自然语言处理、语音识别、金融预测等领域取得了巨大成功，是深度学习 (Deep Learning) 的基础。\n神经网络的核心思想:\n神经网络的核心思想是模拟生物神经系统的信息处理方式。生物神经系统由大量的神经元相互连接而成，神经元之间通过电信号和化学信号传递信息。神经网络试图抽象和模拟这种信息传递和处理机制，通过构建由大量神经元相互连接的网络结构，来实现复杂的信息处理和学习任务。\n神经网络的优势:\n\n强大的非线性建模能力：神经网络通过激活函数的非线性变换，可以学习和表示非常复杂的非线性关系，这是传统线性模型难以实现的。\n高度的并行分布式计算能力：神经网络由大量的神经元并行工作，可以高效地处理大规模数据和复杂计算任务。\n良好的泛化能力：通过合理的网络结构设计和训练方法，神经网络可以学习到数据中的本质规律，具有良好的泛化能力，在未见过的数据上也能表现良好。\n自适应学习能力：神经网络可以通过反向传播算法等方法，自动地从数据中学习，调整网络参数，适应不同的任务和数据。\n\n\n5.2.1 神经网络模型及算法\n神经元模型 (Neuron Model):\n神经元是神经网络的基本单元，也称为 感知机 (Perceptron)。一个典型的神经元模型包括以下几个主要组成部分：\n\n输入 (Input)：神经元接收来自其他神经元或外部环境的输入信号。输入信号可以是数值、向量或张量等形式。 例如，对于图像识别任务，输入可以是图像像素的灰度值；对于自然语言处理任务，输入可以是词向量。\n权重 (Weight)：每个输入连接都对应一个权重 \\(w_{ij}\\)，表示连接的强度或重要性。权重可以是正数（兴奋性连接）或负数（抑制性连接）。 权重是神经元学习的关键参数，通过调整权重，神经元可以选择性地接收和处理不同的输入信号。\n偏置 (Bias)：神经元还接收一个偏置 \\(b_i\\)，也称为 阈值 (Threshold)。偏置是一个常数，用于调整神经元的激活阈值，使得神经元更容易或更不容易被激活。 偏置可以看作是神经元的一个自由度，使得神经元可以更加灵活地进行激活。\n加权求和 (Weighted Summation)：神经元将所有输入信号与对应的权重进行加权求和，再加上偏置，得到神经元的净输入 (Net Input) \\(z_i = \\sum_{j} w_{ij} x_j + b_i\\)。 净输入 \\(z_i\\) 表示神经元接收到的所有输入信号的综合强度。\n激活函数 (Activation Function)：神经元对净输入 \\(z_i\\) 进行非线性变换，通过激活函数 \\(\\sigma(\\cdot)\\) 产生神经元的输出 (Output) \\(a_i = \\sigma(z_i) = \\sigma(\\sum_{j} w_{ij} x_j + b_i)\\)。激活函数是神经网络实现非线性建模的关键。 激活函数引入了非线性因素，使得神经网络可以逼近任意复杂的非线性函数。常用的激活函数包括：\n\nSigmoid 函数：\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)，将输入值映射到 \\((0, 1)\\) 区间，常用于二分类问题的输出层。 Sigmoid 函数的输出值可以解释为概率，例如样本属于正类的概率。\nTanh 函数 (Hyperbolic Tangent Function)：\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1\\)，将输入值映射到 \\((-1, 1)\\) 区间，与 Sigmoid 函数类似，但输出范围不同。 Tanh 函数的输出值以 0 为中心，可能在某些情况下比 Sigmoid 函数更易于训练。\nReLU 函数 (Rectified Linear Unit)：\\(\\text{ReLU}(z) = \\max(0, z)\\)，当输入值大于 0 时，输出值等于输入值；当输入值小于等于 0 时，输出值为 0。ReLU 函数计算简单，收敛速度快，是深度神经网络中最常用的激活函数之一。 ReLU 函数在正区间是线性函数，负区间是常数 0，这种简单的非线性使其在深度网络中表现出色。\nLeaky ReLU 函数 (Leaky Rectified Linear Unit)：\\(\\text{Leaky ReLU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha z, & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个很小的常数，例如 0.01。Leaky ReLU 函数解决了 ReLU 函数在输入值为负数时输出值为 0 导致神经元 “死亡” 的问题。 Leaky ReLU 函数在负区间也保持一定的梯度，有助于信息在网络中更好地传播。\nELU 函数 (Exponential Linear Unit)：\\(\\text{ELU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha (e^z - 1), & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个正的常数。ELU 函数具有 ReLU 函数的优点，同时在输入值为负数时输出值也具有一定的梯度，可以加速神经网络的收敛。 ELU 函数在负区间使用指数函数，可以提供更平滑的输出，并有助于网络的鲁棒性。\nSoftmax 函数：\\(\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\\)，用于多分类问题的输出层。Softmax 函数将一组输入值映射为一组概率值，每个概率值都在 \\((0, 1)\\) 区间，且所有概率值之和为 1。 Softmax 函数可以将神经元的输出转换为类别概率分布，方便进行多分类任务。\n\n输出 (Output)：神经元的输出信号 \\(a_i\\)，可以作为其他神经元的输入，或作为整个神经网络的输出。 神经元的输出是经过激活函数处理后的结果，可以传递给下一层神经元，或者作为最终的预测结果。\n\n多层神经网络 (Multilayer Neural Network, MLNN) / 多层感知机 (Multilayer Perceptron, MLP):\n多层神经网络 (Multilayer Neural Network, MLNN) 或 多层感知机 (Multilayer Perceptron, MLP) 是由多个神经元层相互连接而成的神经网络。多层神经网络通常包括以下几种类型的层：\n\n输入层 (Input Layer)：接收外部输入数据。输入层的神经元不进行任何计算，只是将输入数据传递给下一层。输入层神经元的数量等于输入特征的维度。 例如，如果输入是 28x28 的图像，则输入层通常有 784 个神经元（将图像展平成向量）。\n隐藏层 (Hidden Layer)：位于输入层和输出层之间，可以有一层或多层。隐藏层是神经网络的核心部分，负责提取输入数据的特征，学习输入数据中的复杂模式。隐藏层神经元的数量和层数是神经网络的超参数，需要根据具体问题进行设计和调整。 隐藏层越多，网络可以学习到的特征就越抽象、越复杂。\n输出层 (Output Layer)：产生最终的输出结果。输出层神经元的数量取决于具体的任务类型。例如，二分类问题的输出层通常只有一个神经元，使用 Sigmoid 激活函数，输出样本属于正类的概率；多分类问题的输出层通常有 \\(C\\) 个神经元（\\(C\\) 是类别数量），使用 Softmax 激活函数，输出样本属于每个类别的概率；回归问题的输出层通常只有一个神经元，不使用激活函数或使用 线性激活函数，直接输出预测值。\n\n深度神经网络 (Deep Neural Network, DNN) 指的是具有多个隐藏层的神经网络。 “深度” 指的是网络结构的深度，即隐藏层的层数。 深度神经网络可以学习到更加抽象和层次化的特征表示，从而更好地处理复杂的数据和任务。\n全连接神经网络 (Fully Connected Neural Network, FCNN) / 稠密神经网络 (Dense Neural Network):\n在多层神经网络中，相邻层之间的神经元通常采用 全连接 (Fully Connected) 的方式进行连接，即前一层的每个神经元都与后一层的所有神经元连接。这种连接方式的神经网络称为 全连接神经网络 (Fully Connected Neural Network, FCNN) 或 稠密神经网络 (Dense Neural Network)。全连接神经网络是最基本的神经网络结构，也是许多复杂神经网络的基础。\n前向传播 (Forward Propagation):\n前向传播 (Forward Propagation) 是指输入信号从输入层经过隐藏层逐层传递到输出层的过程。在前向传播过程中，每一层的神经元接收来自前一层的输出，进行加权求和和激活函数处理，然后将输出传递给下一层。通过逐层传递和计算，最终得到输出层的输出，即神经网络的预测结果。\n前向传播的计算步骤:\n\n输入层：将输入数据输入到输入层神经元。\n隐藏层：对于每个隐藏层，依次计算每个神经元的输出。计算过程包括：\n\n加权求和：将前一层所有神经元的输出与连接权重相乘，并加上偏置，得到净输入。\n激活函数：将净输入通过激活函数进行非线性变换，得到神经元的输出。\n\n输出层：计算输出层每个神经元的输出，计算过程与隐藏层类似。输出层神经元的输出即为神经网络的最终预测结果。\n\n反向传播算法 (Backpropagation, BP):\n反向传播算法 (Backpropagation, BP) 是训练多层神经网络最常用和最核心的算法。BP 算法是一种基于梯度下降法 (Gradient Descent) 的误差反向传播算法，通过计算损失函数 (Loss Function) 对网络参数（权重和偏置）的梯度，然后沿着梯度反方向更新参数，迭代地最小化损失函数，从而学习到最优的网络参数。BP 算法实现了误差信号从输出层反向传播到输入层，逐层调整网络参数，使得神经网络能够学习到输入数据中的复杂模式。\n梯度下降法 (Gradient Descent) 的直观解释:\n梯度下降法就像是在山坡上寻找山谷最低点的过程。\n\n确定当前位置（对应于当前的参数值）。\n沿着当前位置最陡峭的方向（负梯度方向）向下走一步（更新参数）。\n重复步骤 2，直到到达山谷最低点（损失函数达到最小值）。\n\n反向传播算法的步骤:\n\n前向传播 (Forward Propagation)：给定输入样本 \\((\\mathbf{x}_i, y_i)\\)，计算每个样本的前向传播输出，得到神经网络的预测值 \\(\\hat{y}_i\\)。前向传播过程是从输入层开始，逐层计算每个神经元的输出，直到输出层。\n计算损失 (Compute Loss)：根据预测值 \\(\\hat{y}_i\\) 和真实值 \\(y_i\\)，计算损失函数值 \\(L_i = L(\\hat{y}_i, y_i)\\)。损失函数衡量了模型预测结果与真实值之间的差异。常用的损失函数包括：\n\n均方误差损失函数 (Mean Squared Error Loss, MSE)：\\(L_{MSE}(\\hat{y}, y) = \\frac{1}{2} (\\hat{y} - y)^2\\)，常用于回归问题。\n交叉熵损失函数 (Cross-Entropy Loss Function)：\\(L_{CE}(\\hat{y}, y) = - [y \\ln \\hat{y} + (1-y) \\ln (1-\\hat{y})]\\)，常用于二分类问题。对于多分类问题，可以使用 多类交叉熵损失函数 (Categorical Cross-Entropy Loss)。\n\n反向传播误差 (Backpropagate Error)：从输出层开始，反向计算每一层的误差项 (Error Term) \\(\\delta_l\\)。误差项 \\(\\delta_l\\) 反映了第 \\(l\\) 层神经元的输出对最终损失的影响程度，是损失函数关于第 \\(l\\) 层神经元净输入 \\(z_l\\) 的梯度。误差项的计算公式为：\n\n输出层误差项：\\(\\delta_{output} = \\frac{\\partial L}{\\partial z_{output}} = \\frac{\\partial L}{\\partial a_{output}} \\odot \\sigma'_{output}(z_{output})\\)，其中 \\(\\odot\\) 表示逐元素乘积 (Element-wise Product)，\\(\\sigma'_{output}(z_{output})\\) 是输出层激活函数 \\(\\sigma_{output}\\) 对净输入 \\(z_{output}\\) 的导数。\n隐藏层误差项：\\(\\delta_l = \\frac{\\partial L}{\\partial z_l} = (\\mathbf{W}_{l+1}^T \\delta_{l+1}) \\odot \\sigma'_{l}(z_{l})\\)，其中 \\(\\mathbf{W}_{l+1}\\) 是第 \\(l\\) 层到第 \\(l+1\\) 层的权重矩阵，\\(\\delta_{l+1}\\) 是第 \\(l+1\\) 层的误差项，\\(\\sigma'_{l}(z_{l})\\) 是第 \\(l\\) 层激活函数 \\(\\sigma_{l}\\) 对净输入 \\(z_{l}\\) 的导数。误差项的计算是从输出层向输入层逐层反向传播的。\n\n计算梯度 (Compute Gradients)：根据误差项 \\(\\delta_l\\)，计算损失函数关于每一层权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\) 的梯度。梯度的计算公式为：\n\n权重梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\delta_l \\mathbf{a}_{l-1}^T\\)，其中 \\(\\mathbf{a}_{l-1}\\) 是第 \\(l-1\\) 层的输出（对于输入层，\\(\\mathbf{a}_0 = \\mathbf{x}\\)）。\n偏置梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\delta_l\\)。\n\n更新参数 (Update Parameters)：沿着梯度反方向更新权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\)，最小化损失函数。参数更新公式为：\n\n权重更新：\\(\\mathbf{W}_l = \\mathbf{W}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\mathbf{W}_l - \\alpha \\delta_l \\mathbf{a}_{l-1}^T\\)\n偏置更新：\\(\\mathbf{b}_l = \\mathbf{b}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\mathbf{b}_l - \\alpha \\delta_l\\) 其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，控制参数更新的步长。\n\n迭代 (Iteration)：重复步骤 1-5，遍历所有训练样本（或一个批次的样本），进行多轮迭代 (Epoch)，直到损失函数收敛或达到预设的迭代次数。\n\n\n\n5.2.2 深度学习 (Deep Learning)\n深度学习 (Deep Learning) 是机器学习的一个分支，本质上就是具有多层隐藏层的神经网络。 深度学习通过构建深层神经网络，学习数据中更加抽象和复杂的特征表示，从而解决更加复杂的任务。 深度学习在图像识别、自然语言处理、语音识别等领域取得了突破性进展，推动了人工智能的快速发展。\n深度学习的关键要素:\n\n深层神经网络结构：深度学习模型通常具有多个隐藏层，可以学习到多层次的特征表示。\n大规模数据集：深度学习模型通常需要大规模的数据集进行训练，才能充分发挥其性能。\n强大的计算能力：深度学习模型的训练通常需要大量的计算资源，例如 GPU (图形处理器)。\n高效的优化算法：深度学习模型的训练需要高效的优化算法，例如 Adam、SGD with Momentum 等。\n\n神经网络的变体模型 (Variations of Neural Networks):\n随着深度学习的发展，研究者们提出了各种各样的神经网络变体模型，以适应不同的任务和数据类型。以下是一些常见的神经网络变体模型：\n\n卷积神经网络 (Convolutional Neural Network, CNN)：\n\n特点：CNN 是一种专门用于处理图像数据的神经网络。CNN 的核心组件是卷积层 (Convolutional Layer) 和 池化层 (Pooling Layer)。卷积层可以自动学习图像中的局部特征，例如边缘、纹理等。池化层可以降低特征图的维度，减少计算量，并提高模型的鲁棒性。\n应用场景：图像分类、目标检测、图像分割、人脸识别等计算机视觉任务。\n\n循环神经网络 (Recurrent Neural Network, RNN)：\n\n特点：RNN 是一种专门用于处理序列数据的神经网络。RNN 的特点是具有循环连接，使得网络可以记忆之前的输入信息，并应用于当前的输出。\n应用场景：自然语言处理 (如文本分类、机器翻译、文本生成)、语音识别、时间序列预测等序列数据处理任务。\n常见变体：长短期记忆网络 (Long Short-Term Memory Network, LSTM) 和 门控循环单元网络 (Gated Recurrent Unit Network, GRU)，它们解决了传统 RNN 在长序列数据中容易出现的梯度消失和梯度爆炸问题。\n\nTransformer 网络:\n\n特点：Transformer 是一种基于自注意力机制 (Self-Attention Mechanism) 的神经网络结构。Transformer 摒弃了传统的 RNN 结构，完全依赖自注意力机制来捕捉输入序列中不同位置之间的关系。Transformer 具有并行计算能力强、可以捕捉长距离依赖关系等优点。\n应用场景：自然语言处理 (如机器翻译、文本摘要、问答系统)、图像识别、语音识别等各种序列数据处理任务。\n重要模型：BERT (Bidirectional Encoder Representations from Transformers)、GPT (Generative Pre-trained Transformer) 等预训练语言模型，在自然语言处理领域取得了革命性的进展。\n\n图神经网络 (Graph Neural Network, GNN)：\n\n特点：GNN 是一种专门用于处理图结构数据的神经网络。GNN 可以学习图中节点和边的特征表示，并进行节点分类、链接预测、图分类等任务。\n应用场景：社交网络分析、知识图谱、推荐系统、生物信息学等图结构数据分析任务。\n\n生成对抗网络 (Generative Adversarial Network, GAN)：\n\n特点：GAN 是一种生成模型，由生成器 (Generator) 和 判别器 (Discriminator) 两个神经网络组成。生成器负责生成假数据，判别器负责区分真假数据。两个网络相互对抗训练，最终生成器可以生成逼真的数据。\n应用场景：图像生成、图像编辑、数据增强、风格迁移等生成式任务。\n\n自编码器 (Autoencoder, AE)：\n\n特点：自编码器 是一种无监督学习模型，用于学习数据的低维表示 (特征)。自编码器由编码器 (Encoder) 和 解码器 (Decoder) 两个神经网络组成。编码器将输入数据压缩到低维空间，解码器将低维表示重构回原始数据。\n应用场景：特征提取、降维、数据去噪、异常检测等无监督学习任务。\n常见变体：变分自编码器 (Variational Autoencoder, VAE)、稀疏自编码器 (Sparse Autoencoder)、降噪自编码器 (Denoising Autoencoder) 等。\n\n\n总而言之，神经网络和深度学习是当前机器学习领域最热门和最重要的方向之一。 掌握神经网络的基本原理和常用模型，对于理解和应用人工智能技术至关重要。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#总结",
    "href": "04_supervised.html#总结",
    "title": "5  监督学习（下）",
    "section": "5.3 总结",
    "text": "5.3 总结\n本讲义深入探讨了集成学习和神经网络模型，主要内容包括：\n\n集成学习: 详细介绍了 Bagging 和 Boosting 两种主要的集成学习方法，以及随机森林和 AdaBoost 算法的原理和特点。\n神经网络: 系统讲解了神经网络的基本结构、前向传播、反向传播算法以及深度学习的概念。\n深度学习变体模型: 简要介绍了卷积神经网络 (CNN)、循环神经网络 (RNN)、Transformer 网络、图神经网络 (GNN) 和生成对抗网络 (GAN) 等深度学习模型的特点和应用场景。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html",
    "href": "05_model_assess.html",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1 模型评估与优化",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型评估与优化",
    "href": "05_model_assess.html#模型评估与优化",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1.1 为什么需要评估模型？\n\n避免过拟合与欠拟合: 模型可能在训练数据上表现很好，但在未见过的数据上表现很差 (过拟合)，或者模型可能无法捕捉到数据中的基本模式 (欠拟合)。\n选择最佳模型: 当我们尝试不同的模型或模型配置时，我们需要一种方法来比较它们的性能并选择最佳模型。\n了解模型性能: 评估可以帮助我们了解模型在不同情况下的表现，例如在不同的数据子集或不同的任务上。\n指导模型改进: 评估结果可以帮助我们识别模型的弱点，并指导我们如何改进模型。\n\n\n\n6.1.2 评估指标\n评估指标的选择取决于具体的机器学习任务类型。\n\n6.1.2.1 分类模型评估指标\n\n准确率 (Accuracy): 分类正确的样本数占总样本数的比例。\n\n适用于类别分布均衡的数据集。\n当类别不平衡时，准确率可能会产生误导。\n\n\\[\n  \\text{Accuracy} = \\frac{\\text{正确分类的样本数}}{\\text{总样本数}}\n  \\]\n精确率 (Precision): 预测为正例的样本中，真正例的比例。\n\n关注模型预测正例的准确性。\n\n\\[\n  \\text{Precision} = \\frac{TP}{TP + FP}\n  \\]\n\nTP (True Positive): 真正例，模型预测为正例，实际也是正例。\nFP (False Positive): 假正例，模型预测为正例，实际是负例。\n\n召回率 (Recall): 所有实际正例中，被模型正确预测为正例的比例。\n\n关注模型发现所有正例的能力。\n\n\\[\n  \\text{Recall} = \\frac{TP}{TP + FN}\n  \\]\n\nFN (False Negative): 假负例，模型预测为负例，实际是正例。\n\nF1 分数 (F1-Score): 精确率和召回率的调和平均值。\n\n综合考虑精确率和召回率。\n当精确率和召回率都很重要时，F1 分数是一个很好的指标。\n\n\\[\n  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n  \\]\nAUC-ROC 曲线: 受试者工作特征 (Receiver Operating Characteristic) 曲线下的面积。\n\nROC 曲线描述了在不同阈值下，真正例率 (TPR) 与假正例率 (FPR) 之间的关系。\nAUC 值越大，模型性能越好。\n适用于评估二分类模型的排序能力。\nTPR (True Positive Rate) 或 灵敏度 (Sensitivity): \\(\\frac{TP}{TP + FN}\\)， 等于召回率 (Recall)。\nFPR (False Positive Rate): \\(\\frac{FP}{FP + TN}\\)\nTN (True Negative): 真负例，模型预测为负例，实际也是负例。\n\n混淆矩阵 (Confusion Matrix): 总结分类模型预测结果的表格。\n\n可以直观地看到模型在每个类别上的预测情况。\n可以用于计算精确率、召回率、F1 分数等指标。\n\n\n\n\n\n\n\n\n\n\n\n预测为正例 (Positive Prediction)\n预测为负例 (Negative Prediction)\n\n\n\n\n实际正例 (Actual Positive)\nTP\nFN\n\n\n实际负例 (Actual Negative)\nFP\nTN\n\n\n\n\n\n6.1.2.2 回归模型评估指标\n\n均方误差 (Mean Squared Error, MSE): 预测值与真实值之差的平方的平均值。\n\n对误差进行平方，可以放大误差较大的样本的影响。\nMSE 越小，模型性能越好。\n\n\\[\n  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n\\(y_i\\): 第 \\(i\\) 个样本的真实值。\n\\(\\hat{y}_i\\): 第 \\(i\\) 个样本的预测值。\n\\(n\\): 样本总数。\n\n均绝对误差 (Mean Absolute Error, MAE): 预测值与真实值之差的绝对值的平均值。\n\n对误差取绝对值，可以避免正负误差相互抵消。\nMAE 越小，模型性能越好。\n对异常值 (outlier) 不敏感。\n\n\\[\n  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n  \\]\n均方根误差 (Root Mean Squared Error, RMSE): 均方误差的平方根。\n\n与 MSE 类似，但 RMSE 的量纲与原始数据一致，更易于解释。\nRMSE 越小，模型性能越好。\n\n\\[\n  \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n  \\]\nR 平方 (R-squared): 模型解释的方差比例。\n\n取值范围为 \\([0, 1]\\)，值越大，模型拟合程度越好。\n\\(R^2 = 1\\) 表示模型完美拟合数据。\n\\(R^2 = 0\\) 表示模型性能与使用均值作为预测值相当。\n可以用于评估模型对数据方差的解释能力。\n\n\\[\n  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n  \\]\n\n\\(SS_{res}\\): 残差平方和 (Sum of Squares of Residuals)，即 \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)。\n\\(SS_{tot}\\): 总平方和 (Total Sum of Squares)，即 \\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)。\n\\(\\bar{y}\\): 真实值的均值。\n\n\n\n\n\n6.1.3 交叉验证 (Cross-Validation)\n\n目的: 更可靠地评估模型的泛化能力，避免模型在特定数据集划分上的偶然性。\n基本思想: 将数据集分成若干份，轮流使用其中一份作为验证集，其余作为训练集进行模型训练和评估。\n常用方法:\n\nk 折交叉验证 (k-Fold Cross-Validation): 将数据集分成 k 份，每次使用其中 1 份作为验证集，其余 k-1 份作为训练集，重复 k 次。最终评估结果是 k 次评估结果的平均值。\n留一交叉验证 (Leave-One-Out Cross-Validation, LOOCV): k 折交叉验证的特殊情况，k 等于样本总数。每次只使用一个样本作为验证集，其余样本作为训练集。\n分层 k 折交叉验证 (Stratified k-Fold Cross-Validation): 在 k 折交叉验证的基础上，保证每个 fold 中各类别样本的比例与原始数据集中的比例大致相同。适用于类别不平衡的数据集。\n\n\n\n\n6.1.4 超参数调优 (Hyperparameter Tuning)\n\n超参数 (Hyperparameters): 模型训练前需要手动设置的参数，例如学习率、正则化系数、决策树的最大深度等。\n目的: 找到最佳的超参数组合，使模型在验证集上获得最佳性能。\n常用方法:\n\n网格搜索 (Grid Search): 预先定义超参数的候选值，然后穷举所有可能的超参数组合，并在验证集上评估每种组合的性能，选择最佳组合。\n随机搜索 (Random Search): 在预定义的超参数空间中随机采样超参数组合，并在验证集上评估性能，选择最佳组合。通常比网格搜索更高效，尤其是在超参数空间较大时。\n贝叶斯优化 (Bayesian Optimization): 使用贝叶斯方法建立超参数与模型性能之间的概率模型，然后根据该模型选择下一组超参数进行评估，以更有效地找到最佳超参数组合。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型优化策略",
    "href": "05_model_assess.html#模型优化策略",
    "title": "6  模型评估与优化",
    "section": "6.2 模型优化策略",
    "text": "6.2 模型优化策略\n\n特征工程 (Feature Engineering): 通过对原始特征进行转换、组合或选择，创建更有效的特征，以提高模型性能。\n\n特征转换: 例如，对数值特征进行标准化、归一化、对数变换、Box-Cox 变换等，使特征更符合模型假设或更易于模型学习。对类别特征进行独热编码、标签编码等。\n特征组合: 将多个特征进行组合，生成新的交叉特征，以捕捉特征之间的交互关系。例如，将年龄和收入进行组合，生成年龄*收入的特征。\n特征选择: 从原始特征中选择最相关的特征子集，去除冗余或不相关的特征，降低模型复杂度，提高模型泛化能力。常用的特征选择方法包括过滤式 (Filter)、包裹式 (Wrapper) 和嵌入式 (Embedded) 方法。\n\n模型选择 (Model Selection): 尝试不同的机器学习模型，例如线性回归、逻辑回归、决策树、支持向量机、神经网络等，选择最适合当前任务的模型。\n\n模型比较: 在同一个数据集上训练不同的模型，并使用交叉验证等方法评估它们的性能，选择性能最佳的模型。\n模型融合: 将多个不同模型的预测结果进行融合，以获得更好的预测性能。例如， stacking、blending 等集成方法。\n\n集成学习 (Ensemble Learning): 将多个弱学习器组合成一个强学习器，例如随机森林、梯度提升树 (GBDT)、XGBoost 等。集成学习通常可以提高模型的稳定性和泛化能力。\n\nBagging: 例如，随机森林 (Random Forest)。通过bootstrap 采样创建多个训练集，在每个训练集上训练一个弱学习器，然后将多个弱学习器的预测结果进行平均或投票。\nBoosting: 例如，梯度提升树 (Gradient Boosting Decision Tree, GBDT)、XGBoost、LightGBM、AdaBoost。 迭代地训练弱学习器，每个弱学习器都试图纠正前一个弱学习器的错误。最终将多个弱学习器加权组合成一个强学习器。\n\n数据增强 (Data Augmentation): 通过对训练数据进行变换 (例如旋转、平移、缩放、裁剪等)，增加训练数据的多样性，提高模型的泛化能力。\n\n图像数据增强: 例如，旋转、平移、缩放、裁剪、翻转、颜色变换、添加噪声等。\n文本数据增强: 例如，同义词替换、随机插入、随机删除、回译等。\n音频数据增强: 例如，添加噪声、时间拉伸、音调变换等。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#正则化-regularization",
    "href": "05_model_assess.html#正则化-regularization",
    "title": "6  模型评估与优化",
    "section": "6.3 正则化 (Regularization)",
    "text": "6.3 正则化 (Regularization)\n正则化是一种在机器学习中常用的技术，用于防止模型过拟合，提高模型的泛化能力。其基本思想是在损失函数中添加一个正则化项，以惩罚模型的复杂度。\n\nL1 正则化 (Lasso Regularization): 向损失函数添加模型权重向量的 L1 范数。\n\n作用: 使模型权重稀疏化，产生稀疏模型，有助于特征选择。\n特点: 可以将一部分权重压缩为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} |w_i|\n  \\]\n\n\\(\\lambda\\): 正则化强度超参数。\n\\(w_i\\): 模型权重。\n\nL2 正则化 (Ridge Regularization): 向损失函数添加模型权重向量的 L2 范数。\n\n作用: 减小模型权重，使模型更平滑，降低模型复杂度。\n特点: 权重趋向于变小，但不会变为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} w_i^2\n  \\]\nElastic Net: 结合 L1 和 L2 正则化的方法。\n\n作用: 结合 L1 和 L2 正则化的优点，既可以进行特征选择，又可以减小模型权重。\n特点: 通过调节 L1 和 L2 正则化项的比例，平衡特征选择和权重衰减的效果。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda_1 \\sum_{i} |w_i| + \\lambda_2 \\sum_{i} w_i^2\n  \\]\n\n\\(\\lambda_1, \\lambda_2\\): L1 和 L2 正则化强度超参数。\n\nDropout: 一种在神经网络中常用的正则化技术。\n\n原理: 在训练过程中，随机地将一部分神经元的输出置为 0。\n作用: 强制网络学习更鲁棒的特征表示，减少神经元之间的共适应性，提高泛化能力。\n特点: 简单有效，计算成本低，常用于深度神经网络。\n\nEarly Stopping (提前终止): 在模型训练过程中，监控验证集上的性能指标，当验证集性能不再提升或开始下降时，提前停止训练。\n\n原理: 随着训练的进行，模型在训练集上的性能会不断提升，但验证集上的性能可能会先提升后下降 (过拟合)。Early Stopping 旨在找到验证集性能最佳的训练迭代次数。\n优点: 简单易用，无需额外计算，可以有效防止过拟合。\n缺点: 可能会错过全局最优解，需要合适的验证集划分。\n\nBatch Normalization (批量归一化): 在神经网络的每一层输入或激活函数之前，对数据进行归一化处理。\n\n原理: 将每一批次 (batch) 的数据归一化到均值为 0，方差为 1 的分布。\n作用: 加速模型训练，提高训练稳定性，减轻内部协变量偏移 (Internal Covariate Shift) 问题，并具有一定的正则化效果。\n特点: 常用于深度神经网络，可以提高模型的泛化能力和鲁棒性。\n\n数据增强 (Data Augmentation): 虽然前面已经作为模型优化策略单独列出，但数据增强也可以被视为一种正则化技术。\n\n原理: 通过增加训练数据的多样性，使模型接触到更多不同的数据样本。\n作用: 提高模型的泛化能力，减少模型对特定训练样本的过拟合。\n特点: 特别适用于图像、文本和音频等数据，是一种有效的数据正则化方法。\n\n模型剪枝 (Pruning): 减小模型复杂度的技术，常用于决策树和神经网络。\n\n决策树剪枝: 通过剪去决策树中不必要的节点，简化决策树结构，防止过拟合。\n神经网络剪枝: 移除神经网络中不重要的连接或神经元，减小模型大小，提高模型效率，并具有一定的正则化效果。\n方法: 可以基于权重大小、梯度大小或其他指标进行剪枝。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#总结",
    "href": "05_model_assess.html#总结",
    "title": "6  模型评估与优化",
    "section": "6.4 总结",
    "text": "6.4 总结\n\n模型评估是机器学习流程中至关重要的一步，它可以帮助我们了解模型性能、选择最佳模型、并指导模型改进。\n选择合适的评估指标取决于具体的机器学习任务类型。\n交叉验证可以更可靠地评估模型的泛化能力。\n超参数调优可以找到最佳的模型配置。\n模型优化是一个迭代过程，需要不断尝试不同的策略来提高模型性能。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "09_ts.html",
    "href": "09_ts.html",
    "title": "7  时间序列监督学习",
    "section": "",
    "text": "7.1 时间序列监督学习的基本概念\n时间序列数据是按时间顺序收集的一系列数据点，如股票价格、气象数据、用户行为记录等。时间序列监督学习是指利用历史时间序列数据预测未来值的机器学习任务。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "09_ts.html#时间序列监督学习的步骤",
    "href": "09_ts.html#时间序列监督学习的步骤",
    "title": "7  时间序列监督学习",
    "section": "7.2 时间序列监督学习的步骤",
    "text": "7.2 时间序列监督学习的步骤\n\n7.2.1 数据收集与预处理\n\n数据收集：获取包含时间戳的序列数据\n数据清洗：处理缺失值、异常值和噪声\n数据标准化：对数据进行归一化或标准化处理\n特征工程：\n\n滞后特征（Lag Features）：使用过去的观测值作为特征\n窗口统计特征：计算滑动窗口内的统计量（均值、方差、最大值等）\n时间特征：提取时间戳中的周期性信息（小时、星期几、月份等）\n差分特征：计算数据点之间的差值，消除趋势\n\n\n\n\n7.2.2 特殊的样本分割方法\n时间序列数据的样本分割与传统监督学习有显著区别：\n\n时间序列分割原则：必须严格按照时间顺序分割数据，确保训练数据在测试数据之前，这是因为未来的数据在实际预测时是不可获取的。\n\n\n7.2.2.1 传统随机分割的问题\n在传统监督学习中，我们通常采用随机分割数据：\n数据: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n随机分割后:\n训练集: [1, 3, 5, 8, 9]\n测试集: [2, 4, 6, 7, 10]\n这种方法在时间序列问题中会导致数据泄露和过于乐观的评估结果，因为模型可能利用未来的信息来预测过去，而这在实际应用中是不可能的。\n\n\n7.2.2.2 时间序列数据分割方法详解\n以股票价格预测为例，假设我们有12个月的每日收盘价数据，希望构建模型来预测未来的价格。\n\n扩展窗口法（Expanding Window）\n\n扩展窗口法是使用所有历史数据进行训练，预测窗口不断向前移动：\n第1轮:\n训练集: [月份1-3]\n测试集: [月份4]\n\n第2轮:\n训练集: [月份1-4]\n测试集: [月份5]\n\n第3轮:\n训练集: [月份1-5]\n测试集: [月份6]\n\n...以此类推\n具体示例：假设我们有2022年1月至12月的股票数据，第一轮使用1-3月数据训练，预测4月；第二轮使用1-4月数据，预测5月；第三轮使用1-5月数据训练，预测6月…最终评估结果为所有测试期的平均表现。\n优势： - 充分利用所有历史数据 - 随着时间推移，模型可获得更多训练数据 - 适合长期趋势预测和季节性数据\n适用场景：宏观经济指标预测、季节性明显的数据（如零售销售）、需要考虑长期模式的预测任务\n\n滑动窗口法（Sliding Window）\n\n滑动窗口法使用固定长度的数据窗口进行训练和测试：\n第1轮:\n训练集: [月份1-3]\n测试集: [月份4]\n\n第2轮:\n训练集: [月份2-4]\n测试集: [月份5]\n\n第3轮:\n训练集: [月份3-5]\n测试集: [月份6]\n\n...以此类推\n具体示例：使用连续3个月的数据预测紧接着的1个月。每次向前滑动一个月，同时保持训练窗口大小不变。如使用2022年1-3月数据训练，预测4月；然后使用2-4月数据训练，预测5月；然后使用3-5月数据训练，预测6月…\n优势： - 适合捕捉近期模式和趋势 - 减少较旧数据的影响 - 计算效率高，训练集大小恒定\n适用场景：快速变化的时间序列，如高频交易数据、社交媒体趋势、短期市场预测\n\n多步预测分割\n\n多步预测分割专注于一次性预测多个未来时间点：\n训练集: [月份1-6]\n测试集: [月份7, 8, 9]  // 一次性预测多个未来月份\n具体示例：使用1-6月的历史数据，同时预测7、8、9月的值，而不是仅预测下一个月。\n实现方式： 1. 直接法：构建单一模型直接预测t+1, t+2, …, t+h的值 模型输入: X_t (t时刻特征)    模型输出: [y_{t+1}, y_{t+2}, ..., y_{t+h}] (未来h个时间点的预测值)\n\n递归法：使用单步预测模型，将前一步的预测结果作为下一步的输入特征\n预测y_{t+1}: 使用[X_1, X_2, ..., X_t]\n预测y_{t+2}: 使用[X_2, ..., X_t, y_{t+1}]\n预测y_{t+3}: 使用[X_3, ..., X_t, y_{t+1}, y_{t+2}]\n\n优势： - 适合需要长期规划的场景 - 可以捕捉时间序列的长期依赖性 - 评估模型在不同预测步长上的性能\n适用场景：需求预测、资源规划、长期投资策略\n\n\n7.2.2.3 时间序列交叉验证的实际示例\n以下是一个完整的实例，说明如何对2022年1月至12月的每日股票价格数据进行时间序列交叉验证：\n数据准备：\n数据集：2022年1月至12月的每日股票价格\n特征：每日开盘价、最高价、最低价、交易量、技术指标等\n目标：预测下一个交易日的收盘价\n扩展窗口法实例：\n轮次1：使用1-3月数据训练，预测4月第一周\n轮次2：使用1-4月数据训练，预测5月第一周\n轮次3：使用1-5月数据训练，预测6月第一周\n...\n轮次8：使用1-10月数据训练，预测11月第一周\n轮次9：使用1-11月数据训练，预测12月第一周\n滑动窗口法实例：\n轮次1：使用1-3月数据训练，预测4月第一周\n轮次2：使用2-4月数据训练，预测5月第一周\n轮次3：使用3-5月数据训练，预测6月第一周\n...\n轮次8：使用8-10月数据训练，预测11月第一周\n轮次9：使用9-11月数据训练，预测12月第一周\n多步预测分割实例：\n任务：使用过去3个月的数据，预测未来1个月的每周价格\n分割方式：\n训练集：1-3月数据\n测试集：4月(4个预测值，对应4周)\n\n然后滑动窗口：\n训练集：2-4月数据\n测试集：5月(4个预测值)\n...以此类推\n\n\n\n7.2.2.4 各方法的对比与选择\n\n\n\n\n\n\n\n\n\n\n分割方法\n训练数据量\n计算成本\n适用场景\n主要优势\n\n\n\n\n扩展窗口法\n不断增加\n高\n长期稳定数据、季节性数据\n充分利用所有历史数据\n\n\n滑动窗口法\n固定\n低\n快速变化的时间序列\n专注于最新趋势，减少旧数据影响\n\n\n多步预测分割\n固定\n中等-高\n需要长期预测的应用\n评估模型在多个未来时间点的预测能力\n\n\n\n\n\n7.2.2.5 选择合适的分割方法的建议\n\n考虑数据特性：\n\n快速变化的数据（如股票价格）→ 滑动窗口\n长期稳定的数据（如宏观经济指标）→ 扩展窗口\n\n考虑预测目标：\n\n短期预测（1-2步）→ 滑动窗口\n长期预测（多步）→ 多步预测分割\n\n考虑计算资源：\n\n资源有限 → 滑动窗口（训练集大小固定）\n资源充足 → 扩展窗口（可利用更多历史数据）\n\n数据量考虑：\n\n数据量小 → 扩展窗口（最大化训练数据利用）\n数据量大 → 可考虑滑动窗口（聚焦最相关数据）\n\n\n\n\n\n7.2.3 模型选择与训练\n\n适合时间序列的模型：\n\n传统统计模型：ARIMA、指数平滑法\n机器学习模型：随机森林、XGBoost、LSTM、Transformer\n混合模型：结合统计模型和机器学习模型的优点\n\n训练注意事项：\n\n考虑时间依赖性\n避免数据泄露（不使用未来信息）\n合理设置预测步长（短期或长期预测）\n\n\n\n\n7.2.4 模型评估与优化\n\n评估指标：\n\nMSE、RMSE、MAE：衡量预测误差\nMAPE：相对误差百分比\n方向准确率：预测趋势变化的正确率\n\n超参数调优：\n\n使用时间序列交叉验证而非随机交叉验证\n考虑时间窗口大小、滞后阶数等特有超参数\n\n\n\n\n7.2.5 模型部署与监控\n\n滚动预测：定期使用新数据更新模型\n模型监控：检测数据分布变化、模型漂移\n自适应更新：根据预测表现动态调整模型",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "09_ts.html#时间序列监督学习与传统监督学习的区别",
    "href": "09_ts.html#时间序列监督学习与传统监督学习的区别",
    "title": "7  时间序列监督学习",
    "section": "7.3 时间序列监督学习与传统监督学习的区别",
    "text": "7.3 时间序列监督学习与传统监督学习的区别\n\n\n\n\n\n\n\n\n方面\n时间序列监督学习\n传统监督学习\n\n\n\n\n数据依赖性\n数据点间存在时间依赖关系\n假设数据点独立同分布\n\n\n样本分割\n严格按时间顺序分割，不可随机打乱\n通常随机分割，可以打乱顺序\n\n\n交叉验证\n使用时间序列交叉验证方法\n使用K折交叉验证等随机方法\n\n\n特征工程\n重视滞后特征、时间特征和窗口特征\n关注静态特征和特征间关系\n\n\n数据泄露\n易发生时序泄露（使用未来信息）\n主要关注特征泄露\n\n\n评估重点\n关注预测的时间稳定性和趋势把握\n关注整体准确率和泛化能力\n\n\n模型更新\n需要频繁更新以适应最新数据\n通常在分布稳定时可长期使用",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "09_ts.html#挑战与解决方案",
    "href": "09_ts.html#挑战与解决方案",
    "title": "7  时间序列监督学习",
    "section": "7.4 挑战与解决方案",
    "text": "7.4 挑战与解决方案\n\n非平稳性：数据分布随时间变化\n\n解决方案：差分、移动平均、时间分解\n\n季节性：数据存在周期性模式\n\n解决方案：季节性分解、季节性特征\n\n长期依赖：当前预测可能依赖于很久之前的数据\n\n解决方案：使用LSTM、注意力机制等处理长期依赖\n\n多变量预测：多个相关时间序列的联合预测\n\n解决方案：多变量模型、变量间关系建模",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "09_ts.html#实践案例预测股票收益率",
    "href": "09_ts.html#实践案例预测股票收益率",
    "title": "7  时间序列监督学习",
    "section": "7.5 实践案例：预测股票收益率",
    "text": "7.5 实践案例：预测股票收益率\n参考文献: Gu, Kelly, and Xiu, (2020). “Empirical Asset Pricing via Machine Learning”, Review of Financial Studies.\n研究问题: 比较不同的机器学习方法在衡量资产风险溢价（预测股票收益）这一经典实证资产定价问题中的表现。\n主要贡献: 1. 设定新基准: 为机器学习方法在衡量风险溢价方面的预测准确性提供新基准，展示了显著优于传统方法的样本外预测能力（高 \\(R^2_{oos}\\)）。 2. 经济价值: 证明使用机器学习预测能为投资者带来巨大的经济收益，部分策略表现可达文献中领先的基于回归策略的两倍。 3. 方法识别与机制: 识别出表现最佳的方法（树模型和神经网络），并将其预测优势归因于捕捉了其他方法忽略的非线性预测因子交互作用。 4. 关键预测因子: 所有方法都认同同一组主要的预测信号，包括动量、流动性和波动率的变体。\n\n7.5.1 数据与特征工程\n\n资产: 近 30,000 只美国个股 (NYSE, AMEX, NASDAQ)。\n时间: 1957年3月 - 2016年12月 (60年)。\n频率: 月度数据。\n原始预测因子:\n\n94 个股票层面特征 (Characteristics)。\n\n年度更新: 61个, 季度更新: 13个, 月度更新: 20个。\n\n74 个行业虚拟变量 (基于 SIC 两位码)。\n8 个宏观经济预测变量 (来自 Welch & Goyal, 2008)\n\ndividend-price ratio (dpr)\nearnings-price ratio (epr)\nbook-to-market ratio (bmr)\nnet equity expansion (ntis)\nTreasury-bill rate (tbl)\nterm spread (tms)\ndefault spread (dfy)\nstock variance (svar)\n\n\n\n\n\n\n\n特征工程:\n\n标准化: 每个时间点，对所有股票层面的特征（94个）进行横截面排序 (Cross-sectional rank)，然后将排序值线性映射到 \\([-1, 1]\\) 区间。\n缺失值处理: 使用当月该特征的横截面中位数填补缺失值。\n滞后处理: 为避免前视偏差 (look-ahead bias)，对会计数据特征（季度、年度）的更新时间进行了滞后处理（月度特征滞后1个月，季度特征至少4个月，年度特征至少6个月）。\n交互项构建: 构建宏观变量 \\(x_t\\) (包含常数项) 与标准化后的股票特征 \\(c_{i,t}\\) 的交互项，作为最终输入机器学习模型的预测变量 \\(z_{i,t} = x_t \\otimes c_{i,t}\\)。总输入特征数 \\(P = 94 \\times (8+1) + 74 = 920\\)。\n\n\n\n\n7.5.2 实证设定\n总体模型: 预测个股超额收益 \\(r_{i,t+1}\\)： \\[r_{i,t+1} = E_t(r_{i,t+1}) + \\epsilon_{i,t+1}\\] 其中条件期望（风险溢价）是预测变量 \\(z_{i,t}\\) 的灵活函数 \\(g^*(\\cdot)\\)： \\[E_t(r_{i,t+1}) = g^*(z_{i,t})\\] * \\(g^*(\\cdot)\\) 函数形式对所有时间和股票保持不变。\n预测变量构建 \\(z_{i,t}\\): * 见“特征工程”部分的交互项构建。 * 此设定允许纯粹的股票层面信息 (通过 \\(c_{i,t}\\) 类似风险暴露 \\(\\beta_{i,t}\\)) 和总体经济状况 (通过 \\(x_t\\) 类似动态风险溢价 \\(\\lambda_t\\)) 进入预期收益模型。\n样本划分与评估: * 训练集: 1957 - 1974 (18年)。 * 验证集: 1975 - 1986 (12年, 用于超参数调优)。 * 测试集: 1987 - 2016 (30年, 用于样本外评估)。 * 更新: 每年重新训练模型，训练集递归增加，验证集滚动。 * 主要评估指标: 样本外预测 \\(R^2\\) (\\(R^2_{oos}\\))。 \\[R^2_{oos} = 1 - \\frac{\\sum_{(i,t)\\in T_3} (r_{i,t+1} - \\hat{r}_{i,t+1})^2}{\\sum_{(i,t)\\in T_3} r^2_{i,t+1}}\\] * \\(R^2_{oos}\\) 定义解释: * 分母使用的是平方超额收益之和 \\(\\sum r^2_{i,t+1}\\)，而不是通常的平方中心化收益之和 (即 \\(\\sum (r_{i,t+1} - \\bar{r})^2\\))。 * 原因: 传统的基于历史均值 (\\(\\bar{r}\\)) 的基准对于个股收益预测来说过于宽松。个股历史平均收益本身波动极大且预测能力很差，通常不如直接预测为零。若以历史均值为基准，会人为地抬高 \\(R^2_{oos}\\)，使得评价标准过低。 * 因此，本文使用的 \\(R^2_{oos}\\) 是将模型预测与零收益预测进行比较，这是一个更严格、更合理的基准。\n\n\n7.5.3 方法论概述\n本文比较了多种机器学习方法预测股票风险溢价的表现，涵盖了从简单到复杂的不同模型类别：\n\n简单线性模型 (OLS): 包括仅使用少数关键特征 (Size, B/M, Momentum) 的模型和使用全部 900+ 特征的模型。\n带惩罚项的线性模型 (Penalized Linear): 如 Elastic Net (结合 Lasso 的变量选择和 Ridge 的系数收缩)。\n降维方法 (Dimension Reduction): 主成分回归 (PCR) 和偏最小二乘法 (PLS)。\n广义线性模型 (Generalized Linear Model, GLM): 使用样条函数捕捉单变量非线性，但不含交互项。\n树模型 (Trees): 包括随机森林 (Random Forest, RF) 和梯度提升回归树 (Gradient Boosted Regression Trees, GBRT)。\n神经网络 (Neural Networks, NN): 测试了从 1 层到 5 层隐藏层的不同深度网络 (NN1-NN5)。\n\n\n\n7.5.4 主要实证结果：个股层面预测\n样本外 \\(R^2_{oos}\\) (月度):\n\nOLS: 使用全部 900+ 预测变量的 OLS 表现很差 (\\(R^2_{oos} &lt; 0\\))，有过拟合问题。OLS-3 (仅 Size, B/M, Mom) 表现尚可 (\\(R^2_{oos} = 0.16\\%\\))。\n正则化/降维: 优于 OLS-3。Elastic Net (\\(0.11\\%\\)), PCR (\\(0.26\\%\\)), PLS (\\(0.27\\%\\)) 能有效处理高维预测变量。\n非线性模型:\n\nGLM: 未能显著超越线性模型 (\\(0.19\\%\\))，表明仅加入单变量非线性作用有限。\n树模型: RF (\\(0.33\\%\\)), GBRT (\\(0.34\\%\\)) 表现优于线性模型。\n神经网络: 表现最佳。NN1 (\\(0.33\\%\\)), NN2 (\\(0.39\\%\\)), NN3 (\\(0.40\\%\\)), NN4 (\\(0.39\\%\\)), NN5 (\\(0.36\\%\\))。\n\n关键: 预测能力的提升主要来自允许预测变量间的非线性交互作用（树和 NN 模型的核心优势）。\n深度: “浅层”学习优于”深层”学习。NN 表现随层数增加先升后降，在 NN3 达到峰值。树模型也倾向于选择较浅的结构。\n\n统计显著性 (Diebold-Mariano Tests):\n\n正则化/降维方法显著优于未约束的 OLS。\n树模型优于线性模型，但优势不显著。\n神经网络显著优于线性和广义线性模型。\n神经网络相对树模型的优势不显著。\n\n\n\n7.5.5 主要实证结果：投资组合层面预测\n基于个股预测构建投资组合预测 (Bottom-up):\n\nS&P 500 指数:\n\nOLS-3 表现不佳 (\\(R^2_{oos} = -0.22\\%\\))。\nGLM 表现提升 (\\(R^2_{oos} = 0.71\\%\\))。\n树模型和 NN 表现优异， \\(R^2_{oos}\\) 在 \\(1.08\\%\\) (NN1) 到 \\(1.80\\%\\) (NN3) 之间。\n\n因子投资组合 (SMB, HML, RMW, CMA, UMD 等):\n\n模式类似 S&P 500，非线性方法（尤其是 NN）表现突出。\nNN3 对所有测试的因子组合都产生了正的 \\(R^2_{oos}\\)。\n\n原因: 投资组合层面预测力更强，因为聚合过程平均掉了部分个股层面的噪音，提升了信号强度。\n\n\n\n7.5.6 主要实证结果：经济显著性\n机器学习预测的经济价值巨大:\n\n市场择时 (Market Timing):\n\n使用 NN 预测进行 S&P 500 择时，年化夏普比率可从买入持有的 0.51 提高到 0.77 (NN3)。\n对多种特征组合的择时策略，夏普比率均有显著提升。\n\n机器学习投资组合 (直接基于预测排序):\n\n构建基于 ML 预测排序的十分位多空组合 (Decile Long-Short)。\n价值加权: NN4 策略月均回报 2.3%，年化夏普比率 1.35。OLS-3 仅为 0.61。\n等权重: 表现更优。NN4 策略年化夏普比率高达 2.45。剔除微盘股后仍有 1.69。\n\n风险调整后收益 (Alpha):\n\n基于 ML 预测构建的投资组合相对 Fama-French 五因子+动量六因子模型，产生了显著的 alpha。\nNN 策略的因子模型解释力相对较低 (R^2 约 10%-30%)，信息比率 (IR) 高。\n\n\n结论: 机器学习预测不仅统计显著，而且能转化为可观的、风险调整后的投资收益。\n\n\n7.5.7 主要实证结果：变量重要性\n哪些预测因子最重要？\n\n方法间一致性: 不同机器学习方法在识别重要预测因子方面高度一致。\n最重要特征类别:\n\n价格趋势 (Price Trends): 最重要。包括短期反转 (mom1m)、12个月动量 (mom12m)、动量变化 (chmom)、行业动量 (indmom)、近期最大回报 (maxret)、长期反转 (mom36m)。\n流动性 (Liquidity): 换手率 (turn)、市值 (mvel1)、美元交易量 (dolvol)、Amihud 非流动性 (ill)、零交易天数 (zerotrade)、买卖价差 (baspread)。\n风险 (Risk): 总/异质波动率 (retvol, idiovol)、市场 beta (beta)、beta 平方 (betasq)。\n估值与基本面: 市盈率 (ep)、市销率 (sp)、资产增长 (agr) 等也有一定作用。\n\n分布: 线性模型的重要性更集中于少数变量 (如动量)，而树和 NN 从更广泛的特征中提取信息。\n宏观变量: 所有模型都认为聚合账面市值比 (bm) 最重要。违约利差 (dfy) 和国债利率 (tbl) 在线性模型中重要，而期限利差 (tms) 和发行活动 (ntis) 在非线性模型中更重要。\n\n\n\n7.5.8 主要实证结果：交互效应\n\n交互的重要性: 树模型和神经网络的优异表现很大程度上归功于它们能捕捉预测变量之间复杂的交互效应。\n可视化示例 (NN3):\n\n特征间交互: 例如，短期反转效应在小盘股中更强且接近线性，而在大盘股中呈凹性。动量效应和低波动率异常在大盘股中更显著。\n特征与宏观变量交互: 例如，市值效应在聚合估值低 (bm 高) 或股权发行少 (ntis 低) 时更强。低波动率异常在高估值和高发行环境中尤为明显。\n\n主要交互来源: 股票近期价格趋势 (如动量、反转) 与聚合资产价格水平 (如估值比率、利率) 的交互最为重要。\n\n\n\n7.5.9 文章结论\n\nML 的价值: 机器学习，特别是神经网络 (NNs) 和树模型 (Trees)，能够显著提高资产风险溢价的衡量精度（即股票收益预测能力）。\n优势来源: 这些方法的优势在于能够灵活地捕捉预测变量之间复杂的非线性交互作用。\n关键驱动因素: 预测能力主要来源于价格趋势 (动量/反转)、流动性、波动率等特征。\n经济显著性: 机器学习预测不仅在统计上显著，而且能带来巨大的经济价值，显著提高投资组合的夏普比率并产生 Alpha。\n模型选择: 在此应用场景下，“浅层”网络 (如 3 层 NN) 表现优于“深层”网络。\n\n启示: 机器学习为理解资产定价提供了有力工具，有助于识别关键驱动因素、探索经济机制，并在金融科技创新中具有巨大潜力。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "09_ts.html#本章节总结",
    "href": "09_ts.html#本章节总结",
    "title": "7  时间序列监督学习",
    "section": "7.6 本章节总结",
    "text": "7.6 本章节总结\n时间序列监督学习与传统监督学习最大的区别在于对数据时间依赖性的处理。在样本分割、特征工程、模型选择和评估方面都需要考虑时间顺序的影响，避免数据泄露，并捕捉时间依赖模式。掌握合适的时间序列分析技术对于处理实际业务中的预测问题至关重要。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>时间序列监督学习</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html",
    "href": "lab03_titanic.html",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "",
    "text": "9 导言",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#泰坦尼克号乘客数据",
    "href": "lab03_titanic.html#泰坦尼克号乘客数据",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "9.1 泰坦尼克号乘客数据",
    "text": "9.1 泰坦尼克号乘客数据\n泰坦尼克号是历史上著名的客轮，1912年首航时与冰山相撞沉没，造成了1500多人遇难。本实践课我们将使用泰坦尼克号乘客数据，尝试预测乘客在这场灾难中的生存情况。\n数据分为两个数据集： - 训练集 train.csv：有关键变量Survived显示是否生还 - 测试集 test.csv：没有Survived变量\n变量定义如下表所示：\n\n\n\n变量\n定义\n取值说明\n\n\n\n\nSurvived\n生存状态\n0 = 未生存, 1 = 生存\n\n\nPclass\n船票等级\n1 = 一等舱, 2 = 二等舱, 3 = 三等舱\n\n\nSex\n性别\n\n\n\nAge\n年龄\n\n\n\nSibSp\n船上兄弟姐妹/配偶数量\n\n\n\nParch\n船上父母/子女数量\n\n\n\nTicket\n船票号码\n\n\n\nFare\n票价\n\n\n\nCabin\n客舱号码\n\n\n\nEmbarked\n登船港口\nC = 瑟堡, Q = 皇后镇, S = 南安普顿",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#机器学习的一般步骤",
    "href": "lab03_titanic.html#机器学习的一般步骤",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "9.2 机器学习的一般步骤",
    "text": "9.2 机器学习的一般步骤\n\n9.2.1 探索性分析\n\n对变量进行初步描述性统计分析，用以检测空值、不合法值、异常值等\n数据可视化展示，发现变量之间的关系\n\n\n\n9.2.2 数据清理与特征工程\n\n特征工程：从现有特征中提取更有价值的信息\n填补缺失值：对缺失数据进行合理的估计和填补\n\n\n\n9.2.3 数据建模与模型选择\n\n建立分类模型（sklearn包）\n\n在训练集上拟合模型\n根据交叉验证的模型评估指标选择超参数\n\n备选模型包括：\n\n逻辑回归模型\n决策树模型\n随机森林\n提升树（XGBoost）\n神经网络\n\n模型评估指标包括：\n\n精度accuracy\n查全率recall与F1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据基本信息与统计描述",
    "href": "lab03_titanic.html#数据基本信息与统计描述",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "10.1 数据基本信息与统计描述",
    "text": "10.1 数据基本信息与统计描述\n我们需要了解数据的基本信息，包括各变量的数据类型和缺失情况：\n\n\n代码\n# 查看数据基本信息\ntrain_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \n 12  train        891 non-null    int64  \ndtypes: float64(2), int64(6), object(5)\nmemory usage: 90.6+ KB\n\n\n对数值型变量进行统计描述，了解其分布特征：\n\n\n代码\n# 统计描述\ntrain_data.describe()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\ntrain\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n891.0\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n1.0\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n0.0\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n1.0\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n1.0\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n1.0\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n1.0\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n1.0\n\n\n\n\n\n\n\n检查各变量的缺失值情况：\n\n\n代码\n# 检查缺失值\nprint(\"缺失值统计:\")\nall_data.isnull().sum()\n\n\n缺失值统计:\n\n\nPassengerId       0\nSurvived        418\nPclass            0\nName              0\nSex               0\nAge             263\nSibSp             0\nParch             0\nTicket            0\nFare              1\nCabin          1014\nEmbarked          2\ntrain             0\ndtype: int64\n\n\n从上面的分析可以看出： - Age（年龄）有一部分缺失 - Cabin（船舱）缺失严重 - Embarked（登船港口）有少量缺失",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#生存情况分析",
    "href": "lab03_titanic.html#生存情况分析",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "10.2 生存情况分析",
    "text": "10.2 生存情况分析\n首先我们看一下整体的生存率：\n\n\n代码\n# 分析生存率\nsurvival_rate = train_data['Survived'].mean() * 100\nprint(f\"总体生存率: {survival_rate:.2f}%\")\n\n\n总体生存率: 38.38%",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据可视化分析",
    "href": "lab03_titanic.html#数据可视化分析",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "10.3 数据可视化分析",
    "text": "10.3 数据可视化分析\n通过可视化，我们可以更直观地了解不同特征与生存率之间的关系：\n\n\n代码\n# 设置图形大小\nplt.figure(figsize=(15, 12))\n\n# 1. 性别与生存率\nplt.subplot(2, 2, 1)\nsns.countplot(x='Sex', hue='Survived', data=train_data)\nplt.title('性别与生存状况')\nplt.xlabel('性别')\nplt.ylabel('人数')\n\n# 2. 船票等级与生存率\nplt.subplot(2, 2, 2)\nsns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.title('船票等级与生存状况')\nplt.xlabel('船票等级')\nplt.ylabel('人数')\n\n# 3. 年龄分布与生存情况\nplt.subplot(2, 2, 3)\nsns.histplot(data=train_data, x='Age', hue='Survived', multiple='stack', bins=20)\nplt.title('年龄分布与生存状况')\nplt.xlabel('年龄')\nplt.ylabel('人数')\n\n# 4. 家庭规模与生存情况\nplt.subplot(2, 2, 4)\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\nsns.countplot(x='FamilySize', hue='Survived', data=train_data)\nplt.title('家庭规模与生存状况')\nplt.xlabel('家庭规模')\nplt.ylabel('人数')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n从以上可视化分析可以看出：\n\n性别与生存率：女性的生存率明显高于男性，这可能与”妇女和儿童优先”的救生原则有关。\n船票等级与生存率：一等舱乘客生存率最高，三等舱乘客生存率最低，表明社会经济地位可能影响了获救机会。\n年龄分布与生存率：儿童的生存率相对较高，而中年人的生存率较低。\n家庭规模与生存率：家庭规模中等(2-4人)的乘客生存率较高，而独自一人或家庭规模过大的乘客生存率较低。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#特征工程",
    "href": "lab03_titanic.html#特征工程",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "11.1 特征工程",
    "text": "11.1 特征工程\n\n11.1.1 提取姓名中的头衔信息\n乘客的姓名中包含头衔信息（如Mr., Mrs., Miss等），这可能与社会地位和性别相关，进而影响生存率：\n\n\n代码\n# 提取姓名中的头衔\nall_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# 统计头衔分布\nprint(\"头衔分布:\")\nall_data['Title'].value_counts()\n\n\n头衔分布:\n\n\nTitle\nMr          757\nMiss        260\nMrs         197\nMaster       61\nRev           8\nDr            8\nCol           4\nMlle          2\nMajor         2\nMs            2\nLady          1\nSir           1\nMme           1\nDon           1\nCapt          1\nCountess      1\nJonkheer      1\nDona          1\nName: count, dtype: int64\n\n\n有些头衔出现次数很少，我们将它们合并为”Rare”类别：\n\n\n代码\n# 合并稀有头衔\nrare_titles = ['Capt', 'Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir', 'Countess']\nall_data['Title'] = all_data['Title'].replace(rare_titles, 'Rare')\nall_data['Title'] = all_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n\n# 合并后的头衔分布\nprint(\"合并后的头衔分布:\")\nall_data['Title'].value_counts()\n\n\n合并后的头衔分布:\n\n\nTitle\nMr        757\nMiss      264\nMrs       198\nMaster     61\nRare       29\nName: count, dtype: int64\n\n\n\n\n11.1.2 创建家庭规模特征\n我们将SibSp（兄弟姐妹/配偶数量）和Parch（父母/子女数量）相加，再加1（乘客自己），创建家庭规模特征：\n\n\n代码\n# 创建家庭规模特征\nall_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n\n# 创建是否独自一人特征\nall_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)\n\n# 查看是否独自一人的分布\nprint(\"是否独自一人的分布:\")\nall_data['IsAlone'].value_counts()\n\n\n是否独自一人的分布:\n\n\nIsAlone\n1    790\n0    519\nName: count, dtype: int64\n\n\n\n\n11.1.3 从Cabin提取信息\n虽然Cabin变量缺失严重，但我们仍可以提取是否有记录Cabin信息作为一个特征：\n\n\n代码\n# 从Cabin提取信息：是否有Cabin记录\nall_data['HasCabin'] = (~all_data['Cabin'].isnull()).astype(int)\n\nprint(\"Cabin记录情况分布:\")\nall_data['HasCabin'].value_counts()\n\n\nCabin记录情况分布:\n\n\nHasCabin\n0    1014\n1     295\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#缺失值处理",
    "href": "lab03_titanic.html#缺失值处理",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "11.2 缺失值处理",
    "text": "11.2 缺失值处理\n\n11.2.1 处理Age缺失值\n对年龄的缺失值，我们使用按Pclass和Sex分组的中位数进行填充：\n\n\n代码\n# Age缺失值填充（按Pclass和Sex分组的中位数）\nage_imputer = all_data.groupby(['Pclass', 'Sex'])['Age'].transform('median')\nall_data['Age'] = all_data['Age'].fillna(age_imputer)\n\n\n\n\n11.2.2 处理Embarked缺失值\n对登船港口缺失值，使用众数填充：\n\n\n代码\n# Embarked缺失值用众数填充\nmost_common_embarked = all_data['Embarked'].mode()[0]\nall_data['Embarked'] = all_data['Embarked'].fillna(most_common_embarked)\n\n\n\n\n11.2.3 处理Fare缺失值\n对票价缺失值，使用相同船票等级的中位数填充：\n\n\n代码\n# Fare缺失值用Pclass中位数填充\nfare_imputer = all_data.groupby('Pclass')['Fare'].transform('median')\nall_data['Fare'] = all_data['Fare'].fillna(fare_imputer)\n\n# 检查缺失值是否都已处理\nprint(\"缺失值处理后的统计:\")\nall_data[['Age', 'Embarked', 'Fare']].isnull().sum()\n\n\n缺失值处理后的统计:\n\n\nAge         0\nEmbarked    0\nFare        0\ndtype: int64",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#数据转换",
    "href": "lab03_titanic.html#数据转换",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "11.3 数据转换",
    "text": "11.3 数据转换\n将分类变量转换为数值型变量，便于模型处理：\n\n\n代码\n# 类别特征转换为数值\nall_data['Sex'] = all_data['Sex'].map({'male': 0, 'female': 1})\nall_data['Embarked'] = all_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\nall_data['Title'] = all_data['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\n\n\n删除不需要的特征：\n\n\n代码\n# 删除不需要的特征\nall_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n\n# 查看处理后的数据结构\nall_data.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\ntrain\nTitle\nFamilySize\nIsAlone\nHasCabin\n\n\n\n\n0\n0.0\n3\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n2\n0\n0\n\n\n1\n1.0\n1\n1\n38.0\n1\n0\n71.2833\n1\n1\n2\n2\n0\n1\n\n\n2\n1.0\n3\n1\n26.0\n0\n0\n7.9250\n0\n1\n1\n1\n1\n0\n\n\n3\n1.0\n1\n1\n35.0\n1\n0\n53.1000\n0\n1\n2\n2\n0\n1\n\n\n4\n0.0\n3\n0\n35.0\n0\n0\n8.0500\n0\n1\n0\n1\n1\n0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#准备训练和测试数据",
    "href": "lab03_titanic.html#准备训练和测试数据",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.1 准备训练和测试数据",
    "text": "12.1 准备训练和测试数据\n\n\n代码\n# 准备训练和测试数据\ntrain_data = all_data[all_data['train'] == 1].drop('train', axis=1)\ntest_data = all_data[all_data['train'] == 0].drop(['train', 'Survived'], axis=1)\n\nX_train = train_data.drop('Survived', axis=1)\ny_train = train_data['Survived'].astype(int)\n\nprint(f\"训练特征形状: {X_train.shape}\")\nprint(f\"训练标签形状: {y_train.shape}\")\nprint(f\"测试特征形状: {test_data.shape}\")\n\n\n训练特征形状: (891, 11)\n训练标签形状: (891,)\n测试特征形状: (418, 11)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#模型训练与评估函数",
    "href": "lab03_titanic.html#模型训练与评估函数",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.2 模型训练与评估函数",
    "text": "12.2 模型训练与评估函数\n定义一个通用函数用于训练模型并评估性能：\n\n\n代码\n# 模型训练与评估函数\ndef train_and_evaluate(model, X, y, cv=5, model_name=\"模型\"):\n    \"\"\"训练模型并评估性能\"\"\"\n    # 交叉验证评估\n    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n    precision = cross_val_score(model, X, y, cv=cv, scoring='precision')\n    recall = cross_val_score(model, X, y, cv=cv, scoring='recall')\n    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')\n    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n    \n    # 输出评估结果\n    print(f\"{model_name}交叉验证结果：\")\n    print(f\"准确率: {accuracy.mean():.4f} (±{accuracy.std():.4f})\")\n    print(f\"精确率: {precision.mean():.4f} (±{precision.std():.4f})\")\n    print(f\"召回率: {recall.mean():.4f} (±{recall.std():.4f})\")\n    print(f\"F1分数: {f1.mean():.4f} (±{f1.std():.4f})\")\n    print(f\"ROC AUC: {roc_auc.mean():.4f} (±{roc_auc.std():.4f})\")\n    \n    # 在全部训练数据上拟合模型\n    model.fit(X, y)\n    return model",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#逻辑回归模型",
    "href": "lab03_titanic.html#逻辑回归模型",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.3 逻辑回归模型",
    "text": "12.3 逻辑回归模型\n\n\n代码\n# 逻辑回归模型\nlogreg_model = LogisticRegression(max_iter=1000, random_state=42)\nlogreg_fitted = train_and_evaluate(logreg_model, X_train, y_train, model_name=\"逻辑回归\")\n\n# 查看特征系数\nlogreg_coef = pd.DataFrame(\n    logreg_fitted.coef_[0],\n    index=X_train.columns,\n    columns=['系数']\n).sort_values('系数', ascending=False)\n\nprint(\"\\n逻辑回归系数（特征重要性）:\")\nlogreg_coef\n\n\n逻辑回归交叉验证结果：\n准确率: 0.8159 (±0.0139)\n精确率: 0.7708 (±0.0385)\n召回率: 0.7454 (±0.0315)\nF1分数: 0.7566 (±0.0159)\nROC AUC: 0.8613 (±0.0194)\n\n逻辑回归系数（特征重要性）:\n\n\n\n\n\n\n\n\n\n系数\n\n\n\n\nSex\n2.149786\n\n\nHasCabin\n0.690087\n\n\nTitle\n0.489232\n\n\nEmbarked\n0.155761\n\n\nFare\n0.002529\n\n\nParch\n-0.025733\n\n\nAge\n-0.042430\n\n\nSibSp\n-0.268012\n\n\nFamilySize\n-0.282378\n\n\nIsAlone\n-0.461523\n\n\nPclass\n-0.827330",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#决策树模型",
    "href": "lab03_titanic.html#决策树模型",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.4 决策树模型",
    "text": "12.4 决策树模型\n\n\n代码\n# 决策树模型\n# 超参数网格搜索\nparam_grid = {\n    'max_depth': range(1, 20),\n    'min_samples_split': range(2, 20)\n}\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_grid = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\nprint(f\"决策树最佳参数: {dt_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {dt_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = range(1, 20)\ntest_scores = []\n\nfor depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=depth, \n                              min_samples_split=dt_grid.best_params_['min_samples_split'],\n                              random_state=42)\n    dt.fit(X_train, y_train)\n    # 使用交叉验证来评估\n    score = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('决策树性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\ndt_fitted = train_and_evaluate(dt_grid.best_estimator_, X_train, y_train, model_name=\"决策树\")\n\n# 特征重要性\ndt_importance = pd.DataFrame(\n    dt_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n决策树特征重要性:\")\ndt_importance\n\n\n决策树最佳参数: {'max_depth': 4, 'min_samples_split': 17}\n最佳交叉验证分数: 0.8260\n决策树交叉验证结果：\n准确率: 0.8260 (±0.0237)\n精确率: 0.8136 (±0.0331)\n召回率: 0.7159 (±0.1060)\nF1分数: 0.7549 (±0.0577)\nROC AUC: 0.8510 (±0.0399)\n\n决策树特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.661197\n\n\nFamilySize\n0.141799\n\n\nFare\n0.131015\n\n\nPclass\n0.041912\n\n\nAge\n0.012085\n\n\nHasCabin\n0.011992\n\n\nSex\n0.000000\n\n\nSibSp\n0.000000\n\n\nParch\n0.000000\n\n\nEmbarked\n0.000000\n\n\nIsAlone\n0.000000",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#随机森林模型",
    "href": "lab03_titanic.html#随机森林模型",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.5 随机森林模型",
    "text": "12.5 随机森林模型\n\n\n代码\n# 随机森林模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [10, 30, 50, 70, 100, 200],\n    'max_depth': [3, 5, 7, 10, 15, 20, None]\n}\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_grid = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nrf_grid.fit(X_train, y_train)\n\nprint(f\"随机森林最佳参数: {rf_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {rf_grid.best_score_:.4f}\")\n\n# 可视化n_estimators对模型性能的影响\nplt.figure(figsize=(10, 6))\nn_estimators = [10, 30, 50, 70, 100, 200]\ntest_scores = []\n\nfor n in n_estimators:\n    rf = RandomForestClassifier(n_estimators=n,\n                              max_depth=rf_grid.best_params_['max_depth'],\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(n_estimators, test_scores)\nplt.xlabel('n_estimators (树的数量)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随树数量的变化')\nplt.grid(True)\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 10, 15, 20]\ntest_scores = []\n\nfor depth in max_depths:\n    rf = RandomForestClassifier(n_estimators=rf_grid.best_params_['n_estimators'],\n                              max_depth=depth,\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nrf_fitted = train_and_evaluate(rf_grid.best_estimator_, X_train, y_train, model_name=\"随机森林\")\n\n# 特征重要性\nrf_importance = pd.DataFrame(\n    rf_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n随机森林特征重要性:\")\nrf_importance\n\n\n随机森林最佳参数: {'max_depth': 5, 'n_estimators': 10}\n最佳交叉验证分数: 0.8294\n随机森林交叉验证结果：\n准确率: 0.8294 (±0.0124)\n精确率: 0.7989 (±0.0240)\n召回率: 0.7453 (±0.0610)\nF1分数: 0.7690 (±0.0281)\nROC AUC: 0.8710 (±0.0378)\n\n随机森林特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.367692\n\n\nSex\n0.146653\n\n\nAge\n0.097934\n\n\nFare\n0.093234\n\n\nHasCabin\n0.089965\n\n\nPclass\n0.085205\n\n\nFamilySize\n0.052974\n\n\nSibSp\n0.032125\n\n\nEmbarked\n0.018703\n\n\nParch\n0.011941\n\n\nIsAlone\n0.003575",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#xgboost模型",
    "href": "lab03_titanic.html#xgboost模型",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.6 XGBoost模型",
    "text": "12.6 XGBoost模型\n\n\n代码\n# XGBoost模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, 9],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n}\n\nxgb_model = xgb.XGBClassifier(random_state=42)\nxgb_grid = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\n\nprint(f\"XGBoost最佳参数: {xgb_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {xgb_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 9, 11]\ntest_scores = []\n\nfor depth in max_depths:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=depth,\n        learning_rate=xgb_grid.best_params_['learning_rate'],\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随最大深度的变化')\nplt.grid(True)\n\n# 可视化learning_rate对模型性能的影响\nplt.figure(figsize=(10, 6))\nlearning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\ntest_scores = []\n\nfor lr in learning_rates:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=xgb_grid.best_params_['max_depth'],\n        learning_rate=lr,\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(learning_rates, test_scores)\nplt.xlabel('learning_rate (学习率)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随学习率的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nxgb_fitted = train_and_evaluate(xgb_grid.best_estimator_, X_train, y_train, model_name=\"XGBoost\")\n\n# 特征重要性\nxgb_importance = pd.DataFrame(\n    xgb_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\nXGBoost特征重要性:\")\nxgb_importance\n\n\nXGBoost最佳参数: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}\n最佳交叉验证分数: 0.8440\nXGBoost交叉验证结果：\n准确率: 0.8440 (±0.0229)\n精确率: 0.8236 (±0.0284)\n召回率: 0.7571 (±0.0667)\nF1分数: 0.7870 (±0.0394)\nROC AUC: 0.8686 (±0.0345)\n\nXGBoost特征重要性:\n\n\n\n\n\n\n\n\n\n重要性\n\n\n\n\nTitle\n0.545888\n\n\nPclass\n0.111701\n\n\nFamilySize\n0.095393\n\n\nHasCabin\n0.082403\n\n\nSex\n0.035206\n\n\nFare\n0.034001\n\n\nAge\n0.027042\n\n\nParch\n0.025975\n\n\nSibSp\n0.022535\n\n\nEmbarked\n0.019856\n\n\nIsAlone\n0.000000",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "lab03_titanic.html#神经网络模型",
    "href": "lab03_titanic.html#神经网络模型",
    "title": "8  泰坦尼克号生存预测实践",
    "section": "12.7 神经网络模型",
    "text": "12.7 神经网络模型\n在本节中，我们将从简单的全连接神经网络开始，逐步增加网络复杂度，并展示模型从欠拟合到过拟合的演变过程，最后通过引入正则化技术来解决过拟合问题。\n首先，我们需要对特征进行标准化，这对神经网络的训练非常重要：\n\n\n代码\n# 对特征进行标准化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ntest_data_scaled = scaler.transform(test_data)\n\n# 划分训练集和验证集\nX_train_nn, X_val, y_train_nn, y_val = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42\n)\n\nprint(f\"神经网络训练集形状: {X_train_nn.shape}\")\nprint(f\"神经网络验证集形状: {X_val.shape}\")\n\n\n神经网络训练集形状: (712, 11)\n神经网络验证集形状: (179, 11)\n\n\n\n12.7.1 简单全连接网络 - 可能欠拟合\n我们先从一个非常简单的全连接网络开始，看看其性能如何：\n\n\n代码\n# 定义简单的全连接网络\ndef create_simple_nn():\n    model = keras.Sequential([\n        layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练简单神经网络\nsimple_nn = create_simple_nn()\nsimple_nn.summary()  # 显示模型结构\n\n# 训练模型\nsimple_history = simple_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nsimple_train_loss, simple_train_acc = simple_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nsimple_val_loss, simple_val_acc = simple_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"简单网络 - 训练集准确率: {simple_train_acc:.4f}\")\nprint(f\"简单网络 - 验证集准确率: {simple_val_acc:.4f}\")\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 8)                 96        \n                                                                 \n dense_1 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 105 (420.00 Byte)\nTrainable params: 105 (420.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n简单网络 - 训练集准确率: 0.8287\n简单网络 - 验证集准确率: 0.7989\n\n\n绘制简单网络的学习曲线：\n\n\n代码\n# 绘制简单网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(simple_history.history['accuracy'])\nplt.plot(simple_history.history['val_accuracy'])\nplt.title('简单网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(simple_history.history['loss'])\nplt.plot(simple_history.history['val_loss'])\nplt.title('简单网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n12.7.2 中等复杂度网络 - 适度拟合\n现在，让我们增加网络的复杂度，添加更多层和更多神经元：\n\n\n代码\n# 定义中等复杂度的网络\ndef create_medium_nn():\n    model = keras.Sequential([\n        layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练中等复杂度神经网络\nmedium_nn = create_medium_nn()\nmedium_nn.summary()  # 显示模型结构\n\n# 训练模型\nmedium_history = medium_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nmedium_train_loss, medium_train_acc = medium_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nmedium_val_loss, medium_val_acc = medium_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"中等网络 - 训练集准确率: {medium_train_acc:.4f}\")\nprint(f\"中等网络 - 验证集准确率: {medium_val_acc:.4f}\")\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 16)                192       \n                                                                 \n dense_3 (Dense)             (None, 8)                 136       \n                                                                 \n dense_4 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 337 (1.32 KB)\nTrainable params: 337 (1.32 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n中等网络 - 训练集准确率: 0.8511\n中等网络 - 验证集准确率: 0.8045\n\n\n绘制中等复杂度网络的学习曲线：\n\n\n代码\n# 绘制中等网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(medium_history.history['accuracy'])\nplt.plot(medium_history.history['val_accuracy'])\nplt.title('中等网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(medium_history.history['loss'])\nplt.plot(medium_history.history['val_loss'])\nplt.title('中等网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n12.7.3 复杂网络 - 可能过拟合\n现在，我们进一步增加网络复杂度，使其具有更多层和更多神经元，观察是否会出现过拟合：\n\n\n代码\n# 定义复杂网络\ndef create_complex_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练复杂神经网络\ncomplex_nn = create_complex_nn()\ncomplex_nn.summary()  # 显示模型结构\n\n# 训练模型\ncomplex_history = complex_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,  # 增加训练轮次以观察过拟合\n    batch_size=16,  # 减小批量大小\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\ncomplex_train_loss, complex_train_acc = complex_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\ncomplex_val_loss, complex_val_acc = complex_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"复杂网络 - 训练集准确率: {complex_train_acc:.4f}\")\nprint(f\"复杂网络 - 验证集准确率: {complex_val_acc:.4f}\")\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 64)                768       \n                                                                 \n dense_6 (Dense)             (None, 32)                2080      \n                                                                 \n dense_7 (Dense)             (None, 16)                528       \n                                                                 \n dense_8 (Dense)             (None, 8)                 136       \n                                                                 \n dense_9 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 3521 (13.75 KB)\nTrainable params: 3521 (13.75 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n复杂网络 - 训练集准确率: 0.8975\n复杂网络 - 验证集准确率: 0.7989\n\n\n绘制复杂网络的学习曲线：\n\n\n代码\n# 绘制复杂网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(complex_history.history['accuracy'])\nplt.plot(complex_history.history['val_accuracy'])\nplt.title('复杂网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(complex_history.history['loss'])\nplt.plot(complex_history.history['val_loss'])\nplt.title('复杂网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n12.7.4 正则化网络 - 解决过拟合\n现在，我们将为复杂网络添加正则化技术，包括 Dropout 和 BatchNormalization，以解决过拟合问题：\n\n\n代码\n# 定义带有正则化的复杂网络\ndef create_regularized_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(32, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(16, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(8, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练带有正则化的神经网络\nregularized_nn = create_regularized_nn()\nregularized_nn.summary()  # 显示模型结构\n\n# 添加早停策略\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)\n\n# 训练模型\nregularized_history = regularized_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,\n    batch_size=16,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n# 评估性能\nregularized_train_loss, regularized_train_acc = regularized_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nregularized_val_loss, regularized_val_acc = regularized_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"正则化网络 - 训练集准确率: {regularized_train_acc:.4f}\")\nprint(f\"正则化网络 - 验证集准确率: {regularized_val_acc:.4f}\")\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_10 (Dense)            (None, 64)                768       \n                                                                 \n batch_normalization (Batch  (None, 64)                256       \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_11 (Dense)            (None, 32)                2080      \n                                                                 \n batch_normalization_1 (Bat  (None, 32)                128       \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_12 (Dense)            (None, 16)                528       \n                                                                 \n batch_normalization_2 (Bat  (None, 16)                64        \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_13 (Dense)            (None, 8)                 136       \n                                                                 \n batch_normalization_3 (Bat  (None, 8)                 32        \n chNormalization)                                                \n                                                                 \n dropout_3 (Dropout)         (None, 8)                 0         \n                                                                 \n dense_14 (Dense)            (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 4001 (15.63 KB)\nTrainable params: 3761 (14.69 KB)\nNon-trainable params: 240 (960.00 Byte)\n_________________________________________________________________\n正则化网络 - 训练集准确率: 0.8427\n正则化网络 - 验证集准确率: 0.8101\n\n\n绘制正则化网络的学习曲线：\n\n\n代码\n# 绘制正则化网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(regularized_history.history['accuracy'])\nplt.plot(regularized_history.history['val_accuracy'])\nplt.title('正则化网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(regularized_history.history['loss'])\nplt.plot(regularized_history.history['val_loss'])\nplt.title('正则化网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n12.7.5 模型比较\n我们来比较不同复杂度和正则化策略下神经网络的性能：\n\n\n代码\n# 比较不同网络的性能\nnn_models = ['简单网络', '中等网络', '复杂网络', '正则化网络']\ntrain_accuracy = [simple_train_acc, medium_train_acc, complex_train_acc, regularized_train_acc]\nval_accuracy = [simple_val_acc, medium_val_acc, complex_val_acc, regularized_val_acc]\n\nplt.figure(figsize=(12, 6))\nx = np.arange(len(nn_models))\nwidth = 0.35\n\nplt.bar(x - width/2, train_accuracy, width, label='训练集准确率')\nplt.bar(x + width/2, val_accuracy, width, label='验证集准确率')\n\nplt.ylabel('准确率')\nplt.title('不同神经网络模型的性能比较')\nplt.xticks(x, nn_models)\nplt.legend()\n\n# 显示差距\nfor i in range(len(nn_models)):\n    gap = train_accuracy[i] - val_accuracy[i]\n    plt.text(i, 0.5, f'差距: {gap:.4f}', ha='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n12.7.6 最终神经网络模型\n基于上述实验，我们选择性能最好的正则化网络作为最终的神经网络模型：\n\n\n代码\n# 使用正则化网络作为最终模型\nnn_model = regularized_nn\n\n# 在完整训练集上重新训练\nX_full_scaled = scaler.transform(X_train)\nnn_model.fit(X_full_scaled, y_train, epochs=50, batch_size=16, verbose=0)\n\n# 生成最终预测\nnn_pred = (nn_model.predict(X_full_scaled) &gt; 0.5).astype(int).flatten()\nnn_accuracy = accuracy_score(y_train, nn_pred)\nprint(f\"最终神经网络在训练集上的准确率: {nn_accuracy:.4f}\")\n\n# 将正则化网络用于测试集预测\ntest_pred_nn = (nn_model.predict(test_data_scaled) &gt; 0.5).astype(int).flatten()\n\n\n 1/28 [&gt;.............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/28 [==============================] - 0s 313us/step\n最终神经网络在训练集上的准确率: 0.8563\n 1/14 [=&gt;............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/14 [==============================] - 0s 323us/step\n\n\n\n\n12.7.7 神经网络模型小结\n通过这一系列实验，我们观察到：\n\n简单网络：容易欠拟合，训练集和验证集性能都不够理想\n中等网络：提高了模型复杂度，性能有所改善\n复杂网络：进一步增加复杂度，在训练集表现良好但可能在验证集上表现下降，出现过拟合\n正则化网络：通过添加Dropout和BatchNormalization等正则化技术，在保持模型复杂度的同时有效减轻了过拟合，使训练集和验证集的性能差距减小\n\n这个过程展示了神经网络建模中的一个关键问题：如何在模型复杂度和泛化能力之间取得平衡。正则化技术是解决这一问题的有效工具。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>泰坦尼克号生存预测实践</span>"
    ]
  },
  {
    "objectID": "06_credit.html",
    "href": "06_credit.html",
    "title": "9  信用评分理论基础",
    "section": "",
    "text": "9.1 信用评分理论基础",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "06_credit.html#信用评分理论基础",
    "href": "06_credit.html#信用评分理论基础",
    "title": "9  信用评分理论基础",
    "section": "",
    "text": "9.1.1 信用评分的概念与意义\n信用评分是一种利用统计模型和机器学习技术，对借款人或交易对手的信用风险进行量化评估的方法。它通过分析借款人的各种信息，如个人特征、财务状况、还款历史等，预测其未来违约的可能性，并以分数的形式呈现。\n信用评分的意义：\n\n风险管理： 帮助金融机构识别和评估信用风险，作为信贷决策的重要依据，降低坏账率。\n效率提升： 自动化审批流程，提高信贷审批效率，降低人工审核成本。\n差异化定价： 根据信用评分对客户进行风险分层，实现差异化定价，优化资源配置。\n金融普惠： 使得更多信用记录不足但信用良好的人群获得金融服务。\n\n\n\n9.1.2 信用风险度量\n信用风险是指借款人未能按时足额偿还债务的可能性。常用的信用风险度量指标包括：\n\n违约概率 (Probability of Default, PD)：指在一定时期内，借款人发生违约的可能性。信用评分模型的核心目标就是预测 PD。\n违约损失率 (Loss Given Default, LGD)：指发生违约时，债权人可能遭受的损失占总风险暴露的比例。\n违约风险暴露 (Exposure at Default, EAD)：指发生违约时，债权人面临的风险敞口总额。\n\n信用风险的期望损失 (Expected Loss, EL) 可以表示为：\n[ EL = PD LGD EAD ]\n信用评分模型主要关注违约概率 PD 的预测。\n\n\n9.1.3 评分卡模型原理\n评分卡模型是一种常用的信用评分工具，它将借款人的各种特征转化为分数，然后将这些分数加总得到一个总分，总分越高表示信用风险越低。\n评分卡模型的基本原理：\n\n特征选择与处理： 选择与信用风险相关的特征，并进行数据清洗、缺失值处理、异常值处理、特征转换等。\n模型选择与训练： 常用的模型包括逻辑回归、决策树、支持向量机、神经网络等。逻辑回归因其可解释性和稳定性在评分卡中应用最为广泛。\n评分转换： 将模型预测的违约概率转换为评分值。常用的转换方法包括对数 odds 转换等。\n模型验证与评估： 评估模型的区分能力、校准性、稳定性和业务有效性。常用的评估指标包括 AUC、KS 统计量、Lift 图、PSI 指数等。\n\n常用评估指标详解：\n\nAUC (Area Under the ROC Curve): 衡量模型区分好坏客户的能力。AUC 值越接近 1，表示模型区分能力越强。ROC 曲线是以假正例率 (FPR) 为横轴，真正例率 (TPR) 为纵轴绘制的曲线。\nKS (Kolmogorov-Smirnov) 统计量: 衡量模型区分度的指标，表示好坏样本累积分布差异的最大值。KS 值越大，表示模型区分能力越强，通常用于选择最佳阈值。\n提升图 (Lift Chart): 展示模型相比随机选择，在识别好/坏客户方面的提升效果。\nPSI (Population Stability Index): 衡量模型在不同时间段或不同数据集上评分分布的稳定性。PSI 过高可能表示模型需要重新训练或调整。\n\n\n\n\n\n9.1.4 经典信用评分理论\n经典的信用评分理论，例如 5C 信用评估体系，为我们推测可能影响信用评分的指标提供了理论基础。5C 体系从五个方面评估借款人的信用状况：\n\n品格 (Character)：借款人的还款意愿和信用历史。\n\n可能指标：\n\n历史还款记录： 是否有逾期记录、逾期次数、逾期时长等。\n稳定性： 居住时长、工作时长、工作稳定性等。\n社交媒体行为： （在某些情况下）社交媒体上的信用相关行为。\n\n\n偿还能力 (Capacity)：借款人的收入水平和偿债能力。\n\n可能指标：\n\n收入水平： 月收入、年收入、工资流水等。\n职业类型： 职业稳定性、行业前景等。\n资产状况： 房产、车辆、存款、投资等。\n负债水平： 负债收入比、信用卡负债、其他贷款等。\n\n\n资本 (Capital)：借款人的净资产和财务实力。\n\n可能指标：\n\n净资产： 总资产减去总负债。\n自有资金比例： 例如，在购房贷款中，首付比例。\n紧急备用金： 可随时动用的资金储备。\n\n\n抵押品 (Collateral)：借款人提供的抵押物或担保物。\n\n可能指标：\n\n抵押物类型： 房产、车辆、设备等。\n抵押物价值： 抵押物的评估价值。\n抵押率： 贷款金额与抵押物价值的比例。\n\n\n环境 (Condition)：外部经济环境和行业状况。\n\n可能指标：\n\n宏观经济指标： GDP 增长率、失业率、通货膨胀率、利率水平等。\n行业景气度： 借款人所在行业的整体发展状况。\n政策法规： 相关的信贷政策、监管法规等。\n\n\n\n\n\n9.1.5 FICO 评分\nFICO 评分是由美国 Fair Isaac Corporation (FICO) 公司开发的一种个人信用评分系统，是目前在美国最广泛应用的信用评分模型。它被广泛应用于信贷审批、贷款定价、风险管理等领域。\nFICO 评分范围：\nFICO 评分通常在 300 到 850 之间。分数越高，表示信用风险越低。\nFICO 评分的主要影响因素 (权重由高到低):\n\n还款历史 (Payment History) (约占 35%): 这是最重要的因素。包括：\n\n是否按时还款 (信用卡、贷款等)。\n逾期记录 (逾期次数、逾期时长、逾期金额)。\n不良记录 (例如，催收、破产等)。\n\n欠款金额 (Amounts Owed) (约占 30%): 包括：\n\n总欠款金额。\n已用信用额度比例 (Utilization Ratio)：已用信用额度 / 总信用额度。 比例越低越好。\n欠款账户数量。\n\n信用历史长度 (Length of Credit History) (约占 15%): 包括：\n\n最早信用账户的开户时间。\n平均信用账户的开户时间。\n信用历史越长，通常评分越高。\n\n新开立信用账户 (New Credit) (约占 10%): 包括：\n\n新开立信用账户的数量。\n短期内申请信用账户的频率。\n频繁申请新的信用账户可能会降低评分。\n\n信用类型 (Credit Mix) (约占 10%): 包括：\n\n拥有的不同类型的信用账户，例如：\n\n循环信用 (Revolving Credit)：信用卡、信用额度贷款等。\n分期付款信用 (Installment Credit)：房屋贷款、汽车贷款、个人贷款等。\n\n拥有多种类型的信用账户，并良好管理，通常对评分有正面影响。\n\n\nFICO 评分的意义：\n\n信贷决策： 银行和其他金融机构使用 FICO 评分来评估借款人的信用风险，决定是否批准贷款、信用卡申请，以及确定贷款利率和额度。\n消费者信用管理： 消费者可以通过了解 FICO 评分的影响因素，更好地管理自己的信用，提高信用评分，从而获得更好的金融服务。\n\n总结：\n信用评分理论基础是构建有效信用评分模型的基石。理解信用评分的概念、意义、风险度量方法和评分卡模型原理，并结合经典信用评估理论，可以帮助我们更好地选择和构建信用评分模型所需的特征指标，从而提升模型的预测能力和业务价值。\n\n\n9.1.6 信用评分面临的挑战与伦理考量\n尽管信用评分在金融领域发挥着重要作用，但也面临着一些挑战和伦理问题：\n\n数据稀疏性与冷启动问题 (Data Sparsity & Cold Start): 对于缺乏信用历史记录的“白户”或“薄档”人群，难以建立准确的评分模型。\n数据质量与准确性 (Data Quality & Accuracy): 评分模型的准确性高度依赖于输入数据的质量。错误或过时的数据可能导致评分偏差。\n模型偏见与公平性 (Model Bias & Fairness): 模型可能无意中学习到数据中存在的社会偏见（如地域、种族、性别歧视），导致对特定群体的评分不公平。确保算法公平性是一个重要的研究方向。\n模型可解释性与透明度 (Interpretability & Transparency): 复杂的机器学习模型（如深度学习）往往缺乏可解释性，使得用户和监管机构难以理解评分决策的原因，这与“负责任的人工智能”原则相悖。\n模型漂移与稳定性 (Model Drift & Stability): 随着时间的推移，经济环境和用户行为会发生变化，导致模型性能下降（模型漂移）。需要定期监控和更新模型。\n数据隐私与安全 (Data Privacy & Security): 信用评分涉及大量敏感个人信息，必须严格遵守相关法律法规（如 GDPR、个人信息保护法），确保数据安全和用户隐私。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "06_credit.html#互联网时代信用评分的新发展",
    "href": "06_credit.html#互联网时代信用评分的新发展",
    "title": "9  信用评分理论基础",
    "section": "9.2 互联网时代信用评分的新发展",
    "text": "9.2 互联网时代信用评分的新发展\n随着互联网技术的发展和普及，越来越多的互联网企业开始构建自己的信用评分模型，以服务于其业务场景，例如消费金融、电商信贷、共享经济等。这些模型通常具有以下特点：\n\n数据来源多样化： 除了传统的金融数据外，还包括大量的互联网行为数据，例如：\n\n电商数据： 购物历史、消费偏好、收货地址、退货记录等。\n社交数据： 社交关系、社交行为、社交媒体信息等（需注意隐私合规）。\n行为数据： App 使用行为、网站浏览行为、地理位置信息等。\n运营商数据： 通话记录、短信记录、流量使用情况等（需注意隐私合规）。\n\n模型算法更复杂： 更多地采用机器学习和人工智能技术，例如：\n\n集成学习模型： GBDT、XGBoost、LightGBM 等。\n深度学习模型： 神经网络、循环神经网络等。\n图神经网络： 用于分析社交网络和关系数据。\n\n评分维度更丰富： 除了传统的信用风险评估，还可能包括：\n\n消费能力评估： 预测用户的消费潜力，用于授信额度确定。\n欺诈风险评估： 识别欺诈行为，保障交易安全。\n用户质量评估： 评估用户的活跃度、忠诚度、价值贡献等。\n\n\n一些典型的互联网企业信用评分模型案例：\n\n蚂蚁金服 - 芝麻信用: 中国最早、也是最知名的互联网信用评分体系之一。\n\n数据来源： 支付宝、淘宝、天猫等阿里系电商和金融平台数据，以及合作机构数据。\n评分维度： 身份特质、行为偏好、履约能力、信用历史、人脉关系五个维度。\n应用场景： 消费金融（花呗、借呗）、免押服务、信用租房、信用出行等。\n\n腾讯 - 腾讯信用分 (已停止更新): 腾讯也曾推出过信用评分产品，但目前已停止更新。\n\n数据来源： 微信、QQ 等社交平台数据，以及腾讯支付、游戏等业务数据。\n评分维度： 财富、安全、守约、消费、社交五个维度。\n应用场景： 微信支付分、部分消费金融场景。\n\n京东 - 京东小白信用: 京东推出的信用评分产品。\n\n数据来源： 京东电商平台数据、京东金融数据等。\n评分维度： 身份、资产、偏好、履约能力、关系五个维度。\n应用场景： 京东白条、购物优惠、会员权益等。\n\nFoursquare - Pilgrim SDK: 美国地理位置社交网络 Foursquare 提供的 Pilgrim SDK，可以基于用户的位置数据进行风险评估。\n\n数据来源： 用户的位置轨迹、签到数据等。\n评分维度： 用户行为的真实性、稳定性、风险偏好等。\n应用场景： 反欺诈、风险控制、个性化推荐等。\n\n\n总结：\n互联网企业信用评分模型是传统信用评分在互联网时代的创新和发展。它们利用更丰富的数据来源、更复杂的算法模型和更多样的评分维度，为互联网业务场景提供了更精准、更全面的信用风险评估和用户画像能力。但同时也面临着数据隐私、算法公平性、模型可解释性等新的挑战。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>信用评分理论基础</span>"
    ]
  },
  {
    "objectID": "07_unsupervised.html",
    "href": "07_unsupervised.html",
    "title": "10  非监督学习技术概览及其金融应用",
    "section": "",
    "text": "10.1 非监督学习简介\n非监督学习 (Unsupervised Learning) 是机器学习的一个分支，其主要特点是训练数据没有预先标记的输出标签。与监督学习不同，非监督学习的目标是从数据本身发现隐藏的结构、模式或关系。它试图理解数据的内在分布和特征，而不是预测一个特定的目标变量。\n非监督学习在探索性数据分析、数据预处理和发现未知模式方面非常有用。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>非监督学习技术概览及其金融应用</span>"
    ]
  },
  {
    "objectID": "07_unsupervised.html#主要非监督学习技术",
    "href": "07_unsupervised.html#主要非监督学习技术",
    "title": "10  非监督学习技术概览及其金融应用",
    "section": "10.2 主要非监督学习技术",
    "text": "10.2 主要非监督学习技术\n以下是一些主要的非监督学习技术及其在金融领域的应用：\n\n10.2.1 1. 聚类 (Clustering)\n聚类是将数据集中的样本划分为若干个相似组（簇）的过程，使得同一簇内的样本彼此相似，而不同簇之间的样本差异较大。\n常用算法：\n\nK-Means (K均值聚类): 基于距离的划分聚类算法，简单高效，适用于大数据集。需要预先指定簇的数量 K。\n层次聚类 (Hierarchical Clustering): 通过构建样本间的层次结构来进行聚类，可以是凝聚式（自底向上）或分裂式（自顶向下）。不需要预先指定簇数，但计算复杂度较高。\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): 基于密度的聚类算法，可以发现任意形状的簇，并能有效处理噪声点。不需要预先指定簇数。\n高斯混合模型 (Gaussian Mixture Model, GMM): 基于概率模型的聚类方法，假设数据由多个高斯分布混合而成。\n\n金融应用场景：\n\n客户细分 (Customer Segmentation): 根据客户的交易行为、人口统计特征、风险偏好等将客户分组，以便进行精准营销、个性化推荐和风险管理。\n异常检测 (Anomaly Detection): 识别与正常模式显著不同的交易或行为，用于欺诈检测、洗钱识别等。\n投资组合构建 (Portfolio Construction): 将具有相似风险收益特征的资产聚类，辅助构建多元化的投资组合。\n\n\n\n10.2.2 2. 降维 (Dimensionality Reduction)\n降维是指在保留数据主要信息的前提下，减少数据特征数量的过程。高维数据往往存在冗余信息、计算复杂度高、可视化困难等问题（“维度灾难”）。\n常用算法：\n\n主成分分析 (Principal Component Analysis, PCA): 最常用的线性降维方法，通过寻找数据方差最大的方向（主成分）来投影数据，实现降维。\n线性判别分析 (Linear Discriminant Analysis, LDA): 一种有监督的降维方法（但也常与非监督场景对比），旨在最大化类间距离和最小化类内距离，常用于分类任务的预处理。(注意：严格来说 LDA 是有监督的，但常与 PCA 一起讨论)\nt-分布随机邻域嵌入 (t-Distributed Stochastic Neighbor Embedding, t-SNE): 非线性降维方法，特别擅长高维数据的可视化，能较好地保留数据的局部结构。\n自编码器 (Autoencoders): 基于神经网络的非线性降维方法，通过编码器将数据压缩到低维表示，再通过解码器重构原始数据。\n\n金融应用场景：\n\n风险因子识别 (Risk Factor Identification): 从大量市场指标中提取主要的风险因子，简化风险模型。\n数据可视化 (Data Visualization): 将高维金融数据（如客户特征、资产特征）降维到二维或三维空间进行可视化，便于理解和分析。\n特征工程 (Feature Engineering): 减少模型输入的特征数量，提高模型训练效率和泛化能力，避免过拟合。\n资产定价 (Asset Pricing): 减少影响资产价格的因素数量，构建更简洁的定价模型。\n\n\n\n10.2.3 3. 文本分析 (Text Analysis) / 自然语言处理 (NLP) - 非监督部分\n虽然 NLP 包含监督和非监督方法，但许多非监督技术在处理金融文本数据中至关重要。\n常用非监督算法/技术：\n\n词嵌入 (Word Embeddings): 如 Word2Vec, GloVe, FastText。将词语表示为低维稠密向量，捕捉词语间的语义关系。\n主题建模 (Topic Modeling): 如潜在狄利克雷分配 (Latent Dirichlet Allocation, LDA - 注意与线性判别分析区分)。从大量文档中自动发现隐藏的主题结构。\n文本聚类 (Text Clustering): 将相似的文本（如新闻、报告、评论）分组。\n\n金融应用场景：\n\n舆情分析 (Sentiment Analysis): 分析新闻、社交媒体、研报等文本数据中的市场情绪或对特定公司/资产的情感倾向。（通常需要结合监督学习进行情感分类，但词嵌入和主题模型是基础）\n信息提取 (Information Extraction): 从财报、公告、合同等非结构化文本中自动提取关键信息（如公司名称、财务数据、关键条款）。\n文档摘要与分类 (Document Summarization & Classification): 自动生成研报摘要，或将大量金融文档按主题分类。\n风险信号挖掘 (Risk Signal Mining): 从新闻或监管文件中识别潜在的风险事件或趋势。\n\n非监督学习为理解和利用没有明确标签的大量金融数据提供了强大的工具集，有助于发现隐藏的模式、简化复杂性并做出更明智的决策。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>非监督学习技术概览及其金融应用</span>"
    ]
  },
  {
    "objectID": "07_cluster.html",
    "href": "07_cluster.html",
    "title": "11  非监督学习：聚类 (Clustering)",
    "section": "",
    "text": "11.1 核心概念\n聚类是一种非监督学习技术，其目标是将数据集中的样本根据它们的相似性划分为若干个组，这些组被称为”簇” (Cluster)。聚类的核心思想是：\n在金融领域，我们可以对各种对象进行聚类，例如：",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "07_cluster.html#核心概念",
    "href": "07_cluster.html#核心概念",
    "title": "11  非监督学习：聚类 (Clustering)",
    "section": "",
    "text": "簇内相似性最大化 (Maximize Intra-cluster Similarity): 同一个簇内的数据点（样本）应该尽可能相似。\n簇间差异性最大化 (Maximize Inter-cluster Dissimilarity): 不同簇之间的数据点应该尽可能不相似。\n\n\n\n资产 (Assets): 如股票、债券，根据它们的风险收益特征、价格波动模式等进行聚类。\n客户 (Customers): 根据客户的交易行为、风险偏好、人口统计学特征等进行聚类。\n市场时期 (Market Regimes): 根据市场波动性、相关性等指标对不同的市场阶段进行聚类。\n\n\n11.1.1 分类 vs 聚类\n分类和聚类是机器学习中两种不同的任务，它们的关键区别在于是否有标签数据（有监督 vs 无监督）。以下通过实例来展示这两者的区别：\n# 加载鸢尾花数据集\ndata = load_iris()\nX = data.data\ny = data.target\n\nplt.figure(figsize=(9, 3.5))\n\n# 左侧：带有类别标签的数据（分类问题）\nplt.subplot(121)\nplt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\nplt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\nplt.xlabel(\"花瓣长度\")\nplt.ylabel(\"花瓣宽度\")\nplt.grid()\nplt.legend()\n\n# 右侧：不带标签的数据（聚类问题）\nplt.subplot(122)\nplt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\nplt.xlabel(\"花瓣长度\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n关键区别：\n\n分类（有监督学习）:\n\n有预先定义的类别标签\n目标是学习将新样本分配到已知类别的规则\n评估基于预测标签与真实标签的比较\n\n聚类（无监督学习）:\n\n没有预先定义的类别标签\n目标是发现数据中的自然分组\n评估基于分组的内部结构特性（如组内紧密度、组间分离度）",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "07_cluster.html#k均值聚类-k-means",
    "href": "07_cluster.html#k均值聚类-k-means",
    "title": "11  非监督学习：聚类 (Clustering)",
    "section": "11.2 K均值聚类 (K-Means)",
    "text": "11.2 K均值聚类 (K-Means)\n\n11.2.1 算法原理\nK-Means 是一种迭代算法，旨在将数据划分为预先指定的 K 个簇。它通过不断更新簇的中心点（质心）并将每个数据点分配给最近的质心来实现。其算法步骤如下：\n\n随机选择 K 个初始质心。\n分配步骤: 将每个数据点分配给距离其最近的质心，形成 K 个簇。\n更新步骤: 重新计算每个簇的质心（通常是簇内所有点的均值）。\n重复步骤 2 和 3，直到质心不再发生显著变化或达到最大迭代次数。\n\n\n\n11.2.2 算法优缺点\n优点:\n\n算法简单，容易理解和实现。\n计算效率高，处理大数据集速度较快。\n\n缺点:\n\n需要预先指定簇的数量 K，而 K 值的选择往往比较困难。\n对初始质心的选择敏感，可能陷入局部最优解。\n对非球状簇、不同大小和密度的簇效果不佳。\n对异常值（Outliers）比较敏感。\n\n\n\n11.2.3 K-Means实践\n让我们通过一个实例来演示K-Means聚类的应用：\n# 生成模拟数据\nblob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n                        [-2.8,  2.8], [-2.8,  1.3]])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nX, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\n\n# 可视化数据\nplt.figure(figsize=(8, 4))\nplt.scatter(X[:, 0], X[:, 1], c='k', s=1)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\nplt.show()\n\n# 使用K-Means聚类\nk = 5\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\nprint(\"预测的簇标签（前10个）:\", y_pred[:10])\nprint(\"簇中心:\", kmeans.cluster_centers_)\n\n\n11.2.4 K-Means的决策边界\nK-Means的决策边界形成了沃罗诺伊图(Voronoi diagram)，即每个区域内的点到对应质心的距离比到其他质心的距离更近。\n# 绘制决策边界（沃罗诺伊图）\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans, X)\nplt.show()\n\n\n11.2.5 K-Means的迭代过程\n下面演示K-Means算法的迭代过程：\n# K均值算法的步骤演示\nkmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n                    random_state=5)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n                    random_state=5)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=3,\n                    random_state=5)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(321)\nplot_data(X)\nplot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.tick_params(labelbottom=False)\nplt.title(\"更新质心（初始随机）\")\n\nplt.subplot(322)\nplot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n                        show_ylabels=False)\nplt.title(\"标记实例\")\n\n# 更多迭代图示...\nplt.show()\n\n\n11.2.6 初始化敏感性问题\nK-Means对初始质心的选择十分敏感，不同的初始点可能导致完全不同的聚类结果：\n# K均值的变异性问题\nkmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\nkmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n\nplot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n                        \"解决方案 1\",\n                        \"解决方案 2（使用不同的随机初始化）\")\n\n# 查看惯性值\nprint(\"解决方案1的惯性:\", kmeans_rnd_init1.inertia_)\nprint(\"解决方案2的惯性:\", kmeans_rnd_init2.inertia_)\n解决方案：使用多次不同的初始化，选择惯性（簇内平方和）最小的结果。scikit-learn中的K-Means默认进行10次不同的初始化。\n\n\n11.2.7 聚类评估方法\n如何评估聚类效果的好坏，以及如何选择合适的参数（如K-Means中的K值）？以下介绍几种常用的评估方法：\n\n11.2.7.1 肘部法则 (Elbow Method)\n原理：\n\n对不同的K值运行K-Means算法，计算每个K值对应的惯性（簇内平方和，WCSS）。\n随着K值的增加，惯性总体呈下降趋势，但下降速率会逐渐变缓。\n寻找图中的”肘部”，即曲线下降速率发生明显变化的点。该点对应的K值被视为较佳选择。\n\n注意事项：\n\n肘部法则是一种视觉方法，有时肘部可能不够明显。\n需要结合业务理解和其他评估指标来确定最终的K值。\n该方法主要适用于球形簇，对于复杂形状的簇可能效果不佳。\n\n肘部法则是一种启发式方法，用于确定K-Means中的最佳聚类数量K：\n# 肘部法则\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"惯性\")\nplt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n            arrowprops=dict(facecolor='black', shrink=0.1))\nplt.text(4.5, 650, \"肘部\", horizontalalignment=\"center\")\nplt.axis([1, 8.5, 0, 1300])\nplt.grid()\nplt.show()\n\n\n11.2.7.2 轮廓系数 (Silhouette Coefficient)\n轮廓系数是评估聚类质量的一种方法，它综合考虑了簇内的紧密度和簇间的分离度。\n计算方法:\n\n对于数据集中的每个样本点 i：\n\n计算 a(i): 点 i 与同簇中其他所有点的平均距离（衡量簇内紧密度）。\n计算 b(i): 点 i 与距离最近的其他簇中所有点的平均距离（衡量簇间分离度）。\n轮廓系数 s(i) = (b(i) - a(i)) / max(a(i), b(i))。\n\n数据集的整体轮廓系数是所有样本点轮廓系数的平均值。\n\n轮廓系数解读:\n\n值域范围：[-1, 1]\n接近 +1：表示样本与自己的簇很匹配，与相邻簇的差距很大，聚类效果好。\n接近 0：表示样本处于两个簇的边界附近。\n接近 -1：表示样本可能被分配到了错误的簇。\n通常认为，平均轮廓系数大于 0.5 是较好的聚类结果，大于 0.7 是优秀的聚类结果。\n\n下面是使用轮廓系数选择最佳K值的例子：\n# 轮廓系数\nsilhouette_scores = [silhouette_score(X, model.labels_)\n                    for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"轮廓分数\")\nplt.axis([1.8, 8.5, 0.55, 0.7])\nplt.grid()\nplt.show()\n更详细的轮廓分析可以通过轮廓图(Silhouette Plot)进行：\n# 轮廓图分析\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"簇\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"轮廓系数\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n\nplt.show()\n在轮廓图中，每个簇都有对应的条形图，条形图的长度表示该簇内每个样本的轮廓系数。较宽且一致的条形图意味着聚类质量较好。\n\n\n11.2.7.3 戴维斯-布尔丁指数 (Davies-Bouldin Index)\n戴维斯-布尔丁指数是另一种评估聚类质量的度量，它关注簇内相似度和簇间差异性的比率：\nfrom sklearn.metrics import davies_bouldin_score\n\ndbi = davies_bouldin_score(X, labels)\nprint(f\"Davies-Bouldin Index: {dbi:.3f}\")\n解读：\n\n值越小越好，表示簇内紧密且簇间分离。\n理想的聚类会产生低的DBI值。\n\n通过综合使用上述评估方法，我们可以更客观、定量地判断聚类算法的效果，选择合适的参数，从而获得更有意义的聚类结果。\n\n\n\n11.2.8 K-Means的局限性\nK-Means对复杂形状的簇表现不佳：\n# 生成一个更难聚类的数据集\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX_difficult = np.r_[X1, X2]\n\n# 两种不同初始化方式的K-means\nkmeans_good = KMeans(n_clusters=3,\n                    init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n                    n_init=1, random_state=42)\nkmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmeans_good.fit(X_difficult)\nkmeans_bad.fit(X_difficult)\n\n# 比较结果\nplt.figure(figsize=(10, 3.2))\nplt.subplot(121)\nplot_decision_boundaries(kmeans_good, X_difficult)\nplt.title(f\"惯性 = {kmeans_good.inertia_:.1f}\")\nplt.subplot(122)\nplot_decision_boundaries(kmeans_bad, X_difficult, show_ylabels=False)\nplt.title(f\"惯性 = {kmeans_bad.inertia_:.1f}\")\nplt.show()\n\n\n11.2.9 DBSCAN聚类\n\n11.2.9.1 算法原理\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一种基于密度的聚类算法，它将密度相连的点划分为一个簇，并且能够发现任意形状的簇，同时自动识别噪声点。\n核心概念:\n\nε (Epsilon): 邻域半径，定义点之间的”近”的概念。\nMinPts: 邻域内最少点数，用于判断密度。\n核心点 (Core Point): 在其 ε 邻域内至少有 MinPts 个点的点。\n边界点 (Border Point): 不是核心点，但在某个核心点的 ε 邻域内的点。\n噪声点 (Noise Point): 既不是核心点也不是边界点的点。\n\n算法步骤:\n\n任选一个未被访问的点 p。\n标记 p 为已访问。\n如果 p 是核心点，创建一个新簇，并将 p 的所有密度可达点加入该簇。\n如果 p 不是核心点，标记为噪声点并继续。\n重复以上步骤，直到所有点都被访问。\n\n\n\n\n11.2.10 算法优缺点\n优点:\n\n不需要预先指定簇的数量。\n能发现任意形状的簇，不限于球形簇。\n能自动识别和处理噪声点。\n对离群点不敏感。\n\n缺点:\n\n参数选择（ε 和 MinPts）有时较为困难。\n对数据集中密度差异较大的簇效果不佳。\n计算复杂度较高（约为 O(n²)），但通常可以通过空间索引优化。\n不能处理高维空间中的”维度灾难”问题。\n\n\n\n11.2.11 DBSCAN实践\n我们通过一个新月形数据集来演示DBSCAN的效果，这种形状对K-Means来说是很难处理的：\n# 创建一个新月形数据集\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\n# 应用DBSCAN\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\n# 查看结果\nprint(\"标签（前10个）:\", dbscan.labels_[:10])\nprint(\"核心样本索引（前10个）:\", dbscan.core_sample_indices_[:10])\n\n# 可视化DBSCAN结果\nplt.figure(figsize=(6, 3))\nplot_dbscan(dbscan, X, size=100)\nplt.show()\n\n\n11.2.12 参数敏感性\nDBSCAN的关键参数是epsilon (ε)和min_samples。以下展示不同epsilon值对聚类结果的影响：\n# 试两个不同的epsilon值\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nplt.show()\n当ε值增大时，更多的点被连接成簇，噪声点减少。但ε值过大也可能导致不同本应分开的簇被合并。\n\n\n11.2.13 参数选择方法\n为DBSCAN选择合适的参数是一个挑战。一种常用的方法是K-距离图（K-distance plot）：\n# 选择 MinPts 值，例如 4\nMinPts = 4\n\n# 找到每个点的 k 个最近邻\nnbrs = NearestNeighbors(n_neighbors=MinPts).fit(X)\ndistances, indices = nbrs.kneighbors(X)\n\n# 距离排序\ndistances = np.sort(distances[:, MinPts-1])\n\n# 绘制 K-距离图\nplt.figure(figsize=(10, 6))\nplt.plot(range(len(distances)), distances)\nplt.title('K-Distance Plot (k={})'.format(MinPts))\nplt.xlabel('Points sorted by distance')\nplt.ylabel('Distance to k-th nearest neighbor')\nplt.grid(True)\nplt.show()\n在K-距离图中，我们寻找曲线的”拐点”，该点对应的距离值可作为ε的值。拐点表示密度发生显著变化的地方。\n\n\n11.2.14 基于DBSCAN的分类\nDBSCAN可以与监督学习方法结合，用于半监督学习：\n# 基于DBSCAN的分类\n# 使用核心点构建KNN分类器\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan2.components_, dbscan2.labels_[dbscan2.core_sample_indices_])\n\n# 预测新点的类别\nX_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\nprint(\"预测标签:\", knn.predict(X_new))\nprint(\"预测概率:\", knn.predict_proba(X_new).round(2))\n\n# 可视化分类边界\nplt.figure(figsize=(6, 3))\nplot_decision_boundaries(knn, X, show_centroids=False)\nplt.scatter(X_new[:, 0], X_new[:, 1], c=\"b\", marker=\"+\", s=200, zorder=10)\nplt.show()\n在这个例子中，我们首先使用DBSCAN找到核心点和它们的簇标签，然后用这些信息训练KNN分类器。这种方法结合了DBSCAN处理复杂形状的能力和KNN的分类能力。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "07_cluster.html#聚类应用实例",
    "href": "07_cluster.html#聚类应用实例",
    "title": "11  非监督学习：聚类 (Clustering)",
    "section": "11.3 聚类应用实例",
    "text": "11.3 聚类应用实例\n\n11.3.1 图像分割应用\nK-Means可用于图像分割，将图像像素分为几个颜色簇，实现颜色量化：\n# 读取图像并应用K-means\nimage = np.asarray(PIL.Image.open(filename))\nX_img = image.reshape(-1, 3)\nsegmented_imgs = []\nn_colors = (10, 8, 6, 4, 2)\n\nfor n_clusters in n_colors:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X_img)\n    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_imgs.append(segmented_img.reshape(image.shape))\n\n# 显示结果\nplt.figure(figsize=(10, 5))\nplt.subplot(2, 3, 1)\nplt.imshow(image)\nplt.title(\"原始图像\")\nplt.axis('off')\n\nfor idx, n_clusters in enumerate(n_colors):\n    plt.subplot(2, 3, 2 + idx)\n    plt.imshow(segmented_imgs[idx] / 255)\n    plt.title(f\"{n_clusters} 种颜色\")\n    plt.axis('off')\n\nplt.show()\n\n\n11.3.2 半监督学习：使用聚类辅助分类\n聚类可以在标记数据稀少的情况下帮助分类任务：\n# 加载digits数据集\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test = X_digits[:1400], X_digits[1400:]\ny_train, y_test = y_digits[:1400], y_digits[1400:]\n\n# 仅使用少量标记数据训练\nn_labeled = 50\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nprint(f\"使用{n_labeled}个标记样本的准确率:\", log_reg.score(X_test, y_test))\n\n# 使用K-means找到代表性数字\nk = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = X_digits_dist.argmin(axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\n# 绘制代表性数字\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n            interpolation=\"bilinear\")\n    plt.axis('off')\nplt.show()\n\n# 手动标记这些代表性数字\ny_representative_digits = np.array([\n    1, 3, 6, 0, 7, 9, 2, 4, 8, 9,\n    5, 4, 7, 1, 2, 6, 1, 2, 5, 1,\n    4, 1, 3, 3, 8, 8, 2, 5, 6, 9,\n    1, 4, 0, 6, 8, 3, 4, 6, 7, 2,\n    4, 1, 0, 7, 5, 1, 9, 9, 3, 7\n])\n\n# 使用代表性数字训练模型\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nprint(\"使用代表性数字的准确率:\", log_reg.score(X_test, y_test))\n\n# 将标签传播到同一簇中的所有实例\ny_train_propagated = np.empty(len(X_train), dtype=np.int64)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n\n# 使用扩展的训练集\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train, y_train_propagated)\nprint(\"使用标签传播后的准确率:\", log_reg.score(X_test, y_test))\n在这个例子中，我们使用K-Means找到代表性样本，只对这些样本手动标记，然后将标签传播到同一簇的所有样本。这种方法可以大大减少标记工作量。\n\n\n11.3.3 金融市场应用：股票板块轮动分析\n聚类技术可以用于识别股票市场中的板块轮动现象：\n目标: 利用聚类技术识别股票市场中可能存在的板块轮动现象或隐藏的股票群体特征。\n步骤:\n\n数据收集:\n\n选择一个股票池，例如上证A股。\n收集这些股票在过去一段时间（如一年）的每日收益率数据。\n\n数据预处理:\n\n处理缺失值。\n对收益率数据进行标准化，以消除量纲影响。\n\n聚类分析:\n\n使用K-Means对股票进行聚类，输入数据为每只股票的日收益率序列。\n使用肘部法则或轮廓系数选择最优的K值。\n\n结果解读与分析:\n\n簇成员分析: 查看每个簇中包含哪些股票，是否对应传统行业板块或揭示跨行业的投资因子。\n簇表现分析: 计算每个簇在不同时间段的表现，识别强势/弱势群体，寻找板块轮动证据。\n\n\n意义: 通过聚类，可以超越传统的行业划分，从数据驱动的角度发现股票之间更深层次的关联性，为投资组合构建和择时策略提供新的视角。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "07_cluster.html#总结",
    "href": "07_cluster.html#总结",
    "title": "11  非监督学习：聚类 (Clustering)",
    "section": "11.4 总结",
    "text": "11.4 总结\n聚类是一种强大的无监督学习技术，可以在没有标签数据的情况下发现数据中的自然分组。本讲义介绍了两种主要的聚类算法：K-Means和DBSCAN，它们各有优缺点：\n\nK-Means: 简单高效，但需要预先指定簇数量，对簇形状有假设，对异常值敏感。\nDBSCAN: 能处理任意形状的簇，自动识别噪声点，不需要预先指定簇数量，但参数选择较为困难。\n\n聚类算法的评估需要使用特定的指标，如轮廓系数、肘部法则等，而不是传统的分类准确率。\n聚类在图像处理、客户分群、异常检测、半监督学习等领域有广泛的应用。在金融领域，聚类可以用于资产分组、市场状态识别、风险管理等多个方面。\n通过本讲义的学习，读者应该能够理解聚类的基本原理，掌握K-Means和DBSCAN算法的使用方法，学会如何评估聚类结果，并能将聚类技术应用到实际问题中。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>非监督学习：聚类 (Clustering)</span>"
    ]
  },
  {
    "objectID": "09_dimension.html",
    "href": "09_dimension.html",
    "title": "12  非监督学习：降维 (Dimensionality Reduction)",
    "section": "",
    "text": "12.1 降维技术\n在处理现实世界的数据时，我们经常会遇到特征维度非常高的情况（即有很多列）。高维度数据不仅会增加模型的计算复杂度、延长训练时间，还可能引入噪声、导致维度灾难 (Curse of Dimensionality)，使得模型难以学习有效的模式，甚至降低性能。本讲我们将专注于降维 (Dimensionality Reduction)技术，尤其是主成分分析 (PCA)为代表的线性降维方法，同时也会简要介绍一些非线性降维方法。这些技术能帮助我们简化数据、去除冗余、提高模型效率和性能。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>非监督学习：降维 (Dimensionality Reduction)</span>"
    ]
  },
  {
    "objectID": "09_dimension.html#降维技术",
    "href": "09_dimension.html#降维技术",
    "title": "12  非监督学习：降维 (Dimensionality Reduction)",
    "section": "",
    "text": "12.1.1 为什么需要降维？\n\n降低计算复杂度: 特征越少，模型训练和预测所需的时间和内存就越少。\n缓解维度灾难: 在高维空间中，数据点变得稀疏，距离度量失去意义，模型更难找到有效的模式。\n去除冗余和噪声: 并非所有特征都是有用的，有些特征可能高度相关（冗余），有些可能是噪声。去除它们有助于提高模型性能。\n提高模型可解释性: 使用更少的关键特征更容易理解模型的决策过程。\n数据可视化: 将高维数据降到 2 维或 3 维，方便我们进行可视化探索。\n\n\n\n12.1.2 降维的分类\n降维技术主要可以分为两大类：\n\n线性降维方法：假设数据位于线性子空间中\n\n主成分分析 (PCA)\n\n非线性降维方法：处理位于非线性流形上的数据\n\nt-分布随机邻域嵌入 (t-SNE)\nUMAP (Uniform Manifold Approximation and Projection)\n\n\n本讲我们将重点讨论最常用的线性降维方法——主成分分析(PCA)。\n\n\n12.1.3 主成分分析 (PCA - Principal Component Analysis) 深入\nPCA 是一种非常流行的无监督线性降维技术，属于特征提取 (Feature Extraction) 的范畴。它不是简单地选择一部分原始特征，而是将原始特征线性组合成一组新的、不相关的主成分 (Principal Components)，这些主成分能最大程度地保留原始数据的方差 (Variance)。\n\n12.1.3.1 原理回顾与深入\n\n目标: 找到一组新的正交（相互垂直）坐标轴（即主成分），使得数据在这些轴上的投影方差最大化。\n第一主成分: 数据投影后方差最大的那个方向。\n第二主成分: 与第一主成分正交，并且是剩余方差最大的方向。\n以此类推: 第 k 个主成分与前 k-1 个主成分都正交，并且是剩余方差最大的方向。\n降维: 选择方差最大的前 k 个主成分来代表原始数据，从而达到降维的目的。这 k 个主成分是原始特征的线性组合。\n\n\n\n12.1.3.2 PCA的数学原理\n从线性代数角度，PCA可以通过以下步骤实现：\n\n数据中心化：将每个特征减去其均值，使得每个特征的均值为0\n计算协方差矩阵：\\(\\Sigma = \\frac{1}{n-1}X^{T}X\\)，其中X是中心化后的数据矩阵\n计算协方差矩阵的特征值和特征向量：求解\\(\\Sigma v = \\lambda v\\)，得到特征值\\(\\lambda\\)和特征向量\\(v\\)\n特征向量排序：根据特征值大小降序排列特征向量\n选择前k个特征向量：构建投影矩阵\\(W\\)\n数据投影：\\(Z = XW\\)，得到降维后的数据\n\n\n\n12.1.3.3 PCA的几何解释\n从几何角度看，PCA寻找的是数据中的主要变化方向。想象一个三维空间中的扁平椭球体数据云：\n\n第一主成分是椭球体最长的轴\n第二主成分是次长的轴\n第三主成分是最短的轴\n\n通过保留变化最大的方向，PCA能够用较少的维度捕捉数据的主要结构。\n\n\n12.1.3.4 PCA的统计理解\n从统计角度看，PCA是一种最大化数据方差的特征转换方法。这背后的假设是，数据的方差越大，包含的信息量就越多。主成分既相互正交（不相关），又能按重要性排序（由特征值大小决定）。\n\n\n12.1.3.5 方差解释率 (Explained Variance Ratio)\nPCA 的一个重要输出是每个主成分能够解释原始数据方差的比例 (explained_variance_ratio_)。\n\n第一个主成分解释的方差比例最高，第二个次之，以此类推。\n所有主成分解释的方差比例之和为 1 (或 100%)。\n通过计算累积方差解释率，我们可以决定需要保留多少个主成分才能保留足够的信息（例如，保留能够解释 95% 或 99% 方差的主成分）。\n\n\n\n12.1.3.6 如何选择主成分数量 (n_components)\n\n根据累积方差解释率: 绘制累积方差解释率随主成分数量变化的曲线，选择能够达到目标方差解释率（如 95%）的最小主成分数量。\n根据业务需求或可视化需求: 如果是为了可视化，通常选择 2 或 3 个主成分。\n作为超参数: 在某些情况下，可以将 n_components 视为一个超参数，通过交叉验证来选择最佳值（例如，看哪个数量的主成分能让后续模型的性能最好）。\n肘部法则(Elbow Method): 绘制主成分数量与累积方差解释率的关系图，找到曲线拐点。\n\n\n\n12.1.3.7 使用 Scikit-learn 实现\n\n\n\n\n\n\n特征缩放\n\n\n\nPCA 对特征的尺度非常敏感。在应用 PCA 之前，必须对数据进行特征缩放 (通常使用 StandardScaler)。\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_digits # 手写数字数据集示例\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# --- 加载数据示例 (手写数字) ---\n# MNIST数据集的简化版本 - Digits数据集\ndigits = load_digits()\nX_digits = digits.data # (1797, 64) - 1797个样本，每个是8x8图像展平后的64个特征\ny_digits = digits.target\nprint(\"原始数据形状:\", X_digits.shape)\nprint(\"类别分布:\", np.bincount(y_digits))\n\n# --- 可视化原始数据 ---\n# 展示几个样本示例\nfig, axes = plt.subplots(2, 10, figsize=(15, 4), \n                         subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(digits.images[i], cmap='gray')\n    ax.set_title(f'{y_digits[i]}')\nplt.suptitle('MNIST Digits数据集示例', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.88)\nplt.show()\n\n# --- 特征缩放 ---\nscaler = StandardScaler()\nX_digits_scaled = scaler.fit_transform(X_digits)\n\n# --- 应用 PCA ---\n# 1. 先不指定 n_components，计算所有主成分的方差解释率\npca_full = PCA(random_state=42)\npca_full.fit(X_digits_scaled)\n\n# 计算累积方差解释率\nexplained_variance_ratio_cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n\n# 绘制累积方差解释率曲线\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(explained_variance_ratio_cumsum) + 1), explained_variance_ratio_cumsum, marker='.', linestyle='--')\nplt.xlabel('主成分数量')\nplt.ylabel('累积方差解释率')\nplt.title('主成分数量与方差解释率关系')\nplt.grid(True)\n# 添加阈值线 (例如 95%)\nplt.axhline(y=0.95, color='r', linestyle='-', label='95% 方差解释率')\nplt.legend(loc='best')\nplt.show()\n\n# 绘制各个主成分的方差解释率\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, 21), pca_full.explained_variance_ratio_[:20] * 100)\nplt.xlabel('主成分')\nplt.ylabel('方差解释率 (%)')\nplt.title('前20个主成分的方差解释率')\nplt.xticks(range(1, 21))\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# 2. 根据目标方差解释率选择 n_components\n# 例如，我们希望保留 95% 的方差\npca_95 = PCA(n_components=0.95, random_state=42) # 直接传入比例\nX_digits_pca_95 = pca_95.fit_transform(X_digits_scaled)\nprint(f\"\\n保留 95% 方差所需的主成分数量: {pca_95.n_components_}\")\nprint(\"降维后数据形状 (95%方差):\", X_digits_pca_95.shape)\n\n# 3. 或者直接指定主成分数量 (例如用于可视化)\npca_2d = PCA(n_components=2, random_state=42)\nX_digits_pca_2d = pca_2d.fit_transform(X_digits_scaled)\nprint(\"\\n降维到 2D 后的数据形状:\", X_digits_pca_2d.shape)\nprint(\"前两个主成分解释的方差比例: {:.2f}%\".format(\n    pca_full.explained_variance_ratio_[:2].sum() * 100))\n\n# --- 可视化降维结果 (2D) 并标注数字标签 ---\nplt.figure(figsize=(12, 10))\nscatter = plt.scatter(X_digits_pca_2d[:, 0], X_digits_pca_2d[:, 1], \n                     c=y_digits, cmap='tab10', \n                     edgecolor='k', alpha=0.8, s=60)\nplt.colorbar(scatter, label='数字标签')\nplt.title('MNIST Digits数据的PCA降维结果 (2个主成分)', fontsize=16)\nplt.xlabel('主成分 1', fontsize=12)\nplt.ylabel('主成分 2', fontsize=12)\n\n# 添加数字标签\nfor i, (x, y) in enumerate(X_digits_pca_2d):\n    # 只标记部分点以避免过度拥挤\n    if i % 50 == 0:  \n        plt.text(x, y, str(y_digits[i]), \n                 fontsize=12, color='black',\n                 bbox=dict(facecolor='white', alpha=0.7))\n\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# 三维可视化 (使用3个主成分)\npca_3d = PCA(n_components=3, random_state=42)\nX_digits_pca_3d = pca_3d.fit_transform(X_digits_scaled)\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_digits_pca_3d[:, 0], X_digits_pca_3d[:, 1], X_digits_pca_3d[:, 2],\n                     c=y_digits, cmap='tab10', \n                     edgecolor='k', s=60, alpha=0.8)\nax.set_title('MNIST Digits数据的PCA降维结果 (3个主成分)', fontsize=16)\nax.set_xlabel('主成分 1', fontsize=12)\nax.set_ylabel('主成分 2', fontsize=12)\nax.set_zlabel('主成分 3', fontsize=12)\nplt.colorbar(scatter, ax=ax, label='数字标签')\nplt.tight_layout()\nplt.show()\n\n\n12.1.3.8 主成分的解释与可视化\n了解主成分的物理意义，对理解数据结构非常重要：\n# 对于图像数据，我们可以可视化主成分的\"外观\"\n# 假设我们有手写数字数据\npca = PCA(n_components=16)\npca.fit(X_digits_scaled)\n\n# 可视化前几个主成分\nfig, axes = plt.subplots(4, 4, figsize=(12, 12),\n                         subplot_kw={'xticks':[], 'yticks':[]})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(8, 8), cmap='viridis')\n    ax.set_title(f\"PC {i+1}\\n({pca.explained_variance_ratio_[i]:.2%})\")\nplt.suptitle('MNIST数据的前16个主成分', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()\n\n# 可视化原始图像及其重构\n# 使用不同数量的主成分进行重构，观察信息保留情况\nn_components_list = [5, 10, 20, 30, 40, 64]\nn_samples = 4\n\nfig, axes = plt.subplots(len(n_components_list) + 1, n_samples, \n                         figsize=(n_samples * 2, (len(n_components_list) + 1) * 2),\n                         subplot_kw={'xticks':[], 'yticks':[]})\n\n# 选择几个样本进行展示\nsample_indices = [42, 100, 500, 800]  # 选择不同数字的样本\n\n# 显示原始图像\nfor i, idx in enumerate(sample_indices):\n    axes[0, i].imshow(X_digits[idx].reshape(8, 8), cmap='gray')\n    axes[0, i].set_title(f'原始图像\\n(数字: {y_digits[idx]})')\n\n# 使用不同数量的主成分重构\nfor row, n_comp in enumerate(n_components_list, 1):\n    pca_recon = PCA(n_components=n_comp).fit(X_digits_scaled)\n    X_reduced = pca_recon.transform(X_digits_scaled)\n    X_restored = pca_recon.inverse_transform(X_reduced)\n    \n    var_explained = sum(pca_recon.explained_variance_ratio_) * 100\n    \n    for i, idx in enumerate(sample_indices):\n        axes[row, i].imshow(X_restored[idx].reshape(8, 8), cmap='gray')\n        axes[row, i].set_title(f'{n_comp}个主成分\\n({var_explained:.1f}%方差)')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.suptitle('使用不同数量主成分重构MNIST图像', fontsize=16)\nplt.show()\n\n\n12.1.3.9 PCA 应用与局限性\n\n应用:\n\n数据压缩: 用更少的维度存储数据，减少存储空间和计算时间。\n噪声去除: 保留方差较大的主成分通常能过滤掉部分噪声。\n可视化: 将高维数据降到 2D 或 3D 进行可视化。\n作为预处理步骤: 将降维后的数据输入到其他机器学习模型中（有时能提高性能，有时会损失信息导致性能下降，需要尝试）。\n\n局限性:\n\n线性假设: PCA 假设数据的主要结构是线性的，对于高度非线性的数据效果可能不佳（可以考虑 Kernel PCA 等非线性方法）。\n可解释性差: 主成分是原始特征的线性组合，其物理意义不如原始特征直观。\n对特征缩放敏感: 必须进行特征缩放。\n\n\n\n\n\n12.1.4 非线性降维方法\n当数据分布在非线性流形上时，线性降维方法可能效果不佳。这时我们需要考虑非线性降维方法。\n\n12.1.4.1 t-SNE (t-Distributed Stochastic Neighbor Embedding)\nt-SNE 是一种非常流行的非线性降维方法，特别适合数据可视化。它尝试在低维空间中保留高维空间中点的局部结构（即相似的点在降维后仍然靠近）。\n\n12.1.4.1.1 t-SNE的核心原理\nt-SNE与PCA的根本区别在于其目标函数：PCA最大化投影方差，而t-SNE尝试保留数据点之间的相似度关系。其工作原理包括：\n\n高维相似度计算：在原始高维空间中，使用高斯分布计算点对之间的条件概率，作为相似度度量：\n\\[p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2/2\\sigma_i^2)}{\\sum_{k \\neq i}\\exp(-||x_i - x_k||^2/2\\sigma_i^2)}\\]\n这里\\(\\sigma_i\\)是根据困惑度(perplexity)参数调整的局部高斯分布的宽度\n低维映射：在低维空间中，使用t分布（而非高斯分布）计算点对之间的相似度：\n\\[q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}\\]\n优化目标：最小化高维空间和低维空间中相似度分布的KL散度：\n\\[KL(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\\]\n\n\n\n12.1.4.1.2 t-SNE的特点与理解\n\n使用t分布的原因：在低维空间使用t分布（重尾分布）而非高斯分布，可以缓解”拥挤问题”（高维空间中适度远距离的点在低维投影中过于靠近）\nperplexity参数：困惑度控制考虑每个点的局部邻域大小，可理解为”有效邻居数量”\n随机初始化：结果依赖于随机初始化，每次运行可能得到不同结果\n注重局部结构：t-SNE特别擅长保留局部结构，但可能扭曲全局关系\n\nfrom sklearn.manifold import TSNE\nimport time\n\n# 展示不同perplexity参数对t-SNE结果的影响\nperplexities = [5, 30, 50, 100]\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\nfor i, perplexity in enumerate(perplexities):\n    t0 = time.time()\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, \n                n_iter=1000, learning_rate='auto', init='pca')\n    X_tsne = tsne.fit_transform(X_digits)\n    t1 = time.time()\n    \n    # 可视化结果\n    scatter = axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], \n                             c=y_digits, cmap='tab10', edgecolor='k', alpha=0.8, s=60)\n    axes[i].set_title(f'perplexity={perplexity} (计算时间: {t1-t0:.2f}秒)', fontsize=14)\n    axes[i].set_xlabel('t-SNE 1', fontsize=12)\n    axes[i].set_ylabel('t-SNE 2', fontsize=12)\n    \n    # 添加数字标签\n    for j, (x, y) in enumerate(X_tsne):\n        if j % 70 == 0:  # 标记部分点\n            axes[i].text(x, y, str(y_digits[j]), \n                         fontsize=12, color='black',\n                         bbox=dict(facecolor='white', alpha=0.7))\n\nplt.colorbar(scatter, ax=axes, label='数字标签')\nplt.suptitle('t-SNE降维结果 - 不同perplexity参数对比', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.94)\nplt.show()\n\n# 深入分析最优t-SNE结果\nbest_perplexity = 30\ntsne_best = TSNE(n_components=2, perplexity=best_perplexity, random_state=42, \n               n_iter=1000, learning_rate='auto', init='pca')\nX_tsne_best = tsne_best.fit_transform(X_digits)\n\n# 详细可视化最佳t-SNE结果\nplt.figure(figsize=(14, 10))\nscatter = plt.scatter(X_tsne_best[:, 0], X_tsne_best[:, 1], \n                     c=y_digits, cmap='tab10', edgecolor='k', alpha=0.8, s=70)\nplt.title(f't-SNE降维结果 (perplexity={best_perplexity})', fontsize=16)\nplt.xlabel('t-SNE 1', fontsize=14)\nplt.ylabel('t-SNE 2', fontsize=14)\n\n# 添加数字标签和聚类边界\nfor i, (x, y) in enumerate(X_tsne_best):\n    if i % 50 == 0:\n        plt.text(x, y, str(y_digits[i]), \n                fontsize=12, color='black',\n                bbox=dict(facecolor='white', alpha=0.7))\n\n# 添加图例和网格\nplt.colorbar(scatter, label='数字标签')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n12.1.4.2 UMAP (Uniform Manifold Approximation and Projection)\nUMAP 是一种较新的非线性降维技术，它基于黎曼几何和代数拓扑，在保持数据全局结构的同时，维持局部结构。比t-SNE更快，且能更好地保留全局结构。\n\n12.1.4.2.1 UMAP的理论基础\nUMAP建立在更坚实的数学基础上，主要基于：\n\n黎曼几何和流形理论：假设高维数据位于低维流形上\n代数拓扑：使用简化拓扑表示数据\n\n\n\n12.1.4.2.2 UMAP的工作原理\nUMAP的算法步骤包括：\n\n构建局部流形近似：为每个数据点构建局部流形的模糊拓扑表示\n\\[\\rho_i = \\min\\{d(x_i, x_j) | 1 \\leq j \\leq k, j \\neq i\\}\\]\n\\[\\sigma_i\\]通过二分搜索求解，使得：\n\\[\\sum_{j=1}^n \\exp(-\\max(0, d(x_i, x_j) - \\rho_i)/\\sigma_i) = \\log_2(k)\\]\n构建高维模糊图：基于局部距离创建模糊图表示\n\\[v_{ij} = \\exp(-\\max(0, d(x_i, x_j) - \\rho_i)/\\sigma_i)\\]\n低维嵌入优化：通过力导向图布局算法，优化低维表示以匹配高维拓扑结构\n\\[v_{ij} \\log\\left(\\frac{v_{ij}}{w_{ij}}\\right) + (1-v_{ij})\\log\\left(\\frac{1-v_{ij}}{1-w_{ij}}\\right)\\]\n\n\n\n12.1.4.2.3 UMAP与t-SNE的关键区别\n\n理论基础：UMAP基于拓扑学和流形理论，t-SNE基于概率论\n全局结构：UMAP通常更好地保留全局结构\n计算效率：UMAP比t-SNE更快，尤其是对大型数据集\n超参数：UMAP的主要参数是n_neighbors（类似于t-SNE的perplexity）和min_dist（控制点的紧密程度）\n可扩展性：UMAP支持新数据点的transform，t-SNE不支持\n\n# 需要安装: pip install umap-learn\nimport umap\nimport time\n\n# 探索不同参数对UMAP结果的影响\nn_neighbors_list = [5, 15, 30, 50]  # 邻居数量影响局部结构保留程度\nmin_dist_list = [0.0, 0.1, 0.5, 0.8]  # 最小距离影响数据点分布紧密度\n\nfig, axes = plt.subplots(len(n_neighbors_list), len(min_dist_list), \n                         figsize=(20, 20))\n\nfor i, n_neighbors in enumerate(n_neighbors_list):\n    for j, min_dist in enumerate(min_dist_list):\n        t0 = time.time()\n        reducer = umap.UMAP(n_components=2, \n                           n_neighbors=n_neighbors, \n                           min_dist=min_dist, \n                           random_state=42)\n        X_umap = reducer.fit_transform(X_digits)\n        t1 = time.time()\n        \n        # 可视化结果\n        scatter = axes[i, j].scatter(X_umap[:, 0], X_umap[:, 1], \n                                    c=y_digits, cmap='tab10', \n                                    edgecolor='k', alpha=0.8, s=40)\n        axes[i, j].set_title(f'n_neighbors={n_neighbors}, min_dist={min_dist}\\n计算时间: {t1-t0:.2f}秒', \n                            fontsize=12)\n        \n        # 仅在最左侧添加y标签，最底部添加x标签\n        if j == 0:\n            axes[i, j].set_ylabel('UMAP 2', fontsize=12)\n        if i == len(n_neighbors_list) - 1:\n            axes[i, j].set_xlabel('UMAP 1', fontsize=12)\n            \nplt.colorbar(scatter, ax=axes.ravel().tolist(), label='数字标签')\nplt.suptitle('UMAP降维结果 - 不同参数组合对比', fontsize=20)\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()\n\n# 深入分析最优UMAP结果\nbest_n_neighbors = 15\nbest_min_dist = 0.1\n\numap_best = umap.UMAP(n_components=2, \n                     n_neighbors=best_n_neighbors, \n                     min_dist=best_min_dist, \n                     random_state=42)\nX_umap_best = umap_best.fit_transform(X_digits)\n\n# 详细可视化最佳UMAP结果\nplt.figure(figsize=(14, 10))\nscatter = plt.scatter(X_umap_best[:, 0], X_umap_best[:, 1], \n                     c=y_digits, cmap='tab10', \n                     edgecolor='k', alpha=0.8, s=70)\nplt.title(f'UMAP降维结果 (n_neighbors={best_n_neighbors}, min_dist={best_min_dist})', \n         fontsize=16)\nplt.xlabel('UMAP 1', fontsize=14)\nplt.ylabel('UMAP 2', fontsize=14)\n\n# 添加数字标签\nfor i, (x, y) in enumerate(X_umap_best):\n    if i % 50 == 0:  # 只标记部分点\n        plt.text(x, y, str(y_digits[i]), \n                fontsize=12, color='black',\n                bbox=dict(facecolor='white', alpha=0.7))\n\nplt.colorbar(scatter, label='数字标签')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n# 3D可视化\numap_3d = umap.UMAP(n_components=3, \n                   n_neighbors=best_n_neighbors, \n                   min_dist=best_min_dist, \n                   random_state=42)\nX_umap_3d = umap_3d.fit_transform(X_digits)\n\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_umap_3d[:, 0], X_umap_3d[:, 1], X_umap_3d[:, 2],\n                    c=y_digits, cmap='tab10', \n                    edgecolor='k', s=40, alpha=0.8)\nax.set_title('MNIST Digits数据的UMAP 3D降维结果', fontsize=16)\nax.set_xlabel('UMAP 1', fontsize=12)\nax.set_ylabel('UMAP 2', fontsize=12)\nax.set_zlabel('UMAP 3', fontsize=12)\nplt.colorbar(scatter, ax=ax, label='数字标签')\nplt.tight_layout()\nplt.show()\nUMAP的优势： - 比t-SNE更快，特别是对大型数据集 - 更好地保留全局和局部结构 - 能够transform新数据 - 理论基础更坚实 - 参数对结果的影响相对更可预测\n\n\n\n\n12.1.5 三种降维方法的理论比较\n为了更清晰地理解PCA、t-SNE和UMAP的区别，我们可以从多个角度进行比较：\n\n12.1.5.1 数学理论基础\n\nPCA：线性代数和方差最大化，基于特征值分解或奇异值分解\nt-SNE：概率论和信息论，基于高维和低维概率分布的KL散度最小化\nUMAP：代数拓扑和流形理论，基于拓扑结构保持\n\n\n\n12.1.5.2 优化目标\n\nPCA：最大化投影方差，保留全局结构\nt-SNE：最小化高维和低维概率分布的KL散度，保留局部结构\nUMAP：优化模糊拓扑表示，平衡局部和全局结构\n\n\n\n12.1.5.3 数据假设\n\nPCA：假设数据位于线性子空间\nt-SNE：无明确的数据分布假设，但注重局部相似性保持\nUMAP：假设数据位于均匀分布的黎曼流形上\n\n\n\n12.1.5.4 计算复杂度\n\nPCA：\\(O(min(nd^2, n^2d))\\)，其中n是样本数，d是特征数\nt-SNE：\\(O(n^2)\\)，优化版本可达到\\(O(n\\log n)\\)\nUMAP：\\(O(n^{1.14})\\)，实际应用中通常快于t-SNE\n\n\n\n12.1.5.5 各自的适用场景\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n数据规模\n局部结构保持\n全局结构保持\n计算速度\n可视化效果\n新数据处理\n\n\n\n\nPCA\n任何规模\n一般\n好\n非常快\n一般\n支持\n\n\nt-SNE\n中小规模\n非常好\n一般\n慢\n优秀\n不支持\n\n\nUMAP\n各种规模\n非常好\n好\n快\n优秀\n支持\n\n\n\n\n\n\n12.1.6 实践与讨论\n\n12.1.6.1 各种降维方法的比较\n让我们比较PCA、t-SNE和UMAP在同一数据集上的表现：\n# 设置图形大小\nplt.figure(figsize=(18, 6))\n\n# PCA\nplt.subplot(1, 3, 1)\nplt.scatter(X_digits_pca_2d[:, 0], X_digits_pca_2d[:, 1], \n           c=y_digits, cmap='tab10', edgecolor='k', alpha=0.7, s=50)\nplt.title('PCA', fontsize=14)\nplt.xlabel('主成分 1')\nplt.ylabel('主成分 2')\nplt.grid(True, linestyle='--', alpha=0.5)\n# 标注部分数字\nfor i, (x, y) in enumerate(X_digits_pca_2d):\n    if i % 200 == 0:\n        plt.text(x, y, str(y_digits[i]), fontsize=12)\n\n# t-SNE\nplt.subplot(1, 3, 2)\nplt.scatter(X_tsne_best[:, 0], X_tsne_best[:, 1], \n           c=y_digits, cmap='tab10', edgecolor='k', alpha=0.7, s=50)\nplt.title('t-SNE', fontsize=14)\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.grid(True, linestyle='--', alpha=0.5)\n# 标注部分数字\nfor i, (x, y) in enumerate(X_tsne_best):\n    if i % 200 == 0:\n        plt.text(x, y, str(y_digits[i]), fontsize=12)\n\n# UMAP\nplt.subplot(1, 3, 3)\nplt.scatter(X_umap_best[:, 0], X_umap_best[:, 1], \n           c=y_digits, cmap='tab10', edgecolor='k', alpha=0.7, s=50)\nplt.title('UMAP', fontsize=14)\nplt.xlabel('UMAP 1')\nplt.ylabel('UMAP 2')\nplt.grid(True, linestyle='--', alpha=0.5)\n# 标注部分数字\nfor i, (x, y) in enumerate(X_umap_best):\n    if i % 200 == 0:\n        plt.text(x, y, str(y_digits[i]), fontsize=12)\n\nplt.colorbar(scatter, ax=plt.gcf().get_axes(), label='数字标签')\nplt.suptitle('MNIST数据集降维方法比较', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.88)\nplt.show()\n\n# 定量比较：计算降维后同类数据点的聚集程度\nfrom sklearn.metrics import silhouette_score\n\n# 计算轮廓系数 (值越高表示聚类效果越好)\nsilhouette_pca = silhouette_score(X_digits_pca_2d, y_digits)\nsilhouette_tsne = silhouette_score(X_tsne_best, y_digits)\nsilhouette_umap = silhouette_score(X_umap_best, y_digits)\n\nprint(f\"各降维方法的轮廓系数比较:\")\nprint(f\"PCA: {silhouette_pca:.4f}\")\nprint(f\"t-SNE: {silhouette_tsne:.4f}\")\nprint(f\"UMAP: {silhouette_umap:.4f}\")\n\n# 计算每种方法的运行时间比较\nimport time\n\n# PCA时间\nt0 = time.time()\npca = PCA(n_components=2, random_state=42)\npca.fit_transform(X_digits_scaled)\npca_time = time.time() - t0\n\n# t-SNE时间\nt0 = time.time()\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\ntsne.fit_transform(X_digits_scaled)\ntsne_time = time.time() - t0\n\n# UMAP时间\nt0 = time.time()\nreducer = umap.UMAP(n_components=2, random_state=42)\nreducer.fit_transform(X_digits_scaled)\numap_time = time.time() - t0\n\nprint(f\"\\n各降维方法的计算时间比较:\")\nprint(f\"PCA: {pca_time:.4f}秒\")\nprint(f\"t-SNE: {tsne_time:.4f}秒\")\nprint(f\"UMAP: {umap_time:.4f}秒\")\n\n\n12.1.6.2 如何选择合适的降维方法？\n选择降维方法时应考虑以下因素：\n\n数据规模：大数据集可能更适合PCA或UMAP，而不是计算密集的t-SNE\n任务目标：\n\n可视化：t-SNE或UMAP通常效果更好\n降噪：PCA\n分类预处理：LDA或PCA\n\n数据结构：\n\n线性结构：PCA或LDA\n非线性流形：t-SNE、UMAP或其他流形学习方法\n\n可解释性需求：PCA的主成分有明确的数学解释，而非线性方法通常解释性较弱\n计算资源：PCA快速且高效，非线性方法计算密集\n\n\n\n12.1.6.3 实践建议\n\n总是从PCA开始：先尝试简单的线性方法，再逐步尝试复杂的非线性方法\n特征缩放非常重要：大多数降维方法对特征尺度敏感\n可视化降维效果：通过可视化了解数据的内在结构\n调整参数：每种方法都有关键参数需要调整（如t-SNE的perplexity，UMAP的n_neighbors）\n结合领域知识：利用对数据的领域理解来评估降维结果\n\n\n\n12.1.6.4 本讲总结\n本讲我们学习了降维的重要性和各种降维技术，从线性方法（如PCA、LDA）到非线性方法（如t-SNE、UMAP）。我们重点讨论了PCA的原理、实现和应用，并简要介绍了其他降维方法的特点和适用场景。\n降维技术在机器学习中扮演着重要角色，它不仅能够减少计算复杂度，还能够帮助我们发现数据中的隐藏结构，提高模型性能，并实现数据可视化。掌握这些技术将使你能够更有效地处理高维数据。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>非监督学习：降维 (Dimensionality Reduction)</span>"
    ]
  },
  {
    "objectID": "11_nlp.html",
    "href": "11_nlp.html",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "",
    "text": "14 文本数据与NLP初步",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#文本数据的特点",
    "href": "11_nlp.html#文本数据的特点",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "14.1 文本数据的特点",
    "text": "14.1 文本数据的特点\n文本数据是一种非常常见但又极其复杂的数据类型，具有以下特点：\n\n高维稀疏性：文本可以表示为向量空间中的点，但这个空间往往有数万维（对应词汇量），而每个文档只使用其中很少的词\n顺序性：词的顺序对语义至关重要（“狗咬人”和”人咬狗”含义完全不同）\n语义性：文本承载复杂的语义信息，存在歧义、隐喻、引用等多种语言现象",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#文本处理流程概览",
    "href": "11_nlp.html#文本处理流程概览",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "14.2 文本处理流程概览",
    "text": "14.2 文本处理流程概览\n文本分析通常遵循以下步骤：\n\n采集：网络爬虫、API接口、数据库、PDF解析等\n清洗：去除HTML标签、特殊字符、错别字纠正等\n表示：将文本转换为机器可理解的形式（向量化）\n建模：应用机器学习算法执行分类、聚类、主题提取等任务",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#中文分词",
    "href": "11_nlp.html#中文分词",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "15.1 中文分词",
    "text": "15.1 中文分词\n与英文不同，中文没有明显的词语分隔符，需要专门的分词工具。\njieba是目前最流行的中文分词工具之一，具有以下特点： - 支持精确模式、全模式和搜索引擎模式 - 允许添加自定义词典 - 具有词性标注功能\n常见问题： - 专业术语、新词识别困难（需添加自定义词典） - 歧义分词（例如”结合成分子”可能被分为”结合/成/分子”或”结合/成分/子”） - 中英文混合文本处理",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#文本清洗与去停用词",
    "href": "11_nlp.html#文本清洗与去停用词",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "15.2 文本清洗与去停用词",
    "text": "15.2 文本清洗与去停用词\n文本清洗是将原始文本转换为标准格式的过程，主要包括： - 去除标点符号、特殊字符和数字 - 去除停用词（如”的”、“了”、“和”等对分析无实质帮助的词语） - 词形还原（如将”running”转为”run”） - 大小写统一（对英文文本）",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#词袋模型详解",
    "href": "11_nlp.html#词袋模型详解",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "15.3 词袋模型详解",
    "text": "15.3 词袋模型详解\n词袋模型(Bag of Words)是自然语言处理中最基础的文本表示方法。该模型将文本视为一组无序的词语集合，完全忽略语法和词序，仅关注词语在文档中是否出现或出现次数。\n\n15.3.1 词袋模型的理论基础\n词袋模型基于一个简化假设：文档的含义可以由其包含的词语而非词语顺序决定。尽管这个假设在许多情况下显然是错误的（例如”狗咬人”和”人咬狗”含义截然不同），但在文本分类、主题识别等任务上仍然取得了不错的效果。\n词袋模型的数学表示如下：\n对于文档集合 \\(D = \\{d_1, d_2, ..., d_n\\}\\)，我们首先构建一个词汇表 \\(V = \\{w_1, w_2, ..., w_m\\}\\)，其中包含所有文档中出现的唯一词语。\n对于文档 \\(d_i\\)，词袋表示为向量 \\(X_i = [x_{i1}, x_{i2}, ..., x_{im}]\\)，其中：\n\\[x_{ij} = \\text{词语 } w_j \\text{ 在文档 } d_i \\text{ 中的出现次数}\\]\n整个文档集合可以表示为文档-词项矩阵（Document-Term Matrix, DTM）：\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1m} \\\\\nx_{21} & x_{22} & \\cdots & x_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{nm}\n\\end{bmatrix}\\]\n\n\n15.3.2 词袋模型的变体\n词袋模型有几种常见的变体，根据计数方式的不同，具有不同的特性：\n\n二值化词袋模型（Binary Bag of Words）：只关心词语是否出现，不关心出现次数 \\[x_{ij} = \\begin{cases}\n1 & \\text{如果词语 } w_j \\text{ 在文档 } d_i \\text{ 中出现} \\\\\n0 & \\text{否则}\n\\end{cases}\\]\n词频统计（Term Frequency）：计算词语出现的次数 \\[x_{ij} = \\text{词语 } w_j \\text{ 在文档 } d_i \\text{ 中的出现次数}\\]\n词频归一化（Normalized Term Frequency）：将词频除以文档长度，以消除文档长度不同带来的影响 \\[x_{ij} = \\frac{\\text{词语 } w_j \\text{ 在文档 } d_i \\text{ 中的出现次数}}{\\text{文档 } d_i \\text{ 的总词数}}\\]\nN元语法模型（N-gram Model）：不仅考虑单个词，还考虑连续的N个词组合，可以部分保留词序信息\n\n一元语法（Unigram）：单个词，如 “发展”、“经济”\n二元语法（Bigram）：两个相邻词，如 “经济 发展”、“促进 增长”\n三元语法（Trigram）：三个相邻词，如 “促进 经济 发展”\n\n\n\n\n15.3.3 词袋模型的向量化过程\n实现词袋模型通常包括以下步骤：\n\n构建词汇表：从所有文档中收集唯一的词语，建立完整的词汇表\n\n可以进行筛选，如去除停用词、低频词\n可能限制词汇表大小，只保留最频繁的 K 个词\n\n统计词频：计算每个文档中每个词语出现的次数\n创建文档向量：每个文档表示为一个向量，向量的长度等于词汇表大小，向量的元素为对应词语在文档中的出现次数或其他统计量\n\n\n\n15.3.4 词袋模型的特点\n\n优点：\n\n简单直观，易于理解和实现\n计算高效，适用于大规模文本数据\n维度确定，便于应用各种机器学习算法\n捕捉文档的主题关键词\n\n缺点：\n\n完全忽略词序和语法，无法捕捉上下文信息\n无法处理词语的多义性和同义词关系\n高维稀疏表示，导致”维度灾难”\n新词问题：测试文档中可能出现训练集中未见过的词",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#特征稀疏性分析",
    "href": "11_nlp.html#特征稀疏性分析",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "15.4 特征稀疏性分析",
    "text": "15.4 特征稀疏性分析\n文本向量的稀疏性是文本分析中的重要特性。在词袋模型表示下，文本向量具有高度稀疏性，即绝大多数元素为零。这一特性有着深远的理论和实践意义。\n\n15.4.1 稀疏性的理论解释\n稀疏性源于以下事实：\n\n词汇量巨大：自然语言的词汇量通常非常大（中文常用词汇约有几万个）\n单个文档用词有限：任何一篇文档通常只使用全部词汇的很小一部分\n齐普夫定律(Zipf’s Law)：自然语言中，词频与词频排名成反比，即大多数词出现次数很少，少数词出现次数很多\n\n齐普夫定律可表示为： \\[f(k) \\propto \\frac{1}{k^s}\\] 其中 \\(f(k)\\) 是排名第 \\(k\\) 的词的频率，\\(s\\) 是接近于1的常数。\n\n\n15.4.2 稀疏性的数学度量\n文本向量的稀疏性可以用密度(density)的倒数来表示，其计算公式为：\n\\[\\text{稀疏度} = 1 - \\frac{\\text{非零元素数量}}{\\text{总元素数量}}\\]\n例如，如果一个10000维的向量中只有100个非零元素，则其稀疏度为: \\[1 - \\frac{100}{10000} = 0.99 = 99\\%\\]\n\n\n15.4.3 稀疏表示的优势和挑战\n优势： - 存储效率高：只需存储非零元素及其位置 - 计算效率高：只需处理非零元素 - 降低过拟合风险：稀疏性可视为一种正则化\n挑战： - 信息密度低：需要更多样本学习有效特征 - 难以直接应用某些算法，如神经网络 - “维度灾难”问题：高维空间中数据点趋于疏远\n\n\n15.4.4 稀疏矩阵的高效存储\n针对稀疏矩阵的特点，常用的存储格式包括：\n\n坐标列表(COO格式)：存储(行索引,列索引,值)三元组\n压缩稀疏行(CSR格式)：存储非零值、列索引和行指针\n压缩稀疏列(CSC格式)：存储非零值、行索引和列指针\n\n以CSR格式为例，其空间复杂度从O(m×n)降低到O(2nnz+m+1)，其中nnz为非零元素数量。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#tf-idf理论基础",
    "href": "11_nlp.html#tf-idf理论基础",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "16.1 TF-IDF理论基础",
    "text": "16.1 TF-IDF理论基础\nTF-IDF（Term Frequency-Inverse Document Frequency）是对词袋模型的重要改进，它不仅考虑词频(TF)，还考虑词语的区分度(IDF)。TF-IDF的核心思想是：如果一个词在某篇文档中出现次数多，但在整个文档集合中出现次数少，那么这个词很可能对该文档的主题具有较高的区分度。\n\n16.1.1 数学定义\nTF-IDF由两部分组成：\n\n词频(Term Frequency, TF)：衡量词语在文档中的重要性\n常见的TF计算方式有：\n\n原始频率： \\[TF(t,d) = f_{t,d}\\] 其中 \\(f_{t,d}\\) 是词语 \\(t\\) 在文档 \\(d\\) 中的出现次数\n归一化频率： \\[TF(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d}f_{t',d}}\\] 即词语出现的次数除以文档中的总词数\n对数归一化： \\[TF(t,d) = \\log(1 + f_{t,d})\\] 通过取对数压缩词频的范围\n\n逆文档频率(Inverse Document Frequency, IDF)：衡量词语提供信息的程度\n\\[IDF(t) = \\log\\frac{N}{DF(t)}\\]\n其中 \\(N\\) 是文档总数，\\(DF(t)\\) 是包含词语 \\(t\\) 的文档数量。为避免分母为零，通常加上平滑项：\n\\[IDF(t) = \\log\\frac{N}{DF(t) + 1} + 1\\]\nTF-IDF权重：将TF和IDF相乘\n\\[TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)\\]\n\n\n\n16.1.2 TF-IDF的理论性质\n\n词频越高，TF值越大：反映了词在文档中的重要性\n文档频率越高，IDF值越小：惩罚了常见词\n独特性：对于罕见但在特定文档中高频的词，给予最高权重\n向量长度归一化：通常结合余弦相似度使用，此时需要对TF-IDF向量进行L2归一化\n\n\n\n16.1.3 TF-IDF的数学基础\n从信息论角度，IDF可以被理解为词语 \\(t\\) 出现的信息量：\n\\[IDF(t) \\propto -\\log P(t)\\]\n其中 \\(P(t)\\) 是从随机选择的文档中抽取到词语 \\(t\\) 的概率。词语越罕见，其信息量越大。\nTF-IDF权重也可以从概率模型推导：它与词语 \\(t\\) 对文档 \\(d\\) 的主题分类贡献度成正比。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#文本相似度的理论基础",
    "href": "11_nlp.html#文本相似度的理论基础",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "16.2 文本相似度的理论基础",
    "text": "16.2 文本相似度的理论基础\n文本相似度是衡量两篇文档内容相似程度的度量，广泛应用于信息检索、文档聚类和分类、推荐系统等领域。\n\n16.2.1 主要相似度度量\n\n欧氏距离(Euclidean Distance)：向量空间中两点之间的直线距离\n\\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\\]\n\n优点：直观、易理解\n缺点：对向量长度敏感，不适合长短不一的文档比较\n\n曼哈顿距离(Manhattan Distance)：向量各维度差的绝对值之和\n\\[d(X,Y) = \\sum_{i=1}^{n}|x_i - y_i|\\]\n\n优点：计算简单，对异常值不敏感\n缺点：同样对向量长度敏感\n\n余弦相似度(Cosine Similarity)：向量夹角的余弦值\n\\[similarity(X,Y) = \\cos(\\theta) = \\frac{X \\cdot Y}{||X|| \\times ||Y||} = \\frac{\\sum_{i=1}^{n}x_i y_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2} \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\]\n\n优点：不受向量长度影响，适合文本相似度计算\n缺点：不考虑向量幅值，只关注方向\n\n杰卡德相似系数(Jaccard Similarity)：集合交集与并集的比值\n\\[J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\n\n优点：适用于二元特征或集合比较\n缺点：不考虑词频信息\n\n\n\n\n16.2.2 为什么余弦相似度适合文本分析？\n余弦相似度在文本分析中特别常用，原因在于：\n\n文档长度不敏感：长文档和短文档可以直接比较\n方向敏感：关注的是词汇分布的模式而非绝对频率\n高效计算：特别适合稀疏向量计算\n范围明确：值域为[-1,1]，便于理解和比较\n\n在TF-IDF向量空间中，两个文档的相似度可以通过计算它们的TF-IDF向量之间的余弦相似度得到，这样既考虑了共同词汇，又关注了词语的区分度。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#词云图的理论基础",
    "href": "11_nlp.html#词云图的理论基础",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "17.1 词云图的理论基础",
    "text": "17.1 词云图的理论基础\n词云图(Word Cloud)是文本数据可视化的常用方法，通过调整词语的大小和颜色来表示其在文本中的重要性。词云图基于以下理论基础：\n\n17.1.1 词重要性的数学表示\n词云图中词语大小的确定通常基于以下指标：\n\n词频(Term Frequency, TF)：词语在文档中出现的次数 \\[TF(t,d) = f_{t,d}\\]\nTF-IDF权重：结合词频和逆文档频率 \\[TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)\\]\n其他自定义权重：如情感分析中的极性强度等\n\n\n\n17.1.2 布局算法\n词云图的布局是一个复杂的问题，主要算法包括：\n\n贪婪算法：从最重要的词开始，按重要性递减顺序放置单词\n力导向布局：将词语视为带电粒子，通过模拟物理力实现分布\n螺旋放置：从中心点开始，沿螺旋线放置词语\n碰撞检测：确保词语之间不重叠\n\n\n\n17.1.3 视觉编码原则\n词云图的设计遵循以下视觉编码原则：\n\n大小编码(Size Encoding)：词语的大小与其重要性成正比\n颜色编码(Color Encoding)：可用来表示词语的类别、情感极性等\n方向编码(Orientation Encoding)：词语的方向可以增加视觉多样性\n位置编码(Position Encoding)：中心位置通常放置最重要的词",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#情感分析的理论基础",
    "href": "11_nlp.html#情感分析的理论基础",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "18.1 情感分析的理论基础",
    "text": "18.1 情感分析的理论基础\n情感分析(Sentiment Analysis)，也称为意见挖掘(Opinion Mining)，是指通过自然语言处理、文本分析和计算语言学等方法来识别、提取和量化文本中主观信息的过程。情感分析试图回答的核心问题是：文本作者对特定主题或实体的态度是什么？\n\n18.1.1 情感分析的主要任务\n情感分析主要包括以下几个层次的任务：\n\n文档级情感分析：确定整个文档的情感倾向（积极、消极或中性）\n句子级情感分析：确定单个句子的情感倾向\n方面级情感分析：识别文本中提到的特定方面/属性及其相关情感\n比较情感分析：比较不同实体之间的情感差异\n\n\n\n18.1.2 情感分析的主要方法\n情感分析的方法大致可分为三类：\n\n基于词典的方法：\n\n依赖预定义的情感词典，如知网HowNet情感词典、大连理工情感词典等\n通过计算情感词的出现频率和强度来评估整体情感\n数学表示：\\[Score(d) = \\frac{\\sum_{t \\in d} s_t \\times w_t}{\\sum_{t \\in d} w_t}\\] 其中\\(s_t\\)是词\\(t\\)的情感得分，\\(w_t\\)是权重（可以是TF-IDF值）\n\n基于机器学习的方法：\n\n监督学习：使用标注数据训练分类器（如SVM、朴素贝叶斯、决策树等）\n半监督学习：结合少量标注数据和大量未标注数据\n深度学习：使用CNN、RNN/LSTM、Transformer等神经网络模型\n\n混合方法：\n\n结合词典和机器学习方法的优点\n使用词典特征增强机器学习模型\n使用规则修正机器学习结果\n\n\n\n\n18.1.3 情感词典的构建原理\n情感词典是情感分析的重要资源，其构建原理包括：\n\n人工标注：语言学专家手动标注词语的情感极性和强度\n基于种子词的扩展：从少量种子情感词出发，利用同义词、反义词关系扩展\n基于共现的统计方法：分析情感词与已知极性词的共现统计\n基于深度学习的词嵌入：使用词向量的相似性发现新的情感词",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "11_nlp.html#主题建模理论",
    "href": "11_nlp.html#主题建模理论",
    "title": "13  文本分析1：词频法与向量空间",
    "section": "19.1 主题建模理论",
    "text": "19.1 主题建模理论\n主题建模(Topic Modeling)是一类无监督机器学习技术，旨在从文档集合中发现抽象”主题”。其核心思想是：每篇文档可以看作是多个主题的混合，而每个主题又是词语上的概率分布。\n\n19.1.1 潜在语义分析(LSA)\n潜在语义分析(Latent Semantic Analysis, LSA)是早期的主题建模方法，基于奇异值分解(SVD)。\n\n数学表示：\n\n将文档-词项矩阵 \\(X\\) 分解为 \\(X \\approx U\\Sigma V^T\\)\n\\(U\\) 是文档-主题矩阵，\\(V\\) 是词-主题矩阵，\\(\\Sigma\\) 是表示主题重要性的对角矩阵\n\n优缺点：\n\n优点：计算简单，容易理解\n缺点：缺乏明确的统计语义解释，可能出现负值\n\n\n\n\n19.1.2 概率潜在语义分析(PLSA)\n概率潜在语义分析(Probabilistic Latent Semantic Analysis, PLSA)引入了概率框架，将文档和词语的关系建模为混合模型。\n\n数学表示：\n\n文档 \\(d\\) 中词 \\(w\\) 的生成概率：\\(P(w|d) = \\sum_{z} P(w|z)P(z|d)\\)\n其中 \\(z\\) 表示潜在主题，\\(P(w|z)\\) 是主题 \\(z\\) 生成词 \\(w\\) 的概率，\\(P(z|d)\\) 是文档 \\(d\\) 中主题 \\(z\\) 的比例\n\n优缺点：\n\n优点：有明确的概率解释，结果更加合理\n缺点：容易过拟合，不是完全贝叶斯模型\n\n\n\n\n19.1.3 潜在狄利克雷分配(LDA)\n潜在狄利克雷分配(Latent Dirichlet Allocation, LDA)是目前最流行的主题建模方法，它是PLSA的贝叶斯版本，引入了主题分布和词分布的先验。\n\n19.1.3.1 LDA的生成过程\nLDA假设文档的生成过程如下：\n\n对于每个文档 \\(d\\)：\n\n从狄利克雷分布 \\(Dir(\\alpha)\\) 中抽取主题比例向量 \\(\\theta_d\\)\n\n对于每个主题 \\(k\\)：\n\n从狄利克雷分布 \\(Dir(\\beta)\\) 中抽取词语分布 \\(\\phi_k\\)\n\n对于文档 \\(d\\) 中的每个词位置 \\(i\\)：\n\n从多项式分布 \\(Mult(\\theta_d)\\) 中抽取主题 \\(z_{di}\\)\n从多项式分布 \\(Mult(\\phi_{z_{di}})\\) 中抽取词语 \\(w_{di}\\)\n\n\n\n\n19.1.3.2 LDA的数学表示\n\n联合概率分布： \\[P(\\mathbf{w}, \\mathbf{z}, \\mathbf{\\theta}, \\mathbf{\\phi}|\\alpha, \\beta) = \\prod_{k=1}^{K} P(\\phi_k|\\beta) \\prod_{d=1}^{D} P(\\theta_d|\\alpha) \\prod_{i=1}^{N_d} P(z_{di}|\\theta_d) P(w_{di}|z_{di}, \\phi)\\]\n推断任务：\n\n给定观察到的词 \\(\\mathbf{w}\\)，推断隐变量 \\(\\mathbf{z}\\), \\(\\mathbf{\\theta}\\), \\(\\mathbf{\\phi}\\)\n使用近似推断算法，如变分推断、吉布斯采样等\n\n模型参数：\n\n\\(\\alpha\\)：主题分布的先验参数，控制文档主题的稀疏性\n\\(\\beta\\)：词分布的先验参数，控制主题中词的稀疏性\n\\(K\\)：主题数量，需要事先指定\n\n\n\n\n19.1.3.3 LDA的优缺点\n优点： - 完整的生成概率模型，理论基础扎实 - 解释性强，主题和词语分布有明确的语义 - 可扩展性好，适用于大规模文档集合 - 有效避免过拟合\n缺点： - 需要预先指定主题数量 - 不考虑词序和语法，丢失部分语义信息 - 对短文本效果较差 - 复杂度较高，训练较慢\n\n\n\n19.1.4 主题一致性评估\n评估主题模型质量的主要指标包括：\n\n困惑度(Perplexity)：衡量模型对未见文档的预测能力 \\[Perplexity(D_{test}) = \\exp \\left(-\\frac{\\sum_{d=1}^{M} \\log p(\\mathbf{w}_d)}{\\sum_{d=1}^{M} N_d}\\right)\\]\n主题一致性(Topic Coherence)：衡量主题内部词语的语义相关性\n\nPMI(Pointwise Mutual Information)：基于词对在同一文档中共现的概率\nNPMI(Normalized PMI)：归一化的PMI\nUCI coherence：基于PMI的一致性度量\nUMass coherence：基于条件概率的一致性度量",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>文本分析1：词频法与向量空间</span>"
    ]
  },
  {
    "objectID": "12_embedding.html",
    "href": "12_embedding.html",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "",
    "text": "15 从稀疏到密集表示",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#bag-of-words模型的局限性",
    "href": "12_embedding.html#bag-of-words模型的局限性",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "15.1 Bag of Words模型的局限性",
    "text": "15.1 Bag of Words模型的局限性\n上一讲中，我们学习了词袋模型（Bag of Words）和TF-IDF，这些是文本分析的基础方法。然而，这些方法存在明显局限性：\n\n丢失词序信息：词袋模型完全忽略词语出现的顺序。例如”政府调控房价”和”房价调控政府”在词袋表示中是完全相同的\n语义鸿沟问题：无法捕捉词与词之间的语义关系，如同义词、上下位词等\n维度灾难：高维稀疏向量（维度等于词汇量大小）导致计算效率低下\n未登录词问题：无法处理训练集中未出现过的词语\n\n词袋模型的这些局限性促使研究者寻找更先进的文本表示方法，词向量（Word Embedding）正是这一探索的重要成果。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#词向量的直觉理解",
    "href": "12_embedding.html#词向量的直觉理解",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "15.2 词向量的直觉理解",
    "text": "15.2 词向量的直觉理解\n词向量（Word Embedding）是将词语映射到一个低维稠密实数向量空间的技术，通常维度在50-300之间。与词袋模型不同，词向量具有以下特点：\n\n稠密表示：向量中的每个维度都有非零值\n语义编码：向量的不同维度隐含地编码了词语的语义特征\n相似性可计算：语义相近的词在向量空间中距离较近\n\n例如，“银行”和”金融”在向量空间中距离较近，而”银行”和”蔬菜”则距离较远。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#分布式假设词向量的理论基础",
    "href": "12_embedding.html#分布式假设词向量的理论基础",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "15.3 分布式假设：词向量的理论基础",
    "text": "15.3 分布式假设：词向量的理论基础\n词向量背后的核心理论是分布式假设（Distributional Hypothesis），这一理论由语言学家J.R. Firth在1957年提出：\n\n“You shall know a word by the company it keeps.”（一个词的含义取决于它的伙伴词）\n\n这一假设认为：上下文相似的词，其语义也相似。例如，“银行”和”金融机构”经常出现在相似的上下文中，因此它们可能具有相似的语义。\n基于分布式假设，词向量学习的核心任务可以归纳为：学习一个映射函数，使得在语料库中上下文相似的词在向量空间中的位置也相近。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#密集表示的数学性质",
    "href": "12_embedding.html#密集表示的数学性质",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "15.4 密集表示的数学性质",
    "text": "15.4 密集表示的数学性质\n从数学角度看，密集表示的优势在于：\n\n降维性：从稀疏高维空间（词汇量大小，如5万维）降至低维空间（如300维）\n连续性：连续向量空间允许进行向量代数运算，如类比推理\n泛化能力：能够更好地泛化到未见过的例子\n\n数学上，稀疏向量与密集向量的对比如下：\n\n稀疏向量（Sparse Vector）：\\(\\mathbf{v} = [0, 0, 1, 0, ..., 0, 2, 0]\\)，大多数元素为0\n密集向量（Dense Vector）：\\(\\mathbf{v} = [0.2, -0.6, 0.5, 0.9, ..., -0.1, 0.3]\\)，大多数元素非0\n\n稠密向量表示的直观优势可以通过一个简单的类比来理解：假设我们要描述一个人，可以使用二元特征（是/否问题，对应稀疏表示）如”是否戴眼镜”、“是否有胡子”等，也可以使用连续特征（对应稠密表示）如身高、体重、年龄等。连续特征通常能更精确、更紧凑地描述对象。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#word2vec简介",
    "href": "12_embedding.html#word2vec简介",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "16.1 Word2Vec简介",
    "text": "16.1 Word2Vec简介\nWord2Vec是Mikolov等人于2013年提出的一种高效学习词向量的方法，它通过浅层神经网络从大规模语料库中学习词语的分布式表示。Word2Vec迅速成为NLP领域的里程碑技术，为后续深度学习在NLP中的应用奠定了基础。\nWord2Vec的核心思想是：通过预测上下文中的词来学习词语的向量表示。基于这一思想，Word2Vec提出了两种模型：\n\nSkip-gram模型：预测上下文词\nCBOW（Continuous Bag of Words）模型：预测目标词",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#skip-gram模型详解",
    "href": "12_embedding.html#skip-gram模型详解",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "16.2 Skip-gram模型详解",
    "text": "16.2 Skip-gram模型详解\nSkip-gram模型的目标是：给定中心词，预测其上下文词。\n\n16.2.1 模型结构\nSkip-gram模型的网络结构如下：\n\n输入层：中心词的one-hot编码，维度为词汇量大小\\(|V|\\)\n隐藏层：不含激活函数的全连接层，维度为词向量维度\\(d\\)\n输出层：预测上下文词概率的softmax层，维度为词汇量大小\\(|V|\\)\n\n其数学表示为：\n\\[p(w_o|w_i) = \\frac{\\exp(v_{w_o}^{\\prime T} \\cdot v_{w_i})}{\\sum_{w=1}^{|V|} \\exp(v_w^{\\prime T} \\cdot v_{w_i})}\\]\n其中： - \\(w_i\\)是中心词 - \\(w_o\\)是上下文词 - \\(v_{w_i}\\)是中心词的词向量（输入向量） - \\(v_{w_o}^{\\prime}\\)是上下文词的词向量（输出向量） - \\(|V|\\)是词汇表大小\n\n\n16.2.2 训练过程\nSkip-gram模型的训练过程如下：\n\n从语料库中抽取中心词\\(w_i\\)及其上下文窗口内的词\\(w_o\\)\n最大化预测上下文词的条件概率\\(p(w_o|w_i)\\)\n对所有词对\\((w_i, w_o)\\)，优化目标函数：\n\n\\[J(\\theta) = \\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j}|w_t)\\]\n其中： - \\(T\\)是语料库中的词数 - \\(c\\)是上下文窗口大小 - \\(\\theta\\)是模型参数",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#cbow模型详解",
    "href": "12_embedding.html#cbow模型详解",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "16.3 CBOW模型详解",
    "text": "16.3 CBOW模型详解\nCBOW（Continuous Bag of Words）模型与Skip-gram相反，其目标是：给定上下文词，预测中心词。\n\n16.3.1 模型结构\nCBOW模型的网络结构如下：\n\n输入层：多个上下文词的one-hot编码，每个维度为词汇量大小\\(|V|\\)\n隐藏层：不含激活函数的全连接层，维度为词向量维度\\(d\\)\n输出层：预测中心词概率的softmax层，维度为词汇量大小\\(|V|\\)\n\nCBOW模型首先对上下文词的向量取平均：\n\\[\\hat{v} = \\frac{1}{2c} \\sum_{-c \\leq j \\leq c, j \\neq 0} v_{w_{t+j}}\\]\n然后预测中心词的概率：\n\\[p(w_t|\\hat{v}) = \\frac{\\exp(v_{w_t}^{\\prime T} \\cdot \\hat{v})}{\\sum_{w=1}^{|V|} \\exp(v_w^{\\prime T} \\cdot \\hat{v})}\\]\n\n\n16.3.2 Skip-gram与CBOW对比\n两种模型各有优缺点：\n\nSkip-gram:\n\n更适合小型语料库\n对低频词表现更好\n计算复杂度较高\n\nCBOW:\n\n训练速度更快\n对高频词表现更好\n在大型语料库上更稳定",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#负采样negative-sampling技术",
    "href": "12_embedding.html#负采样negative-sampling技术",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "16.4 负采样（Negative Sampling）技术",
    "text": "16.4 负采样（Negative Sampling）技术\nWord2Vec的一个主要计算瓶颈是softmax函数，其计算复杂度与词汇量成正比。为解决这一问题，Mikolov等人提出了负采样（Negative Sampling）技术。\n\n16.4.1 负采样原理\n负采样将多分类问题转化为二分类问题：\n\n对于真实的词对\\((w_i, w_o)\\)，将其标记为正样本（标签为1）\n对于每个正样本，随机采样\\(k\\)个负样本\\((w_i, w_n)\\)，其中\\(w_n\\)是随机词（标签为0）\n使用逻辑回归来判断词对是否真实共现\n\n优化目标变为：\n\\[J(\\theta) = \\log \\sigma(v_{w_o}^{\\prime T} \\cdot v_{w_i}) + \\sum_{j=1}^{k} \\mathbb{E}_{w_j \\sim P_n(w)} [\\log \\sigma(-v_{w_j}^{\\prime T} \\cdot v_{w_i})]\\]\n其中： - \\(\\sigma\\)是sigmoid函数 - \\(P_n(w)\\)是负样本的噪声分布，通常为词频的3/4次方\n\n\n16.4.2 负采样的优势\n负采样技术带来的主要优势包括：\n\n计算效率：将复杂度从\\(O(|V|)\\)降至\\(O(k)\\)，其中\\(k \\ll |V|\\)（通常\\(k=5-20\\)）\n稀疏更新：每次只更新少量词向量，加速收敛\n控制学习难度：通过调整负样本数量控制任务难度\n\n负采样是Word2Vec能够在大规模语料库上高效训练的关键技术之一。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#词向量空间的语义特性",
    "href": "12_embedding.html#词向量空间的语义特性",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "16.5 词向量空间的语义特性",
    "text": "16.5 词向量空间的语义特性\nWord2Vec训练得到的词向量空间具有丰富的语义特性，这些特性使得词向量成为各种NLP任务的强大特征。\n\n16.5.1 语义相关性\n相似概念在向量空间中距离较近。例如：\n\n“银行”和”金融”距离近\n“苹果”(水果)和”橙子”距离近\n“苹果”(公司)和”微软”距离近\n\n这种相似性可以通过余弦相似度定量衡量：\n\\[similarity(w_1, w_2) = \\cos(\\theta) = \\frac{v_{w_1} \\cdot v_{w_2}}{||v_{w_1}|| \\cdot ||v_{w_2}||}\\]\n\n\n16.5.2 语义计算\n词向量空间中最令人惊讶的特性是支持向量代数运算，可以进行”语义计算”：\n\\[v(\\text{\"king\"}) - v(\\text{\"man\"}) + v(\\text{\"woman\"}) \\approx v(\\text{\"queen\"})\\]\n这意味着我们可以通过向量运算回答类比问题：“man之于woman，相当于king之于什么？”\n其他例子包括： - \\(v(\\text{\"中国\"}) - v(\\text{\"北京\"}) + v(\\text{\"法国\"}) \\approx v(\\text{\"巴黎\"})\\) - \\(v(\\text{\"比特币\"}) - v(\\text{\"数字\"}) + v(\\text{\"实物\"}) \\approx v(\\text{\"黄金\"})\\)\n这些语义运算的存在表明词向量确实捕获了复杂的语义关系，而非简单的共现统计。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#金融领域的词向量应用",
    "href": "12_embedding.html#金融领域的词向量应用",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "17.1 金融领域的词向量应用",
    "text": "17.1 金融领域的词向量应用\n在金融领域，词向量技术已被广泛应用于多种任务：\n\n情感分析：分析金融新闻、社交媒体对市场情绪的影响\n风险评估：从文本数据中提取风险信号\n主题发现：自动识别财经报道中的热点话题\n市场预测：结合文本特征进行市场走势预测\n\n金融文本的特殊性（专业术语多、实体关系复杂）使得通用词向量模型可能表现不佳，因此针对金融领域训练的词向量至关重要。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#训练金融领域词向量",
    "href": "12_embedding.html#训练金融领域词向量",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "17.2 训练金融领域词向量",
    "text": "17.2 训练金融领域词向量\n以下我们将使用政府工作报告和其他财经语料训练Word2Vec模型，展示其在金融领域的应用。\n\n\n代码\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jieba\nfrom gensim.models import Word2Vec\nimport re\n\n# 加载政府工作报告数据\ngovreport = pd.read_csv(\"labs/NLP/data/govreport.csv\")\n\n# 设置中文显示\nplt.rcParams['font.sans-serif'] = ['Songti SC']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n# 加载停用词\nwith open(\"labs/NLP/data/ChineseStopWords.txt\", 'r', encoding='utf-8') as f:\n    stop_words = {line.strip() for line in f}\n\n# 文本清洗并分词\ndef preprocess_text(text):\n    # 去除标点符号和数字\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', text)\n    # 分词\n    words = jieba.cut(text)\n    # 去除停用词和空白\n    return [word for word in words if word.strip() and word not in stop_words]\n\n# 处理所有文档\ncorpus = []\nfor _, row in govreport.iterrows():\n    words = preprocess_text(row['texts'])\n    corpus.append(words)\n\nprint(f\"语料库包含{len(corpus)}篇文档\")\n\n\n\n17.2.1 训练Word2Vec模型\n\n\n代码\n# 训练Word2Vec模型\nmodel = Word2Vec(\n    sentences=corpus,\n    vector_size=100,  # 词向量维度\n    window=5,         # 上下文窗口大小\n    min_count=5,      # 忽略低频词的阈值\n    sg=1,             # 使用Skip-gram模型\n    negative=5,       # 负采样数量\n    epochs=10,        # 训练轮数\n    seed=42\n)\n\n# 保存模型\nmodel.save(\"gov_report_word2vec.model\")\n\n# 查看词汇量\nprint(f\"模型包含{len(model.wv)}个词语\")\n\n# 查看一些词向量示例\nprint(\"'经济'的词向量:\")\nprint(model.wv['经济'])\n\n\n\n\n17.2.2 探索词向量空间\n通过查找最相似的词，我们可以验证词向量空间是否捕捉到了金融语义：\n\n\n代码\n# 查找与\"经济\"最相似的词\nsimilar_words = model.wv.most_similar('经济', topn=10)\nprint(\"与'经济'最相似的词:\")\nfor word, similarity in similar_words:\n    print(f\"{word}: {similarity:.4f}\")\n\n# 查找更多词的相似词\nfor query in ['金融', '创新', '改革', '发展']:\n    print(f\"\\n与'{query}'最相似的词:\")\n    for word, similarity in model.wv.most_similar(query, topn=5):\n        print(f\"{word}: {similarity:.4f}\")\n\n\n\n\n17.2.3 词向量的语义运算\n我们可以尝试在金融词向量空间中进行语义运算：\n\n\n代码\n# 词向量运算示例\ntry:\n    result = model.wv.most_similar(\n        positive=['改革', '创新'],\n        negative=['传统'],\n        topn=5\n    )\n    print(\"\\n'改革'+'创新'-'传统'的结果:\")\n    for word, similarity in result:\n        print(f\"{word}: {similarity:.4f}\")\nexcept KeyError as e:\n    print(f\"词汇不在模型中: {e}\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#可视化词向量空间",
    "href": "12_embedding.html#可视化词向量空间",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "17.3 可视化词向量空间",
    "text": "17.3 可视化词向量空间\n由于词向量通常是高维的（如100维），无法直接可视化。我们需要使用降维技术将其映射到2D空间：\n\n\n代码\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# 选择一些重要的金融和经济词汇\nkey_words = []\nfor word in ['经济', '金融', '改革', '创新', '发展', '企业', '市场', '投资', \n             '消费', '增长', '就业', '收入', '债务', '减税', '风险', '数字', \n             '科技', '产业', '结构', '调控', '开放', '政策', '监管', '服务',\n             '银行', '证券', '保险', '互联网', '环保', '低碳']:\n    if word in model.wv:\n        key_words.append(word)\n\n# 获取这些词的向量\nword_vectors = [model.wv[word] for word in key_words]\n\n# 使用t-SNE降维到2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nembeddings_2d = tsne.fit_transform(word_vectors)\n\n# 可视化\nplt.figure(figsize=(12, 10))\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0)\n\n# 添加词语标签\nfor i, word in enumerate(key_words):\n    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]), \n                 fontsize=12, alpha=0.8)\n\nplt.title('金融经济词汇的词向量空间可视化')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#实战案例政策热点分析",
    "href": "12_embedding.html#实战案例政策热点分析",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "17.4 实战案例：政策热点分析",
    "text": "17.4 实战案例：政策热点分析\n我们可以结合词向量技术分析政府工作报告中的政策热点演变：\n\n\n代码\n# 以几个关键政策词为中心，分析其在不同年份报告中的语义环境\nfocus_words = ['创新', '改革', '开放', '就业', '风险']\n\n# 为每个年份创建一个语义环境分析\nyears = sorted(govreport['Year'].unique())\nsemantic_evolution = {}\n\n# 对每个焦点词，分析其在各年份报告中的最相似词\nfor focus in focus_words:\n    if focus not in model.wv:\n        continue\n        \n    print(f\"\\n'{focus}'的语义演变:\")\n    # 获取最相似的10个词\n    similar_words = [word for word, _ in model.wv.most_similar(focus, topn=10)]\n    \n    # 查看这些词在各年份报告中的频率\n    for year in years:\n        year_text = govreport[govreport['Year'] == year]['texts'].iloc[0]\n        year_words = preprocess_text(year_text)\n        \n        # 计算焦点词及相似词在该年报告中的出现次数\n        focus_count = year_words.count(focus)\n        similar_counts = {word: year_words.count(word) for word in similar_words}\n        \n        # 排序并展示前5个高频相似词\n        top_similar = sorted(similar_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n        \n        if focus_count &gt; 0:\n            print(f\"{year}年 - '{focus}'出现{focus_count}次，相关词:\")\n            for word, count in top_similar:\n                if count &gt; 0:\n                    print(f\"  {word}: {count}次\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#与其他nlp模型的集成",
    "href": "12_embedding.html#与其他nlp模型的集成",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "17.5 与其他NLP模型的集成",
    "text": "17.5 与其他NLP模型的集成\n词向量作为特征可以与各种机器学习模型集成，用于更复杂的NLP任务：\n\n\n代码\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nimport numpy as np\n\n# 示例：使用词向量增强TF-IDF特征进行分类任务\n# (本例为概念演示，实际应用需要真实标签)\n\n# 为演示目的，我们假设2015-2019年为一类，2020-2023年为另一类\ngovreport['label'] = govreport['Year'].apply(lambda x: 1 if x &gt;= 2020 else 0)\n\n# 构建文档向量（简单方法：词向量的平均）\ndef document_vector(doc):\n    # 分词并过滤\n    words = preprocess_text(doc)\n    # 只考虑模型中有的词\n    words = [word for word in words if word in model.wv]\n    if len(words) == 0:\n        return np.zeros(model.vector_size)\n    # 计算词向量的平均\n    return np.mean([model.wv[word] for word in words], axis=0)\n\n# 为每个文档创建向量表示\nX = np.array([document_vector(text) for text in govreport['texts']])\ny = govreport['label'].values\n\n# 简单训练分类器（实际应用中应使用交叉验证）\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X, y)\n\n# 查看特征重要性\nfeature_importances = clf.feature_importances_\nprint(\"\\n词向量特征的重要性分布:\")\nplt.figure(figsize=(10, 6))\nplt.hist(feature_importances, bins=20)\nplt.title('词向量特征重要性分布')\nplt.xlabel('特征重要性')\nplt.ylabel('特征数量')\nplt.show()\n\n# 预测（为演示使用训练集，实际应用需要独立测试集）\ny_pred = clf.predict(X)\nprint(\"\\n分类性能：\")\nprint(f\"准确率: {metrics.accuracy_score(y, y_pred):.4f}\")\nprint(f\"F1得分: {metrics.f1_score(y, y_pred):.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#中文预训练词向量模型",
    "href": "12_embedding.html#中文预训练词向量模型",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.1 中文预训练词向量模型",
    "text": "18.1 中文预训练词向量模型\n\n18.1.1 1. 腾讯AI Lab词向量（Chinese Word Vectors）\n腾讯AI Lab发布的中文词向量是目前应用最广泛的中文预训练词向量之一。\n\n训练语料：由8亿多条句子、超过200亿词汇组成\n词汇量：约800万个词、词组和实体\n向量维度：200维\n特点：覆盖面广，质量高，适用于多领域任务\n\n\n\n代码\n# 示例：加载腾讯词向量\nimport gensim\n\n# 下载地址：https://ai.tencent.com/ailab/nlp/en/embedding.html\n# 假设已下载并解压到./data/Tencent_AILab_ChineseEmbedding.txt\ntencent_model = gensim.models.KeyedVectors.load_word2vec_format(\n    './data/Tencent_AILab_ChineseEmbedding.txt', \n    binary=False\n)\n\n# 查找相似词\nprint(\"与'金融'最相似的词:\")\nfor word, similarity in tencent_model.most_similar('金融', topn=5):\n    print(f\"{word}: {similarity:.4f}\")\n\n\n\n\n18.1.2 2. 哈工大/讯飞联合实验室词向量（HIT-SCIR Chinese Word Vectors）\n\n训练语料：人民日报语料库和其他新闻语料\n词汇量：约100万个词\n向量维度：300维\n特点：对专业术语和实体识别有较好的表现\n\n\n\n18.1.3 3. 百度百科词向量（Baidu Encyclopedia Word Vectors）\n\n训练语料：基于百度百科的语料\n词汇量：约200万个词\n向量维度：300维\n特点：对百科类内容和常识性知识表示较好",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#英文预训练词向量模型",
    "href": "12_embedding.html#英文预训练词向量模型",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.2 英文预训练词向量模型",
    "text": "18.2 英文预训练词向量模型\n\n18.2.1 1. Google News词向量\n由Google在约1000亿单词的Google News数据集上训练的Word2Vec模型。\n\n词汇量：约300万个词和短语\n向量维度：300维\n特点：通用性强，被广泛应用于英文NLP研究和应用\n\n\n\n代码\n# 示例：加载Google News词向量\nfrom gensim.models import KeyedVectors\n\n# 下载地址：https://code.google.com/archive/p/word2vec/\n# 假设已下载并解压到./data/GoogleNews-vectors-negative300.bin\ngoogle_model = KeyedVectors.load_word2vec_format(\n    './data/GoogleNews-vectors-negative300.bin', \n    binary=True\n)\n\n# 查找相似词\nprint(\"与'finance'最相似的词:\")\nfor word, similarity in google_model.most_similar('finance', topn=5):\n    print(f\"{word}: {similarity:.4f}\")\n\n\n\n\n18.2.2 2. GloVe（Global Vectors for Word Representation）\n由斯坦福NLP小组开发的词向量模型，训练自维基百科和网络文本。\n\n训练语料：CommonCrawl（840B tokens）、Wikipedia（6B tokens）等\n词汇量：根据语料大小从40万到200万不等\n向量维度：50到300维不等\n特点：结合了全局矩阵分解和局部上下文窗口方法的优点\n\n\n\n代码\n# 示例：加载GloVe词向量\nimport numpy as np\n\n# 下载地址：https://nlp.stanford.edu/projects/glove/\n# 假设已下载并解压到./data/glove.6B.300d.txt\ndef load_glove_vectors(file_path):\n    word_vectors = {}\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            word_vectors[word] = vector\n    return word_vectors\n\nglove_vectors = load_glove_vectors('./data/glove.6B.300d.txt')\n\n# 计算词相似度（简化版）\ndef cosine_similarity(vec1, vec2):\n    dot = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    return dot / (norm1 * norm2)\n\n# 查找相似词（简化版，实际应用中需要更高效的实现）\ndef find_similar_words(word, vectors, topn=5):\n    if word not in vectors:\n        return []\n    target_vector = vectors[word]\n    similarities = []\n    for w, vec in vectors.items():\n        if w != word:\n            sim = cosine_similarity(target_vector, vec)\n            similarities.append((w, sim))\n    return sorted(similarities, key=lambda x: x[1], reverse=True)[:topn]\n\n# 输出与'finance'最相似的词\nsimilar_to_finance = find_similar_words('finance', glove_vectors, 5)\nprint(\"与'finance'最相似的词 (GloVe):\")\nfor word, similarity in similar_to_finance:\n    print(f\"{word}: {similarity:.4f}\")\n\n\n\n\n18.2.3 3. FastText\n由Facebook AI Research开发，在维基百科语料上训练。\n\n词汇量：约200万个词\n向量维度：300维\n特点：利用词的子词信息，能更好地处理罕见词和未登录词\n\n\n\n代码\n# 示例：加载FastText词向量\nfrom gensim.models.fasttext import FastText\n\n# 下载地址：https://fasttext.cc/docs/en/english-vectors.html\n# 假设已下载并解压到./data/wiki-news-300d-1M.vec\nfasttext_model = KeyedVectors.load_word2vec_format(\n    './data/wiki-news-300d-1M.vec'\n)\n\n# 查找相似词\nprint(\"与'finance'最相似的词 (FastText):\")\nfor word, similarity in fasttext_model.most_similar('finance', topn=5):\n    print(f\"{word}: {similarity:.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#预训练模型的表现比较",
    "href": "12_embedding.html#预训练模型的表现比较",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.3 预训练模型的表现比较",
    "text": "18.3 预训练模型的表现比较\n不同预训练词向量模型在各种任务上的表现各有优劣。下面我们将从多个维度对它们进行比较：\n\n18.3.1 1. 语义捕捉能力\n通过测试几组典型的语义关系来比较不同模型：\n\n\n代码\n# 语义关系测试：国家-首都\ndef test_capital_country_relation(model, language='en'):\n    if language == 'en':\n        # 英文测试对\n        test_pairs = [\n            ('france', 'paris'),\n            ('germany', 'berlin'),\n            ('japan', 'tokyo'),\n            ('china', 'beijing'),\n            ('italy', 'rome')\n        ]\n    else:\n        # 中文测试对\n        test_pairs = [\n            ('法国', '巴黎'),\n            ('德国', '柏林'),\n            ('日本', '东京'),\n            ('中国', '北京'),\n            ('意大利', '罗马')\n        ]\n    \n    # 测试向量关系\n    for country, capital in test_pairs:\n        try:\n            result = model.most_similar(\n                positive=[capital, 'country' if language == 'en' else '国家'],\n                negative=['capital' if language == 'en' else '首都'],\n                topn=1\n            )\n            print(f\"{capital} : {country} = {result[0][0]} : {result[0][1]:.4f}\")\n        except:\n            print(f\"无法测试 {country}-{capital} 关系\")\n            \n# 测试Google模型（英文）\nprint(\"Google News 词向量的国家-首都关系测试：\")\ntest_capital_country_relation(google_model)\n\n# 测试腾讯模型（中文）\nprint(\"\\n腾讯AI Lab 词向量的国家-首都关系测试：\")\ntest_capital_country_relation(tencent_model, language='zh')\n\n\n\n\n18.3.2 2. 领域适应性\n不同模型在特定领域（如金融、医疗、法律等）的表现评估：\n\n\n代码\n# 金融领域词汇测试\nfinancial_terms_en = ['stock', 'bond', 'market', 'investment', 'risk']\nfinancial_terms_zh = ['股票', '债券', '市场', '投资', '风险']\n\n# 测试英文模型\nprint(\"Google模型在金融领域词汇相似性：\")\nfor term in financial_terms_en:\n    try:\n        similar_words = google_model.most_similar(term, topn=3)\n        print(f\"{term}: {', '.join([f'{w}({s:.2f})' for w, s in similar_words])}\")\n    except:\n        print(f\"{term}: 不在词汇表中\")\n\n# 测试中文模型\nprint(\"\\n腾讯模型在金融领域词汇相似性：\")\nfor term in financial_terms_zh:\n    try:\n        similar_words = tencent_model.most_similar(term, topn=3)\n        print(f\"{term}: {', '.join([f'{w}({s:.2f})' for w, s in similar_words])}\")\n    except:\n        print(f\"{term}: 不在词汇表中\")\n\n\n\n\n18.3.3 3. 处理未登录词能力\nFastText由于使用子词信息，对未登录词有独特优势：\n\n\n代码\n# 测试处理未登录词的能力\noov_words_en = ['cryptocurrencies', 'fintech', 'blockchain']\noov_words_zh = ['区块链', '数字货币', '智能投顾']\n\n# 对于FastText模型，即使词不在训练集中，也能生成向量\ndef test_oov_words(model, words):\n    for word in words:\n        try:\n            vector = model[word]\n            similar_words = model.most_similar(word, topn=3)\n            print(f\"{word}: 在词表中，相似词: {', '.join([w for w, _ in similar_words])}\")\n        except:\n            print(f\"{word}: 不在词表中\")\n\nprint(\"FastText对未登录词的处理能力：\")\ntest_oov_words(fasttext_model, oov_words_en)\n\n\n\n\n18.3.4 4. 多语言词向量对齐\n为了支持跨语言应用，可以将不同语言的词向量空间对齐：\n\n\n代码\n# 跨语言词向量对齐示例（概念演示）\ndef simple_translate(word, en_model, zh_model, en_to_zh_dictionary):\n    \"\"\"简化的跨语言词查找\"\"\"\n    if word in en_to_zh_dictionary:\n        return en_to_zh_dictionary[word]\n    \n    # 获取英文词向量\n    if word not in en_model:\n        return \"未找到英文词\"\n    \n    en_vector = en_model[word]\n    \n    # 在中文词空间中寻找最近的词\n    max_sim = -1\n    best_word = None\n    \n    # 实际应用中需要更高效的实现，这里仅为演示\n    for zh_word in list(zh_model.key_to_index.keys())[:1000]:  # 限制搜索范围\n        zh_vector = zh_model[zh_word]\n        sim = cosine_similarity(en_vector, zh_vector)\n        if sim &gt; max_sim:\n            max_sim = sim\n            best_word = zh_word\n    \n    return best_word if max_sim &gt; 0.5 else \"未找到匹配的中文词\"",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#预训练模型在不同任务中的选择指南",
    "href": "12_embedding.html#预训练模型在不同任务中的选择指南",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.4 预训练模型在不同任务中的选择指南",
    "text": "18.4 预训练模型在不同任务中的选择指南\n根据不同应用场景，我们推荐选择的预训练词向量模型：\n\n\n\n\n\n\n\n\n\n应用场景\n推荐中文模型\n推荐英文模型\n理由\n\n\n\n\n通用文本分类\n腾讯AI Lab\nGloVe 300d\n覆盖面广，向量维度适中\n\n\n命名实体识别\n哈工大词向量\nFastText\n对实体名称和罕见词有更好表现\n\n\n情感分析\n腾讯AI Lab\nGoogle News\n对语义细微差别表现更好\n\n\n专业领域(如金融)\n领域特定模型\n领域特定模型\n通用模型对专业术语表示不足\n\n\n处理网络文本\n搜狗新闻词向量\nFastText\n对网络流行语和新词表现更好",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#选择或训练自己的词向量模型",
    "href": "12_embedding.html#选择或训练自己的词向量模型",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.5 选择或训练自己的词向量模型",
    "text": "18.5 选择或训练自己的词向量模型\n在实际应用中，我们需要根据具体任务和数据特点选择合适的预训练模型或决定是否需要训练自己的模型：\n\n使用预训练模型的情况：\n\n数据量有限，无法支持有效训练\n任务是通用领域，预训练模型已足够好\n计算资源有限\n需要快速开发原型系统\n\n训练自己的模型的情况：\n\n有大量特定领域的文本数据\n应用领域有特殊术语或表达方式\n现有预训练模型表现不佳\n有足够的计算资源\n\n微调预训练模型的折中方案：\n\n从预训练模型开始，用领域数据继续训练\n保留通用语言知识，同时学习领域特定表示\n资源需求适中，效果通常不错",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#结论",
    "href": "12_embedding.html#结论",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "18.6 结论",
    "text": "18.6 结论\n预训练词向量模型为NLP任务提供了便捷的起点，无需从头训练就能获得高质量的词语表示。根据我们的测试，腾讯AI Lab词向量和GloVe模型在通用任务中表现最佳，而FastText在处理未登录词方面具有明显优势。\n对于金融文本分析，我们建议：如果数据量充足，可以在通用预训练模型基础上，使用金融领域文本进行进一步训练，以获得更符合领域特性的词向量表示；如果资源有限，可以选择腾讯AI Lab等高质量预训练模型作为基础，然后结合任务特点设计适当的特征工程。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#相同文本的两种表示方法",
    "href": "12_embedding.html#相同文本的两种表示方法",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.1 相同文本的两种表示方法",
    "text": "19.1 相同文本的两种表示方法\n首先，让我们回顾两种方法对同一文档的不同表示方式：\n\n\n代码\nimport pandas as pd\nimport numpy as np\nimport jieba\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n\n# 加载政府工作报告数据\ngovreport = pd.read_csv(\"labs/NLP/data/govreport.csv\")\n\n# 加载停用词\nwith open(\"labs/NLP/data/ChineseStopWords.txt\", 'r', encoding='utf-8') as f:\n    stop_words = {line.strip() for line in f}\n\n# 文本清洗与分词函数\ndef preprocess_text(text):\n    # 去除标点符号和数字\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', text)\n    # 分词\n    words = jieba.cut(text)\n    # 去除停用词和空白\n    return [word for word in words if word.strip() and word not in stop_words]\n\n# 处理所有文档\ncorpus = []\ncorpus_raw = []  # 用于TF-IDF\nyears = []\nfor _, row in govreport.iterrows():\n    year = row['Year']\n    text = row['texts']\n    words = preprocess_text(text)\n    corpus.append(words)\n    corpus_raw.append(' '.join(words))\n    years.append(year)\n\n# 1. TF-IDF表示\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(corpus_raw)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n\n# 2. Word2Vec表示\nw2v_model = Word2Vec(\n    corpus, \n    vector_size=100,\n    window=5,\n    min_count=2,\n    sg=1,  # 使用Skip-gram\n    epochs=10\n)\n\n# 文档向量化（取平均）\ndef get_doc_vector(doc_words, model):\n    # 过滤不在模型中的词\n    doc_words = [word for word in doc_words if word in model.wv]\n    if len(doc_words) == 0:\n        return np.zeros(model.vector_size)\n    # 计算词向量的平均\n    return np.mean([model.wv[word] for word in doc_words], axis=0)\n\n# 计算每个文档的词向量表示\ndoc_vectors_w2v = [get_doc_vector(doc, w2v_model) for doc in corpus]\n\n# 打印示例\nreport_idx = 2  # 2021年报告\nprint(f\"{years[report_idx]}年政府工作报告的不同表示方法:\")\nprint(\"\\nTF-IDF表示（稀疏向量，只显示前10个非零元素）:\")\ntfidf_vec = tfidf_matrix[report_idx].toarray()[0]\nnonzero_idxs = np.nonzero(tfidf_vec)[0][:10]\nfor idx in nonzero_idxs:\n    print(f\"{tfidf_feature_names[idx]}: {tfidf_vec[idx]:.4f}\")\n\nprint(\"\\nWord2Vec表示（密集向量，显示前10个维度）:\")\nw2v_vec = doc_vectors_w2v[report_idx]\nfor i in range(10):\n    print(f\"维度{i+1}: {w2v_vec[i]:.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#文本相似度计算比较",
    "href": "12_embedding.html#文本相似度计算比较",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.2 文本相似度计算比较",
    "text": "19.2 文本相似度计算比较\n词频法和词向量在计算文档相似度时有明显差异：\n\n\n代码\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. 基于TF-IDF的相似度\ntfidf_similarity = cosine_similarity(tfidf_matrix)\n\n# 2. 基于Word2Vec的相似度\nw2v_similarity = cosine_similarity(doc_vectors_w2v)\n\n# 创建相似度对比DataFrame\nsimilarity_comparison = pd.DataFrame({\n    'Years': years,\n    'TF-IDF Similarity with 2021': tfidf_similarity[2],  # 以2021年为参考\n    'Word2Vec Similarity with 2021': w2v_similarity[2]   # 以2021年为参考\n})\n\n# 打印相似度比较\nprint(\"不同年份与2021年报告的相似度比较:\")\nprint(similarity_comparison)\n\n# 可视化比较\nplt.figure(figsize=(12, 6))\nplt.bar(similarity_comparison['Years'], similarity_comparison['TF-IDF Similarity with 2021'], \n        alpha=0.7, label='TF-IDF相似度')\nplt.bar(similarity_comparison['Years'], similarity_comparison['Word2Vec Similarity with 2021'], \n        alpha=0.7, label='Word2Vec相似度', color='orange')\nplt.axhline(y=1, color='r', linestyle='--', alpha=0.3)  # 2021年与自身对比线\nplt.xlabel('年份')\nplt.ylabel('与2021年报告的相似度')\nplt.title('词频法与词向量的文本相似度比较')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\n\n\n相似度计算结果分析:\n\nTF-IDF相似度：\n\n基于词汇重叠度，相同词汇出现越多，相似度越高\n对关键词敏感，但没有语义理解\n相似度变化跨度较大，更容易区分文档\n\nWord2Vec相似度：\n\n基于语义空间的接近程度，能捕捉同义词和相关概念\n所有文档相似度普遍较高，因为它们都处于相似的语义空间\n变化更平滑，体现语义连续性\n\n\n例如，即使2020年疫情报告的词汇与其他年份有明显不同，但在Word2Vec中相似度依然较高，因为整体语义主题（政府工作）是相似的。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#关键词提取比较",
    "href": "12_embedding.html#关键词提取比较",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.3 关键词提取比较",
    "text": "19.3 关键词提取比较\n两种方法在关键词提取上也有明显不同：\n\n\n代码\n# 1. TF-IDF关键词提取\ndef get_tfidf_top_words(tfidf_matrix, feature_names, doc_idx, top_n=10):\n    tfidf_vec = tfidf_matrix[doc_idx].toarray()[0]\n    top_idxs = tfidf_vec.argsort()[-top_n:][::-1]\n    return [(feature_names[idx], tfidf_vec[idx]) for idx in top_idxs]\n\n# 2. Word2Vec关键词提取 (基于词向量中心性)\ndef get_w2v_central_words(doc_words, model, top_n=10):\n    # 只考虑模型中有的词\n    doc_words = [w for w in doc_words if w in model.wv]\n    if len(doc_words) == 0:\n        return []\n    \n    # 计算每个词与文档其他词的平均相似度 (中心性)\n    word_centrality = {}\n    for word in doc_words:\n        # 与其他词的相似度之和\n        total_sim = sum(model.wv.similarity(word, other_word) \n                        for other_word in doc_words if other_word != word)\n        # 平均相似度\n        word_centrality[word] = total_sim / (len(doc_words) - 1) if len(doc_words) &gt; 1 else 0\n    \n    # 返回中心性最高的词\n    return sorted(word_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n# 比较2023年报告的关键词提取结果\nreport_idx = 4  # 2023年\nprint(f\"{years[report_idx]}年政府工作报告关键词提取比较:\")\n\ntfidf_top_words = get_tfidf_top_words(tfidf_matrix, tfidf_feature_names, report_idx)\nprint(\"\\nTF-IDF提取的关键词:\")\nfor word, score in tfidf_top_words:\n    print(f\"{word}: {score:.4f}\")\n\nw2v_top_words = get_w2v_central_words(corpus[report_idx], w2v_model)\nprint(\"\\nWord2Vec提取的关键词 (基于中心性):\")\nfor word, score in w2v_top_words:\n    print(f\"{word}: {score:.4f}\")\n\n\n关键词提取结果分析:\n\nTF-IDF关键词：\n\n提取文档中特有的、区分度高的词\n往往是该文档特有的专有名词或低频词\n关注”独特性”而非”重要性”\n\nWord2Vec关键词：\n\n提取文档的语义中心词，与其他词语义联系最紧密的词\n往往是文档主题的核心词，且在语义网络中起枢纽作用\n关注”中心性”而非”频率”或”独特性”\n\n\n例如，TF-IDF可能会提取”十四五”这样的特定术语，而Word2Vec可能会提取”发展”这样的核心概念词。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#语义关联发现比较",
    "href": "12_embedding.html#语义关联发现比较",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.4 语义关联发现比较",
    "text": "19.4 语义关联发现比较\n词频法和词向量在发现词语关联关系上有本质区别：\n\n\n代码\n# 1. 词频法的共现分析\nfrom collections import Counter\nimport networkx as nx\n\ndef build_cooccurrence_network(corpus, target_words, window_size=5):\n    \"\"\"构建目标词的共现网络\"\"\"\n    cooccur = Counter()\n    \n    for doc in corpus:\n        for i, word in enumerate(doc):\n            if word in target_words:\n                # 获取窗口内的词\n                context = doc[max(0, i-window_size):i] + doc[i+1:min(len(doc), i+window_size+1)]\n                for context_word in context:\n                    if context_word in target_words and context_word != word:\n                        # 记录共现次数\n                        pair = tuple(sorted([word, context_word]))\n                        cooccur[pair] += 1\n    \n    # 创建网络\n    G = nx.Graph()\n    for word in target_words:\n        G.add_node(word)\n    \n    # 添加边和权重\n    for (word1, word2), weight in cooccur.items():\n        if weight &gt; 0:\n            G.add_edge(word1, word2, weight=weight)\n    \n    return G, cooccur\n\n# 2. Word2Vec的语义相似性网络\ndef build_semantic_network(model, target_words, threshold=0.3):\n    \"\"\"构建基于词向量相似度的语义网络\"\"\"\n    G = nx.Graph()\n    \n    # 过滤不在模型中的词\n    valid_words = [w for w in target_words if w in model.wv]\n    \n    for word in valid_words:\n        G.add_node(word)\n    \n    # 添加边和权重\n    for i, word1 in enumerate(valid_words):\n        for word2 in valid_words[i+1:]:\n            similarity = model.wv.similarity(word1, word2)\n            if similarity &gt; threshold:\n                G.add_edge(word1, word2, weight=similarity)\n    \n    return G\n\n# 分析目标词之间的关系\ntarget_words = ['发展', '经济', '创新', '改革', '开放', '就业', '民生', '环保', '科技', '数字']\n\n# 1. 创建共现网络\ncooccur_net, cooccur_counter = build_cooccurrence_network(corpus, target_words)\n\n# 2. 创建语义网络\nsemantic_net = build_semantic_network(w2v_model, target_words)\n\n# 共现矩阵可视化\ncooccur_matrix = np.zeros((len(target_words), len(target_words)))\nfor i, word1 in enumerate(target_words):\n    for j, word2 in enumerate(target_words):\n        if i != j:\n            pair = tuple(sorted([word1, word2]))\n            cooccur_matrix[i, j] = cooccur_counter[pair]\n\n# 语义相似度矩阵\nsemantic_matrix = np.zeros((len(target_words), len(target_words)))\nfor i, word1 in enumerate(target_words):\n    for j, word2 in enumerate(target_words[i+1:], i+1):\n        if word1 in w2v_model.wv and word2 in w2v_model.wv:\n            similarity = w2v_model.wv.similarity(word1, word2)\n            semantic_matrix[i, j] = semantic_matrix[j, i] = similarity\n\n# 可视化比较\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# 共现网络热图\nim1 = axes[0].imshow(cooccur_matrix, cmap='Blues')\naxes[0].set_title('词频法：共现关系')\naxes[0].set_xticks(range(len(target_words)))\naxes[0].set_yticks(range(len(target_words)))\naxes[0].set_xticklabels(target_words)\naxes[0].set_yticklabels(target_words)\nplt.colorbar(im1, ax=axes[0], label='共现次数')\n\n# 语义网络热图\nim2 = axes[1].imshow(semantic_matrix, cmap='Reds')\naxes[1].set_title('Word2Vec：语义相似度')\naxes[1].set_xticks(range(len(target_words)))\naxes[1].set_yticks(range(len(target_words)))\naxes[1].set_xticklabels(target_words)\naxes[1].set_yticklabels(target_words)\nplt.colorbar(im2, ax=axes[1], label='余弦相似度')\n\nplt.tight_layout()\nplt.show()\n\n\n语义关联分析结果比较:\n\n词频法（共现分析）：\n\n基于词语在文本中出现的物理距离\n只能发现直接共现的关系，不能泛化\n需要大量文本才能得到可靠的统计结果\n无法发现从未共现但语义相关的词\n\nWord2Vec（语义网络）：\n\n基于分布式表示学习的语义空间距离\n能发现间接关联，即使两个词从未共现\n能发现语义层次和类比关系\n受预训练语料的影响大\n\n\n例如，在政府工作报告中，“环保”和”低碳”可能很少直接共现，但在Word2Vec的语义空间中会很接近。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#主题和情感分析的差异",
    "href": "12_embedding.html#主题和情感分析的差异",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.5 主题和情感分析的差异",
    "text": "19.5 主题和情感分析的差异\n两种方法在主题建模和情感分析上也有明显区别：\n\n\n代码\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# 1. 基于词频的主题建模 (LDA)\n# 使用词频矩阵\ncv = CountVectorizer()\nbow_matrix = cv.fit_transform(corpus_raw)\nfeature_names = cv.get_feature_names_out()\n\n# LDA模型\nlda = LatentDirichletAllocation(n_components=3, random_state=42)\nlda.fit(bow_matrix)\n\n# 2. 基于Word2Vec的主题聚类\nfrom sklearn.cluster import KMeans\n\n# 对文档向量进行聚类\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(doc_vectors_w2v)\n\n# 输出LDA主题词\nprint(\"基于词频的LDA主题词:\")\nfor topic_idx, topic in enumerate(lda.components_):\n    print(f\"主题 #{topic_idx+1}:\")\n    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n    print(\", \".join(top_words))\n\n# 输出Word2Vec聚类中心最近的词\nprint(\"\\nWord2Vec聚类中心最近的词:\")\nfor i in range(3):\n    # 获取聚类中心\n    center = kmeans.cluster_centers_[i]\n    \n    # 找出与中心最接近的词\n    word_dists = []\n    for word in w2v_model.wv.index_to_key:\n        dist = np.linalg.norm(w2v_model.wv[word] - center)\n        word_dists.append((word, dist))\n    \n    top_words = sorted(word_dists, key=lambda x: x[1])[:10]\n    print(f\"聚类 #{i+1}:\")\n    print(\", \".join(word for word, _ in top_words))\n\n\n主题分析结果比较:\n\n基于词频的LDA：\n\n基于词语在文档中的共现统计构建主题\n主题是词语的概率分布，每个词有明确的主题概率\n结果可解释性强，但受制于表面统计\n\n基于Word2Vec的聚类：\n\n基于文档在语义空间中的分布进行聚类\n主题体现为语义空间中的区域，边界更加模糊\n能发现更抽象的语义关联，但解释性较弱",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#综合比较与应用建议",
    "href": "12_embedding.html#综合比较与应用建议",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "19.6 综合比较与应用建议",
    "text": "19.6 综合比较与应用建议\n通过以上案例比较，我们总结词频法和词向量在政府工作报告分析中的优缺点：\n\n\n\n比较维度\n词频法 (Bag of Words/TF-IDF)\n词向量 (Word2Vec)\n\n\n\n\n数据表示\n高维稀疏向量(~万维)\n低维稠密向量(~百维)\n\n\n语义捕捉\n基于表面词频统计，无语义\n基于分布式假设，有语义\n\n\n计算复杂度\n低，适合大规模文档\n中等，训练需要时间\n\n\n内存占用\n大（稀疏矩阵）\n小（稠密向量）\n\n\n新词处理\n无法处理未见词\n也无法直接处理(FastText可以)\n\n\n相似度计算\n仅基于词重叠\n基于语义相似\n\n\n关键词提取\n偏向特有词\n偏向中心词\n\n\n语义关联\n仅能发现共现关系\n能发现间接语义关联\n\n\n应用场景\n文档分类、信息检索\n语义搜索、推荐系统\n\n\n\n\n19.6.1 应用建议\n基于我们对政府工作报告的分析经验，针对不同任务推荐的方法：\n\n文档去重或精确匹配：使用词频表示\n文档语义检索或推荐：使用词向量表示\n特有术语或政策提取：使用TF-IDF方法\n政策主题语义聚类：使用Word2Vec\n综合分析：可以同时使用两种方法并结合结果\n\n在实际应用中，选择合适的文本表示方法往往取决于具体任务需求、可用资源和期望的结果特性。词频法和词向量并非互斥，而是互补的分析视角。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#词向量的优缺点",
    "href": "12_embedding.html#词向量的优缺点",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "20.1 词向量的优缺点",
    "text": "20.1 词向量的优缺点\n\n20.1.1 优点\n\n语义丰富：捕获了词语间的语义关系\n维度可控：典型地为50-300维，远低于词汇量大小\n泛化能力：能处理未见过的词组合\n通用性：可用于各种NLP任务的特征提取\n\n\n\n20.1.2 局限性\n\n多义词问题：无法区分同一个词的不同含义（如”苹果”可以是水果或公司）\n上下文依赖：固定的词向量无法根据上下文调整\n预训练依赖：需要大量语料预训练\n领域专一性：通用领域训练的词向量在专业领域效果可能不佳",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#进阶方向",
    "href": "12_embedding.html#进阶方向",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "20.2 进阶方向",
    "text": "20.2 进阶方向\n\n上下文相关的词表示：如ELMo、BERT等模型能根据上下文动态生成词表示\n多语言词向量：跨语言的词向量对齐，支持多语言应用\n领域适应：将通用词向量迁移到特定领域\n可解释性研究：理解词向量空间的维度含义",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "12_embedding.html#本讲小结",
    "href": "12_embedding.html#本讲小结",
    "title": "14  文本分析2：词向量与深度学习基础",
    "section": "20.3 本讲小结",
    "text": "20.3 本讲小结\n本讲我们从Bag of Words模型的局限性出发，介绍了词向量的概念、原理和应用：\n\n词向量通过低维稠密向量表示词语，克服了传统方法的局限\nWord2Vec通过Skip-gram和CBOW两种模型高效学习词向量\n负采样等技术大幅提高了训练效率\n词向量空间具有丰富的语义特性，支持相似性计算和向量代数运算\n在金融文本分析中，词向量可以发现政策热点、分析语义变化等",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>文本分析2：词向量与深度学习基础</span>"
    ]
  },
  {
    "objectID": "13_llm.html",
    "href": "13_llm.html",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "",
    "text": "16 从静态向量到动态表示",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#word2vec的局限性",
    "href": "13_llm.html#word2vec的局限性",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "16.1 Word2Vec的局限性",
    "text": "16.1 Word2Vec的局限性\n上一讲中，我们学习了Word2Vec等词向量技术，它通过分布式表示极大提升了NLP的表示能力。然而，静态词向量仍然存在明显局限性：\n\n一词一向量问题：每个词只对应一个固定的向量，无法处理一词多义。例如”苹果”在”我吃了一个苹果”和”苹果公司发布新产品”中的含义完全不同。\n上下文无关：词向量无法捕捉词语在特定上下文中的含义变化。例如”银行存款”和”河流的银行”中，“银行”的含义有很大差异。\n长距离依赖问题：无法捕捉句子中相距较远的词之间的依赖关系。例如”他说中文，因为他在中国生活了很多年”中，第二个”他”与第一个”他”指代相同。\n表达能力有限：固定维度的向量难以编码复杂的语言知识和语法结构。\n\n这些局限性促使研究者探索更先进的表示方法，能够根据上下文动态调整词语的表示。这一探索最终导致了BERT等基于Transformer的语言模型的诞生。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#上下文感知的词表示",
    "href": "13_llm.html#上下文感知的词表示",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "16.2 上下文感知的词表示",
    "text": "16.2 上下文感知的词表示\n上下文感知的词表示（Contextualized Word Representations）是指词语的向量表示会根据其所处的上下文动态变化。与静态词向量不同，它具有以下特点：\n\n动态表示：同一个词在不同上下文中具有不同的向量表示\n语义消歧：能够根据上下文区分多义词的不同含义\n句法感知：能够捕捉词语在句子中的句法功能\n长距离依赖：能够建模句子中远距离词语之间的关系\n\n这种表示方法的核心思想是：一个词的含义不仅取决于它自身，更取决于它的上下文环境。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#语言模型理解上下文的基础",
    "href": "13_llm.html#语言模型理解上下文的基础",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "16.3 语言模型：理解上下文的基础",
    "text": "16.3 语言模型：理解上下文的基础\n上下文感知表示的关键在于语言模型（Language Model）。语言模型是一种能够计算文本序列概率的模型，其基本任务是预测序列中的下一个词：\n\\[P(w_t | w_1, w_2, ..., w_{t-1})\\]\n不同类型的语言模型处理上下文的方式不同：\n\n传统n-gram语言模型：只考虑有限历史，如\\(P(w_t | w_{t-2}, w_{t-1})\\)\n循环神经网络(RNN)语言模型：通过隐藏状态递归编码全部历史\n双向语言模型：同时考虑左侧和右侧上下文\nTransformer语言模型：通过注意力机制直接建模所有位置间的依赖关系\n\n预训练语言模型的出现为NLP带来了革命性变化，它通过在大规模语料上无监督预训练，学习通用的语言表示，然后再针对下游任务进行微调。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#从elmo到bert的演进",
    "href": "13_llm.html#从elmo到bert的演进",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "16.4 从ELMo到BERT的演进",
    "text": "16.4 从ELMo到BERT的演进\n上下文感知的词表示技术的发展经历了几个里程碑：\n\n16.4.1 ELMo (2018)\nELMo (Embeddings from Language Models) 是上下文词表示的早期尝试，由Peters等人在2018年提出。其特点包括：\n\n使用双层双向LSTM结构\n将前向和后向语言模型结合\n使用不同层的表示的加权组合作为最终表示\n有效解决了一词多义问题\n\nELMo的表示公式为：\n\\[ELMo_k^{task} = E(R_k; \\Theta^{task}) = \\gamma^{task} \\sum_{j=0}^{L} s_j^{task} \\mathbf{h}_{k,j}^{LM}\\]\n其中，\\(\\mathbf{h}_{k,j}^{LM}\\)是第k个词在第j层的表示。\n\n\n16.4.2 GPT (2018)\nOpenAI的GPT (Generative Pre-Training) 模型采用了单向Transformer结构：\n\n仅使用前向语言模型（只看左侧上下文）\n基于Transformer解码器架构\n首次展示了大规模预训练+微调的范式\n\nGPT采用的预训练目标是预测下一个词：\n\\[L(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)\\]\n\n\n16.4.3 BERT (2018)\nBERT (Bidirectional Encoder Representations from Transformers) 由Google在2018年提出，成为上下文词表示的里程碑工作：\n\n使用双向Transformer编码器\n采用掩码语言模型(Masked LM)预训练\n同时使用下一句预测(NSP)任务\n极大提升了NLP任务的性能上限\n\nBERT的预训练目标是预测被掩码的词：\n\\[L(\\mathcal{D}) = \\sum_{i \\in \\mathcal{M}} \\log P(w_i | w_{\\neg \\mathcal{M}}; \\Theta)\\]\n其中，\\(\\mathcal{M}\\)是被掩码的词的位置集合。\n这一演进体现了以下趋势： - 从浅层网络到深层Transformer架构 - 从单向上下文到双向上下文 - 从特征提取器到通用语言模型 - 从任务相关到预训练-微调范式\n接下来，我们将深入理解BERT模型的内部工作原理。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#transformer架构bert的基础",
    "href": "13_llm.html#transformer架构bert的基础",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "17.1 Transformer架构：BERT的基础",
    "text": "17.1 Transformer架构：BERT的基础\nBERT建立在Transformer架构之上，这是由Vaswani等人在2017年提出的一种完全基于注意力机制的神经网络结构。在深入BERT之前，我们需要先理解Transformer的基本组件。\n\n17.1.1 自注意力机制\n自注意力（Self-Attention）是Transformer的核心组件，它允许模型在处理某个位置时，考虑序列中所有位置的信息。其计算过程如下：\n\n将输入向量\\(X\\)分别转换为查询(Query)、键(Key)和值(Value)三个矩阵： \\[Q = XW^Q, K = XW^K, V = XW^V\\]\n计算注意力得分并归一化： \\[Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\]\n其中，\\(\\sqrt{d_k}\\)是缩放因子，用于防止梯度消失。\n\n自注意力机制的优势在于： - 可以捕捉任意距离的依赖关系 - 计算复杂度相对RNN低 - 允许并行计算\n\n\n17.1.2 多头注意力\n为了增强模型的表示能力，Transformer使用了多头注意力（Multi-Head Attention）：\n\\[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\\]\n其中，\\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\\)\n多头注意力允许模型: - 在不同子空间中学习不同的关注模式 - 同时关注位置和语义信息 - 提供更丰富的特征表示\n\n\n17.1.3 位置编码\n由于自注意力机制本身不包含位置信息，Transformer引入了位置编码（Positional Encoding）来将序列顺序信息注入模型：\n\\[PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\\] \\[PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\\]\n其中，\\(pos\\)是位置，\\(i\\)是维度。\n\n\n17.1.4 前馈神经网络\nTransformer中每个子层还包含一个前馈神经网络（Feed-Forward Network），由两个线性变换组成：\n\\[FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\\]\n\n\n17.1.5 Transformer编码器结构\n一个完整的Transformer编码器层包含： 1. 多头自注意力机制 2. 层归一化（Layer Normalization） 3. 前馈神经网络 4. 残差连接（Residual Connection）\n这些组件按以下方式组合： \\[\\hat{h} = LayerNorm(x + MultiHeadAttention(x))\\] \\[h = LayerNorm(\\hat{h} + FFN(\\hat{h}))\\]\nBERT使用了Transformer的编码器部分，通常包含12层（BERT-Base）或24层（BERT-Large）。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#bert模型详解",
    "href": "13_llm.html#bert模型详解",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "17.2 BERT模型详解",
    "text": "17.2 BERT模型详解\nBERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，旨在学习深层的双向语言表示。\n\n17.2.1 BERT的输入表示\nBERT的输入由三种嵌入的总和组成：\n\n词嵌入（Token Embeddings）：WordPiece词表中的词元对应的嵌入\n段嵌入（Segment Embeddings）：区分句子对中的第一句和第二句\n位置嵌入（Position Embeddings）：表示词元在序列中的位置\n\n每个输入序列以特殊标记[CLS]开始，以[SEP]分隔不同句子。\n\n\n\nBERT输入表示\n\n\n\n\n17.2.2 BERT的预训练任务\nBERT通过两个无监督任务进行预训练：\n\n掩码语言模型（Masked Language Model，MLM）：\n\n随机掩盖输入中15%的词元\n其中80%用[MASK]替换，10%用随机词替换，10%保持不变\n训练模型预测被掩盖的原始词元\n这使得BERT能够学习双向上下文表示\n\n下一句预测（Next Sentence Prediction，NSP）：\n\n给定两个句子，预测第二句是否是第一句的真实后续\n训练数据中50%是真实的连续句子，50%是随机句子对\n这使得BERT能够理解句子间的关系\n\n\n\n\n17.2.3 BERT的模型变体\nBERT有两个主要变体：\n\nBERT-Base：\n\n12层Transformer编码器\n12个注意力头\n768维隐藏层\n1.1亿参数\n\nBERT-Large：\n\n24层Transformer编码器\n16个注意力头\n1024维隐藏层\n3.4亿参数\n\n\n\n\n17.2.4 BERT的微调方式\n预训练后的BERT可以通过简单的任务特定层进行微调，适用于多种下游任务：\n\n序列级任务（如分类）：使用[CLS]标记的最终隐藏状态\n词元级任务（如NER）：使用每个词元的最终隐藏状态\n句子对任务（如问答）：同时输入问题和段落，识别答案跨度\n\n微调过程通常只需要少量标注数据和训练轮次，极大地降低了NLP任务的门槛。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#bert的内部工作机制",
    "href": "13_llm.html#bert的内部工作机制",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "17.3 BERT的内部工作机制",
    "text": "17.3 BERT的内部工作机制\n通过深入分析BERT的内部表示，研究者发现BERT的不同层捕捉了不同类型的语言知识：\n\n17.3.1 层次化语言知识\n\n底层（1-4层）：捕捉表面语法特征、词性、局部依赖等\n中层（5-8层）：编码短语级语义和共指关系\n高层（9-12层）：处理长距离依赖和更抽象的语义关系\n\n\n\n17.3.2 注意力头的专业化\nBERT的不同注意力头专注于不同类型的语言信息：\n\n语法头：关注句法依赖关系\n语义头：关注语义相关的词\n共指头：关注指代同一实体的表达\n\n\n\n17.3.3 BERT的表示空间\nBERT的表示空间表现出interessting的性质：\n\n各向异性：嵌入向量集中在狭窄的锥体中，而非均匀分布\n语义区分：相似概念在表示空间中形成聚类\n线性结构：某些语义关系可以通过向量差来表示\n\n这些特性使得BERT能够有效地编码复杂的语言知识，并为下游任务提供丰富的特征表示。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#bert的后续演进",
    "href": "13_llm.html#bert的后续演进",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "17.4 BERT的后续演进",
    "text": "17.4 BERT的后续演进\nBERT发布后，研究者提出了许多改进版本，主要集中在以下几个方向：\n\n17.4.1 预训练任务优化\n\nRoBERTa：移除NSP任务，使用更大批量和更多数据训练\nALBERT：参数共享和分解嵌入，降低模型大小\nELECTRA：用判别式替换检测训练，提高效率\n\n\n\n17.4.2 知识增强\n\nKnowBERT：集成知识库信息\nERNIE：加入实体和短语级掩码\nFinBERT：针对金融领域的专业知识训练\n\n\n\n17.4.3 模型架构改进\n\nSpanBERT：掩盖连续的文本片段而非单个词\nXLNet：使用排列语言模型，解决掩码带来的预训练-微调不一致\nDeBERTa：解耦注意力机制，增强位置编码\n\n这些改进进一步推动了预训练语言模型的发展，为下一代更强大的模型如GPT系列奠定了基础。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#transformer架构的扩展",
    "href": "13_llm.html#transformer架构的扩展",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "18.1 Transformer架构的扩展",
    "text": "18.1 Transformer架构的扩展\n虽然BERT在NLP领域带来了巨大进步，但它仍然存在一些局限性，如无法进行生成任务和处理长文本。为了克服这些限制，研究者们对Transformer架构进行了多方面扩展。\n\n18.1.1 编码器-解码器结构\n编码器-解码器（Encoder-Decoder）结构是机器翻译等序列到序列任务的标准架构：\n\n编码器：处理输入序列，生成上下文表示\n解码器：基于编码器输出生成目标序列\n交叉注意力：解码器通过注意力机制访问编码器的输出\n\n代表模型： - T5：将所有NLP任务统一为文本到文本的转换 - BART：通过降噪自编码器预训练\n\n\n18.1.2 仅解码器架构\n仅解码器（Decoder-only）架构专注于生成任务，通过自回归方式预测下一个词：\n\n单向自注意力：每个位置只能看到其前面的位置\n自回归生成：逐词生成输出序列\n缩放规模：通过扩大模型规模提升能力\n\n代表模型： - GPT系列：从GPT-1到GPT-4，规模和能力不断增长 - LLaMA：开源的大型语言模型，有效降低了资源需求\n\n\n18.1.3 长距离建模\n处理长文本的能力是大语言模型的关键挑战之一，研究者提出了多种解决方案：\n\n稀疏注意力：如Longformer，只关注局部窗口和全局标记\n循环机制：如Transformer-XL，跨段传递隐藏状态\n线性复杂度：如Linformer，通过低秩近似降低计算量\n扩展上下文窗口：如DeepSeek，将上下文窗口扩展到128K",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#大型语言模型的关键创新",
    "href": "13_llm.html#大型语言模型的关键创新",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "18.2 大型语言模型的关键创新",
    "text": "18.2 大型语言模型的关键创新\n大型语言模型（LLMs）相比传统BERT模型有几个关键创新：\n\n18.2.1 规模扩展\n深度学习研究表明，模型规模与性能呈现”幂律”关系，增加参数量能带来显著性能提升：\n\n从亿到千亿参数：BERT-Large有3.4亿参数，而GPT-4估计有超过1万亿参数\n计算资源增长：训练大模型需要数千GPU/TPU，消耗数百万美元\n预训练数据扩展：从GB级语料到TB级语料\n\n\n\n18.2.2 涌现能力\n大语言模型最惊人的特性是涌现能力（Emergent Abilities）——在达到一定规模后突然出现的能力：\n\n指令跟随：理解并执行自然语言指令\n思维链推理：通过分步骤推理解决复杂问题\n上下文学习：从少量示例中学习新任务\n多模态理解：结合文本与图像等多种模态信息\n\n\n\n18.2.3 提示工程与思维链推理\n大语言模型的使用方式也发生了革命性变化：\n\n提示工程（Prompt Engineering）：\n\n通过精心设计的提示引导模型行为\n不同于传统的微调范式\n允许灵活调整模型输出\n\n思维链推理（Chain-of-Thought）：\n\n让模型先生成推理过程，再给出结论\n显著提高模型解决复杂问题的能力\n公式：\\(\\text{Prompt} + \\text{思考过程} \\to \\text{更准确的结果}\\)\n\n上下文学习（In-context Learning）：\n\n在提示中包含示例，引导模型学习模式\n无需参数更新，即可适应新任务\n示例：给出几个情感分类示例，模型可泛化到新文本",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#代表性大型语言模型",
    "href": "13_llm.html#代表性大型语言模型",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "18.3 代表性大型语言模型",
    "text": "18.3 代表性大型语言模型\n\n18.3.1 GPT系列\n由OpenAI开发的GPT（Generative Pre-trained Transformer）系列是大型语言模型的代表：\n\nGPT-1（2018）：\n\n1.17亿参数\n首次展示预训练+微调范式\n在多个NLP任务上获得突破\n\nGPT-2（2019）：\n\n15亿参数\n展示了零样本学习能力\n文本生成质量有显著提升\n\nGPT-3（2020）：\n\n1750亿参数\n展示了惊人的少样本学习能力\n可以执行之前未见过的任务\n\nGPT-4（2023）：\n\n参数规模未公开，估计超过1万亿\n多模态能力，支持图像输入\n接近人类专家水平的表现\n\n\n\n\n18.3.2 开源大型语言模型\n除了GPT系列，开源社区也开发了多种高性能大语言模型：\n\nLLaMA系列：\n\n由Meta AI开发\n参数规模从7B到65B不等\n性能接近闭源商业模型\n衍生了许多优秀模型如Vicuna和Alpaca\n\n国产大模型：\n\nChatGLM：清华大学与智谱AI合作开发的双语模型\nDeepSeek：深度求索开发，专注长序列处理\nQwen：阿里云开发，性能优异的开源模型\n\n多模态模型：\n\nCLIP：连接图像和文本的表示学习\nGPT-4V：具有视觉理解能力的GPT-4变体\nGemini：Google的多模态大语言模型",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#大语言模型的金融应用",
    "href": "13_llm.html#大语言模型的金融应用",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "18.4 大语言模型的金融应用",
    "text": "18.4 大语言模型的金融应用\n大语言模型在金融领域有广泛的应用潜力：\n\n18.4.1 信息提取与分析\n\n报告解析：\n\n自动提取财报中的关键财务指标\n总结长篇研报要点\n识别风险披露声明\n\n市场情感分析：\n\n分析新闻报道的市场情绪\n提取投资者情绪信号\n预测市场波动\n\n事件提取：\n\n从财经新闻中识别重大事件\n构建事件知识图谱\n分析事件之间的因果关系\n\n\n\n\n18.4.2 金融文本生成\n\n研究报告生成：\n\n基于数据自动生成财务分析\n创建行业趋势报告\n生成个股评论\n\n监管合规：\n\n生成合规声明和披露\n检查文档是否符合监管要求\n自动更新合规文件\n\n客户交互：\n\n智能金融顾问\n个性化投资建议\n金融知识普及\n\n\n\n\n18.4.3 无监督学习辅助\n\n文本聚类：\n\n通过嵌入向量聚类发现主题\n识别相似公告和报告\n发现市场关注热点\n\n异常检测：\n\n识别异常金融叙述\n发现财报中的可疑部分\n预警潜在风险信号\n\n主题提取：\n\n无监督发现文档主题\n总结长文本的核心观点\n追踪主题随时间的演变",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#bert与word2vec的实验对比",
    "href": "13_llm.html#bert与word2vec的实验对比",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "19.1 BERT与Word2Vec的实验对比",
    "text": "19.1 BERT与Word2Vec的实验对比\n在本节中，我们将使用政府工作报告数据，对比BERT与Word2Vec在文本表示上的差异。这一对比将帮助我们理解上下文感知表示相对于静态词向量的优势。\n\n19.1.1 数据准备与预处理\n与上一讲类似，我们首先加载和预处理政府工作报告数据：\n\n\n代码\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jieba\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nfrom transformers import BertModel, BertTokenizer\n\n# 加载政府工作报告数据\ngovreport = pd.read_csv(\"labs/NLP/data/govreport.csv\")\n\n# 设置中文显示\nplt.rcParams['font.sans-serif'] = ['Songti SC']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n# 加载停用词\nwith open(\"labs/NLP/data/ChineseStopWords.txt\", 'r', encoding='utf-8') as f:\n    stop_words = {line.strip() for line in f}\n\n# 文本清洗并分词\ndef preprocess_text(text):\n    # 去除标点符号和数字\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', text)\n    # 分词\n    words = jieba.cut(text)\n    # 去除停用词和空白\n    return [word for word in words if word.strip() and word not in stop_words]\n\n# 处理所有文档\ncorpus = []\nyears = []\nfor _, row in govreport.iterrows():\n    words = preprocess_text(row['texts'])\n    corpus.append(words)\n    years.append(row['Year'])\n\nprint(f\"语料库包含{len(corpus)}篇文档\")\n\n# 加载预训练的Word2Vec模型（假设我们已经训练好了，这里直接加载）\nfrom gensim.models import Word2Vec\nw2v_model = Word2Vec.load(\"gov_report_word2vec.model\")\n\n\n\n\n19.1.2 加载BERT模型\n我们将使用中文预训练BERT模型：\n\n\n代码\n# 加载预训练的中文BERT模型\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\nmodel = BertModel.from_pretrained('bert-base-chinese')\nmodel.eval()  # 设置为评估模式\n\n# 定义函数获取BERT词向量\ndef get_bert_embeddings(text, tokenizer, model, max_length=512):\n    # 对文本进行分词和转换\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length, padding='max_length')\n    \n    # 获取BERT输出\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # 提取最后一层的隐藏状态（词向量）\n    last_hidden_states = outputs.last_hidden_state\n    \n    # 提取[CLS]标记的向量作为句子表示\n    cls_embedding = last_hidden_states[:, 0, :].numpy()\n    \n    # 提取所有词元的向量\n    token_embeddings = last_hidden_states.numpy()\n    \n    return {\n        'cls_embedding': cls_embedding,  # 句子表示\n        'token_embeddings': token_embeddings,  # 词元表示\n        'tokens': tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])  # 对应的词元\n    }\n\n\n\n\n19.1.3 多义词表示对比\n我们将对比BERT和Word2Vec在处理多义词上的能力差异：\n\n\n代码\n# 选择一个多义词进行分析，如\"发展\"\nambiguous_word = \"发展\"\n\n# 从报告中找出含有该词的不同上下文\ncontexts = []\nfor i, doc in enumerate(corpus):\n    # 将分词列表转换为文本\n    text = \"\".join(doc)\n    # 在原文中寻找该词\n    if ambiguous_word in text:\n        # 寻找包含目标词的短句\n        sentences = re.split(r'[，。！？；]', text)\n        for sentence in sentences:\n            if ambiguous_word in sentence and 10 &lt; len(sentence) &lt; 50:\n                contexts.append((years[i], sentence))\n                if len(contexts) &gt;= 5:  # 只取5个上下文示例\n                    break\n    if len(contexts) &gt;= 5:\n        break\n\n# 1. Word2Vec的表示（静态）\nw2v_vector = w2v_model.wv[ambiguous_word]\n\n# 2. BERT的表示（动态，依赖上下文）\nbert_vectors = []\nfor year, context in contexts:\n    # 获取BERT表示\n    bert_result = get_bert_embeddings(context, tokenizer, model)\n    # 找到目标词的位置\n    target_positions = [i for i, token in enumerate(bert_result['tokens']) \n                        if ambiguous_word in token and '##' not in token]\n    \n    if target_positions:\n        # 提取目标词在该上下文中的表示\n        target_idx = target_positions[0]\n        bert_vector = bert_result['token_embeddings'][0, target_idx, :]\n        bert_vectors.append((year, context, bert_vector))\n\n# 计算BERT表示之间的相似度\nif bert_vectors:\n    bert_similarities = np.zeros((len(bert_vectors), len(bert_vectors)))\n    for i in range(len(bert_vectors)):\n        for j in range(len(bert_vectors)):\n            bert_similarities[i, j] = cosine_similarity(\n                bert_vectors[i][2].reshape(1, -1), \n                bert_vectors[j][2].reshape(1, -1)\n            )[0, 0]\n    \n    # 可视化BERT表示的相似度矩阵\n    plt.figure(figsize=(10, 8))\n    plt.imshow(bert_similarities, cmap='YlOrRd')\n    plt.colorbar(label='余弦相似度')\n    plt.title(f'\"{ambiguous_word}\"在不同上下文中的BERT表示相似度')\n    \n    # 设置坐标轴标签\n    context_labels = [f\"{year}: {context[:10]}...\" for year, context, _ in bert_vectors]\n    plt.xticks(range(len(context_labels)), context_labels, rotation=45, ha='right')\n    plt.yticks(range(len(context_labels)), context_labels)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nWord2Vec中'{ambiguous_word}'的静态表示是固定的，无法区分不同上下文\")\n    print(f\"而BERT可以为'{ambiguous_word}'在不同上下文中生成不同的表示\")\n    print(\"相似度矩阵展示了这些表示之间的差异，相似度较低的对应不同语义用法\")\n\n\n\n\n19.1.4 生成句子向量比较\n我们将对比Word2Vec和BERT生成的句子向量在文本相似度任务上的表现：\n\n\n代码\n# 为每个报告生成句子向量\n# 1. 基于Word2Vec (简单平均)\ndef get_w2v_sentence_vector(tokens, model):\n    valid_tokens = [t for t in tokens if t in model.wv]\n    if not valid_tokens:\n        return np.zeros(model.vector_size)\n    return np.mean([model.wv[t] for t in valid_tokens], axis=0)\n\n# 2. 基于BERT的[CLS]标记\ndef get_bert_sentence_vector(text, tokenizer, model):\n    result = get_bert_embeddings(text, tokenizer, model)\n    return result['cls_embedding'][0]\n\n# 生成向量\nw2v_doc_vectors = [get_w2v_sentence_vector(doc, w2v_model) for doc in corpus]\nbert_doc_vectors = []\n\nfor i, _ in govreport.iterrows():\n    # 取报告的前512个字符（BERT输入长度限制）\n    text_sample = govreport.iloc[i]['texts'][:512]\n    bert_vec = get_bert_sentence_vector(text_sample, tokenizer, model)\n    bert_doc_vectors.append(bert_vec)\n\n# 计算报告之间的相似度\nw2v_similarities = cosine_similarity(w2v_doc_vectors)\nbert_similarities = cosine_similarity(bert_doc_vectors)\n\n# 可视化报告相似度矩阵比较\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# Word2Vec相似度矩阵\nim1 = axes[0].imshow(w2v_similarities, cmap='Blues')\naxes[0].set_title('Word2Vec: 报告相似度矩阵')\naxes[0].set_xticks(range(len(years)))\naxes[0].set_yticks(range(len(years)))\naxes[0].set_xticklabels(years)\naxes[0].set_yticklabels(years)\nplt.colorbar(im1, ax=axes[0])\n\n# BERT相似度矩阵\nim2 = axes[1].imshow(bert_similarities, cmap='Reds')\naxes[1].set_title('BERT: 报告相似度矩阵')\naxes[1].set_xticks(range(len(years)))\naxes[1].set_yticks(range(len(years)))\naxes[1].set_xticklabels(years)\naxes[1].set_yticklabels(years)\nplt.colorbar(im2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n19.1.5 关键词提取对比\n比较BERT和Word2Vec在关键词提取任务上的差异：\n\n\n代码\n# 基于BERT的关键词提取\ndef extract_bert_keywords(text, tokenizer, model, top_n=10):\n    # 将文本分成短句\n    sentences = re.split(r'[，。！？；]', text)\n    sentences = [s for s in sentences if len(s) &gt; 5]\n    \n    if not sentences:\n        return []\n    \n    # 对每个句子获取词元表示\n    all_tokens = []\n    all_embeddings = []\n    \n    for sentence in sentences[:20]:  # 限制句子数量\n        result = get_bert_embeddings(sentence, tokenizer, model)\n        tokens = result['tokens']\n        embeddings = result['token_embeddings'][0]\n        \n        # 过滤掉特殊标记和重复词\n        valid_indices = []\n        valid_tokens = []\n        for i, token in enumerate(tokens):\n            if token not in ['[CLS]', '[SEP]', '[PAD]'] and '##' not in token:\n                if token not in valid_tokens:\n                    valid_indices.append(i)\n                    valid_tokens.append(token)\n        \n        all_tokens.extend([tokens[i] for i in valid_indices])\n        all_embeddings.extend([embeddings[i] for i in valid_indices])\n    \n    if not all_tokens:\n        return []\n    \n    # 计算每个词与文档中心的余弦相似度\n    doc_center = np.mean(all_embeddings, axis=0)\n    token_scores = []\n    \n    for i, token in enumerate(all_tokens):\n        if token in stop_words or len(token) &lt; 2:\n            continue\n        score = cosine_similarity([all_embeddings[i]], [doc_center])[0][0]\n        token_scores.append((token, score))\n    \n    # 按分数排序并返回前N个关键词\n    unique_tokens = {}\n    for token, score in token_scores:\n        if token not in unique_tokens or score &gt; unique_tokens[token]:\n            unique_tokens[token] = score\n    \n    sorted_tokens = sorted(unique_tokens.items(), key=lambda x: x[1], reverse=True)\n    return sorted_tokens[:top_n]\n\n# 选择2023年报告进行分析\nreport_idx = govreport[govreport['Year'] == 2023].index[0]\nreport_text = govreport.loc[report_idx, 'texts']\nreport_tokens = corpus[report_idx]\n\n# Word2Vec关键词（使用上一讲的中心性方法）\ndef get_w2v_central_words(tokens, model, top_n=10):\n    # 过滤不在模型中的词\n    valid_tokens = [t for t in tokens if t in model.wv]\n    \n    if not valid_tokens:\n        return []\n    \n    # 计算中心性\n    word_centrality = {}\n    for word in set(valid_tokens):\n        if len(word) &lt; 2:  # 跳过单字词\n            continue\n        similarities = [model.wv.similarity(word, t) for t in valid_tokens if t != word]\n        if similarities:\n            word_centrality[word] = sum(similarities) / len(similarities)\n    \n    # 返回中心性最高的词\n    return sorted(word_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n# 提取关键词\nw2v_keywords = get_w2v_central_words(report_tokens, w2v_model)\nbert_keywords = extract_bert_keywords(report_text[:2000], tokenizer, model)  # 限制文本长度\n\n# 打印结果比较\nprint(\"Word2Vec提取的关键词:\")\nfor word, score in w2v_keywords:\n    print(f\"{word}: {score:.4f}\")\n\nprint(\"\\nBERT提取的关键词:\")\nfor word, score in bert_keywords:\n    print(f\"{word}: {score:.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#使用预训练模型进行主题分析",
    "href": "13_llm.html#使用预训练模型进行主题分析",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "19.2 使用预训练模型进行主题分析",
    "text": "19.2 使用预训练模型进行主题分析\n接下来，我们探索如何使用BERT进行政府工作报告的主题分析。\n\n19.2.1 基于BERT表示的文本聚类\n\n\n代码\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# 对BERT文档向量进行聚类\nn_clusters = 3\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\ncluster_labels = kmeans.fit_predict(bert_doc_vectors)\n\n# 将聚类结果与年份对应\ncluster_df = pd.DataFrame({\n    'Year': years,\n    'Cluster': cluster_labels\n})\n\n# 对结果进行可视化（使用PCA降维）\npca = PCA(n_components=2)\nbert_2d = pca.fit_transform(bert_doc_vectors)\n\nplt.figure(figsize=(12, 8))\nfor cluster in range(n_clusters):\n    # 找出属于当前簇的点\n    cluster_points = bert_2d[cluster_labels == cluster]\n    cluster_years = [years[i] for i in range(len(years)) if cluster_labels[i] == cluster]\n    \n    # 绘制点\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}', alpha=0.7)\n    \n    # 添加年份标签\n    for i, point in enumerate(cluster_points):\n        plt.annotate(cluster_years[i], xy=point, fontsize=10)\n\nplt.title('基于BERT表示的政府工作报告聚类')\nplt.xlabel('PCA维度1')\nplt.ylabel('PCA维度2')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 分析每个簇的主题特征\nfor cluster in range(n_clusters):\n    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster]\n    cluster_years = [years[i] for i in cluster_indices]\n    \n    print(f\"\\n簇 {cluster} 包含年份: {', '.join(map(str, cluster_years))}\")\n    \n    # 合并该簇的所有文档\n    cluster_text = \" \".join([govreport.iloc[i]['texts'][:500] for i in cluster_indices])\n    \n    # 提取该簇的主题词\n    cluster_keywords = extract_bert_keywords(cluster_text, tokenizer, model, top_n=15)\n    print(\"主题词:\")\n    for word, score in cluster_keywords:\n        print(f\"  {word}: {score:.4f}\")\n\n\n\n\n19.2.2 文本主题随时间的演变分析\n\n\n代码\n# 对每年报告提取主题词，分析主题随时间的演变\nyears_list = sorted(set(years))\nyearly_topics = {}\n\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts'][:2000]  # 限制长度\n    \n    # 提取主题词\n    topics = extract_bert_keywords(text, tokenizer, model, top_n=10)\n    yearly_topics[year] = topics\n\n# 跟踪某些关键词随时间的变化\nfocus_words = ['创新', '发展', '改革', '民生', '科技', '数字', '经济']\nword_trends = {word: [] for word in focus_words}\n\nfor year in years_list:\n    # 创建当年主题词的字典\n    year_word_scores = {word: score for word, score in yearly_topics[year]}\n    \n    # 记录焦点词的出现情况\n    for word in focus_words:\n        if word in year_word_scores:\n            word_trends[word].append((year, year_word_scores[word]))\n        else:\n            word_trends[word].append((year, 0))\n\n# 可视化关键词趋势\nplt.figure(figsize=(14, 8))\nfor word, trend in word_trends.items():\n    years_data = [t[0] for t in trend]\n    scores = [t[1] for t in trend]\n    plt.plot(years_data, scores, marker='o', linewidth=2, label=word)\n\nplt.title('政府工作报告中关键概念的重要性变化')\nplt.xlabel('年份')\nplt.ylabel('主题重要性得分')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.xticks(years_list)\nplt.tight_layout()\nplt.show()\n\n\n\n\n19.2.3 语义相似度异常点检测\n\n\n代码\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# 使用局部异常因子(LOF)算法检测异常点\nlof = LocalOutlierFactor(n_neighbors=2, contamination=0.1)\noutliers = lof.fit_predict(bert_doc_vectors)\n\n# 异常点是-1，正常点是1\nanomaly_indices = [i for i, label in enumerate(outliers) if label == -1]\nanomaly_years = [years[i] for i in anomaly_indices]\n\nprint(f\"检测到的异常报告: {', '.join(map(str, anomaly_years))}\")\n\n# 可视化异常点\nplt.figure(figsize=(12, 8))\nplt.scatter(bert_2d[:, 0], bert_2d[:, 1], c=['red' if x == -1 else 'blue' for x in outliers], \n            alpha=0.7, label=['Anomaly' if x == -1 else 'Normal' for x in outliers])\n\n# 添加年份标签\nfor i, point in enumerate(bert_2d):\n    plt.annotate(years[i], xy=point, fontsize=10)\n\nplt.title('政府工作报告语义异常检测')\nplt.xlabel('PCA维度1')\nplt.ylabel('PCA维度2')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 分析异常报告的特点\nif anomaly_indices:\n    for idx in anomaly_indices:\n        year = years[idx]\n        print(f\"\\n{year}年报告被检测为异常，分析其特点:\")\n        \n        # 提取该报告的特有词汇\n        text = govreport.iloc[idx]['texts'][:2000]\n        keywords = extract_bert_keywords(text, tokenizer, model, top_n=10)\n        \n        print(\"特征词:\")\n        for word, score in keywords:\n            print(f\"  {word}: {score:.4f}\")\n        \n        # 计算与其他报告的平均相似度\n        similarities = []\n        for i in range(len(bert_doc_vectors)):\n            if i != idx:\n                sim = cosine_similarity([bert_doc_vectors[idx]], [bert_doc_vectors[i]])[0][0]\n                similarities.append((years[i], sim))\n        \n        avg_similarity = sum(sim for _, sim in similarities) / len(similarities)\n        print(f\"与其他报告的平均相似度: {avg_similarity:.4f}\")\n        \n        # 找出最不相似的报告\n        least_similar = min(similarities, key=lambda x: x[1])\n        print(f\"最不相似的报告是{least_similar[0]}年，相似度为{least_similar[1]:.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#使用finbert进行金融文本分类",
    "href": "13_llm.html#使用finbert进行金融文本分类",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "20.1 使用FinBERT进行金融文本分类",
    "text": "20.1 使用FinBERT进行金融文本分类\nFinBERT是一种针对金融领域进行微调的BERT变体，专门为金融文本分析而设计。\n\n20.1.1 安装与加载FinBERT\n\n\n代码\n# 安装所需库\n# !pip install transformers sentencepiece matplotlib\n# !pip install finbert\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 加载FinBERT模型\ntokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\nmodel.eval()  # 设置为评估模式\n\n\n\n\n20.1.2 金融情感分析\nFinBERT的一个主要用途是金融文本的情感分析，可以将文本分类为正面、负面或中性：\n\n\n代码\ndef analyze_sentiment(text, model, tokenizer):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    \n    # FinBERT情感标签: 0=negative, 1=neutral, 2=positive\n    sentiment_labels = [\"负面\", \"中性\", \"正面\"]\n    scores = predictions[0].numpy()\n    \n    results = []\n    for i in range(len(sentiment_labels)):\n        results.append((sentiment_labels[i], float(scores[i])))\n    \n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n# 测试几个金融相关的文本片段\ntest_texts = [\n    \"今年以来，我国经济稳中向好，经济增长好于预期，通胀水平保持稳定。\",\n    \"受外部需求减弱影响，出口增长放缓，企业经营压力加大，就业形势更加严峻。\",\n    \"科技创新成为经济高质量发展的强大动力，数字经济蓬勃发展。\",\n    \"金融风险明显增加，部分企业债务违约，需要加强风险防控。\",\n    \"资本市场改革持续推进，投资者信心有所恢复，市场预期逐步改善。\"\n]\n\n# 分析情感\nsentiments = []\nfor text in test_texts:\n    sentiment = analyze_sentiment(text, model, tokenizer)\n    sentiments.append(sentiment)\n    print(f\"文本: {text}\")\n    for label, score in sentiment:\n        print(f\"  {label}: {score:.4f}\")\n    print()\n\n# 可视化情感分析结果\nfig, ax = plt.subplots(figsize=(12, 8))\nx = np.arange(len(test_texts))\nwidth = 0.25\n\n# 提取各情感得分\nnegative_scores = [sentiment[2][1] for sentiment in sentiments]\nneutral_scores = [sentiment[1][1] for sentiment in sentiments]\npositive_scores = [sentiment[0][1] for sentiment in sentiments]\n\n# 绘制条形图\nax.bar(x - width, positive_scores, width, label='正面')\nax.bar(x, neutral_scores, width, label='中性')\nax.bar(x + width, negative_scores, width, label='负面')\n\nax.set_ylabel('情感得分')\nax.set_title('FinBERT金融文本情感分析')\nax.set_xticks(x)\nax.set_xticklabels([f'文本{i+1}' for i in range(len(test_texts))])\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n20.1.3 对政府工作报告进行金融情感分析\n接下来，我们使用FinBERT分析政府工作报告中的金融相关段落：\n\n\n代码\n# 从政府工作报告中提取金融相关段落\ndef extract_finance_paragraphs(text, keywords=None):\n    if keywords is None:\n        keywords = ['经济', '金融', '财政', '税收', '货币', '银行', '债务', '投资', \n                  '证券', '股市', '外汇', '通胀', '增长', '风险', '改革']\n    \n    # 将文本分成段落\n    paragraphs = text.split('\\n')\n    \n    # 过滤出含有金融关键词的段落\n    finance_paragraphs = []\n    for para in paragraphs:\n        if len(para) &lt; 10:  # 跳过短段落\n            continue\n        if any(keyword in para for keyword in keywords):\n            finance_paragraphs.append(para)\n    \n    return finance_paragraphs\n\n# 按年份分析政府工作报告的金融情感\nyearly_sentiments = {}\n\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取金融段落\n    finance_paras = extract_finance_paragraphs(text)\n    \n    # 不分析段落过少的年份\n    if len(finance_paras) &lt; 3:\n        continue\n    \n    # 分析每个段落的情感\n    para_sentiments = []\n    for para in finance_paras[:10]:  # 限制段落数量\n        sentiment = analyze_sentiment(para, model, tokenizer)\n        para_sentiments.append(sentiment)\n    \n    # 计算平均情感分数\n    avg_positive = np.mean([s[0][1] for s in para_sentiments])\n    avg_neutral = np.mean([s[1][1] for s in para_sentiments])\n    avg_negative = np.mean([s[2][1] for s in para_sentiments])\n    \n    yearly_sentiments[year] = {\n        \"正面\": avg_positive,\n        \"中性\": avg_neutral,\n        \"负面\": avg_negative\n    }\n\n# 可视化情感随时间的变化\nplt.figure(figsize=(14, 8))\nyears = sorted(yearly_sentiments.keys())\npositive_scores = [yearly_sentiments[year][\"正面\"] for year in years]\nneutral_scores = [yearly_sentiments[year][\"中性\"] for year in years]\nnegative_scores = [yearly_sentiments[year][\"负面\"] for year in years]\n\nplt.plot(years, positive_scores, 'g-', marker='o', linewidth=2, label='正面')\nplt.plot(years, neutral_scores, 'b-', marker='s', linewidth=2, label='中性')\nplt.plot(years, negative_scores, 'r-', marker='^', linewidth=2, label='负面')\n\nplt.title('政府工作报告金融段落情感变化趋势')\nplt.xlabel('年份')\nplt.ylabel('情感强度')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.xticks(years)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#使用通用大语言模型的零样本分类",
    "href": "13_llm.html#使用通用大语言模型的零样本分类",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "20.2 使用通用大语言模型的零样本分类",
    "text": "20.2 使用通用大语言模型的零样本分类\n除了专业领域模型，我们还可以利用通用大语言模型的强大能力进行零样本分类，无需额外训练。\n\n20.2.1 安装与设置大语言模型\n\n\n代码\n# 安装所需库\n# !pip install openai\n# !pip install deepseek\n\n# 导入必要的库\nimport openai\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom deepseek import DeepSeekAPI\n\n# 设置API密钥（请使用自己的API密钥）\n# openai.api_key = \"your-api-key-here\"  # GPT-4/3.5\n# deepseek_api = DeepSeekAPI(\"your-deepseek-api-key\")  # DeepSeek\n\n\n\n\n20.2.2 零样本文本分类\n我们可以使用大语言模型进行零样本分类，无需提供训练数据：\n\n\n代码\ndef classify_text_with_gpt(text, categories, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型进行零样本文本分类\"\"\"\n    prompt = f\"\"\"请将以下文本分类到这些类别之一: {', '.join(categories)}。\n    只需回复类别名称，不要添加任何解释或标点符号。\n    \n    文本: {text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个精确的文本分类助手。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,\n    )\n    \n    return response.choices[0].message.content.strip()\n\ndef classify_text_with_deepseek(text, categories, model=\"deepseek-chat\"):\n    \"\"\"使用DeepSeek模型进行零样本文本分类\"\"\"\n    prompt = f\"\"\"请将以下文本分类到这些类别之一: {', '.join(categories)}。\n    只需回复类别名称，不要添加任何解释或标点符号。\n    \n    文本: {text}\n    \"\"\"\n    \n    response = deepseek_api.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个精确的文本分类助手。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 示例：将政府工作报告段落分类为不同政策领域\npolicy_categories = [\"经济发展\", \"科技创新\", \"民生改善\", \"环境保护\", \"改革开放\", \"风险防控\"]\n\n# 从多个年份的报告中选取段落\nsample_paragraphs = []\nsample_years = [2019, 2020, 2021, 2022, 2023]\n\nfor year in sample_years:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取段落\n    paragraphs = [p for p in text.split('\\n') if len(p) &gt; 50 and len(p) &lt; 200]\n    if paragraphs:\n        # 随机选择一个段落\n        import random\n        selected_para = random.choice(paragraphs)\n        sample_paragraphs.append((year, selected_para))\n\n# 使用大语言模型进行分类\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\nclassification_results = []\nfor year, para in sample_paragraphs:\n    # category = classify_text_with_gpt(para, policy_categories)\n    category = classify_text_with_deepseek(para, policy_categories)\n    classification_results.append((year, para[:50] + \"...\", category))\n\n# 打印分类结果\nprint(\"大语言模型零样本分类结果:\")\nfor year, para_preview, category in classification_results:\n    print(f\"{year}年段落: {para_preview}\")\n    print(f\"分类: {category}\\n\")\n\"\"\"\n\n\n\n\n20.2.3 使用大模型生成金融文本摘要\n大语言模型在摘要生成方面表现出色，可以用于提取政府工作报告中的金融政策要点：\n\n\n代码\ndef generate_summary_with_gpt(text, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型生成摘要\"\"\"\n    prompt = f\"\"\"请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:\n\n    {text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的金融政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=300\n    )\n    \n    return response.choices[0].message.content.strip()\n\ndef generate_summary_with_deepseek(text, model=\"deepseek-chat\"):\n    \"\"\"使用DeepSeek模型生成摘要\"\"\"\n    prompt = f\"\"\"请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:\n\n    {text}\n    \"\"\"\n    \n    response = deepseek_api.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的金融政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=300\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 为近几年的报告生成金融政策摘要\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\npolicy_summaries = {}\nfor year in [2021, 2022, 2023]:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取金融段落\n    finance_paras = extract_finance_paragraphs(text)\n    finance_text = \"\\n\".join(finance_paras[:10])  # 限制输入长度\n    \n    # 生成摘要\n    # summary = generate_summary_with_gpt(finance_text)\n    summary = generate_summary_with_deepseek(finance_text)\n    policy_summaries[year] = summary\n\n# 打印摘要结果\nfor year, summary in policy_summaries.items():\n    print(f\"{year}年政府工作报告金融政策要点:\")\n    print(summary)\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\"\"\"",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#使用大语言模型进行高级文本分析",
    "href": "13_llm.html#使用大语言模型进行高级文本分析",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "20.3 使用大语言模型进行高级文本分析",
    "text": "20.3 使用大语言模型进行高级文本分析\n大语言模型还可以用于更复杂的无监督文本分析任务。\n\n20.3.1 提示式主题建模\n使用提示工程（Prompt Engineering）引导大语言模型进行主题发现：\n\n\n代码\ndef extract_topics_with_gpt(texts, n_topics=5, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型进行提示式主题建模\"\"\"\n    # 合并文本，但限制长度以适应API限制\n    combined_text = \"\\n\\n\".join(texts)\n    if len(combined_text) &gt; 4000:\n        combined_text = combined_text[:4000] + \"...\"\n    \n    prompt = f\"\"\"作为一个文本挖掘专家，请分析以下多个文本片段，识别其中的{n_topics}个主要主题。\n    每个主题请提供一个简短标题和3-5个关键词。\n    请仅输出主题和关键词，不要有其他解释。\n    \n    文本片段:\n    {combined_text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的文本挖掘专家。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.2,\n        max_tokens=500\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 从多个年份的报告中提取金融段落\nall_finance_paras = []\nfor year in years_list[-5:]:  # 只取最近5年\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    finance_paras = extract_finance_paragraphs(text)\n    all_finance_paras.extend(finance_paras[:5])  # 每年取5个段落\n\n# 使用大语言模型进行主题建模\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\ntopics = extract_topics_with_gpt(all_finance_paras)\nprint(\"大语言模型识别的主题:\")\nprint(topics)\n\"\"\"\n\n\n\n\n20.3.2 思维链分析\n使用思维链（Chain-of-Thought）技术引导大语言模型进行深度分析：\n\n\n代码\ndef analyze_with_cot(text, question, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用思维链技术进行深度分析\"\"\"\n    prompt = f\"\"\"请分析以下政府工作报告文本，回答问题。\n    请先逐步思考，然后给出最终答案。\n    \n    文本:\n    {text}\n    \n    问题: {question}\n    \n    逐步思考:\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=800\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 示例：分析最新报告中的政策转变\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\n# 获取2023年报告\nlatest_idx = govreport[govreport['Year'] == 2023].index[0]\nlatest_text = govreport.loc[latest_idx, 'texts']\n\n# 限制文本长度\nanalysis_text = latest_text[:3000]\n\n# 定义分析问题\nanalysis_question = \"这份政府工作报告对金融风险防控政策有哪些新的调整？这些调整与前几年相比有何变化？\"\n\n# 进行思维链分析\nanalysis_result = analyze_with_cot(analysis_text, analysis_question)\nprint(\"思维链分析结果:\")\nprint(analysis_result)\n\"\"\"\n\n\n\n\n20.3.3 嵌入空间与向量检索\n大语言模型的嵌入向量可用于高级语义检索：\n\n\n代码\ndef get_embedding_with_openai(text, model=\"text-embedding-ada-002\"):\n    \"\"\"获取OpenAI的文本嵌入向量\"\"\"\n    if not text.strip():\n        return np.zeros(1536)  # OpenAI embeddings are 1536-dimensional\n    \n    # 限制文本长度\n    if len(text) &gt; 8000:\n        text = text[:8000]\n    \n    response = openai.Embedding.create(\n        input=text,\n        model=model\n    )\n    \n    embedding = response['data'][0]['embedding']\n    return np.array(embedding)\n\n# 为政府工作报告段落生成嵌入向量\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\n# 准备段落\nall_paragraphs = []\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 分段\n    paragraphs = [p for p in text.split('\\n') if len(p) &gt; 50]\n    for para in paragraphs[:10]:  # 每年最多10个段落\n        all_paragraphs.append({\n            'year': year,\n            'content': para\n        })\n\n# 生成嵌入向量\nfor i, para in enumerate(all_paragraphs):\n    embedding = get_embedding_with_openai(para['content'])\n    all_paragraphs[i]['embedding'] = embedding\n    \n# 计算相似度矩阵\nn_paras = len(all_paragraphs)\nsimilarity_matrix = np.zeros((n_paras, n_paras))\n\nfor i in range(n_paras):\n    for j in range(n_paras):\n        if i == j:\n            similarity_matrix[i, j] = 1.0\n        else:\n            similarity_matrix[i, j] = cosine_similarity(\n                [all_paragraphs[i]['embedding']], \n                [all_paragraphs[j]['embedding']]\n            )[0, 0]\n\n# 找出每个段落最相似的其他段落\nfor i, para in enumerate(all_paragraphs):\n    similar_indices = np.argsort(similarity_matrix[i])[-3:-1][::-1]  # 排除自身，取前2个\n    \n    print(f\"{para['year']}年段落:\")\n    print(para['content'][:100] + \"...\\n\")\n    \n    print(\"最相似的段落:\")\n    for idx in similar_indices:\n        sim_para = all_paragraphs[idx]\n        sim_score = similarity_matrix[i, idx]\n        print(f\"- {sim_para['year']}年 (相似度: {sim_score:.4f}):\")\n        print(sim_para['content'][:100] + \"...\\n\")\n    \n    print(\"-\"*80 + \"\\n\")\n\"\"\"",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#应用大语言模型的最佳实践",
    "href": "13_llm.html#应用大语言模型的最佳实践",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "20.4 应用大语言模型的最佳实践",
    "text": "20.4 应用大语言模型的最佳实践\n在金融文本分析中应用大语言模型时，应注意以下几点：\n\n20.4.1 提示设计技巧\n\n明确任务界定：\n\n清晰指定分析目标和期望输出格式\n使用领域专业术语增强精确性\n\n思维链设计：\n\n引导模型分步思考复杂问题\n要求模型先分析再总结\n\n角色设定：\n\n指定模型扮演”金融分析师”等专业角色\n增强输出的专业性和针对性\n\n\n\n\n20.4.2 金融特定优化\n\n上下文补充：\n\n提供行业背景信息增强理解\n明确时间线帮助模型理解经济周期\n\n多模型比较：\n\n使用通用模型和金融专业模型对比结果\n综合优势获得更全面分析\n\n人机协作：\n\n将模型输出作为专业分析的起点\n关键决策仍需人类专家判断\n\n\n\n\n20.4.3 局限性与注意事项\n\n事实准确性：\n\n大语言模型可能产生”幻觉”，输出虚构内容\n关键数据和结论需要人工验证\n\n偏见风险：\n\n模型可能继承训练数据中的偏见\n金融分析需要客观中立\n\n时效性限制：\n\n模型知识截止日期后的事件需要通过提示补充\n定期更新分析以反映最新情况",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#从静态向量到大语言模型的演进",
    "href": "13_llm.html#从静态向量到大语言模型的演进",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "21.1 从静态向量到大语言模型的演进",
    "text": "21.1 从静态向量到大语言模型的演进\n本讲我们从Word2Vec的局限性出发，介绍了BERT等Transformer模型的原理，以及大语言模型的应用：\n\n表示方法演进：从静态词向量到上下文感知的动态表示\n架构演进：从浅层神经网络到深层Transformer架构\n规模演进：从百万参数到千亿参数\n应用演进：从特征提取到端到端文本理解与生成",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#无监督学习的新范式",
    "href": "13_llm.html#无监督学习的新范式",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "21.2 无监督学习的新范式",
    "text": "21.2 无监督学习的新范式\n大语言模型为无监督学习带来了新的范式：\n\n零样本学习：无需额外标注数据，直接分类新数据\n上下文学习：通过提示中的示例引导模型学习模式\n涌现能力：模型规模增长带来质的飞跃\n提示工程：通过设计提示引导模型行为",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#金融文本分析的未来方向",
    "href": "13_llm.html#金融文本分析的未来方向",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "21.3 金融文本分析的未来方向",
    "text": "21.3 金融文本分析的未来方向\n大语言模型在金融文本分析中的未来方向包括：\n\n多模态融合：结合文本、数值、图表等多种数据\n实时适应：持续学习最新市场信息和政策变化\n可解释性增强：提高模型决策的透明度\n领域知识增强：融入更多金融专业知识",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#进阶学习资源",
    "href": "13_llm.html#进阶学习资源",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "21.4 进阶学习资源",
    "text": "21.4 进阶学习资源\n\n理论深入：\n\n《Attention Is All You Need》 - Transformer原始论文\n《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》\n\n实践教程：\n\nHugging Face Transformers 库文档\nOpenAI GPT API 文档\n\n金融NLP资源：\n\nFinBERT 和 FinGPT 项目\n金融领域预训练模型集合",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "13_llm.html#本讲小结",
    "href": "13_llm.html#本讲小结",
    "title": "15  文本分析3：大语言模型及其应用",
    "section": "21.5 本讲小结",
    "text": "21.5 本讲小结\n本讲我们从Word2Vec的局限性出发，介绍了BERT和Transformer架构的原理，以及大语言模型在金融文本分析中的应用：\n\n从静态词向量到动态上下文表示的演进\nTransformer架构与自注意力机制的工作原理\nBERT等预训练模型的内部结构和应用方法\n大语言模型的关键创新与涌现能力\n实践案例：使用BERT和大语言模型分析政府工作报告\n\n通过这些内容，我们理解了现代NLP技术在金融文本分析中的强大能力，以及如何将这些技术应用于实际金融分析任务。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>文本分析3：大语言模型及其应用</span>"
    ]
  },
  {
    "objectID": "project1_LC.html",
    "href": "project1_LC.html",
    "title": "16  项目1：借贷违约风险评估",
    "section": "",
    "text": "16.1 项目背景\nLending Club（NYSE：LC）创立于2006年，是一家在线撮合借款和出借的P2P平台，公司位于美国旧金山。公司上线运营初期仅提供个人贷款（personal loans）服务，后增加了医疗信贷（patient loans）、车贷分期（auto refinancing loans）。2014年12月12日开始在纽交所挂牌交易，成为当年最大的科技股IPO，2014年前后公司增加小微企业贷（small business loans）服务。该公司报告称，截至2015年12月31日，已通过其平台发放了159.8亿美元的贷款。\n借款人可以在LC平台上申请1,000美元到40,000美元之间的无担保个人贷款。标准贷款期限为三年。投资者可以在LC网站上搜索和浏览贷款清单，并根据提供的有关借款人、贷款金额、贷款等级和贷款目的的信息选择他们想要投资的贷款。投资者从这些贷款的利息中获利。\nLC负责贷款的审批和定价，贷款对应票据凭证的发行，以及贷后月度收款付款以及逾期后催收等服务。贷款的实际发放者是一家注册在犹他州的商业银行Web Bank。贷款产生的违约风险、提前还款和再投资风险，都由投资者自行承担。\nLC自行开发了风险评估和定价模型。公司会采用来自两个以上信用局的FICO评分（由美国Fair Isaac公司开发出的个人信用评级法），有时候借款人满足以上所有要求，他们也可能被拒绝。LC可能会要求验证借款人的其他信息。虽然LC的贷款审批只需7天-14天，但目前只有10%的贷款申请被批准，约90%的贷款申请被拒绝。\nLending Club的收入来源为交易手续费、服务费和管理费。交易手续费是向借款人收取的费用；服务费是向投资者收取的费用；管理费是管理投资基金和其他管理账户的费用。交易手续费是Lending Club收入的主要来源。\n尽管被视为金融科技行业的先驱和最大的此类公司之一，LC在2016年初遇到了问题，难以吸引投资者，公司的一些贷款丑闻以及董事会对首席执行官雷诺拉普朗什披露信息的担忧导致其股价大幅下跌和拉普朗什辞职。\n2020年，LC收购了Radius Bank，并宣布将关闭其P2P借贷平台。现有账户持有人将继续对现有票据收取利息，直到每笔贷款还清或违约，但没有新贷款可用于个人投资。也不再可能像以前那样通过二级市场出售现有贷款。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目目标",
    "href": "project1_LC.html#项目目标",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.2 项目目标",
    "text": "16.2 项目目标\n本项目旨在利用Lending Club提供的历史贷款数据，构建机器学习模型以预测贷款是否会违约。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#数据简介",
    "href": "project1_LC.html#数据简介",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.3 数据简介",
    "text": "16.3 数据简介\nLending Club贷款数据，覆盖2007.6-2018.12时间段，包含以下几类信息：\n\n贷款基本信息：\n\nid：贷款唯一标识符\nissue_d：贷款发布时间\nloan_amnt：贷款金额\nterm：贷款期限（36或60个月）\nint_rate：贷款利率\ninstallment：每月还款额\ngrade & sub_grade：LC给出的信用评级\nloan_status：贷款状态（是否违约）\npurpose：贷款目的\n\n借款人信息：\n\nemp_title：工作职位\nemp_length：工作年限\nannual_inc：年收入\ndti：债务收入比(DTI)\nhome_ownership：房产拥有状态\n\n信用数据：\n\nfico_range_low & fico_range_high：FICO分数范围\nopen_acc：开放信用账户数\nrevol_bal：循环信用余额\nrevol_util：循环额度利用率\n\n\n原始数据集包含145个变量和约200万条记录。本项目将使用其中的子集进行分析。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#样本与变量选择",
    "href": "project1_LC.html#样本与变量选择",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.4 样本与变量选择",
    "text": "16.4 样本与变量选择\n\n时间范围：选择2013-2014年发放的、期限为3年的贷款数据。这些贷款在2018年底已全部结束，因此有完整的还款结果。\n特征选择原则：\n\n剔除所有贷后信息，因为这些信息在贷款发放时并不可得，包括：\n\n包含recover字段的变量（与回收相关）\n包含settlement字段的变量（与结算相关）\n包含pymnt字段的变量（与付款相关）\n以total_rec开始的变量（与收款总额相关）\n以out_prncp开始的变量（与未偿本金相关）\n\n只保留那些在贷款申请和审批过程中可获得的信息，以构建具有实际预测价值的模型",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目步骤建议仅供参考",
    "href": "project1_LC.html#项目步骤建议仅供参考",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.5 项目步骤建议（仅供参考）",
    "text": "16.5 项目步骤建议（仅供参考）\n\n16.5.1 数据清理与特征工程\n\n数据探索分析：\n\n计算各变量的基本统计量（均值、中位数、标准差等）\n检测并处理缺失值、异常值和不合法值\n分析目标变量的分布情况，评估类别不平衡程度\n\n数据预处理：\n\n缺失值处理：根据变量类型选择适当的填补方法（均值、中位数、众数或特殊值）\n异常值处理：识别并处理离群点（可使用箱线图、Z-score等方法）\n特征变换：将分类变量转换为哑变量，对数值变量进行标准化或归一化\n\n特征工程：\n\n特征选择：去除低方差特征、高度相关特征或具有较多缺失值的特征\n特征创建：根据业务理解创建新的特征（如各类比率、差值等）\n特征重要性评估：使用统计方法或模型预测能力评估特征重要性\n\n数据可视化：\n\n绘制变量分布图，分析变量与目标的关系\n使用散点图、热力图等展示变量间相关性\n生成变量重要性图表\n\n\n\n\n16.5.2 数据建模与模型评估\n\n数据集划分：\n\n训练集（60%）：用于模型训练\n验证集（20%）：用于超参数调优\n测试集（20%）：用于最终模型评估，模拟真实应用场景\n\n处理类别不平衡：\n\n尝试欠抽样（减少多数类样本）或过抽样（增加少数类样本）技术\n考虑SMOTE等合成样本生成方法\n调整类别权重或使用集成学习方法\n\n模型构建与选择： 尝试以下几种分类模型并进行比较：\n\nLogistic回归：基准模型，易于解释\n决策树：能够捕捉非线性关系，提供决策规则\n随机森林：降低过拟合风险，提高预测稳定性\n梯度提升树（如XGBoost、LightGBM）：通常具有较高的预测准确率\n\n模型调优：\n\n使用网格搜索或随机搜索方法确定最优超参数\n利用交叉验证评估模型稳定性\n根据验证集表现选择最佳模型配置\n\n模型评估：\n\n计算多种评估指标：\n\n混淆矩阵：TP、TN、FP、FN\n精度（Accuracy）：整体分类正确率\n查准率（Precision）：预测为违约中实际违约的比例\n查全率（Recall）：实际违约中被成功预测的比例\nF1分数：Precision和Recall的调和平均\nROC曲线与AUC值：评估模型在不同阈值下的性能\nKS统计量：衡量模型区分好坏客户的能力\n\n分析模型的业务价值：计算不同决策阈值下的潜在收益和损失\n\n模型解释：\n\n分析特征重要性\n部分依赖图或SHAP值分析，理解特征对预测的影响\n提出基于模型的业务洞见和建议",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#提交要求",
    "href": "project1_LC.html#提交要求",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.6 提交要求",
    "text": "16.6 提交要求\n\n项目报告：\n\n项目背景和目标的理解\n数据探索分析结果及发现\n特征工程和数据预处理的详细步骤\n模型构建、评估和比较的过程与结果\n最终模型的性能分析和业务意义解读\n项目总结与进一步改进建议\n报告长度建议不超过10页\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n说明对违约预测有显著影响的变量、特征工程、模型选择\n包含关键可视化图表\n演示时间控制在8分钟以内\n展示时间：2025年4月21日上课时\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据导入、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目报告和项目代码打包为一个ZIP文件\n报告提交截止日期：2025年5月5日23:59",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#评分标准",
    "href": "project1_LC.html#评分标准",
    "title": "16  项目1：借贷违约风险评估",
    "section": "16.7 评分标准",
    "text": "16.7 评分标准\n\n项目报告（40分）\n\n背景与目标理解：准确阐述项目背景、业务逻辑和预测目标\n数据探索分析：缺失值/异常值处理得当，可视化分析深入，数据分布描述清晰\n特征工程：特征选择合理，创造有效新特征，编码转换方法正确\n模型构建与评估：模型选择恰当，评估指标完整，对比分析深入\n总结建议：结论有数据支撑，改进建议具有实操性\n\n代码质量（30分）\n\n完整性：包含数据清洗、特征工程、建模评估全流程\n可重复性：代码可直接运行并复现结果\n规范性：代码结构清晰，有详细注释说明\n数据处理：缺失值/异常值处理逻辑正确\n模型实现：正确使用机器学习库，参数设置合理\n\n模型性能（20分）\n\n基准模型：实现合理的基准模型（如逻辑回归）\n优化模型：通过特征工程/参数调优显著提升性能\n模型对比：尝试3种以上模型并进行横向比较\n\n课堂展示（10分）\n\n内容组织：逻辑清晰，重点突出，时间控制得当\n可视化呈现：图表专业，信息传达有效\n问答环节：准确回答评委提问",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html",
    "href": "project2a_tspred.html",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "",
    "text": "17.1 项目背景\n上海证券交易所(上交所)成立于1990年11月26日，是中国大陆第一家证券交易所，与深圳证券交易所、北京证券交易所并称为中国大陆三大证券交易所。上证综指(Shanghai Composite Index)是以上海证券交易所挂牌上市的所有股票为样本，以发行量为权数计算的加权综合股价指数，是反映上海证券市场整体走势的重要指标。\n上证综指自1991年7月15日正式发布，基日为1990年12月19日，基点为100点。作为中国股市最具代表性的指数之一，上证综指的走势不仅反映了中国资本市场的整体状况，也在很大程度上反映了中国宏观经济的发展态势。\n金融时间序列预测一直是金融数据分析和量化投资领域的重要研究方向。相比传统的截面数据分析，时间序列数据具有明显的时序依赖性，这使得时间序列预测面临更多的挑战。特别是金融市场数据，其高波动性、非线性特征以及受多种复杂因素影响的特点，进一步增加了预测的难度。\n随着机器学习技术的发展，各种先进的预测方法被应用于金融时间序列分析，从传统的ARIMA、GARCH模型，到现代的支持向量机(SVM)、随机森林、深度学习网络等，为金融时间序列预测提供了更多可能性。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目目标",
    "href": "project2a_tspred.html#项目目标",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.2 项目目标",
    "text": "17.2 项目目标\n本项目旨在利用机器学习方法对上证综指的收益率进行时间序列预测。具体目标包括：\n\n理解并处理金融时间序列数据的特性，如平稳性、季节性、趋势等\n构建和评估不同的时间序列预测模型\n比较传统统计方法与现代机器学习方法在金融时间序列预测中的表现\n探索能够提高预测准确性的特征工程方法\n分析预测结果的经济意义和实际应用价值",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据简介",
    "href": "project2a_tspred.html#数据简介",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.3 数据简介",
    "text": "17.3 数据简介\n本项目将使用上证综指的历史数据，时间范围为2000年1月至2023年12月，包含以下几类信息：\n\n基础价格数据（CSMAR、WIND）：\n\nDate：交易日期\nOpen：开盘价\nHigh：最高价\nLow：最低价\nClose：收盘价\nAdj Close：调整后收盘价\nVolume：成交量\n\n派生指标（CSMAR、WIND）：\n\nReturns：日收益率（当日收盘价相对前一日收盘价的百分比变化）\nVolatility：基于历史窗口计算的波动率\n\n技术指标（需自行构建）：\n\n移动平均线(MA)：不同时间窗口的简单移动平均和指数移动平均\n相对强弱指数(RSI)\n布林带(Bollinger Bands)\nMACD(Moving Average Convergence Divergence)\n\n宏观经济数据（CSMAR、WIND）：\n\n中国GDP增长率\nCPI（消费者价格指数）\n利率\n汇率（美元/人民币）",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据特点与挑战",
    "href": "project2a_tspred.html#数据特点与挑战",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.4 数据特点与挑战",
    "text": "17.4 数据特点与挑战\n金融时间序列数据具有以下特点，这些都给预测带来了挑战：\n\n非平稳性：金融市场数据通常表现出非平稳特性，即统计特性随时间变化\n高噪声：市场价格受多种随机因素影响，含有大量噪声\n异方差性：金融数据的波动性往往会随时间聚集，表现为波动聚类现象\n胖尾分布：收益率分布常常表现出比正态分布更胖的尾部，意味着极端事件发生的概率更高\n长期记忆和短期记忆：金融时间序列可能同时表现出短期和长期的相关性特征",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目步骤建议仅供参考",
    "href": "project2a_tspred.html#项目步骤建议仅供参考",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.5 项目步骤建议（仅供参考）",
    "text": "17.5 项目步骤建议（仅供参考）\n\n17.5.1 数据预处理与探索性分析\n\n数据获取与清洗：\n\n下载上证综指历史数据\n处理缺失值（如节假日）\n检测并处理异常值\n确保时间索引的连续性和正确性\n\n探索性数据分析：\n\n绘制时间序列图，观察长期趋势和季节性模式\n计算并分析基本统计量（均值、标准差、偏度、峰度等）\n检验序列的平稳性（ADF检验、KPSS检验等）\n分析自相关性（ACF和PACF图）\n检测异方差性（ARCH效应检验）\n\n特征工程：\n\n构建基于不同滞后期的特征\n计算技术分析指标作为特征（如移动平均、相对强弱指数等）\n引入宏观经济变量作为外生变量\n特征选择与降维（如主成分分析）\n考虑时间结构特征（如一周中的某天、月份效应等）\n\n数据转换：\n\n差分变换以实现平稳性\n标准化或归一化处理\n考虑非线性变换（如对数变换）\n处理数据频率不匹配问题（如将宏观经济月度数据转换为日度数据）\n\n\n\n\n17.5.2 模型构建与评估\n\n传统时间序列模型：\n\n自回归模型(AR)\n移动平均模型(MA)\n自回归移动平均模型(ARMA)\n自回归积分移动平均模型(ARIMA)\n广义自回归条件异方差模型(GARCH)及其变种\n\n机器学习模型：\n\n支持向量回归(SVR)\n随机森林(RF)\n梯度提升树(如XGBoost、LightGBM)\nK近邻回归(KNN)\n神经网络模型(如MLP、RNN、LSTM)\n\n集成方法与混合模型：\n\n组合不同模型的预测结果\n构建混合模型融合时间序列模型和机器学习模型的优势\n\n交叉验证策略：\n\n使用时间序列交叉验证方法（如时间滚动窗口法）\n避免数据泄露\n考虑不同长度的训练窗口和预测窗口\n\n评估指标：\n\n均方误差(MSE)\n平均绝对误差(MAE)\n平均绝对百分比误差(MAPE)\n方向准确率（预测涨跌方向的准确度）\n信息系数(IC)和累积信息系数(CIC)\n\n\n\n\n17.5.3 预测结果分析与应用\n\n模型解释性分析：\n\n分析特征重要性\n理解模型学习到的模式\n分析不同市场条件下的预测表现\n\n交易策略构建：\n\n基于预测结果设计简单交易策略\n回测策略表现\n考虑交易成本和滑点\n计算策略风险调整后收益（如夏普比率）\n\n鲁棒性分析：\n\n在不同市场环境下测试模型性能\n分析模型对极端事件的适应能力\n考虑参数变化对预测结果的影响",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#提交要求",
    "href": "project2a_tspred.html#提交要求",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.6 提交要求",
    "text": "17.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据获取、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#评分标准",
    "href": "project2a_tspred.html#评分标准",
    "title": "17  项目2A：上证综指收益率时间序列预测",
    "section": "17.7 评分标准",
    "text": "17.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示数据分析、特征工程、模型构建和评估的完整流程\n技术理解：准确阐述金融时间序列特性及采用模型的优缺点\n结果分析：深入解读预测结果的经济意义和实际应用价值\n可视化呈现：包含时间序列分解图、ACF/PACF图、模型预测对比图等专业图表\n创新性：展示对模型局限性的思考或改进尝试\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含数据预处理、特征生成、模型训练、回测评估全流程\n时序处理：正确实现时间序列分割、滚动窗口验证，避免数据泄露\n可重复性：设置随机种子，保证结果可复现\n规范性：代码模块化设计，有清晰的函数注释和文档说明\n可视化：实现关键时序分析和预测结果的可视化功能\n\n模型性能（30分）\n\n基准模型：实现ARIMA/GARCH等传统时序模型作为基准\n优化模型：通过特征工程/模型融合提升预测精度\n模型多样性：至少包含3种不同类别模型（如ARIMA、LSTM、XGBoost）\n评估全面性：同时报告点预测精度（MSE）和方向准确性指标\n策略应用：设计基于预测结果的简单交易策略并评估其表现",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>项目2A：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2b_text.html",
    "href": "project2b_text.html",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "",
    "text": "18.1 项目背景\n随着数字化转型的加速，网络安全已成为现代企业面临的最重要风险之一。数据泄露、系统入侵、勒索软件攻击等安全事件不仅会导致直接的经济损失，还会损害企业声誉、客户信任，甚至引发监管处罚和法律诉讼。据相关统计，中国企业平均数据泄露成本持续上升，重大网络安全事件对企业造成的负面影响也日益加剧。\n中国证监会及相关监管机构近年来逐步加强了对上市公司信息披露的要求，特别是在企业风险管理方面。《公开发行证券的公司信息披露内容与格式准则》要求上市公司在年度报告中披露可能面临的各类风险，包括网络安全风险。2021年，《数据安全法》和《个人信息保护法》的实施进一步提高了企业对网络安全和数据保护的合规要求，使得上市公司在财务报表中更加详细地披露相关风险信息。\n这些文本披露为研究者提供了丰富的数据源，使我们能够通过自然语言处理和机器学习技术，从财务报表文本中提取、量化和分析企业的网络安全风险。这种基于文本的风险分析方法，不仅可以帮助投资者识别潜在的高风险企业，还可以帮助监管机构发现网络安全披露不足的公司，并为企业自身提供行业基准和改进方向。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目目标",
    "href": "project2b_text.html#项目目标",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.2 项目目标",
    "text": "18.2 项目目标\n本项目旨在通过对中国上市公司财务报表的文本分析，构建模型以评估和预测企业的网络安全风险水平。具体目标包括：\n\n掌握中文财务文本数据的获取和预处理方法\n学习和应用中文文本分析的核心技术，包括文本特征提取、情感分析和主题建模\n构建能够评估企业网络安全风险水平的预测模型\n分析不同行业、不同规模企业的网络安全风险特征和趋势\n探索网络安全风险与企业财务表现、股价波动等因素之间的关系",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据简介",
    "href": "project2b_text.html#数据简介",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.3 数据简介",
    "text": "18.3 数据简介\n本项目将使用中国A股上市公司的财务报表数据，时间范围为2018年至2023年，主要包含以下数据来源：\n\n上市公司公告文本（巨潮网）：\n\n年度报告文本\n半年度报告文本\n季度报告文本\n临时公告和重大事件公告\n\n网络安全事件数据（需自行获取）：\n\n国家互联网应急中心(CNCERT)发布的安全事件报告\n公开报道的重大网络安全事件\n行业安全报告中记录的数据泄露和系统入侵事件\n\n财务和市场数据（CSMAR、WIND）：\n\n股价和股票回报数据\n财务比率和业绩指标\n市值和行业分类\n\n文本特征数据（需自行构建）：\n\n中文网络安全相关词汇表和术语库\n行业特定风险指标\n中文情感和语调词典",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据特点与挑战",
    "href": "project2b_text.html#数据特点与挑战",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.4 数据特点与挑战",
    "text": "18.4 数据特点与挑战\n中文财务报表文本分析面临以下特点和挑战：\n\n中文文本特性：中文没有空格分词，需要特殊的分词技术；同时存在词义多样、同义词丰富等特点\n非结构化数据：财务报表文本是典型的非结构化数据，需要特殊的处理和特征提取方法\n专业术语丰富：财务和网络安全领域均有大量专业术语，普通NLP模型可能难以准确理解\n披露规范差异：与欧美市场相比，中国上市公司在风险披露方面的规范和实践存在差异\n行业差异：不同行业面临的网络安全风险类型和程度各不相同\n时间演变：网络安全威胁和披露要求随时间不断变化\n因果关系复杂：网络安全风险与企业表现之间的因果关系难以确定",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目步骤建议仅供参考",
    "href": "project2b_text.html#项目步骤建议仅供参考",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.5 项目步骤建议（仅供参考）",
    "text": "18.5 项目步骤建议（仅供参考）\n\n18.5.1 数据收集与预处理\n\n文本数据获取：\n\n从巨潮网下载上市公司年报、半年报、季报文件\n提取文件中与风险相关的章节（尤其是”风险因素”和”管理层讨论与分析”部分）\n识别并提取与网络安全相关的段落\n\n中文文本清洗与标准化：\n\n对PDF文件进行清洗\n中文分词处理\n去除停用词\n简繁体转换（如有必要）\n标点符号和特殊字符处理\n\n创建标记数据集：\n\n标记历史上发生过重大网络安全事件的企业\n根据事件严重程度和影响创建风险等级标签\n构建时间序列标签，反映风险随时间变化\n\n\n\n\n18.5.2 特征工程与文本分析\n\n网络安全关键词提取：\n\n基于行业专家知识和相关法规构建中文网络安全术语词典（如”数据泄露”、“勒索软件”、“网络攻击”等）\n计算网络安全相关术语的出现频率、密度和分布位置\n对比不同时期同一企业的网络安全关键词变化\n分析风险披露部分中网络安全词汇占比\n\n风险披露的复杂性与模糊性：\n\n计算风险描述的语言复杂度（如句长、复合句比例）\n分析模糊表述词汇使用（如”可能”、“或许”、“不确定”等）\n对比明确风险描述与模糊风险描述比例\n构建模糊性指数，衡量风险披露的具体程度\n\n财务报表特定结构特征：\n\n提取风险因素章节位置和长度\n分析网络安全风险在整体风险披露中的相对位置\n计算网络安全风险披露的篇幅占比\n构建章节重要性特征（如风险因素被放在报告前部的企业可能对风险更重视）\n\n\n\n\n18.5.3 模型构建与评估\n\n基础文本聚类分析：\n\n使用简单K-means聚类（sklearn库）将企业按风险披露特征分为几个组\n尝试不同的聚类数量（如3-5类），观察企业风险分组情况\n计算并可视化各聚类中心，理解不同企业组的风险特点\n使用词云图直观展示各类企业风险表述的关键词差异\n\n简单风险指标构建：\n\n统计每份报告中网络安全关键词出现的数量作为风险指标\n计算风险词汇占总词数的百分比，作为风险关注度指标\n设计简单的风险评分公式：结合关键词频率、模糊词使用程度等\n对比不同企业的风险指标，建立相对排名\n\n基础主题分析：\n\n使用Python的gensim库实现简单的LDA主题模型\n设置3-5个主题，识别网络安全风险中的主要话题\n分析不同企业的主题分布情况\n观察主题随时间的变化趋势\n\n简易验证方法：\n\n随机抽取样本进行人工检查，验证模型发现的模式是否合理\n与公开报道的网络安全事件进行对比（作为间接验证）\n绘制散点图，观察企业在风险指标上的分布是否符合预期\n使用统计检验（如t检验）比较不同行业企业的风险指标差异\n\n\n\n\n18.5.4 分析与应用\n\n跨行业比较分析：\n\n比较中国不同行业上市公司的网络安全风险特征\n识别高风险行业特征\n分析中国特色行业（如互联网、金融科技等）的风险特点\n\n风险与财务表现关系：\n\n分析网络安全风险与股价波动的关系\n研究风险披露与企业估值的关联\n测量网络安全事件对企业长期财务影响\n\n披露质量评估：\n\n评估中国企业网络安全风险披露的完整性\n分析披露内容与实际风险的一致性\n比较不同市场板块（如主板、科创板、创业板）企业的披露差异\n\n案例研究：\n\n选择典型企业进行深入分析\n研究重大网络安全事件前后的披露变化\n分析最佳实践和常见缺陷",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#提交要求",
    "href": "project2b_text.html#提交要求",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.6 提交要求",
    "text": "18.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表和案例分析\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码\n代码应包含从数据获取、中文文本处理、特征工程到后续应用的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#评分标准",
    "href": "project2b_text.html#评分标准",
    "title": "18  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "18.7 评分标准",
    "text": "18.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示中文文本数据获取、预处理、特征提取和风险评估的完整流程\n技术理解：准确阐述中文财务文本分析的特点和所采用NLP技术的优缺点\n结果分析：深入解读企业网络安全风险评估结果及其行业差异\n可视化呈现：包含关键词频统计图、主题分布图、风险评分对比图等专业可视化图表\n创新性：展示特色词典构建或针对中文财务文本的特殊处理方法\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含文本获取、中文分词、特征提取、模型构建和结果评估全流程\n中文处理：正确实现中文文本的分词、去停用词和特征提取流程\n可重复性：设置随机种子，保证结果可复现，包含必要的词典文件\n规范性：代码模块化设计，有清晰的函数注释和文档说明（特别是中文注释）\n可视化：实现文本分析关键结果的可视化功能，包括词云、主题分布等\n\n模型性能（30分）\n\n特征工程：构建全面的中文网络安全相关特征，包括关键词频率、语义特征等\n风险指标：成功构建有效的网络安全风险评估指标体系\n模型多样性：至少实现3种不同文本分析方法（如词频统计、主题模型、情感分析）\n评估全面性：从多角度（行业对比、时间趋势、案例分析）评估分析结果\n实际应用：将文本分析结果与企业财务表现或市场反应进行关联分析",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "review.html",
    "href": "review.html",
    "title": "19  《数据挖掘与机器学习》期末考试复习指南",
    "section": "",
    "text": "各位同学：\n大家好！本学期《数据挖掘与机器学习》课程即将结束，为帮助大家更好地准备期末考试，特整理本复习指南。期末考试旨在考察大家对课程核心理论知识的理解和应用能力。\n考试形式： 闭卷笔试 题型分布（参考）： 选择题、判断题、简答题、案例分析题 考试范围： 主要覆盖课程讲义及课堂讲解的核心内容。\n复习要点：\n一、 机器学习基础概念与流程 * 理解监督学习、无监督学习的基本概念和典型任务（分类、回归、聚类等）。 * 掌握典型的机器学习项目流程：问题定义 -&gt; 数据收集 -&gt; 数据预处理与特征工程 -&gt; 模型选择与训练 -&gt; 模型评估与调优 -&gt; 模型部署与监控。\n二、 数据预处理与特征工程 * 特征工程: 理解其在机器学习流程中的位置和重要性。 * 类别特征编码: 掌握常用方法，如独热编码（One-Hot Encoding）、标签编码（Label Encoding）的原理、优缺点和适用场景。 * 特征缩放: 了解为何需要特征缩放（如标准化、归一化），哪些算法对其敏感（如基于距离的算法、梯度下降优化的算法），哪些不太敏感（如基于树的模型）。\n三、 监督学习 * 算法原理与应用: * 逻辑回归: 理解其主要用途是解决二分类问题，能输出概率。 * 决策树: 了解其基本构建思想（基于信息增益或基尼不纯度等进行分裂）。 * 集成学习: * 理解Bagging（如随机森林基础）的基本思想（自助采样、并行训练、聚合结果）和主要目标（降低方差）。 * 理解Boosting（如GBDT）的基本思想（串行训练、关注错误/残差、加权组合）和基本工作原理。 * 掌握GBDT与随机森林在构建方式（串行 vs 并行）、依赖关系和目标侧重（降偏差 vs 降方差）上的主要区别。 * 神经网络: 了解隐藏层常用激活函数（如ReLU）的作用是引入非线性。 * 模型评估与选择: * 偏差-方差权衡: 理解偏差（Bias）、方差（Variance）的概念，及其与欠拟合（高偏差）、过拟合（高方差）的对应关系，掌握模型复杂度与两者权衡的基本思想。 * 评估指标: * 掌握准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数（F1-Score）的定义、计算方式和物理意义。 * 理解在类别不平衡数据场景下，准确率可能失效，为何需要重点关注Precision, Recall, F1等指标。 * 掌握ROC曲线绘制的是TPR vs FPR的关系，理解AUC值的含义（0.5代表随机，越接近1越好）。 * 验证方法: * 理解交叉验证（Cross-Validation，如K折）的基本原理和相比简单划分的优势（更稳健的评估、更有效利用数据）。 * 理解处理时间序列数据时为何不能用随机交叉验证（破坏时序性导致数据泄漏），掌握至少一种适用方法（如向前滚动划分/Walk-Forward）的原理与优缺点。 * 正则化: * 理解L1 (Lasso) 和 L2 (Ridge) 正则化的主要目的（作为一种模型复杂度控制手段，降低过拟合风险）。 * 了解L1正则化倾向于产生稀疏权重，具有特征选择的效果。\n四、 无监督学习及相关技术 * 聚类: * 理解聚类的核心目标（发现数据内在分组结构，使得簇内相似、簇间相异）。 * 了解聚类在金融客户分析中的应用（如客户细分及其价值）。 * （若涉及K-Means）了解其基本步骤和主要局限性（如需指定K值、对初始点敏感、假设球状簇等）。 * 降维: * 理解维度灾难（Curse of Dimensionality）的概念。 * 了解降维的主要目标（克服维度灾难、降低计算成本、去噪、可视化等）。 * 掌握经典的线性降维方法PCA的基本思想（最大化方差投影）。 * 能够识别常见的降维算法（如PCA, t-SNE, UMAP）并与其他类型算法（如KNN）区分开。 * 文本表示与分析基础: * 文本预处理: 理解分词（Tokenization）的基本概念。 * 文本表示方法: * 了解词袋模型（Bag-of-Words）及其忽略词序的主要局限性。 * 掌握TF-IDF的计算原理（TF * IDF）及其相比简单词频的优势（降低常见词权重，提升关键词权重）。了解其在金融文本分析中的应用。 * 了解词嵌入（Word Embeddings，如Word2Vec）的基本思想（将词映射为低维稠密向量，捕捉语义）。 * 主题模型: 了解其目标（从大量文档中发现隐藏的主题结构）。\n五、 复习建议 * 回归基础： 重点回顾和理解课程讲义及核心概念，确保基础扎实。 * 理解原理： 不仅要记住算法名称，更要理解其背后的基本原理、适用场景及优缺点。 * 联系实际： 思考课程中介绍的各种技术如何在金融场景（风控、量化、客户分析、文本处理等）中应用。 * 查漏补缺： 对照复习要点，检查自己是否有遗漏或理解不清的地方，及时复习巩固。\n预祝各位同学期末考试顺利，取得理想成绩！",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>《数据挖掘与机器学习》期末考试复习指南</span>"
    ]
  }
]