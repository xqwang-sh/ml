[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据挖掘与机器学习课程讲义",
    "section": "",
    "text": "本讲义将系统地介绍机器学习的基本概念、主要模型以及实践应用。\n引言:\n机器学习是人工智能的一个重要分支，它是指从数据中自动学习规律和模式，并利用这些规律和模式进行预测和决策的过程。机器学习在量化投资、金融科技等领域有广泛应用。由于课程时间有限，本讲义将重点介绍机器学习中的监督学习与无监督学习，以及其在金融预测中的应用。\n课程内容:\n\n机器学习基础\n金融数据获取与数据分析基础\n监督学习（上）\n监督学习（下）\n模型评估与优化\n\n课程项目:\n\n借贷违约风险评估\n\n下面两个项目二选一：\n2a. 股票价格预测 2b. 财务报表文本分析\n使用说明:\n\n本讲义使用 Quarto 创建，可以方便地生成 HTML, PDF, ePub 等多种格式。\n点击左侧导航栏可以浏览不同章节的内容。\n\n希望本讲义能帮助您更好地学习和理解机器学习！",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html",
    "href": "01_ml_basic.html",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1 机器学习简介",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习简介",
    "href": "01_ml_basic.html#机器学习简介",
    "title": "2  机器学习基础",
    "section": "",
    "text": "2.1.1 什么是机器学习？\n机器学习是人工智能领域中一个非常热门且快速发展的分支。简单来说，机器学习就是让计算机通过学习数据，而不是依赖明确的编程指令，来完成特定的任务或解决问题。想象一下，我们教小孩子认识猫和狗，不是告诉他们猫和狗的具体特征（比如多少根胡须，耳朵的形状），而是给他们看大量的猫和狗的图片，告诉他们哪些是猫，哪些是狗。通过不断学习，孩子就能自己总结出猫和狗的区别，并且能够识别新的猫和狗。机器学习的原理与之类似，它使用算法来解析数据，从中学习，然后利用学到的知识对新数据做出预测或决策。\n\n\n2.1.2 机器学习的主要特点\n\n数据驱动: 机器学习模型的核心是数据。模型从数据中学习规律，数据越多、质量越高，模型通常就越强大。\n自动学习: 机器学习系统能够自动地从数据中发现模式和规律，无需人工明确指定规则。\n持续优化: 机器学习模型可以通过不断学习新的数据来提升性能，使其能够适应变化的环境。\n泛化能力: 训练好的模型不仅能处理训练数据，还能对未见过的新数据进行预测或决策。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习的主要类型",
    "href": "01_ml_basic.html#机器学习的主要类型",
    "title": "2  机器学习基础",
    "section": "2.2 机器学习的主要类型",
    "text": "2.2 机器学习的主要类型\n\n2.2.1 监督学习 (Supervised Learning)\n监督学习就像是有一位老师（监督者）指导计算机学习。我们提供给计算机带有”标签”的数据，标签就是我们希望模型预测的答案。例如，如果我们想让模型识别图片中的水果是苹果还是香蕉，我们就需要提供大量已经标记好（苹果或香蕉）的水果图片给模型学习。模型学习的目标就是找到输入数据（水果图片）和输出标签（苹果或香蕉）之间的关系。\n主要特点:\n\n需要使用带有标签的数据进行训练。\n目标是学习输入特征与输出标签之间的映射关系。\n主要解决分类和回归问题。分类问题是预测数据属于哪个类别（例如，垃圾邮件检测），回归问题是预测一个连续的数值（例如，房价预测）。\n\n现实生活案例:\n\n垃圾邮件检测: 通过分析邮件的内容（关键词、发件人等）来判断邮件是否为垃圾邮件。\n图像识别: 识别图片中的物体，例如人脸识别、交通标志识别等。\n语音识别: 将语音转换成文字。\n\n金融领域应用示例:\n\n信用评分: 根据用户的个人信息和交易记录预测其信用等级。\n股票价格预测: 预测股票未来价格的涨跌趋势。\n客户流失预测: 预测哪些客户可能在未来一段时间内停止使用某项金融服务。\n\n\n\n2.2.2 无监督学习 (Unsupervised Learning)\n无监督学习则像是让计算机在没有老师指导的情况下，自己去探索数据的内在结构和模式。我们提供给计算机的数据没有标签，模型需要自己去发现数据中的隐藏信息。例如，给计算机一大堆新闻报道，让它自己将这些新闻按照主题进行分类，这就是一个无监督学习的任务。\n主要特点:\n\n使用没有标签的数据进行学习。\n目标是发现数据中的内在结构、模式或关系。\n常用于聚类、降维和关联规则挖掘等任务。聚类是将相似的数据点 grouping 在一起，降维是在保留数据主要信息的同时减少数据的维度，关联规则挖掘是发现数据中不同项之间的关联关系。\n\n现实生活案例:\n\n客户分群: 根据用户的购买行为将用户分成不同的群体，以便进行个性化营销。\n社交网络分析: 分析社交网络中用户之间的关系，发现社区结构或影响力中心。\n异常检测: 在大量数据中找出异常或不正常的点，例如信用卡欺诈检测。\n\n金融领域应用示例:\n\n投资组合风险分析: 通过聚类分析将不同的投资资产进行分类，评估投资组合的风险。\n市场细分: 将市场上的客户按照不同的特征进行细分，以便更好地了解市场需求。\n交易异常检测: 检测金融市场中不正常的交易行为，例如内幕交易或市场操纵。\n\n\n\n2.2.3 强化学习 (Reinforcement Learning)\n强化学习更像是训练一只宠物。我们不直接告诉宠物应该做什么，而是通过奖励或惩罚来引导它学习。计算机作为一个”智能体”，在与环境的交互中不断尝试不同的动作。如果某个动作让它达到了目标（例如，在游戏中获得高分，或者在交易中获得盈利），我们就给予奖励；如果动作不好，就给予惩罚。通过不断地试错和学习，智能体最终学会如何在特定环境中做出最优的决策，以获得最大的累积奖励。\n主要特点:\n\n通过与环境的交互进行学习。\n通过奖励和惩罚来指导学习方向。\n目标是学习在特定环境中采取最优的行动策略，以最大化累积奖励。\n适合解决序贯决策问题，即一系列连续决策的问题。\n\n现实生活案例:\n\n游戏AI: 训练AI玩游戏，例如围棋、象棋、电子游戏等。\n机器人控制: 训练机器人完成各种任务，例如自动驾驶、物体抓取等。\n推荐系统优化: 通过用户与推荐系统的交互（点击、购买等）来优化推荐策略。\n\n金融领域应用示例:\n\n自动化交易: 开发自动交易程序，根据市场情况自动进行买卖操作。\n投资组合管理: 动态调整投资组合，以最大化收益并控制风险。\n订单执行优化: 优化股票交易的订单执行策略，以降低交易成本。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#机器学习项目流程",
    "href": "01_ml_basic.html#机器学习项目流程",
    "title": "2  机器学习基础",
    "section": "2.3 机器学习项目流程",
    "text": "2.3 机器学习项目流程\n一个完整的机器学习项目通常包含以下几个关键步骤：\n\n数据收集与预处理:\n\n数据获取: 收集项目所需的数据。数据来源可能包括数据库、文件、网络爬虫、传感器等等。\n数据清洗: 处理数据中的缺失值、异常值、重复值和错误数据，确保数据质量。\n特征工程: 从原始数据中提取有用的特征，或者创建新的特征，以便模型更好地学习。特征工程是机器学习项目中非常重要的一步，好的特征能够显著提升模型性能。\n\n模型选择与训练:\n\n选择合适的算法: 根据问题的类型（分类、回归、聚类等）和数据的特点，选择合适的机器学习算法。例如，对于分类问题可以选择逻辑回归、支持向量机、决策树、随机森林等算法。\n划分数据集: 将数据集划分为训练集、验证集和测试集。训练集用于训练模型，验证集用于调整模型参数，测试集用于评估模型的泛化能力。\n模型训练与调参: 使用训练集数据训练模型，并使用验证集调整模型参数，例如超参数优化。目标是找到在验证集上表现最好的模型参数。\n\n模型评估与优化:\n\n性能评估: 使用测试集评估模型的性能。根据问题的类型选择合适的评估指标，例如准确率、精确率、召回率、F1 值（分类问题），均方误差、平均绝对误差（回归问题）等。\n模型调优: 如果模型性能不理想，需要进一步分析原因，并进行模型调优。调优方法可能包括：调整模型参数、尝试不同的算法、改进特征工程、增加数据量等。\n结果分析: 分析模型的预测结果，理解模型的优点和不足，为后续的模型改进提供方向。\n\n模型部署与监控:\n\n模型部署: 将训练好的模型部署到实际应用环境中。部署方式可能包括将模型集成到应用程序中、部署为 Web 服务等。\n性能监控: 在模型上线运行后，需要持续监控模型的性能。因为实际应用环境中的数据分布可能会发生变化（即”概念漂移”），导致模型性能下降。\n定期更新: 根据监控结果，定期使用新的数据重新训练模型，或者调整模型参数，以保持模型的性能和适应性。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "01_ml_basic.html#总结",
    "href": "01_ml_basic.html#总结",
    "title": "2  机器学习基础",
    "section": "2.4 总结",
    "text": "2.4 总结\n\n机器学习是一种强大的数据分析和预测工具，能够从数据中自动学习模式和规律。\n监督学习、无监督学习和强化学习是机器学习的三种主要类型，它们适用于不同的问题场景。\n机器学习在金融领域有着广泛的应用前景，可以用于风险管理、投资决策、客户服务等多个方面。\n成功应用机器学习需要一个完整的项目流程，包括数据准备、模型构建、评估优化和部署监控等环节。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>机器学习基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html",
    "href": "lab02_data.html",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "3.1 内容概要",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#内容概要",
    "href": "lab02_data.html#内容概要",
    "title": "3  金融数据获取与数据分析基础",
    "section": "",
    "text": "金融数据获取\n\n股票、债券、期货市场数据\n数据接口 (Tushare, Yahoo Finance)\n上市公司财务报表数据\n金融文本数据\n\nPython数据分析基础\n\nNumPy, Pandas 常用功能\n数据预处理与清洗\n探索性数据分析 (EDA)\n\nAI辅助编程实践\n\n代码生成、解释、优化\n最佳实践案例",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#金融数据获取",
    "href": "lab02_data.html#金融数据获取",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.2 金融数据获取",
    "text": "3.2 金融数据获取\n\n3.2.1 股票市场数据\n\n数据类型：\n\n基本行情数据：股票代码、名称、交易所、行业\n交易数据：开盘价、收盘价、最高价、最低价、成交量、成交额\n财务数据：资产负债表、利润表、现金流量表、财务指标 (ROE, EPS, PE)\n股东信息：股东户数、十大股东\n分红送股：分红金额、送股比例\n\n常用数据源：\n\nTushare (tushare.pro): 国内股票数据接口，数据全面，API友好 (稍后详细介绍)\nYahoo Finance (finance.yahoo.com): 全球股票数据，免费API (yfinance Python库)\n交易所官方API: 上海证券交易所 (sse.com.cn), 深圳证券交易所 (szse.cn) - 数据权威，但API可能较为复杂\n券商API: 部分券商提供API接口，方便交易和数据获取 (例如：同花顺, 东方财富)\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商，数据质量高，但价格昂贵 (机构常用)\n\n\n\n\n3.2.2 债券市场数据\n\n数据类型：\n\n债券基本信息：债券代码、名称、发行人、债券类型、票面利率、到期日\n债券交易数据：成交价、收益率、成交量\n债券估值数据：中债估值、市场估值\n债券评级：评级机构、评级结果\n债券发行数据：发行规模、发行利率\n\n常用数据源：\n\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n中债网 (chinabond.com.cn): 中国债券信息网，官方数据\n交易所债券信息平台: 上海证券交易所债券信息平台, 深圳证券交易所债券信息平台\n券商研究报告: 部分券商研报会提供债券市场数据和分析\n\n\n\n\n3.2.3 期货市场数据\n\n数据类型：\n\n期货合约信息：合约代码、标的资产、交易单位、最小变动价位、交割月份\n期货交易数据：开盘价、收盘价、最高价、最低价、成交量、持仓量\n期货指数数据：商品期货指数、股指期货指数\n期货仓单数据：仓单数量、注册仓单、有效预报\n期货持仓排名：期货交易所公布的持仓排名数据\n\n常用数据源：\n\nCTP接口: 期货公司提供的交易接口，可以获取实时行情和历史数据 (专业交易者常用)\n同花顺, 文华财经: 金融软件，提供期货行情和数据\n期货交易所网站: 各期货交易所 (例如：上海期货交易所, 大连商品交易所, 郑州商品交易所) 网站通常提供数据下载\nWind (wind.com.cn), Bloomberg (bloomberg.com): 专业金融数据服务商\n\n\n\n\n3.2.4 数据接口使用：Tushare\n\nTushare Pro (tushare.pro): 注册认证后可获取更丰富的数据和更高的API访问权限 (收费)\n安装: pip install tushare\n初始化: 需要token (注册Tushare Pro后获取)\nimport tushare as ts\n\n# 初始化 pro 接口\npro = ts.pro_api('YOUR_TOKEN') # 替换为你的token\n常用API示例：\n\n获取股票列表: pro.stock_basic()\n获取股票日线行情: ts.get_k_data('600519', start='2023-01-01', end='2023-01-31') (旧接口) 或 pro.daily(ts_code='600519.SH', start_date='20230101', end_date='20230131') (Pro接口)\n获取公司财务报表: pro.fina_indicator(ts_code='600519.SH', period='20221231')\n更多API: 参考 Tushare 官方文档 (https://tushare.pro/document/2)\n\n注意事项:\n\nAPI访问频率限制: 免费用户有访问频率限制，避免频繁调用\n数据权限: 不同级别用户权限不同，部分数据需要Pro会员\n数据质量: 注意核对数据质量，不同接口数据可能存在差异\n\n\n\n\n3.2.5 数据接口使用：Yahoo Finance\n\nyfinance 库主要用于获取海外股票数据，国内A股数据质量可能不如 Tushare 等国内接口，因此本课程示例主要使用 Tushare 获取A股数据。 Yahoo Finance 示例如下，如果需要分析海外股票，可以使用 yfinance。\nimport yfinance as yf\n\n# 下载 苹果 (AAPL) 股票数据\naapl = yf.Ticker(\"AAPL\")\n\n# 获取历史数据\nhist = aapl.history(period=\"5y\") # 5年历史数据\nprint(hist.head())\n\n# 获取公司信息\ninfo = aapl.info\nprint(info)\n\n# 获取分红信息\ndividends = aapl.dividends\nprint(dividends)\n\n# 更多功能参考 yfinance 文档\n优点: 免费，全球股票数据，使用简单 (如果分析海外股票)\n缺点: A 股数据质量可能不如国内专业数据源，API 稳定性可能不如官方接口，文档相对简单，A 股代码可能需要调整\n\n\n\n3.2.6 上市公司财务报表数据\n\n数据类型:\n\n资产负债表: 反映公司在特定时点的资产、负债和所有者权益状况\n利润表: 反映公司在特定期间的经营成果 (收入、成本、利润)\n现金流量表: 反映公司在特定期间的现金流入和流出\n财务指标: 根据财务报表计算的各种指标，例如：盈利能力指标 (ROE, ROA, 净利润率), 偿债能力指标 (资产负债率, 流动比率), 运营能力指标 (存货周转率, 应收账款周转率), 成长能力指标 (营业收入增长率, 净利润增长率)\n\n数据来源:\n\nCSMAR (csmar.com): 国泰安，国内权威的金融数据库，数据质量高，但收费，高校和研究机构常用\nCNRDS (cnrds.com): 中国研究数据服务平台，国内较全面的研究数据平台，数据覆盖范围广，部分数据收费，学术研究常用\nWind (wind.com.cn): 专业金融数据服务商，提供全面的财务报表和财务指标数据，收费昂贵，金融机构常用\n巨潮资讯网 (cninfo.com.cn): 免费的上市公司公告平台，包含上市公司定期报告 (年报、季报)，可以从中获取财务报表数据，但需要自行解析和整理",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#python数据分析基础",
    "href": "lab02_data.html#python数据分析基础",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.3 Python数据分析基础",
    "text": "3.3 Python数据分析基础\n\n3.3.1 NumPy 基础\n\nNumPy: 基于Python的科学计算库，提供高效的多维数组对象和工具，用于数据分析和科学计算\n核心功能:\n\n数组操作: 创建、操作、转换数组\n数学运算: 线性代数、傅里叶变换、随机数生成\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n常用操作:\nimport numpy as np\nimport pandas as pd\n\n# 假设您已下载 茅台 (600519.SH) 近5日收盘价数据到 moutai_daily.csv\n# CSV 文件包含 Date 和 Close 列\nmoutai_daily_df = pd.read_csv('moutai_daily.csv')\nclose_prices = moutai_daily_df['Close'].values\n\n# 计算平均收盘价\navg_price = np.mean(close_prices)\nprint(f\"平均收盘价: {avg_price:.2f}\")\n\n# 计算收盘价的标准差\nstd_price = np.std(close_prices)\nprint(f\"收盘价标准差: {std_price:.2f}\")\n\n# 计算每日涨跌幅 (假设前一日收盘价在 CSV 中也存在)\nprevious_close_prices = moutai_daily_df['Close'].shift(1).fillna(method='bfill').values #  向前填充第一个缺失值\nprice_change_ratio = (close_prices[1:] - previous_close_prices[1:]) / previous_close_prices[1:]\nprint(f\"每日涨跌幅: {price_change_ratio}\")\n\n\n\n3.3.2 2.2 Pandas 基础\n\nPandas (Panel Data): 基于NumPy的数据分析库，提供 Series (一维带标签数组) 和 DataFrame (二维表格型数据) 数据结构\n核心功能:\n\n数据结构: Series 和 DataFrame，方便数据表示和操作\n数据清洗: 处理缺失值、重复值、异常值\n数据预处理: 数据转换、数据标准化、特征工程\n数据分析: 数据选择、过滤、排序、分组聚合、透视表\n数据IO: 读取和写入各种数据格式 (CSV, Excel, SQL, JSON, HTML)\n\n实践示例: 茅台股票数据分析:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tushare as ts\n\n# 设置Tushare token\nts.set_token('你的token')\npro = ts.pro_api()\n\n# 获取茅台股票数据（最近60个交易日）\ndf = pro.daily(ts_code='600519.SH', \n              start_date='20240101',\n              end_date='20240331')\n\n# 基础数据处理\ndf['trade_date'] = pd.to_datetime(df['trade_date'])  # 转换日期格式\ndf = df.sort_values('trade_date')  # 按日期排序\n\n# 计算基本指标\ndf['daily_return'] = df['close'].pct_change()  # 日收益率\ndf['MA5'] = df['close'].rolling(window=5).mean()  # 5日均线\ndf['MA20'] = df['close'].rolling(window=20).mean()  # 20日均线\ndf['volatility'] = df['daily_return'].rolling(window=20).std() * np.sqrt(252)  # 20日年化波动率\n\n# 数据分析示例\nprint(\"\\n基本统计信息:\")\nprint(df[['close', 'daily_return', 'volatility']].describe())\n\nprint(\"\\n交易量最大的5天:\")\nprint(df.nlargest(5, 'vol')[['trade_date', 'close', 'vol']])\n\n# 计算每周平均收盘价和成交量\nweekly_stats = df.set_index('trade_date').resample('W').agg({\n    'close': 'mean',\n    'vol': 'sum'\n})\nprint(\"\\n每周统计:\")\nprint(weekly_stats.head())\n\n# 可视化分析\nplt.figure(figsize=(15, 10))\n\n# 绘制K线图和均线\nplt.subplot(2, 1, 1)\nplt.plot(df['trade_date'], df['close'], label='收盘价')\nplt.plot(df['trade_date'], df['MA5'], label='5日均线')\nplt.plot(df['trade_date'], df['MA20'], label='20日均线')\nplt.title('贵州茅台股价走势')\nplt.legend()\nplt.grid(True)\n\n# 绘制成交量和波动率\nplt.subplot(2, 1, 2)\nplt.bar(df['trade_date'], df['vol'], alpha=0.5, label='成交量')\nplt.plot(df['trade_date'], df['volatility'] * 1000000, 'r', label='波动率(放大1000000倍)')\nplt.title('成交量和波动率')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n练习建议:\n\n尝试修改上述代码，计算不同时间窗口的均线（如10日、30日均线）\n添加其他技术指标的计算（如RSI、MACD）\n尝试对比茅台与其他白酒股的表现\n探索不同的可视化方式（如蜡烛图）\n\n\n\n\n3.3.3 数据预处理与清洗\n\n数据质量问题:\n\n缺失值 (Missing Values): 数据记录中某些字段为空 (例如：股票停牌日可能成交量为缺失值)\n异常值 (Outliers): 与其他数据明显偏离的值 (例如：交易数据中的错误记录)\n重复值 (Duplicates): 重复的数据记录\n数据不一致 (Inconsistent Data): 同一信息在不同数据源中表示不一致\n数据类型错误 (Data Type Errors): 例如：数值型字段存储为字符串\n\n数据预处理步骤:\n\n数据清洗 (Data Cleaning): 处理缺失值、异常值、重复值、数据不一致等\n数据转换 (Data Transformation): 数据类型转换、数据格式转换、数据编码 (例如：One-Hot Encoding)\n数据标准化/归一化 (Data Scaling/Normalization): 将数据缩放到特定范围，消除量纲影响 (例如：Min-Max Scaling, Standardization)\n特征选择/特征构建 (Feature Selection/Feature Engineering): 选择重要特征，构建新特征 (后续章节详细介绍)\n\n\n\n\n3.3.4 探索性数据分析 (EDA)\n\n目的: 初步了解数据特征、发现数据规律、为后续建模提供方向\n常用方法:\n\n描述性统计: 均值、中位数、标准差、分位数、最大值、最小值等，了解数据分布和集中趋势 (例如：分析股票收盘价的统计特征)\n数据可视化: 直方图、箱线图、散点图、折线图、热力图等，直观展示数据分布、关系和异常 (例如：绘制股票价格走势图、成交量直方图)\n相关性分析: 计算特征之间的相关性，了解特征之间的关系 (例如：分析股票收益率与成交量之间的相关性)\n分组分析: 按类别分组，比较不同组别的数据特征差异 (例如：按行业分组，比较不同行业股票的盈利能力)\n\n常用可视化工具:\n\nMatplotlib: Python 基础绘图库，功能强大，定制性强\nSeaborn: 基于Matplotlib的高级可视化库，更美观，更方便绘制统计图表\nPlotly: 交互式可视化库，可创建动态图表",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#ai-辅助编程实践",
    "href": "lab02_data.html#ai-辅助编程实践",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.4 AI 辅助编程实践",
    "text": "3.4 AI 辅助编程实践\n\n3.4.1 代码生成与解释\n\nAI 代码生成工具: Cursor, GitHub Copilot 等\n代码生成: 使用 Ctrl+K (或 Cmd+K) 快捷键，根据自然语言描述或代码上下文，自动生成代码片段或完整函数。 例如，在注释中输入 “用 Tushare 获取贵州茅台 (600519.SH) 2023年至今的日线数据”，AI 可以自动生成相应的 Python 代码。\n代码解释: 使用 Ctrl+L (或 Cmd+L) 快捷键，AI 工具可以解释选定代码的功能和逻辑，帮助理解代码。 例如，选中一段复杂的 Pandas 股票数据处理代码，使用 Ctrl+L 可以快速了解代码的功能。\n\n\n\n3.4.2 代码优化与调试\n\nAI 代码优化: AI 工具可以分析代码，提供性能优化建议，例如：\n\n代码效率优化: 改进算法、减少循环、使用向量化运算 (例如：优化股票数据批量处理循环)。 可以使用 Ctrl+I (或 Cmd+I) 快捷键，请求 AI 检查代码并给出优化建议。\n代码可读性优化: 代码格式化、变量命名建议、代码重构建议。 Ctrl+I 同样可以用于代码可读性优化。\n\nAI 代码调试: AI 工具可以辅助代码调试，例如：\n\n错误检测: 静态代码分析，检测潜在错误 (例如：检查股票数据处理代码中是否存在数据类型错误)。 某些 AI 工具可以在编写代码时实时检测错误。\nBug 定位: 根据错误信息和代码上下文，帮助定位 Bug。 AI 可以分析错误堆栈信息，并结合代码逻辑，给出可能的 Bug 位置和修复建议。\n智能提示: 提供代码补全、参数提示、错误提示，减少编码错误。\n\n\n\n\n3.4.3 最佳实践案例\n\n案例 1: 生成基础数据分析代码\n\n场景: 需要生成一段完整的代码，用于获取茅台股票数据并进行基本分析。\n操作: 在代码编辑器中输入以下注释，然后按下 Ctrl+K: python     # 使用tushare获取贵州茅台(600519.SH)最近30天的日线数据     # 计算日收益率、5日均线、20日均线     # 使用matplotlib绘制K线图，并在图上显示均线\n预期生成代码:\nimport tushare as ts\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 初始化tushare\nts.set_token('你的token')\npro = ts.pro_api()\n\n# 获取数据\nend_date = pd.Timestamp.today().strftime('%Y%m%d')\nstart_date = (pd.Timestamp.today() - pd.Timedelta(days=30)).strftime('%Y%m%d')\ndf = pro.daily(ts_code='600519.SH', start_date=start_date, end_date=end_date)\ndf['trade_date'] = pd.to_datetime(df['trade_date'])\ndf = df.sort_values('trade_date')\n\n# 计算指标\ndf['daily_return'] = df['close'].pct_change()\ndf['MA5'] = df['close'].rolling(window=5).mean()\ndf['MA20'] = df['close'].rolling(window=20).mean()\n\n# 绘图\nplt.figure(figsize=(12, 6))\nplt.plot(df['trade_date'], df['close'], label='收盘价')\nplt.plot(df['trade_date'], df['MA5'], label='5日均线')\nplt.plot(df['trade_date'], df['MA20'], label='20日均线')\nplt.title('贵州茅台股价走势')\nplt.xlabel('日期')\nplt.ylabel('价格')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n案例 2: 生成技术分析代码\n\n场景: 通过解释MACD指标的计算原理，生成更复杂的技术分析代码。\n操作: 输入以下注释，然后按下 Ctrl+L: python     # 请解释MACD指标的计算原理，并生成一个完整的函数来计算茅台股票的MACD指标     # 需要包含：     # 1. MACD的计算（快线、慢线、柱状图）     # 2. 买卖信号判断     # 3. 结果可视化\n预期生成代码:\ndef analyze_macd(stock_data, short_window=12, long_window=26, signal_window=9):\n    \"\"\"\n    计算并可视化MACD指标\n    - DIF(MACD线): 快速EMA与慢速EMA的差\n    - DEA(信号线): DIF的移动平均\n    - MACD柱: DIF与DEA的差值\n    \"\"\"\n    # 计算MACD\n    exp1 = stock_data['close'].ewm(span=short_window, adjust=False).mean()\n    exp2 = stock_data['close'].ewm(span=long_window, adjust=False).mean()\n    stock_data['DIF'] = exp1 - exp2\n    stock_data['DEA'] = stock_data['DIF'].ewm(span=signal_window, adjust=False).mean()\n    stock_data['MACD'] = 2 * (stock_data['DIF'] - stock_data['DEA'])\n\n    # 生成买卖信号\n    stock_data['SIGNAL'] = 0\n    stock_data.loc[stock_data['DIF'] &gt; stock_data['DEA'], 'SIGNAL'] = 1\n    stock_data.loc[stock_data['DIF'] &lt; stock_data['DEA'], 'SIGNAL'] = -1\n\n    # 可视化\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n\n    # 绘制股价\n    ax1.plot(stock_data['trade_date'], stock_data['close'])\n    ax1.set_title('茅台股价与MACD信号')\n    ax1.grid(True)\n\n    # 绘制MACD\n    ax2.plot(stock_data['trade_date'], stock_data['DIF'], label='DIF')\n    ax2.plot(stock_data['trade_date'], stock_data['DEA'], label='DEA')\n    ax2.bar(stock_data['trade_date'], stock_data['MACD'], label='MACD柱')\n    ax2.grid(True)\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return stock_data\n\n# 调用函数\ndf = analyze_macd(df)\n\n案例 3: 生成性能优化代码\n\n场景: 优化股票数据分析代码，生成更高效的实现。\n操作: 输入以下注释和代码，然后按下 Ctrl+I:\n# 请优化以下计算波动率的代码，使用向量化操作提高性能\n# 需要：\n# 1. 使用pandas的向量化操作\n# 2. 添加并行计算支持\n# 3. 增加性能监控\n\ndef calculate_volatility(close_prices):\n    volatility = []\n    for i in range(len(close_prices)-20):\n        window = close_prices[i:i+20]\n        daily_returns = []\n        for j in range(1, len(window)):\n            daily_return = (window[j] - window[j-1]) / window[j-1]\n            daily_returns.append(daily_return)\n        vol = np.std(daily_returns) * np.sqrt(252)\n        volatility.append(vol)\n    return volatility\n预期生成代码:\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef calculate_volatility_optimized(close_prices):\n    \"\"\"\n    优化后的波动率计算函数\n    - 使用pandas的向量化操作\n    - 支持并行计算\n    - 包含性能监控\n    \"\"\"\n    start_time = time.time()\n\n    # 计算日收益率（向量化操作）\n    returns = pd.Series(close_prices).pct_change().dropna()\n\n    # 使用rolling计算滚动波动率\n    volatility = returns.rolling(window=20).std() * np.sqrt(252)\n\n    # 并行计算示例（处理多个股票时使用）\n    def parallel_volatility(stock_returns):\n        return stock_returns.rolling(window=20).std() * np.sqrt(252)\n\n    with ThreadPoolExecutor() as executor:\n        # 这里可以处理多个股票的数据\n        results = list(executor.map(parallel_volatility, [returns]))\n\n    end_time = time.time()\n    print(f\"计算耗时: {end_time - start_time:.4f} 秒\")\n\n    return volatility\n\n# 性能对比\nprices = df['close'].values\n\n# 测试原始版本\nstart = time.time()\nvol1 = calculate_volatility(prices)\nprint(f\"原始版本耗时: {time.time() - start:.4f} 秒\")\n\n# 测试优化版本\nstart = time.time()\nvol2 = calculate_volatility_optimized(prices)\nprint(f\"优化版本耗时: {time.time() - start:.4f} 秒\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "lab02_data.html#总结",
    "href": "lab02_data.html#总结",
    "title": "3  金融数据获取与数据分析基础",
    "section": "3.5 总结",
    "text": "3.5 总结\n\n数据是机器学习的基石：高质量的数据是构建有效模型的关键。\n金融数据获取多样化：掌握不同数据源和API接口，灵活获取所需数据，包括股票、债券、期货、财务报表和金融文本数据。\nPython 数据分析是必备技能：熟练运用 NumPy 和 Pandas 进行金融数据处理和分析。\nEDA 帮助理解数据：通过探索性数据分析，发现金融数据规律，为建模提供方向。\nAI 辅助编程提升效率：善用 AI 工具，提高金融数据获取和分析效率。 熟练使用AI工具，可以显著提升开发效率。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>金融数据获取与数据分析基础</span>"
    ]
  },
  {
    "objectID": "03_supervised.html",
    "href": "03_supervised.html",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "4.1 监督学习简介\n监督学习是机器学习的一个重要分支，它是指从带有标签的数据中自动学习规律和模式，并利用这些规律和模式对新数据进行预测和决策的过程。在监督学习中，我们拥有一个包含输入特征 \\(\\mathbf{x}\\) 和对应输出标签 \\(y\\) 的数据集，模型的目标是学习一个从输入特征到输出标签的映射关系。监督学习在量化投资、金融科技等领域有广泛应用，例如：\n由于课程时间有限，本讲义将重点介绍监督学习中的回归、分类和集成学习，以及它们在金融预测中的应用。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习简介",
    "href": "03_supervised.html#监督学习简介",
    "title": "4  监督学习（上）",
    "section": "",
    "text": "风险评估：根据客户的历史信用数据（特征）预测其信用风险等级（标签）。\n欺诈检测：基于交易记录（特征）识别欺诈交易（标签）。\n量化交易：预测股票价格走势（标签）以辅助交易决策（特征）。\n客户细分：根据客户特征（特征）预测客户所属类别（标签），进行精准营销。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#监督学习模型详解",
    "href": "03_supervised.html#监督学习模型详解",
    "title": "4  监督学习（上）",
    "section": "4.2 监督学习模型详解",
    "text": "4.2 监督学习模型详解\n\n4.2.1 回归问题描述\n回归问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测连续数值型的结果 \\(y\\)。 给定训练数据 \\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(\\mathbf{x}_i\\) 是第 \\(i\\) 个样本的特征向量， \\(y_i\\) 是对应的真实值， \\(N\\) 是样本数量。我们的目标是找到一个函数 \\(f\\)，使得对于新的输入 \\(\\mathbf{x}\\)，模型预测值 \\(\\hat{y} = f(\\mathbf{x})\\) 尽可能接近真实值 \\(y\\)。假设真实值 \\(y_i\\) 与预测函数 \\(f(\\mathbf{x}_i)\\) 之间存在如下关系：\n\\[y_i = f(\\mathbf{x}_i) + \\epsilon_i\\]\n其中 \\(\\epsilon_i\\) 代表随机误差项，通常假设其服从均值为 0 的正态分布。在实际应用中，我们通常将观测值堆叠成矩阵和向量的形式，方便模型表达和计算：\n\n\\(N \\times 1\\) 维结果向量 \\(\\mathbf{y} = (y_1, y_2, ..., y_N)^T\\)\n\\(N \\times K\\) 维特征矩阵 \\(\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N)^T\\)，每一行代表一个样本，每一列代表一个特征。\n\\(N \\times 1\\) 维误差向量 \\(\\mathbf{\\epsilon} = (\\epsilon_1, \\epsilon_2, ..., \\epsilon_N)^T\\)\n\n回归模型可以简洁地写为： \\(\\mathbf{y} = f(\\mathbf{X}) + \\mathbf{\\epsilon}\\)。我们的目标是通过训练数据学习到函数 \\(f\\) 的具体形式，从而能够对新的样本 \\(\\mathbf{x}\\) 进行预测。\n\n\n4.2.2 线性回归 (Linear Regression)\n线性回归模型是最简单且应用广泛的回归模型。它假设结果变量 \\(y\\) 与特征向量 \\(\\mathbf{x}\\) 之间存在线性关系。线性回归模型易于理解和实现，是许多复杂模型的基础。\n模型表达式:\n线性回归模型假设预测函数 \\(f(\\mathbf{x})\\) 是特征 \\(\\mathbf{x}\\) 的线性组合，模型表达式如下：\n\\(y = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\)\n或者对于单个样本 \\(i\\)，可以表示为：\n\\(y_i = \\mathbf{x}_i^T \\mathbf{\\beta} + \\epsilon_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_K x_{iK} + \\epsilon_i\\)\n其中： - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是 \\((K+1) \\times 1\\) 维回归系数向量，\\(\\beta_0\\) 是截距项（bias），\\(\\beta_1, ..., \\beta_K\\) 是特征的系数。为了方便表示，我们通常在特征矩阵 \\(\\mathbf{X}\\) 中添加一列全为 1 的列向量，对应于截距项 \\(\\beta_0\\)。 - \\(\\mathbf{x}_i = (1, x_{i1}, x_{i2}, ..., x_{iK})^T\\) 是 \\((K+1) \\times 1\\) 维增广特征向量，包含了常数项 1 和原始特征。 - \\(\\epsilon_i\\) 是误差项。\n最优化方法：最小二乘法 (OLS)\n线性回归的目标是找到最优的回归系数 \\(\\mathbf{\\beta}\\)，使得模型的预测值 \\(\\mathbf{X}\\mathbf{\\beta}\\) 与真实值 \\(\\mathbf{y}\\) 之间的误差平方和 (Sum of Squared Errors, SSE) 最小。最小二乘法 (Ordinary Least Squares, OLS) 是一种常用的求解线性回归模型参数的方法。其目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^T \\mathbf{\\beta})^2\\]\n为了求解最优的 \\(\\mathbf{\\beta}\\)，我们可以对目标函数 \\(L(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到正规方程 (Normal Equation)：\n\\[\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = \\mathbf{X}^T\\mathbf{y}\\]\n如果矩阵 \\(\\mathbf{X}^T\\mathbf{X}\\) 可逆（即满秩），则可以得到普通最小二乘 (OLS) 估计量的解析解：\n\\[\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n高维环境下的过拟合问题与正则化:\n在高维环境中，当特征数量 \\(K\\) 相对于观测数量 \\(N\\) 来说较大时（例如 \\(K &gt; N\\)，或 \\(K\\) 接近 \\(N\\)），OLS 估计可能会出现过拟合 (Overfitting) 问题。过拟合是指模型在训练集上表现得非常好（例如，训练误差很小），但在未见过的测试集上泛化能力很差，预测性能下降。这是因为在高维情况下，模型参数过多，容易捕捉到训练数据中的噪声和随机波动，而不是真实的 underlying pattern。\n为了解决过拟合问题，提高模型的泛化能力，可以引入正则化 (Regularization) 方法。正则化通过在损失函数中添加惩罚项，限制模型复杂度，从而避免模型过度拟合训练数据。常用的正则化方法包括 岭回归 (Ridge Regression) 和 Lasso 回归 (Lasso Regression)。\n\n\n4.2.3 岭回归 (Ridge Regression)\n岭回归是一种改进的线性回归方法，也称为 \\(L_2\\) 正则化线性回归。它通过在最小二乘法的损失函数中添加 \\(L_2\\) 范数惩罚项来对回归系数进行 shrinkage (收缩)，限制回归系数的大小，从而降低模型的复杂度和过拟合风险。岭回归特别适用于处理多重共线性问题，即特征之间存在高度相关性的情况。\n模型表达式:\n岭回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Ridge}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} \\right] \\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差 (Mean Squared Error, MSE) 项，衡量模型预测值与真实值之间的平均误差平方。 - \\(\\lambda \\mathbf{\\beta}^T\\mathbf{\\beta} = \\lambda ||\\mathbf{\\beta}||_2^2 = \\lambda \\sum_{j=0}^{K} \\beta_j^2\\) 是 \\(L_2\\) 范数惩罚项，也称为 权重衰减项 (Weight Decay)。它惩罚回归系数 \\(\\mathbf{\\beta}\\) 的平方和，迫使系数趋向于较小的值。 - \\(\\lambda \\ge 0\\) 是正则化参数 (Regularization Parameter)，也称为 惩罚系数。它控制惩罚项的强度。\\(\\lambda\\) 越大，惩罚越强，回归系数越趋向于 0。当 \\(\\lambda = 0\\) 时，岭回归退化为普通线性回归。\n估计方法:\n类似于线性回归，我们可以对岭回归的目标函数 \\(L_{Ridge}(\\mathbf{\\beta})\\) 关于 \\(\\mathbf{\\beta}\\) 求导，并令导数等于 0，得到岭回归的估计结果：\n\\[\\hat{\\mathbf{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})^{-1} \\mathbf{X}^T\\mathbf{y}\\]\n其中 \\(\\mathbf{I}_{K+1}\\) 是 \\((K+1) \\times (K+1)\\) 单位矩阵。通过向 \\(\\mathbf{X}^T\\mathbf{X}\\) 添加对角矩阵 \\(\\lambda \\mathbf{I}_{K+1}\\)（即”岭”），可以使得在求逆运算时，即使 \\(\\mathbf{X}^T\\mathbf{X}\\) 接近奇异矩阵（例如，当存在多重共线性时），\\((\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{K+1})\\) 仍然具有较好的可逆性，保证了解的稳定性。并且 \\(\\lambda \\mathbf{I}_{K+1}\\) 的存在将导致回归系数 \\(\\hat{\\mathbf{\\beta}}_{Ridge}\\) 向零收缩。\n岭回归的特点:\n\n\\(L_2\\) 正则化：使用 \\(L_2\\) 范数惩罚项，将回归系数向零收缩，但不会精确地变为 0。\n缓解多重共线性：通过引入正则化项，降低了模型对特征之间相关性的敏感度，可以缓解多重共线性问题，提高模型稳定性。\n降低过拟合风险：通过限制模型复杂度，有效降低过拟合风险，提高模型的泛化能力。\n无法进行特征选择：岭回归会缩小所有特征的系数，但不会将任何系数精确地设置为 0，因此无法进行特征选择。\n\n\n\n4.2.4 Lasso 回归 (Lasso Regression)\nLasso (Least Absolute Shrinkage and Selection Operator) 回归是另一种常用的正则化线性回归方法，也称为 \\(L_1\\) 正则化线性回归。与岭回归不同，Lasso 回归使用 \\(L_1\\) 范数惩罚项进行正则化。 \\(L_1\\) 正则化不仅可以进行系数 shrinkage，更重要的是，它具有 特征选择 (Feature Selection) 的能力，可以将一些不重要特征的回归系数压缩为 精确的 0，从而得到 稀疏模型 (Sparse Model)。稀疏模型更易于解释，并且可以提高模型的泛化能力。\n模型表达式:\nLasso 回归的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{Lasso}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma \\sum_{j=1}^{K} |\\beta_j| \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 仍然是均方误差项。 - \\(\\gamma \\sum_{j=1}^{K} |\\beta_j| = \\gamma ||\\mathbf{\\beta}_{1:K}||_1 = \\gamma (|\\beta_1| + |\\beta_2| + ... + |\\beta_K|)\\) 是 \\(L_1\\) 范数惩罚项，注意这里只惩罚了特征系数 \\(\\beta_1, ..., \\beta_K\\)，不惩罚截距项 \\(\\beta_0\\)。 \\(L_1\\) 范数惩罚项迫使一些回归系数变为 0。 - \\(\\gamma \\ge 0\\) 是 正则化参数，控制 \\(L_1\\) 惩罚项的强度。\\(\\gamma\\) 越大，惩罚越强，更多的回归系数会被压缩为 0。\n估计方法:\n与岭回归不同，Lasso 回归的目标函数由于包含 \\(L_1\\) 范数项，在 \\(\\beta_j = 0\\) 处不可导，因此 没有解析解。通常需要使用数值优化算法（如坐标轴下降法 (Coordinate Descent)、近端梯度下降法 (Proximal Gradient Descent)）进行求解。\nLasso 回归的特点:\n\n\\(L_1\\) 正则化：使用 \\(L_1\\) 范数惩罚项，不仅可以进行系数 shrinkage，还可以将一些不重要特征的回归系数压缩为精确的 0，实现特征选择。\n稀疏模型：Lasso 回归可以产生稀疏模型，即模型中只有少数特征的系数非零，这有助于模型解释和提高泛化能力。\n特征选择能力：在特征选择方面优于岭回归。Lasso 回归可以自动选择重要的特征，去除冗余和不相关的特征。\n适用于高维稀疏数据：Lasso 回归特别适用于处理高维稀疏数据，例如文本数据、基因数据等。\n\n\n\n4.2.5 弹性网 (Elastic Net)\n弹性网 (Elastic Net) 是一种结合了岭回归和 Lasso 回归的正则化方法，可以看作是岭回归和 Lasso 回归的折衷。弹性网同时使用 \\(L_1\\) 范数和 \\(L_2\\) 范数惩罚项进行正则化。弹性网同时使用 \\(L_1\\) 正则化和 \\(L_2\\) 正则化，综合利用 \\(L_1\\) 正则化的特征选择能力和 \\(L_2\\) 正则化的稳定性和 shrinkage 能力。在某些情况下，弹性网的性能优于单独的岭回归和 Lasso 回归，尤其是在特征之间高度相关时，弹性网表现更稳定。\n模型表达式:\n弹性网的目标函数为：\n\\[\\min_{\\mathbf{\\beta}} L_{ElasticNet}(\\mathbf{\\beta}) = \\min_{\\mathbf{\\beta}} \\left[ \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) + \\gamma_1 \\sum_{j=1}^{K} |\\beta_j| + \\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta} \\right]\\]\n其中： - \\(\\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\\) 是均方误差项。 - \\(\\gamma_1 \\sum_{j=1}^{K} |\\beta_j|\\) 是 \\(L_1\\) 范数惩罚项，用于特征选择和产生稀疏模型。 - \\(\\gamma_2 \\mathbf{\\beta}^T\\mathbf{\\beta}\\) 是 \\(L_2\\) 范数惩罚项，用于系数 shrinkage 和缓解多重共线性。 - \\(\\gamma_1 \\ge 0\\) 和 \\(\\gamma_2 \\ge 0\\) 分别是 \\(L_1\\) 正则化参数 和 \\(L_2\\) 正则化参数，控制两种惩罚项的强度。通常需要通过交叉验证等方法来选择合适的 \\(\\gamma_1\\) 和 \\(\\gamma_2\\) 值。\n弹性网的特点:\n\n结合 \\(L_1\\) 和 \\(L_2\\) 正则化：弹性网同时使用 \\(L_1\\) 和 \\(L_2\\) 范数惩罚项，结合了两者的优点。\n既可以进行特征选择，又可以进行系数 shrinkage：弹性网既可以像 Lasso 回归一样进行特征选择，将一些不重要特征的系数压缩为 0，又可以像岭回归一样进行系数 shrinkage，缩小系数的整体大小，提高模型稳定性。\n性能更稳定：在某些情况下，弹性网的预测性能和鲁棒性优于岭回归和 Lasso 回归。\n处理特征高度相关性：当特征之间高度相关时，Lasso 回归可能随机选择其中一个特征，而弹性网倾向于选择一组相关的特征，表现更稳定。\n\n\n\n4.2.6 分类问题描述\n分类问题旨在通过由 \\(K \\times 1\\) 维向量 \\(\\mathbf{x}\\) 表示的 \\(K\\) 个观测到的预测变量（特征）来预测离散类别型的结果 \\(y\\)。分类问题的目标是学习一个模型，将输入样本 \\(\\mathbf{x}\\) 划分到预定义的类别中。根据类别数量的不同，分类问题可以分为：\n\n二分类 (Binary Classification)：预测结果 \\(y\\) 只有两个类别，通常表示为 \\(y \\in \\{0, 1\\}\\) (或 \\(y \\in \\{-1, +1\\}\\))。例如，判断邮件是否为垃圾邮件（是/否），预测用户是否会点击广告（点击/不点击），识别交易是否为欺诈交易（欺诈/正常）。\n多分类 (Multiclass Classification)：预测结果 \\(y\\) 有两个以上的类别，表示为 \\(y \\in \\{C_1, C_2, ..., C_L\\}\\)，其中 \\(C_i\\) 是类别标签， \\(L \\ge 3\\) 是类别数量。例如，图像分类（猫、狗、鸟、鱼等），文本分类（政治、经济、体育、娱乐等），客户类型分类（高价值客户、中价值客户、低价值客户）。\n\n对于多分类问题，常用的处理策略是 “拆解法” (Decomposition)，即将多分类任务拆解为若干个二分类任务求解。常见的拆解策略包括 一对一 (One-vs-One, OvO)、一对多 (One-vs-Rest, OvR) 和 多对多 (Many-vs-Many, MvM) 等。\n\n\n4.2.7 类别不平衡问题\n在分类任务中，经常会遇到不同类别的训练样本数量差别很大的情况，即 类别不平衡 (Class Imbalance) 问题。例如，在欺诈检测、罕见病诊断、自然灾害预测等领域，少数类样本 (Minority Class)（如欺诈交易、患病样本、地震）的数量通常远远少于多数类样本 (Majority Class)（如正常交易、健康样本、非地震）。类别不平衡问题会严重影响模型的学习效果，使得模型更倾向于预测样本数量较多的类别，而对少数类别的识别率很低。\n类别不平衡的影响:\n\n模型偏向多数类：模型在训练过程中更容易学习到多数类样本的特征，而忽略少数类样本的特征，导致模型预测结果偏向多数类。\n整体分类精度虚高：由于多数类样本数量占优，即使模型将所有样本都预测为多数类，也可能获得较高的整体分类精度 (Accuracy)。但这种高精度是没有意义的，因为模型对少数类的识别能力很差。\n评估指标失效：常用的评估指标（如准确率 Accuracy）在类别不平衡数据集上可能失效，无法真实反映模型的性能。我们需要使用更合适的评估指标，例如 精确率 (Precision)、召回率 (Recall)、F1 值 (F1-score)、AUC 值 (Area Under ROC Curve) 等。\n\n类别不平衡的解决方案:\n为了解决类别不平衡问题，提高模型对少数类别的识别能力，常用的解决方案包括：\n\n再缩放 (Rescaling) / 阈值调整 (Threshold Adjustment)：不改变原始模型，而是调整分类阈值 (Classification Threshold)，使得模型在类别不平衡时也能做出合理的预测。例如，对于逻辑回归或 SVM 等输出概率的模型，默认的分类阈值通常为 0.5。当类别不平衡时，可以将预测为正例的阈值从 0.5 调整为更小的值，例如 \\(\\frac{m^{+}}{m^{-} + m^{+}}\\), 其中 \\(m^{+}\\) 和 \\(m^{-}\\) 分别是正类（少数类）和负类（多数类）样本的数量。降低阈值会使得模型更容易将样本预测为正类，从而提高少数类的召回率。\n重采样 (Resampling)：通过改变训练集中不同类别样本的比例来缓解类别不平衡问题。重采样方法包括 欠抽样 (Undersampling) 和 过抽样 (Oversampling)。\n\n欠抽样 (Undersampling)：减少多数类样本的数量，随机删除一部分多数类样本，使得正负类样本数量接近平衡。欠抽样方法简单易行，但可能会丢失一部分多数类样本的信息，适用于数据量较大的情况。\n过抽样 (Oversampling)：增加少数类样本的数量，例如通过复制少数类样本或生成合成样本（如 SMOTE (Synthetic Minority Over-sampling Technique)）。过抽样方法可以保留所有原始多数类样本的信息，但可能会导致过拟合，适用于数据量较小的情况。SMOTE 算法通过在少数类样本之间进行插值生成新的合成样本，可以有效缓解过拟合问题。\n\n阈值移动 (Threshold-moving)：这是一种代价敏感学习 (Cost-sensitive learning) 的思想。基于原始训练集进行学习，但在用训练好的分类器进行预测时，根据类别不平衡的程度调整决策阈值。例如，如果少数类样本的误分类代价更高，则可以将决策阈值向多数类方向移动，使得模型更倾向于将样本预测为少数类。\n代价敏感学习 (Cost-sensitive learning)：为不同类别的误分类设置不同的代价 (Cost)，使得模型在训练时更加关注少数类样本，最小化总的期望代价而不是最小化分类错误率。例如，可以使用代价矩阵 (Cost Matrix) 来定义不同误分类情况的代价，然后在训练过程中根据代价矩阵调整模型的学习策略。\n集成学习方法：一些集成学习方法，如 集成学习 (Ensemble Learning) 方法，例如 EasyEnsemble、BalanceCascade 等，通过将数据集划分为多个子集，在每个子集上训练基学习器，然后集成多个基学习器的预测结果，可以有效提高模型在类别不平衡数据集上的性能。\n\n\n\n4.2.8 逻辑回归 (Logistic Regression)\n逻辑回归 (Logistic Regression) 是一种广泛使用的二分类模型。虽然名字带有”回归”，但逻辑回归实际上是一种分类算法，主要用于解决二分类问题。逻辑回归模型简单高效，易于解释，是许多分类问题的 baseline 模型。\n模型表达式:\n逻辑回归模型基于线性回归的思想，但通过引入 Sigmoid 函数 (Sigmoid Function) 或 Logistic 函数，将线性回归的输出值映射到 \\((0, 1)\\) 区间，使其具有概率意义，用于表示样本属于正类的概率。\n逻辑回归模型的表达式如下：\n\\[P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}}\\]\n其中： - \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\) 表示给定特征向量 \\(\\mathbf{x}\\) 和模型参数 \\(\\mathbf{\\beta}\\) 的条件下，样本属于正类 (y=1) 的概率。 - \\(\\mathbf{x} = (1, x_1, x_2, ..., x_K)^T\\) 是增广特征向量。 - \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_K)^T\\) 是模型参数，与线性回归中的回归系数类似。 - \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) 是 Sigmoid 函数，也称为 Logistic 函数。Sigmoid 函数将任意实数 \\(z\\) 映射到 \\((0, 1)\\) 区间，函数图像呈 S 形。当 \\(z \\rightarrow +\\infty\\) 时，\\(\\sigma(z) \\rightarrow 1\\)；当 \\(z \\rightarrow -\\infty\\) 时，\\(\\sigma(z) \\rightarrow 0\\)；当 \\(z = 0\\) 时，\\(\\sigma(z) = 0.5\\)。\n对于二分类问题，逻辑回归模型预测样本属于正类的概率 \\(P(y=1|\\mathbf{x}; \\mathbf{\\beta})\\)，则样本属于负类的概率为 \\(P(y=0|\\mathbf{x}; \\mathbf{\\beta}) = 1 - P(y=1|\\mathbf{x}; \\mathbf{\\beta}) = 1 - \\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = \\sigma(-\\mathbf{x}^T \\mathbf{\\beta}) = \\frac{e^{-\\mathbf{x}^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}^T \\mathbf{\\beta}}} = \\frac{1}{1 + e^{\\mathbf{x}^T \\mathbf{\\beta}}}\\)。\n模型训练：最大似然估计 (Maximum Likelihood Estimation, MLE)\n逻辑回归模型的训练目标是最大化训练数据的似然函数 (Likelihood Function)，即找到一组模型参数 \\(\\mathbf{\\beta}\\)，使得在给定这组参数下，训练数据出现的概率最大。对于二分类问题，逻辑回归的似然函数可以表示为：\n\\[L(\\mathbf{\\beta}) = \\prod_{i=1}^{N} [P(y_i=1|\\mathbf{x}_i; \\mathbf{\\beta})]^{y_i} [P(y_i=0|\\mathbf{x}_i; \\mathbf{\\beta})]^{1-y_i} = \\prod_{i=1}^{N} [\\sigma(\\mathbf{x}_i^T \\mathbf{\\beta})]^{y_i} [\\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]^{1-y_i}\\]\n为了方便优化，通常将似然函数取对数，得到对数似然函数 (Log-Likelihood Function)：\n\\[\\ell(\\mathbf{\\beta}) = \\ln L(\\mathbf{\\beta}) = \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})] = \\sum_{i=1}^{N} [y_i \\ln \\frac{1}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}} + (1-y_i) \\ln \\frac{e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\beta}}}]\\]\n我们的目标是最大化对数似然函数 \\(\\ell(\\mathbf{\\beta})\\)，等价于最小化负对数似然函数 (Negative Log-Likelihood Function)：\n\\[J(\\mathbf{\\beta}) = -\\ell(\\mathbf{\\beta}) = - \\sum_{i=1}^{N} [y_i \\ln \\sigma(\\mathbf{x}_i^T \\mathbf{\\beta}) + (1-y_i) \\ln \\sigma(-\\mathbf{x}_i^T \\mathbf{\\beta})]\\]\n负对数似然函数 \\(J(\\mathbf{\\beta})\\) 也称为 交叉熵损失函数 (Cross-Entropy Loss Function) 或 Logistic Loss Function。\n最优化方法：梯度下降法 (Gradient Descent)\n逻辑回归模型通常使用梯度下降法 (Gradient Descent) 或其变种（如 随机梯度下降 (SGD)、小批量梯度下降 (Mini-batch GD)、Adam 等）来求解最优参数 \\(\\mathbf{\\beta}\\)，最小化交叉熵损失函数 \\(J(\\mathbf{\\beta})\\)。梯度下降法是一种迭代优化算法，通过不断沿着损失函数梯度 负方向 更新参数，逐步逼近最优解。\n决策边界 (Decision Boundary):\n逻辑回归模型的决策边界是线性的。当 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 时，\\(\\sigma(\\mathbf{x}^T \\mathbf{\\beta}) = 0.5\\)，模型预测样本属于正类和负类的概率均为 0.5。因此，线性方程 \\(\\mathbf{x}^T \\mathbf{\\beta} = 0\\) 定义了逻辑回归模型的决策边界，将特征空间划分为正类区域和负类区域。\n\n\n4.2.9 支持向量机 (Support Vector Machine, SVM)\n支持向量机 (Support Vector Machine, SVM) 是一种强大且广泛应用于分类和回归问题的监督学习模型。SVM 的核心思想是找到一个最优超平面 (Optimal Hyperplane)，将不同类别的样本最大程度地分开，同时使得分类间隔 (Margin) 最大化。SVM 在高维空间和非线性分类问题中表现出色，通过核技巧 (Kernel Trick) 可以有效地处理非线性可分数据。\n线性可分支持向量机 (Linearly Separable SVM) / 硬间隔 SVM (Hard Margin SVM):\n对于线性可分 (Linearly Separable) 的数据集，即存在一个超平面可以将不同类别的样本完全分开的情况，我们可以构建线性可分支持向量机，也称为 硬间隔 SVM。硬间隔 SVM 旨在找到一个最大间隔超平面，将两类样本完全正确地分开，并且使得间隔最大化。间隔是指超平面到最近的样本点（称为 支持向量 (Support Vector)）的距离。\n模型表达式:\n给定线性可分的训练数据集 \\(D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\)，其中 \\(y_i \\in \\{-1, +1\\}\\)。线性可分 SVM 的目标是找到一个超平面 \\((\\mathbf{w}, b)\\)，使得：\n\n正确分类: 所有样本都被正确分类，即对于 \\(y_i = +1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\ge +1\\)；对于 \\(y_i = -1\\) 的样本，有 \\(\\mathbf{w}^T \\mathbf{x}_i + b \\le -1\\)。可以将两个不等式统一为： \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\)。\n间隔最大化: 最大化分类间隔 \\(Margin = \\frac{2}{||\\mathbf{w}||}\\)，等价于最小化 \\(||\\mathbf{w}||^2 = \\mathbf{w}^T \\mathbf{w}\\)。\n\n因此，线性可分 SVM 的最优化问题可以表示为：\n\\[\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1, \\quad i = 1, 2, ..., N\\]\n这是一个凸二次规划 (Convex Quadratic Programming, QP) 问题，可以使用现成的 QP 求解器求解。\n线性不可分支持向量机 (Linearly Inseparable SVM) / 软间隔 SVM (Soft Margin SVM):\n在实际应用中，很多数据集不是线性可分的，即不存在一个超平面可以将不同类别的样本完全分开。为了处理线性不可分数据，我们需要引入软间隔 SVM，也称为 线性支持向量机。软间隔 SVM 允许模型在一些样本上分类错误，但希望尽可能减少分类错误，同时保持间隔最大化。\n模型表达式:\n软间隔 SVM 通过引入松弛变量 (Slack Variables) \\(\\xi_i \\ge 0\\)，允许一些样本不满足硬间隔约束 \\(y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1\\)。松弛变量 \\(\\xi_i\\) 表示第 \\(i\\) 个样本违反约束的程度。软间隔 SVM 的最优化问题变为：\n\\[\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i \\quad \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0, \\quad i = 1, 2, ..., N\\]\n其中： - \\(\\frac{1}{2} ||\\mathbf{w}||^2\\) 仍然是间隔最大化项。 - \\(C \\sum_{i=1}^{N} \\xi_i\\) 是惩罚项，表示对误分类的惩罚。 \\(\\xi_i\\) 越大，误分类程度越高，惩罚越大。 - \\(C &gt; 0\\) 是 惩罚参数 (Penalty Parameter)，也称为 正则化参数。 \\(C\\) 控制对误分类的惩罚程度。 \\(C\\) 越大，对误分类的惩罚越大，模型越倾向于减小误分类，但可能会导致间隔变小，容易过拟合； \\(C\\) 越小，对误分类的惩罚越小，模型更容忍误分类，间隔可能更大，泛化能力可能更好。 \\(C\\) 的选择需要通过交叉验证等方法进行调优。\n核函数 (Kernel Function):\n对于非线性可分 (Nonlinearly Separable) 的数据集，SVM 可以通过 核函数 (Kernel Function) 将数据映射到高维空间 (High-Dimensional Space)，使得在高维空间中数据变得线性可分，然后在高维空间中寻找最优超平面。核技巧 (Kernel Trick) 的强大之处在于，我们不需要显式地计算高维空间的特征向量，只需要定义一个核函数 \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)，它可以计算原始空间中两个向量 \\(\\mathbf{x}_i\\) 和 \\(\\mathbf{x}_j\\) 映射到高维空间后的内积。常用的核函数包括：\n\n线性核 (Linear Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\\)。线性核实际上没有进行特征映射，适用于线性可分数据。\n多项式核 (Polynomial Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d\\)。多项式核可以将数据映射到多项式特征空间，适用于多项式关系的数据。其中 \\(\\gamma &gt; 0, r \\ge 0, d \\ge 1\\) 是核参数。\n高斯核 / RBF 核 (Gaussian Kernel / Radial Basis Function Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)\\)。高斯核是最常用的核函数之一，可以将数据映射到无限维空间，适用于各种类型的数据，尤其是局部性模式的数据。其中 \\(\\gamma &gt; 0\\) 是核参数，控制核函数的宽度。\nSigmoid 核 (Sigmoid Kernel): \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)\\)。Sigmoid 核类似于神经网络中的 Sigmoid 激活函数，SVM 使用 Sigmoid 核时，其行为类似于多层感知机神经网络。其中 \\(\\gamma &gt; 0, r &lt; 0\\) 是核参数。\n\n最优化方法：对偶问题与 SMO 算法\nSVM 的优化问题（无论是硬间隔还是软间隔）通常转化为 对偶问题 (Dual Problem) 进行求解。求解对偶问题的好处包括： 1. 更容易求解：对偶问题通常比原始问题更容易求解。 2. 引入核函数：在对偶问题中，目标函数和约束条件只涉及到样本之间的内积，可以方便地引入核函数，将线性 SVM 扩展到非线性 SVM。\n求解 SVM 对偶问题的高效算法是 SMO (Sequential Minimal Optimization) 算法。SMO 算法是一种启发式算法，它将大规模 QP 问题分解为一系列小规模 QP 子问题，通过迭代地优化两个变量，高效地求解 SVM 模型。\n\n\n4.2.10 决策树 (Decision Tree)\n决策树 (Decision Tree) 是一种树形结构的分类或回归模型。决策树模型直观易懂，易于解释，并且可以处理类别型和数值型特征，无需进行特征缩放。决策树模型的核心思想是基于特征对数据集进行递归划分，构建一棵树状的决策规则，用于对新样本进行分类或预测。\n决策树由节点 (Node) 和 有向边 (Directed Edge) 组成。节点分为两种类型： - 内部节点 (Internal Node)：表示一个特征或属性的测试条件，用于决定样本的划分方向。 - 叶节点 (Leaf Node / Terminal Node)：表示最终的决策结果，即类别标签（分类树）或预测值（回归树）。\n有向边代表划分规则，从父节点指向子节点。从根节点到每个叶节点的路径都对应着一条决策规则。\n决策树的学习过程主要包括三个步骤：特征选择、树的生成 和 树的剪枝。\n回归树 (Regression Tree):\n回归树 (Regression Tree) 用于预测连续数值型目标变量。例如，预测房价、股票价格等。\n模型构建:\n回归树的构建过程是一个递归的二叉树构建过程，也称为 CART (Classification and Regression Tree) 树。CART 树是一种二叉树，内部节点根据特征取值将数据集划分为两个子集，叶节点输出预测值。回归树的构建过程如下：\n\n选择划分特征和划分点：从所有特征和所有可能的划分点中，选择一个最优的特征 \\(j\\) 和切分点 \\(s\\)，将当前节点的数据集划分为两个区域 \\(R_1(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j \\le s\\}\\) 和 \\(R_2(j,s) = \\{\\mathbf{x}|\\mathbf{x}_j &gt; s\\}\\)。\n最小化平方误差：选择最优划分属性 \\(j\\) 和划分点 \\(s\\) 的目标是最小化划分后的平方误差 (Squared Error)，即使得划分后的两个子区域内样本的目标变量值尽可能接近。对于给定的特征 \\(j\\) 和切分点 \\(s\\)，遍历所有可能的 \\((j, s)\\) 对，计算划分后的平方误差，选择使得平方误差最小的 \\((j, s)\\) 对作为最优划分。平方误差的计算公式为：\n\n\\[\\min_{j,s} \\left[ \\min_{c_1} \\sum_{\\mathbf{x}_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{\\mathbf{x}_i \\in R_2(j,s)} (y_i - c_2)^2\\right]\\]\n其中 \\(c_1\\) 和 \\(c_2\\) 分别是区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\) 的预测值。对于给定的区域 \\(R_m(j,s)\\)，最优的预测值 \\(\\hat{c}_m\\) 是该区域内样本目标变量的均值：\n\\[\\hat{c}_m = \\text{ave}(y_i|\\mathbf{x}_i \\in R_m(j,s)) = \\frac{1}{|R_m(j,s)|} \\sum_{\\mathbf{x}_i \\in R_m(j,s)} y_i\\]\n\n递归划分：对划分后的两个子区域 \\(R_1(j,s)\\) 和 \\(R_2(j,s)\\)，递归地重复步骤 1 和 2，继续选择最优特征和切分点进行划分，直到满足停止条件。停止条件通常包括：\n\n节点内样本数量小于某个预设阈值。\n节点内样本的目标变量方差或平方误差小于某个阈值。\n没有更多特征可用于划分，或所有特征都已用完。\n\n生成叶节点：当满足停止条件时，将当前节点作为叶节点，并计算叶节点的预测值，通常为叶节点内样本目标变量的均值。\n\n分类树 (Classification Tree):\n分类树 (Classification Tree) 用于预测离散类别型目标变量。例如，判断用户是否会流失、识别图像中的物体类别等。\n模型构建:\n分类树的构建过程与回归树类似，也是一个递归的二叉树构建过程。不同之处在于，分类树在选择最优特征和切分点时，使用的划分指标不同，以及叶节点的预测值类型不同。分类树常用的划分指标包括 信息增益 (Information Gain)、信息增益率 (Information Gain Ratio) 和 基尼指数 (Gini Index)。目标是使得划分后的子节点数据尽可能 “纯净” (Pure)，即属于同一类别的样本比例尽可能高。\n划分指标:\n\n信息增益 (Information Gain)：基于信息熵 (Entropy) 的划分指标。信息熵衡量了数据集的混乱程度或不确定性。信息增益表示使用特征 \\(A\\) 对数据集 \\(D\\) 进行划分后，数据集 \\(D\\) 的信息熵减少的程度。信息增益越大，说明使用特征 \\(A\\) 划分数据集的效果越好。常用的基于信息增益的决策树算法是 ID3 算法。\n信息增益率 (Information Gain Ratio)：为了克服信息增益对取值数目较多的特征的偏好，C4.5 算法引入了信息增益率。信息增益率在信息增益的基础上，除以特征 \\(A\\) 本身的熵，对特征取值数目较多的情况进行惩罚。常用的基于信息增益率的决策树算法是 C4.5 算法。\n基尼指数 (Gini Index)：基尼指数衡量了数据集的纯度。基尼指数越小，数据集纯度越高。CART 算法使用基尼指数作为分类树的划分指标。\n\n叶节点预测值:\n分类树的叶节点输出类别标签，通常是叶节点内样本数量最多的类别（多数表决）。\n决策树的特点:\n\n优点：\n\n易于理解和解释：决策树模型直观易懂，决策规则清晰可见，易于向业务人员解释。\n可以处理类别型和数值型特征：无需对特征进行预处理，如独热编码、标准化等。\n无需特征缩放：决策树模型对特征的尺度不敏感，无需进行特征缩放。\n可以处理缺失值：决策树模型可以处理包含缺失值的数据。\n可以进行特征选择：决策树模型在构建过程中会自动选择重要的特征进行划分。\n\n缺点：\n\n容易过拟合：决策树模型容易在训练集上过拟合，导致泛化能力差。可以通过剪枝 (Pruning) 等方法缓解过拟合问题。\n不稳定：决策树模型对训练数据敏感，训练数据的微小变化可能导致树结构发生很大变化。\n忽略特征之间的相关性：决策树模型在选择划分特征时，每次只考虑一个特征，忽略了特征之间的相关性。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "03_supervised.html#总结",
    "href": "03_supervised.html#总结",
    "title": "4  监督学习（上）",
    "section": "4.3 总结",
    "text": "4.3 总结\n本讲义主要介绍了监督学习的基本概念和常用模型，包括：\n\n监督学习概述: 介绍了监督学习的定义、应用场景以及与量化投资的结合。\n回归模型: 详细讲解了线性回归和岭回归模型，包括模型表达式、最小二乘法、正则化以及模型特点。\n分类模型: 深入探讨了支持向量机 (SVM) 和决策树模型，包括模型原理、核函数、优化方法以及模型优缺点。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>监督学习（上）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html",
    "href": "04_supervised.html",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "5.1 集成学习 (Ensemble Learning)\n集成学习 (Ensemble Learning) 是一种将多个弱学习器 (Weak Learner) 组合成一个强学习器 (Strong Learner) 的技术。 弱学习器通常指预测性能略优于随机猜测的模型，例如单层决策树、简单的线性模型等。集成学习的核心思想是 “集思广益”，即通过组合多个弱学习器的预测结果，来获得比单个强学习器更全面、更鲁棒的预测能力。\n集成学习模型通常具有比单个学习器更好的预测性能和泛化能力。 这是因为集成学习可以通过以下方式降低模型的误差：\n集成学习的核心思想是 “三个臭皮匠，顶个诸葛亮”，即通过集体智慧来提高模型的性能。常用的集成学习方法包括 Bagging (Bootstrap Aggregating)、Boosting (提升法) 和 Stacking (堆叠法)。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#集成学习-ensemble-learning",
    "href": "04_supervised.html#集成学习-ensemble-learning",
    "title": "5  监督学习（下）",
    "section": "",
    "text": "降低方差 (Variance Reduction)：Bagging 等方法通过并行训练多个基学习器，并对它们的预测结果进行平均或投票，可以有效降低模型的方差，提高模型的稳定性。\n降低偏差 (Bias Reduction)：Boosting 等方法通过串行训练多个基学习器，每个基学习器都试图纠正前一个基学习器的错误，可以有效降低模型的偏差，提高模型的预测精度。\n提高鲁棒性 (Robustness Improvement)：集成学习模型通常不易过拟合，对异常值和噪声数据具有更强的鲁棒性。\n\n\n\n5.1.1 Bagging (Bootstrap Aggregating)\nBagging (Bootstrap Aggregating)，也称为自助采样聚合，是一种基于 自助采样 (Bootstrap Sampling) 的集成学习方法。Bagging 的核心思想是 并行集成，即同时训练多个独立的基学习器，然后通过投票 (Voting) 或 平均 (Averaging) 的方式将它们的预测结果集成起来。Bagging 可以有效降低模型的方差 (Variance)，提高模型的稳定性和泛化能力，尤其适用于容易过拟合的基学习器，如决策树、神经网络等。\n自助采样 (Bootstrap Sampling) 的直观解释:\n想象一下，你有一个装满弹珠的罐子（代表原始数据集），你想估计罐子里弹珠颜色的分布。自助采样就像是：\n\n有放回地从罐子里随机抽取一些弹珠，记录它们的颜色，然后放回罐子。\n重复步骤 1 多次，每次抽取都形成一个新的弹珠样本集（子数据集）。\n\n由于是有放回的抽样，每次抽取的子数据集都可能包含重复的弹珠，也可能缺少原始数据集中的某些弹珠。但从整体上看，每个子数据集都近似地代表了原始数据集的分布。\nBagging 降低方差的原理:\nBagging 通过自助采样创建多个略有不同的训练数据集，并在每个数据集上独立训练一个基学习器。由于每个基学习器都是在不同的数据集上训练的，它们之间具有一定的差异性。当对多个基学习器的预测结果进行平均或投票时，可以有效地平滑掉单个基学习器预测结果中的随机波动，从而降低整体模型的方差。\n算法流程:\n\n自助采样 (Bootstrap Sampling)：从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建一个子数据集 \\(D_t\\)，也称为 自助样本集 (Bootstrap Sample)。重复 \\(T\\) 次，得到 \\(T\\) 个独立的子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。每个子数据集的大小与原始数据集相同，但样本分布略有不同。由于是有放回抽样，因此每个子数据集中可能包含重复样本，也可能缺少原始数据集中的某些样本。一般来说，每个子数据集大约包含原始数据集 63.2% 的样本。\n训练基学习器 (Base Learner)：在每个子数据集 \\(D_t\\) 上独立地训练一个基学习器 \\(h_t\\)。基学习器可以是同质的（例如，都使用决策树），也可以是异质的（例如，使用决策树、神经网络、SVM 等不同的模型）。常用的基学习器是决策树，此时的 Bagging 集成学习方法就是 随机森林 (Random Forest)。\n集成预测 (Ensemble Prediction)：对于分类问题，Bagging 通常使用 投票法 (Voting) 进行预测，即将 \\(T\\) 个基学习器预测结果中出现次数最多的类别作为最终预测结果。对于回归问题，Bagging 通常使用 平均法 (Averaging) 进行预测，即将 \\(T\\) 个基学习器预测结果的平均值作为最终预测结果。\n\n随机森林 (Random Forest):\n随机森林 (Random Forest, RF) 是一种非常流行且强大的 基于 Bagging 思想的集成学习模型，以 决策树 (Decision Tree) 为基学习器。随机森林在 Bagging 的基础上，进一步引入了 特征随机选择 (Random Feature Selection)，使得基学习器之间具有更高的差异性 (Diversity)，从而进一步提高集成的性能。随机森林具有高精度、高效率、鲁棒性强、不易过拟合等优点，被广泛应用于分类、回归和特征重要性评估等任务。\n特征随机选择 (Random Feature Selection) 的直观解释:\n在构建决策树时，传统的决策树会在所有特征中选择最优特征进行节点分裂。而特征随机选择则是在每个节点分裂时，先随机选择一部分特征，然后只在这部分特征中选择最优特征。\n特征随机选择提高模型性能的原理:\n特征随机选择进一步增加了基学习器之间的差异性。由于每个决策树只使用一部分随机选择的特征进行训练，即使在相同的子数据集上训练，不同的决策树也会学习到不同的特征子集，从而降低基学习器之间的相关性，提高集成的效果。 此外，特征随机选择还有助于降低模型的过拟合风险，并提高模型的泛化能力。\n随机森林的构建过程:\n\n自助采样 (Bootstrap Sampling)：与 Bagging 相同，从原始数据集 \\(D\\) 中有放回地随机抽取 \\(N\\) 个样本，构建 \\(T\\) 个子数据集 \\(\\{D_1, D_2, ..., D_T\\}\\)。\n训练基学习器 (决策树)：在每个子数据集 \\(D_t\\) 上训练一个决策树 \\(h_t\\)。与传统的决策树不同，随机森林在训练决策树的过程中，引入了特征随机选择。具体来说，在决策树的每个节点分裂时，不是从所有特征中选择最优特征，而是先随机选择 \\(m\\) 个特征构成一个特征子集（通常 \\(m &lt;&lt; K\\)，例如 \\(m = \\sqrt{K}\\)），然后从这个特征子集中选择最优特征进行分裂。这里的 \\(m\\) 是一个超参数，需要预先设定。特征随机选择进一步增加了基学习器之间的差异性，使得随机森林的集成效果更好。\n集成预测 (Ensemble Prediction)：与 Bagging 相同，对于分类问题使用投票法，对于回归问题使用平均法。\n\n随机森林的特点:\n\n高精度：随机森林通常比单个决策树具有更高的预测精度。\n鲁棒性强：随机森林对异常值和噪声数据具有较好的鲁棒性。\n不易过拟合：随机森林通过 Bagging 和特征随机选择，有效降低了模型的方差，不易过拟合。\n泛化能力强：随机森林具有较强的泛化能力，在测试集上表现良好。\n可处理高维数据：随机森林可以处理高维数据，无需进行特征选择。\n可评估特征重要性：随机森林可以评估每个特征在模型中的重要性，用于特征选择和特征理解。\n实现简单，易于并行化：随机森林的构建过程简单高效，基学习器之间相互独立，易于并行化处理，训练速度快。\n\n\n\n5.1.2 Boosting (提升法)\nBoosting (提升法) 是一种与 Bagging 不同的集成学习方法。Boosting 的核心思想是 串行集成，即迭代地训练一系列的基学习器，每个基学习器都试图纠正前一个基学习器的错误。Boosting 方法通过加权样本或调整预测结果的方式，逐步提升模型的性能。Boosting 方法可以有效降低模型的偏差 (Bias) 和方差，得到高精度的集成模型，尤其适用于弱学习器，如浅层决策树 (Decision Stump)。常用的 Boosting 算法包括 AdaBoost (Adaptive Boosting)、GBDT (Gradient Boosting Decision Tree)、XGBoost (Extreme Gradient Boosting)、LightGBM (Light Gradient Boosting Machine)、CatBoost (Categorical Boosting) 等。\nBoosting 的串行集成思想的直观解释:\nBoosting 就像是一个团队合作解决问题的过程，其中：\n\n第一个弱学习器先尝试解决问题，但可能做得不够好，会犯一些错误。\n后续的弱学习器会仔细研究第一个学习器犯的错误，并针对这些错误进行改进，尝试纠正之前的错误。\n每个弱学习器都在前一个学习器的基础上进行提升，逐步提高整体的预测能力。\n\nBoosting 迭代提升模型性能的原理:\nBoosting 方法通过迭代训练，每一轮迭代都关注前一轮模型预测错误的样本，并调整样本权重或模型权重，使得后续的模型更加关注难以分类或预测的样本。 这样不断地迭代和调整，逐步将弱学习器提升为强学习器，最终得到一个高精度、高性能的集成模型。\nAdaBoost (Adaptive Boosting):\nAdaBoost (Adaptive Boosting, 自适应提升) 是一种经典的 Boosting 算法。AdaBoost 的核心思想是 “关注错误样本” 和 “加权基学习器”。AdaBoost 通过迭代地训练基学习器，每轮迭代都更加关注前一轮基学习器预测错误的样本，提高错误样本的权重，降低正确样本的权重，使得后续的基学习器更加关注难以分类的样本。同时，AdaBoost 为每个基学习器赋予一个权重，预测性能好的基学习器权重较高，预测性能差的基学习器权重较低。最终模型是所有基学习器的加权线性组合。AdaBoost 算法主要用于二分类问题。\n“关注错误样本” 和 “加权基学习器” 的直观解释:\n\n关注错误样本：在每一轮迭代中，AdaBoost 会提高上一轮分类错误的样本的权重，使得后续的基学习器更加关注这些难分样本，努力将它们分类正确。 这就像老师在辅导学生时，会更加关注那些经常犯错的学生，帮助他们改正错误，提高学习成绩。\n加权基学习器：AdaBoost 会根据每个基学习器的预测性能，赋予不同的权重。预测性能好的基学习器，例如错误率低的基学习器，会被赋予更高的权重，在最终的预测中起更大的作用。 这就像专家团队中，更信任那些经验丰富、能力强的专家的意见。\n\nAdaBoost 算法流程 (二分类):\n\n初始化样本权重 (Initialize Sample Weights)：为每个样本赋予相同的初始权重 \\(w_{1i} = 1/N, i = 1, 2, ..., N\\)。初始时，所有样本的权重相同，表示所有样本同等重要。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n训练基学习器 (Train Base Learner)：使用带有样本权重的训练数据集 \\(\\{( \\mathbf{x}_i, y_i, w_{ti} )\\}_{i=1}^{N}\\) 训练一个基学习器 \\(h_t(\\mathbf{x})\\)。在第 \\(t\\) 轮迭代中，基学习器 \\(h_t\\) 基于样本权重 \\(w_{ti}\\) 进行训练，使得加权训练误差最小化。基学习器通常选择弱学习器，如决策树桩 (Decision Stump)，即单层决策树。\n计算基学习器权重 (Calculate Base Learner Weight)：计算基学习器 \\(h_t\\) 在训练集上的加权错误率 \\(e_t = P(h_t(\\mathbf{x}_i) \\ne y_i) = \\sum_{i=1}^{N} w_{ti} I(h_t(\\mathbf{x}_i) \\ne y_i)\\)。 \\(e_t\\) 表示被基学习器 \\(h_t\\) 误分类的样本的权重之和。如果 \\(e_t &gt; 0.5\\)，则停止迭代，因为此时基学习器的性能甚至不如随机猜测。然后计算基学习器 \\(h_t\\) 的权重 \\(\\alpha_t = \\frac{1}{2} \\ln(\\frac{1-e_t}{e_t})\\)。当 \\(e_t\\) 越小时，\\(\\alpha_t\\) 越大，说明基学习器 \\(h_t\\) 的预测性能越好，权重越高。\n更新样本权重 (Update Sample Weights)：根据基学习器 \\(h_t\\) 的预测结果更新样本权重，提高误分类样本的权重，降低正确分类样本的权重，使得后续的基学习器更加关注难以分类的样本。样本权重更新公式为： \\[w_{t+1, i} = \\frac{w_{ti}}{Z_t} \\times \\begin{cases} e^{-\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) = y_i \\\\ e^{\\alpha_t}, & \\text{if } h_t(\\mathbf{x}_i) \\ne y_i \\end{cases} = \\frac{w_{ti}}{Z_t} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\] 其中 \\(Z_t = \\sum_{i=1}^{N} w_{ti} e^{-\\alpha_t y_i h_t(\\mathbf{x}_i)}\\) 是归一化因子 (Normalization Factor)，使得 \\(\\sum_{i=1}^{N} w_{t+1, i} = 1\\)，保证样本权重之和为 1。 \\(y_i \\in \\{-1, +1\\}\\) 是样本的真实标签， \\(h_t(\\mathbf{x}_i) \\in \\{-1, +1\\}\\) 是基学习器的预测标签。如果样本被正确分类 (\\(y_i h_t(\\mathbf{x}_i) = +1\\))，则样本权重乘以 \\(e^{-\\alpha_t} &lt; 1\\)，权重降低；如果样本被误分类 (\\(y_i h_t(\\mathbf{x}_i) = -1\\))，则样本权重乘以 \\(e^{\\alpha_t} &gt; 1\\)，权重提高。\n\n构建最终模型 (Build Final Model)：经过 \\(T\\) 轮迭代后，得到 \\(T\\) 个基学习器 \\(\\{h_1, h_2, ..., h_T\\}\\) 及其对应的权重 \\(\\{\\alpha_1, \\alpha_2, ..., \\alpha_T\\}\\)。最终模型是基学习器的加权线性组合：对于新样本 \\(\\mathbf{x}\\)，最终模型的预测结果为： \\(H(\\mathbf{x}) = \\text{sign}(\\sum_{t=1}^{T} \\alpha_t h_t(\\mathbf{x}))\\)。 \\(\\text{sign}(z)\\) 是符号函数，当 \\(z &gt; 0\\) 时，\\(\\text{sign}(z) = +1\\)；当 \\(z &lt; 0\\) 时，\\(\\text{sign}(z) = -1\\)；当 \\(z = 0\\) 时，\\(\\text{sign}(z) = 0\\) 或 \\(+1\\) 或 \\(-1\\)，通常取 \\(+1\\) 或 \\(-1\\)。\n\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT):\n梯度提升决策树 (Gradient Boosting Decision Tree, GBDT) 是一种非常强大且广泛应用的 Boosting 算法，以 决策树 (Decision Tree) 为基学习器。GBDT 的核心思想是 梯度提升 (Gradient Boosting)，也称为 梯度下降提升 (Gradient Descent Boosting)。GBDT 使用 梯度提升算法，通过迭代地训练决策树来拟合负梯度残差 (Negative Gradient Residuals)，逐步逼近真实的目标函数。GBDT 可以用于回归和分类问题，尤其在非线性和复杂的预测问题中表现出色。GBDT 是许多高级 Boosting 算法（如 XGBoost、LightGBM、CatBoost）的基础。\n梯度提升 (Gradient Boosting) 的直观解释:\n梯度提升的思想可以理解为函数空间的梯度下降。在传统的梯度下降中，我们是在参数空间中沿着负梯度方向迭代优化参数，以最小化损失函数。而在梯度提升中，我们是在函数空间中沿着负梯度方向迭代优化模型函数，以逼近真实的目标函数。\nGBDT 拟合负梯度残差的原理:\nGBDT 的每一轮迭代都训练一个新的决策树，目标是拟合当前模型预测结果与真实值之间的残差。更精确地说，GBDT 拟合的是损失函数的负梯度，即残差的某种形式。通过不断地拟合残差，GBDT 逐步减小模型的预测误差，提高模型的预测精度。\nGBDT 算法流程 (以回归问题为例):\nGBDT 算法流程 (以回归问题为例):\n\n初始化模型 (Initialize Model)：初始化一个弱学习器（例如，常数模型 (Constant Model)，即所有样本的预测值都为一个常数） \\(f_0(\\mathbf{x}) = \\text{average}(y_i)\\)。 \\(f_0(\\mathbf{x})\\) 是初始模型的预测函数，通常使用训练集样本目标变量的均值作为初始预测值。\n迭代训练基学习器 (Iterative Training of Base Learners)：进行 \\(T\\) 轮迭代， \\(t = 1, 2, ..., T\\)：\n\n计算负梯度残差 (Compute Negative Gradient Residuals)：对于每个样本 \\(i = 1, 2, ..., N\\)，计算负梯度残差 \\(r_{ti} = y_i - f_{t-1}(\\mathbf{x}_i)\\)。 \\(f_{t-1}(\\mathbf{x}_i)\\) 是前一轮迭代得到的模型 \\(f_{t-1}\\) 对样本 \\(\\mathbf{x}_i\\) 的预测值。 \\(r_{ti}\\) 表示真实值 \\(y_i\\) 与当前模型预测值 \\(f_{t-1}(\\mathbf{x}_i)\\) 之间的差异，即模型在样本 \\(\\mathbf{x}_i\\) 上的预测误差。负梯度残差 \\(r_{ti}\\) 可以看作是本轮迭代需要拟合的目标，即模型需要在本轮迭代中学习如何纠正前一轮的预测误差。\n训练决策树 (Train Decision Tree)：使用 \\((\\mathbf{x}_i, r_{ti})_{i=1}^{N}\\) 作为训练数据，训练一个决策树 \\(h_t(\\mathbf{x})\\)，拟合负梯度残差 \\(r_{ti}\\)。决策树 \\(h_t(\\mathbf{x})\\) 的叶节点区域 \\(\\{R_{tj}\\}_{j=1}^{J_t}\\) 将特征空间划分为 \\(J_t\\) 个互不相交的区域， \\(J_t\\) 是决策树 \\(h_t\\) 的叶节点数量。\n确定叶节点区域的输出值 (Determine Leaf Node Output Values)：对于决策树 \\(h_t(\\mathbf{x})\\) 的每个叶节点区域 \\(R_{tj}\\)，计算该区域内样本负梯度残差的平均值 \\(c_{tj} = \\text{average}_{\\mathbf{x}_i \\in R_{tj}} (r_{ti}) = \\frac{1}{|R_{tj}|} \\sum_{\\mathbf{x}_i \\in R_{tj}} r_{ti}\\)。 \\(c_{tj}\\) 是决策树 \\(h_t\\) 在叶节点区域 \\(R_{tj}\\) 上的预测值，表示模型在本轮迭代中需要在区域 \\(R_{tj}\\) 内进行的调整量。\n更新模型 (Update Model)：更新模型 \\(f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj})\\)，其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，也称为 shrinkage 参数，通常取值范围为 \\((0, 1]\\)，例如 0.1、0.01 等。 \\(\\alpha\\) 控制每个基学习器的步长，减小学习率可以降低模型对后续基学习器的依赖程度，提高模型的泛化能力，但需要更多的迭代次数。 \\(I(\\mathbf{x} \\in R_{tj})\\) 是指示函数，当样本 \\(\\mathbf{x}\\) 属于叶节点区域 \\(R_{tj}\\) 时， \\(I(\\mathbf{x} \\in R_{tj}) = 1\\)，否则 \\(I(\\mathbf{x} \\in R_{tj}) = 0\\)。 \\(f_t(\\mathbf{x})\\) 是本轮迭代更新后的模型，它是在前一轮模型 \\(f_{t-1}(\\mathbf{x})\\) 的基础上，加上本轮训练的决策树 \\(h_t(\\mathbf{x})\\) 的加权结果，逐步逼近真实的目标函数。\n\n得到最终模型 (Obtain Final Model)：经过 \\(T\\) 轮迭代后，得到最终的 GBDT 模型 \\(f_T(\\mathbf{x}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\sum_{j=1}^{J_t} \\alpha c_{tj} I(\\mathbf{x} \\in R_{tj}) = f_0(\\mathbf{x}) + \\sum_{t=1}^{T} \\alpha h_t(\\mathbf{x})\\)。最终模型是所有基学习器的加权和。\n\nGBDT 算法流程 (以分类问题为例):\nGBDT 用于分类问题时，算法流程与回归问题类似，主要区别在于： - 损失函数不同：回归问题通常使用平方误差损失函数 (Squared Error Loss)，分类问题通常使用对数似然损失函数 (Log-Likelihood Loss) 或 指数损失函数 (Exponential Loss) 等。 - 负梯度计算不同：不同损失函数的负梯度计算方式不同。例如，对于二分类问题，如果使用对数似然损失函数，则负梯度残差的计算公式与回归问题不同。 - 叶节点区域的输出值确定方式不同：分类问题中，叶节点区域的输出值通常不是负梯度残差的平均值，而是根据具体的损失函数和优化目标确定。例如，对于二分类问题，可以使用对数几率 (Log Odds) 或 类别概率 作为叶节点输出值。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#神经网络-neural-network",
    "href": "04_supervised.html#神经网络-neural-network",
    "title": "5  监督学习（下）",
    "section": "5.2 神经网络 (Neural Network)",
    "text": "5.2 神经网络 (Neural Network)\n神经网络 (Neural Network, NN)，更精确地说是人工神经网络 (Artificial Neural Network, ANN)，是一种模拟生物神经系统结构的计算模型，由大量相互连接的神经元 (Neuron) 组成。神经网络可以学习复杂的非线性关系，具有强大的模式识别、函数逼近和自适应能力。神经网络在图像识别、自然语言处理、语音识别、金融预测等领域取得了巨大成功，是深度学习 (Deep Learning) 的基础。\n神经网络的核心思想:\n神经网络的核心思想是模拟生物神经系统的信息处理方式。生物神经系统由大量的神经元相互连接而成，神经元之间通过电信号和化学信号传递信息。神经网络试图抽象和模拟这种信息传递和处理机制，通过构建由大量神经元相互连接的网络结构，来实现复杂的信息处理和学习任务。\n神经网络的优势:\n\n强大的非线性建模能力：神经网络通过激活函数的非线性变换，可以学习和表示非常复杂的非线性关系，这是传统线性模型难以实现的。\n高度的并行分布式计算能力：神经网络由大量的神经元并行工作，可以高效地处理大规模数据和复杂计算任务。\n良好的泛化能力：通过合理的网络结构设计和训练方法，神经网络可以学习到数据中的本质规律，具有良好的泛化能力，在未见过的数据上也能表现良好。\n自适应学习能力：神经网络可以通过反向传播算法等方法，自动地从数据中学习，调整网络参数，适应不同的任务和数据。\n\n\n5.2.1 神经网络模型及算法\n神经元模型 (Neuron Model):\n神经元是神经网络的基本单元，也称为 感知机 (Perceptron)。一个典型的神经元模型包括以下几个主要组成部分：\n\n输入 (Input)：神经元接收来自其他神经元或外部环境的输入信号。输入信号可以是数值、向量或张量等形式。 例如，对于图像识别任务，输入可以是图像像素的灰度值；对于自然语言处理任务，输入可以是词向量。\n权重 (Weight)：每个输入连接都对应一个权重 \\(w_{ij}\\)，表示连接的强度或重要性。权重可以是正数（兴奋性连接）或负数（抑制性连接）。 权重是神经元学习的关键参数，通过调整权重，神经元可以选择性地接收和处理不同的输入信号。\n偏置 (Bias)：神经元还接收一个偏置 \\(b_i\\)，也称为 阈值 (Threshold)。偏置是一个常数，用于调整神经元的激活阈值，使得神经元更容易或更不容易被激活。 偏置可以看作是神经元的一个自由度，使得神经元可以更加灵活地进行激活。\n加权求和 (Weighted Summation)：神经元将所有输入信号与对应的权重进行加权求和，再加上偏置，得到神经元的净输入 (Net Input) \\(z_i = \\sum_{j} w_{ij} x_j + b_i\\)。 净输入 \\(z_i\\) 表示神经元接收到的所有输入信号的综合强度。\n激活函数 (Activation Function)：神经元对净输入 \\(z_i\\) 进行非线性变换，通过激活函数 \\(\\sigma(\\cdot)\\) 产生神经元的输出 (Output) \\(a_i = \\sigma(z_i) = \\sigma(\\sum_{j} w_{ij} x_j + b_i)\\)。激活函数是神经网络实现非线性建模的关键。 激活函数引入了非线性因素，使得神经网络可以逼近任意复杂的非线性函数。常用的激活函数包括：\n\nSigmoid 函数：\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)，将输入值映射到 \\((0, 1)\\) 区间，常用于二分类问题的输出层。 Sigmoid 函数的输出值可以解释为概率，例如样本属于正类的概率。\nTanh 函数 (Hyperbolic Tangent Function)：\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1\\)，将输入值映射到 \\((-1, 1)\\) 区间，与 Sigmoid 函数类似，但输出范围不同。 Tanh 函数的输出值以 0 为中心，可能在某些情况下比 Sigmoid 函数更易于训练。\nReLU 函数 (Rectified Linear Unit)：\\(\\text{ReLU}(z) = \\max(0, z)\\)，当输入值大于 0 时，输出值等于输入值；当输入值小于等于 0 时，输出值为 0。ReLU 函数计算简单，收敛速度快，是深度神经网络中最常用的激活函数之一。 ReLU 函数在正区间是线性函数，负区间是常数 0，这种简单的非线性使其在深度网络中表现出色。\nLeaky ReLU 函数 (Leaky Rectified Linear Unit)：\\(\\text{Leaky ReLU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha z, & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个很小的常数，例如 0.01。Leaky ReLU 函数解决了 ReLU 函数在输入值为负数时输出值为 0 导致神经元 “死亡” 的问题。 Leaky ReLU 函数在负区间也保持一定的梯度，有助于信息在网络中更好地传播。\nELU 函数 (Exponential Linear Unit)：\\(\\text{ELU}(z) = \\begin{cases} z, & \\text{if } z &gt; 0 \\\\ \\alpha (e^z - 1), & \\text{if } z \\le 0 \\end{cases}\\)，其中 \\(\\alpha\\) 是一个正的常数。ELU 函数具有 ReLU 函数的优点，同时在输入值为负数时输出值也具有一定的梯度，可以加速神经网络的收敛。 ELU 函数在负区间使用指数函数，可以提供更平滑的输出，并有助于网络的鲁棒性。\nSoftmax 函数：\\(\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\\)，用于多分类问题的输出层。Softmax 函数将一组输入值映射为一组概率值，每个概率值都在 \\((0, 1)\\) 区间，且所有概率值之和为 1。 Softmax 函数可以将神经元的输出转换为类别概率分布，方便进行多分类任务。\n\n输出 (Output)：神经元的输出信号 \\(a_i\\)，可以作为其他神经元的输入，或作为整个神经网络的输出。 神经元的输出是经过激活函数处理后的结果，可以传递给下一层神经元，或者作为最终的预测结果。\n\n多层神经网络 (Multilayer Neural Network, MLNN) / 多层感知机 (Multilayer Perceptron, MLP):\n多层神经网络 (Multilayer Neural Network, MLNN) 或 多层感知机 (Multilayer Perceptron, MLP) 是由多个神经元层相互连接而成的神经网络。多层神经网络通常包括以下几种类型的层：\n\n输入层 (Input Layer)：接收外部输入数据。输入层的神经元不进行任何计算，只是将输入数据传递给下一层。输入层神经元的数量等于输入特征的维度。 例如，如果输入是 28x28 的图像，则输入层通常有 784 个神经元（将图像展平成向量）。\n隐藏层 (Hidden Layer)：位于输入层和输出层之间，可以有一层或多层。隐藏层是神经网络的核心部分，负责提取输入数据的特征，学习输入数据中的复杂模式。隐藏层神经元的数量和层数是神经网络的超参数，需要根据具体问题进行设计和调整。 隐藏层越多，网络可以学习到的特征就越抽象、越复杂。\n输出层 (Output Layer)：产生最终的输出结果。输出层神经元的数量取决于具体的任务类型。例如，二分类问题的输出层通常只有一个神经元，使用 Sigmoid 激活函数，输出样本属于正类的概率；多分类问题的输出层通常有 \\(C\\) 个神经元（\\(C\\) 是类别数量），使用 Softmax 激活函数，输出样本属于每个类别的概率；回归问题的输出层通常只有一个神经元，不使用激活函数或使用 线性激活函数，直接输出预测值。\n\n深度神经网络 (Deep Neural Network, DNN) 指的是具有多个隐藏层的神经网络。 “深度” 指的是网络结构的深度，即隐藏层的层数。 深度神经网络可以学习到更加抽象和层次化的特征表示，从而更好地处理复杂的数据和任务。\n全连接神经网络 (Fully Connected Neural Network, FCNN) / 稠密神经网络 (Dense Neural Network):\n在多层神经网络中，相邻层之间的神经元通常采用 全连接 (Fully Connected) 的方式进行连接，即前一层的每个神经元都与后一层的所有神经元连接。这种连接方式的神经网络称为 全连接神经网络 (Fully Connected Neural Network, FCNN) 或 稠密神经网络 (Dense Neural Network)。全连接神经网络是最基本的神经网络结构，也是许多复杂神经网络的基础。\n前向传播 (Forward Propagation):\n前向传播 (Forward Propagation) 是指输入信号从输入层经过隐藏层逐层传递到输出层的过程。在前向传播过程中，每一层的神经元接收来自前一层的输出，进行加权求和和激活函数处理，然后将输出传递给下一层。通过逐层传递和计算，最终得到输出层的输出，即神经网络的预测结果。\n前向传播的计算步骤:\n\n输入层：将输入数据输入到输入层神经元。\n隐藏层：对于每个隐藏层，依次计算每个神经元的输出。计算过程包括：\n\n加权求和：将前一层所有神经元的输出与连接权重相乘，并加上偏置，得到净输入。\n激活函数：将净输入通过激活函数进行非线性变换，得到神经元的输出。\n\n输出层：计算输出层每个神经元的输出，计算过程与隐藏层类似。输出层神经元的输出即为神经网络的最终预测结果。\n\n反向传播算法 (Backpropagation, BP):\n反向传播算法 (Backpropagation, BP) 是训练多层神经网络最常用和最核心的算法。BP 算法是一种基于梯度下降法 (Gradient Descent) 的误差反向传播算法，通过计算损失函数 (Loss Function) 对网络参数（权重和偏置）的梯度，然后沿着梯度反方向更新参数，迭代地最小化损失函数，从而学习到最优的网络参数。BP 算法实现了误差信号从输出层反向传播到输入层，逐层调整网络参数，使得神经网络能够学习到输入数据中的复杂模式。\n梯度下降法 (Gradient Descent) 的直观解释:\n梯度下降法就像是在山坡上寻找山谷最低点的过程。\n\n确定当前位置（对应于当前的参数值）。\n沿着当前位置最陡峭的方向（负梯度方向）向下走一步（更新参数）。\n重复步骤 2，直到到达山谷最低点（损失函数达到最小值）。\n\n反向传播算法的步骤:\n\n前向传播 (Forward Propagation)：给定输入样本 \\((\\mathbf{x}_i, y_i)\\)，计算每个样本的前向传播输出，得到神经网络的预测值 \\(\\hat{y}_i\\)。前向传播过程是从输入层开始，逐层计算每个神经元的输出，直到输出层。\n计算损失 (Compute Loss)：根据预测值 \\(\\hat{y}_i\\) 和真实值 \\(y_i\\)，计算损失函数值 \\(L_i = L(\\hat{y}_i, y_i)\\)。损失函数衡量了模型预测结果与真实值之间的差异。常用的损失函数包括：\n\n均方误差损失函数 (Mean Squared Error Loss, MSE)：\\(L_{MSE}(\\hat{y}, y) = \\frac{1}{2} (\\hat{y} - y)^2\\)，常用于回归问题。\n交叉熵损失函数 (Cross-Entropy Loss Function)：\\(L_{CE}(\\hat{y}, y) = - [y \\ln \\hat{y} + (1-y) \\ln (1-\\hat{y})]\\)，常用于二分类问题。对于多分类问题，可以使用 多类交叉熵损失函数 (Categorical Cross-Entropy Loss)。\n\n反向传播误差 (Backpropagate Error)：从输出层开始，反向计算每一层的误差项 (Error Term) \\(\\delta_l\\)。误差项 \\(\\delta_l\\) 反映了第 \\(l\\) 层神经元的输出对最终损失的影响程度，是损失函数关于第 \\(l\\) 层神经元净输入 \\(z_l\\) 的梯度。误差项的计算公式为：\n\n输出层误差项：\\(\\delta_{output} = \\frac{\\partial L}{\\partial z_{output}} = \\frac{\\partial L}{\\partial a_{output}} \\odot \\sigma'_{output}(z_{output})\\)，其中 \\(\\odot\\) 表示逐元素乘积 (Element-wise Product)，\\(\\sigma'_{output}(z_{output})\\) 是输出层激活函数 \\(\\sigma_{output}\\) 对净输入 \\(z_{output}\\) 的导数。\n隐藏层误差项：\\(\\delta_l = \\frac{\\partial L}{\\partial z_l} = (\\mathbf{W}_{l+1}^T \\delta_{l+1}) \\odot \\sigma'_{l}(z_{l})\\)，其中 \\(\\mathbf{W}_{l+1}\\) 是第 \\(l\\) 层到第 \\(l+1\\) 层的权重矩阵，\\(\\delta_{l+1}\\) 是第 \\(l+1\\) 层的误差项，\\(\\sigma'_{l}(z_{l})\\) 是第 \\(l\\) 层激活函数 \\(\\sigma_{l}\\) 对净输入 \\(z_{l}\\) 的导数。误差项的计算是从输出层向输入层逐层反向传播的。\n\n计算梯度 (Compute Gradients)：根据误差项 \\(\\delta_l\\)，计算损失函数关于每一层权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\) 的梯度。梯度的计算公式为：\n\n权重梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\delta_l \\mathbf{a}_{l-1}^T\\)，其中 \\(\\mathbf{a}_{l-1}\\) 是第 \\(l-1\\) 层的输出（对于输入层，\\(\\mathbf{a}_0 = \\mathbf{x}\\)）。\n偏置梯度：\\(\\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\delta_l\\)。\n\n更新参数 (Update Parameters)：沿着梯度反方向更新权重 \\(\\mathbf{W}_l\\) 和偏置 \\(\\mathbf{b}_l\\)，最小化损失函数。参数更新公式为：\n\n权重更新：\\(\\mathbf{W}_l = \\mathbf{W}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_l} = \\mathbf{W}_l - \\alpha \\delta_l \\mathbf{a}_{l-1}^T\\)\n偏置更新：\\(\\mathbf{b}_l = \\mathbf{b}_l - \\alpha \\frac{\\partial L}{\\partial \\mathbf{b}_l} = \\mathbf{b}_l - \\alpha \\delta_l\\) 其中 \\(\\alpha\\) 是 学习率 (Learning Rate)，控制参数更新的步长。\n\n迭代 (Iteration)：重复步骤 1-5，遍历所有训练样本（或一个批次的样本），进行多轮迭代 (Epoch)，直到损失函数收敛或达到预设的迭代次数。\n\n\n\n5.2.2 深度学习 (Deep Learning)\n深度学习 (Deep Learning) 是机器学习的一个分支，本质上就是具有多层隐藏层的神经网络。 深度学习通过构建深层神经网络，学习数据中更加抽象和复杂的特征表示，从而解决更加复杂的任务。 深度学习在图像识别、自然语言处理、语音识别等领域取得了突破性进展，推动了人工智能的快速发展。\n深度学习的关键要素:\n\n深层神经网络结构：深度学习模型通常具有多个隐藏层，可以学习到多层次的特征表示。\n大规模数据集：深度学习模型通常需要大规模的数据集进行训练，才能充分发挥其性能。\n强大的计算能力：深度学习模型的训练通常需要大量的计算资源，例如 GPU (图形处理器)。\n高效的优化算法：深度学习模型的训练需要高效的优化算法，例如 Adam、SGD with Momentum 等。\n\n神经网络的变体模型 (Variations of Neural Networks):\n随着深度学习的发展，研究者们提出了各种各样的神经网络变体模型，以适应不同的任务和数据类型。以下是一些常见的神经网络变体模型：\n\n卷积神经网络 (Convolutional Neural Network, CNN)：\n\n特点：CNN 是一种专门用于处理图像数据的神经网络。CNN 的核心组件是卷积层 (Convolutional Layer) 和 池化层 (Pooling Layer)。卷积层可以自动学习图像中的局部特征，例如边缘、纹理等。池化层可以降低特征图的维度，减少计算量，并提高模型的鲁棒性。\n应用场景：图像分类、目标检测、图像分割、人脸识别等计算机视觉任务。\n\n循环神经网络 (Recurrent Neural Network, RNN)：\n\n特点：RNN 是一种专门用于处理序列数据的神经网络。RNN 的特点是具有循环连接，使得网络可以记忆之前的输入信息，并应用于当前的输出。\n应用场景：自然语言处理 (如文本分类、机器翻译、文本生成)、语音识别、时间序列预测等序列数据处理任务。\n常见变体：长短期记忆网络 (Long Short-Term Memory Network, LSTM) 和 门控循环单元网络 (Gated Recurrent Unit Network, GRU)，它们解决了传统 RNN 在长序列数据中容易出现的梯度消失和梯度爆炸问题。\n\nTransformer 网络:\n\n特点：Transformer 是一种基于自注意力机制 (Self-Attention Mechanism) 的神经网络结构。Transformer 摒弃了传统的 RNN 结构，完全依赖自注意力机制来捕捉输入序列中不同位置之间的关系。Transformer 具有并行计算能力强、可以捕捉长距离依赖关系等优点。\n应用场景：自然语言处理 (如机器翻译、文本摘要、问答系统)、图像识别、语音识别等各种序列数据处理任务。\n重要模型：BERT (Bidirectional Encoder Representations from Transformers)、GPT (Generative Pre-trained Transformer) 等预训练语言模型，在自然语言处理领域取得了革命性的进展。\n\n图神经网络 (Graph Neural Network, GNN)：\n\n特点：GNN 是一种专门用于处理图结构数据的神经网络。GNN 可以学习图中节点和边的特征表示，并进行节点分类、链接预测、图分类等任务。\n应用场景：社交网络分析、知识图谱、推荐系统、生物信息学等图结构数据分析任务。\n\n生成对抗网络 (Generative Adversarial Network, GAN)：\n\n特点：GAN 是一种生成模型，由生成器 (Generator) 和 判别器 (Discriminator) 两个神经网络组成。生成器负责生成假数据，判别器负责区分真假数据。两个网络相互对抗训练，最终生成器可以生成逼真的数据。\n应用场景：图像生成、图像编辑、数据增强、风格迁移等生成式任务。\n\n自编码器 (Autoencoder, AE)：\n\n特点：自编码器 是一种无监督学习模型，用于学习数据的低维表示 (特征)。自编码器由编码器 (Encoder) 和 解码器 (Decoder) 两个神经网络组成。编码器将输入数据压缩到低维空间，解码器将低维表示重构回原始数据。\n应用场景：特征提取、降维、数据去噪、异常检测等无监督学习任务。\n常见变体：变分自编码器 (Variational Autoencoder, VAE)、稀疏自编码器 (Sparse Autoencoder)、降噪自编码器 (Denoising Autoencoder) 等。\n\n\n总而言之，神经网络和深度学习是当前机器学习领域最热门和最重要的方向之一。 掌握神经网络的基本原理和常用模型，对于理解和应用人工智能技术至关重要。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "04_supervised.html#总结",
    "href": "04_supervised.html#总结",
    "title": "5  监督学习（下）",
    "section": "5.3 总结",
    "text": "5.3 总结\n本讲义深入探讨了集成学习和神经网络模型，主要内容包括：\n\n集成学习: 详细介绍了 Bagging 和 Boosting 两种主要的集成学习方法，以及随机森林和 AdaBoost 算法的原理和特点。\n神经网络: 系统讲解了神经网络的基本结构、前向传播、反向传播算法以及深度学习的概念。\n深度学习变体模型: 简要介绍了卷积神经网络 (CNN)、循环神经网络 (RNN)、Transformer 网络、图神经网络 (GNN) 和生成对抗网络 (GAN) 等深度学习模型的特点和应用场景。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>监督学习（下）</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html",
    "href": "05_model_assess.html",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1 模型评估与优化",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型评估与优化",
    "href": "05_model_assess.html#模型评估与优化",
    "title": "6  模型评估与优化",
    "section": "",
    "text": "6.1.1 为什么需要评估模型？\n\n避免过拟合与欠拟合: 模型可能在训练数据上表现很好，但在未见过的数据上表现很差 (过拟合)，或者模型可能无法捕捉到数据中的基本模式 (欠拟合)。\n选择最佳模型: 当我们尝试不同的模型或模型配置时，我们需要一种方法来比较它们的性能并选择最佳模型。\n了解模型性能: 评估可以帮助我们了解模型在不同情况下的表现，例如在不同的数据子集或不同的任务上。\n指导模型改进: 评估结果可以帮助我们识别模型的弱点，并指导我们如何改进模型。\n\n\n\n6.1.2 评估指标\n评估指标的选择取决于具体的机器学习任务类型。\n\n6.1.2.1 分类模型评估指标\n\n准确率 (Accuracy): 分类正确的样本数占总样本数的比例。\n\n适用于类别分布均衡的数据集。\n当类别不平衡时，准确率可能会产生误导。\n\n\\[\n  \\text{Accuracy} = \\frac{\\text{正确分类的样本数}}{\\text{总样本数}}\n  \\]\n精确率 (Precision): 预测为正例的样本中，真正例的比例。\n\n关注模型预测正例的准确性。\n\n\\[\n  \\text{Precision} = \\frac{TP}{TP + FP}\n  \\]\n\nTP (True Positive): 真正例，模型预测为正例，实际也是正例。\nFP (False Positive): 假正例，模型预测为正例，实际是负例。\n\n召回率 (Recall): 所有实际正例中，被模型正确预测为正例的比例。\n\n关注模型发现所有正例的能力。\n\n\\[\n  \\text{Recall} = \\frac{TP}{TP + FN}\n  \\]\n\nFN (False Negative): 假负例，模型预测为负例，实际是正例。\n\nF1 分数 (F1-Score): 精确率和召回率的调和平均值。\n\n综合考虑精确率和召回率。\n当精确率和召回率都很重要时，F1 分数是一个很好的指标。\n\n\\[\n  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n  \\]\nAUC-ROC 曲线: 受试者工作特征 (Receiver Operating Characteristic) 曲线下的面积。\n\nROC 曲线描述了在不同阈值下，真正例率 (TPR) 与假正例率 (FPR) 之间的关系。\nAUC 值越大，模型性能越好。\n适用于评估二分类模型的排序能力。\nTPR (True Positive Rate) 或 灵敏度 (Sensitivity): \\(\\frac{TP}{TP + FN}\\)， 等于召回率 (Recall)。\nFPR (False Positive Rate): \\(\\frac{FP}{FP + TN}\\)\nTN (True Negative): 真负例，模型预测为负例，实际也是负例。\n\n混淆矩阵 (Confusion Matrix): 总结分类模型预测结果的表格。\n\n可以直观地看到模型在每个类别上的预测情况。\n可以用于计算精确率、召回率、F1 分数等指标。\n\n\n\n\n\n\n\n\n\n\n\n预测为正例 (Positive Prediction)\n预测为负例 (Negative Prediction)\n\n\n\n\n实际正例 (Actual Positive)\nTP\nFN\n\n\n实际负例 (Actual Negative)\nFP\nTN\n\n\n\n\n\n6.1.2.2 回归模型评估指标\n\n均方误差 (Mean Squared Error, MSE): 预测值与真实值之差的平方的平均值。\n\n对误差进行平方，可以放大误差较大的样本的影响。\nMSE 越小，模型性能越好。\n\n\\[\n  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n\\(y_i\\): 第 \\(i\\) 个样本的真实值。\n\\(\\hat{y}_i\\): 第 \\(i\\) 个样本的预测值。\n\\(n\\): 样本总数。\n\n均绝对误差 (Mean Absolute Error, MAE): 预测值与真实值之差的绝对值的平均值。\n\n对误差取绝对值，可以避免正负误差相互抵消。\nMAE 越小，模型性能越好。\n对异常值 (outlier) 不敏感。\n\n\\[\n  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n  \\]\n均方根误差 (Root Mean Squared Error, RMSE): 均方误差的平方根。\n\n与 MSE 类似，但 RMSE 的量纲与原始数据一致，更易于解释。\nRMSE 越小，模型性能越好。\n\n\\[\n  \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n  \\]\nR 平方 (R-squared): 模型解释的方差比例。\n\n取值范围为 \\([0, 1]\\)，值越大，模型拟合程度越好。\n\\(R^2 = 1\\) 表示模型完美拟合数据。\n\\(R^2 = 0\\) 表示模型性能与使用均值作为预测值相当。\n可以用于评估模型对数据方差的解释能力。\n\n\\[\n  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n  \\]\n\n\\(SS_{res}\\): 残差平方和 (Sum of Squares of Residuals)，即 \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)。\n\\(SS_{tot}\\): 总平方和 (Total Sum of Squares)，即 \\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)。\n\\(\\bar{y}\\): 真实值的均值。\n\n\n\n\n\n6.1.3 交叉验证 (Cross-Validation)\n\n目的: 更可靠地评估模型的泛化能力，避免模型在特定数据集划分上的偶然性。\n基本思想: 将数据集分成若干份，轮流使用其中一份作为验证集，其余作为训练集进行模型训练和评估。\n常用方法:\n\nk 折交叉验证 (k-Fold Cross-Validation): 将数据集分成 k 份，每次使用其中 1 份作为验证集，其余 k-1 份作为训练集，重复 k 次。最终评估结果是 k 次评估结果的平均值。\n留一交叉验证 (Leave-One-Out Cross-Validation, LOOCV): k 折交叉验证的特殊情况，k 等于样本总数。每次只使用一个样本作为验证集，其余样本作为训练集。\n分层 k 折交叉验证 (Stratified k-Fold Cross-Validation): 在 k 折交叉验证的基础上，保证每个 fold 中各类别样本的比例与原始数据集中的比例大致相同。适用于类别不平衡的数据集。\n\n\n\n\n6.1.4 超参数调优 (Hyperparameter Tuning)\n\n超参数 (Hyperparameters): 模型训练前需要手动设置的参数，例如学习率、正则化系数、决策树的最大深度等。\n目的: 找到最佳的超参数组合，使模型在验证集上获得最佳性能。\n常用方法:\n\n网格搜索 (Grid Search): 预先定义超参数的候选值，然后穷举所有可能的超参数组合，并在验证集上评估每种组合的性能，选择最佳组合。\n随机搜索 (Random Search): 在预定义的超参数空间中随机采样超参数组合，并在验证集上评估性能，选择最佳组合。通常比网格搜索更高效，尤其是在超参数空间较大时。\n贝叶斯优化 (Bayesian Optimization): 使用贝叶斯方法建立超参数与模型性能之间的概率模型，然后根据该模型选择下一组超参数进行评估，以更有效地找到最佳超参数组合。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#模型优化策略",
    "href": "05_model_assess.html#模型优化策略",
    "title": "6  模型评估与优化",
    "section": "6.2 模型优化策略",
    "text": "6.2 模型优化策略\n\n特征工程 (Feature Engineering): 通过对原始特征进行转换、组合或选择，创建更有效的特征，以提高模型性能。\n\n特征转换: 例如，对数值特征进行标准化、归一化、对数变换、Box-Cox 变换等，使特征更符合模型假设或更易于模型学习。对类别特征进行独热编码、标签编码等。\n特征组合: 将多个特征进行组合，生成新的交叉特征，以捕捉特征之间的交互关系。例如，将年龄和收入进行组合，生成年龄*收入的特征。\n特征选择: 从原始特征中选择最相关的特征子集，去除冗余或不相关的特征，降低模型复杂度，提高模型泛化能力。常用的特征选择方法包括过滤式 (Filter)、包裹式 (Wrapper) 和嵌入式 (Embedded) 方法。\n\n模型选择 (Model Selection): 尝试不同的机器学习模型，例如线性回归、逻辑回归、决策树、支持向量机、神经网络等，选择最适合当前任务的模型。\n\n模型比较: 在同一个数据集上训练不同的模型，并使用交叉验证等方法评估它们的性能，选择性能最佳的模型。\n模型融合: 将多个不同模型的预测结果进行融合，以获得更好的预测性能。例如， stacking、blending 等集成方法。\n\n集成学习 (Ensemble Learning): 将多个弱学习器组合成一个强学习器，例如随机森林、梯度提升树 (GBDT)、XGBoost 等。集成学习通常可以提高模型的稳定性和泛化能力。\n\nBagging: 例如，随机森林 (Random Forest)。通过bootstrap 采样创建多个训练集，在每个训练集上训练一个弱学习器，然后将多个弱学习器的预测结果进行平均或投票。\nBoosting: 例如，梯度提升树 (Gradient Boosting Decision Tree, GBDT)、XGBoost、LightGBM、AdaBoost。 迭代地训练弱学习器，每个弱学习器都试图纠正前一个弱学习器的错误。最终将多个弱学习器加权组合成一个强学习器。\n\n数据增强 (Data Augmentation): 通过对训练数据进行变换 (例如旋转、平移、缩放、裁剪等)，增加训练数据的多样性，提高模型的泛化能力。\n\n图像数据增强: 例如，旋转、平移、缩放、裁剪、翻转、颜色变换、添加噪声等。\n文本数据增强: 例如，同义词替换、随机插入、随机删除、回译等。\n音频数据增强: 例如，添加噪声、时间拉伸、音调变换等。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#正则化-regularization",
    "href": "05_model_assess.html#正则化-regularization",
    "title": "6  模型评估与优化",
    "section": "6.3 正则化 (Regularization)",
    "text": "6.3 正则化 (Regularization)\n正则化是一种在机器学习中常用的技术，用于防止模型过拟合，提高模型的泛化能力。其基本思想是在损失函数中添加一个正则化项，以惩罚模型的复杂度。\n\nL1 正则化 (Lasso Regularization): 向损失函数添加模型权重向量的 L1 范数。\n\n作用: 使模型权重稀疏化，产生稀疏模型，有助于特征选择。\n特点: 可以将一部分权重压缩为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} |w_i|\n  \\]\n\n\\(\\lambda\\): 正则化强度超参数。\n\\(w_i\\): 模型权重。\n\nL2 正则化 (Ridge Regularization): 向损失函数添加模型权重向量的 L2 范数。\n\n作用: 减小模型权重，使模型更平滑，降低模型复杂度。\n特点: 权重趋向于变小，但不会变为 0。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda \\sum_{i} w_i^2\n  \\]\nElastic Net: 结合 L1 和 L2 正则化的方法。\n\n作用: 结合 L1 和 L2 正则化的优点，既可以进行特征选择，又可以减小模型权重。\n特点: 通过调节 L1 和 L2 正则化项的比例，平衡特征选择和权重衰减的效果。\n\n\\[\n  \\text{Loss}_{regularized} = \\text{Loss}_{original} + \\lambda_1 \\sum_{i} |w_i| + \\lambda_2 \\sum_{i} w_i^2\n  \\]\n\n\\(\\lambda_1, \\lambda_2\\): L1 和 L2 正则化强度超参数。\n\nDropout: 一种在神经网络中常用的正则化技术。\n\n原理: 在训练过程中，随机地将一部分神经元的输出置为 0。\n作用: 强制网络学习更鲁棒的特征表示，减少神经元之间的共适应性，提高泛化能力。\n特点: 简单有效，计算成本低，常用于深度神经网络。\n\nEarly Stopping (提前终止): 在模型训练过程中，监控验证集上的性能指标，当验证集性能不再提升或开始下降时，提前停止训练。\n\n原理: 随着训练的进行，模型在训练集上的性能会不断提升，但验证集上的性能可能会先提升后下降 (过拟合)。Early Stopping 旨在找到验证集性能最佳的训练迭代次数。\n优点: 简单易用，无需额外计算，可以有效防止过拟合。\n缺点: 可能会错过全局最优解，需要合适的验证集划分。\n\nBatch Normalization (批量归一化): 在神经网络的每一层输入或激活函数之前，对数据进行归一化处理。\n\n原理: 将每一批次 (batch) 的数据归一化到均值为 0，方差为 1 的分布。\n作用: 加速模型训练，提高训练稳定性，减轻内部协变量偏移 (Internal Covariate Shift) 问题，并具有一定的正则化效果。\n特点: 常用于深度神经网络，可以提高模型的泛化能力和鲁棒性。\n\n数据增强 (Data Augmentation): 虽然前面已经作为模型优化策略单独列出，但数据增强也可以被视为一种正则化技术。\n\n原理: 通过增加训练数据的多样性，使模型接触到更多不同的数据样本。\n作用: 提高模型的泛化能力，减少模型对特定训练样本的过拟合。\n特点: 特别适用于图像、文本和音频等数据，是一种有效的数据正则化方法。\n\n模型剪枝 (Pruning): 减小模型复杂度的技术，常用于决策树和神经网络。\n\n决策树剪枝: 通过剪去决策树中不必要的节点，简化决策树结构，防止过拟合。\n神经网络剪枝: 移除神经网络中不重要的连接或神经元，减小模型大小，提高模型效率，并具有一定的正则化效果。\n方法: 可以基于权重大小、梯度大小或其他指标进行剪枝。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "05_model_assess.html#总结",
    "href": "05_model_assess.html#总结",
    "title": "6  模型评估与优化",
    "section": "6.4 总结",
    "text": "6.4 总结\n\n模型评估是机器学习流程中至关重要的一步，它可以帮助我们了解模型性能、选择最佳模型、并指导模型改进。\n选择合适的评估指标取决于具体的机器学习任务类型。\n交叉验证可以更可靠地评估模型的泛化能力。\n超参数调优可以找到最佳的模型配置。\n模型优化是一个迭代过程，需要不断尝试不同的策略来提高模型性能。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>模型评估与优化</span>"
    ]
  },
  {
    "objectID": "project1_LC.html",
    "href": "project1_LC.html",
    "title": "7  项目1：借贷违约风险评估",
    "section": "",
    "text": "7.1 项目背景\nLending Club（NYSE：LC）创立于2006年，是一家在线撮合借款和出借的P2P平台，公司位于美国旧金山。公司上线运营初期仅提供个人贷款（personal loans）服务，后增加了医疗信贷（patient loans）、车贷分期（auto refinancing loans）。2014年12月12日开始在纽交所挂牌交易，成为当年最大的科技股IPO，2014年前后公司增加小微企业贷（small business loans）服务。该公司报告称，截至2015年12月31日，已通过其平台发放了159.8亿美元的贷款。\n借款人可以在LC平台上申请1,000美元到40,000美元之间的无担保个人贷款。标准贷款期限为三年。投资者可以在LC网站上搜索和浏览贷款清单，并根据提供的有关借款人、贷款金额、贷款等级和贷款目的的信息选择他们想要投资的贷款。投资者从这些贷款的利息中获利。\nLC负责贷款的审批和定价，贷款对应票据凭证的发行，以及贷后月度收款付款以及逾期后催收等服务。贷款的实际发放者是一家注册在犹他州的商业银行Web Bank。贷款产生的违约风险、提前还款和再投资风险，都由投资者自行承担。\nLC自行开发了风险评估和定价模型。公司会采用来自两个以上信用局的FICO评分（由美国Fair Isaac公司开发出的个人信用评级法），有时候借款人满足以上所有要求，他们也可能被拒绝。LC可能会要求验证借款人的其他信息。虽然LC的贷款审批只需7天-14天，但目前只有10%的贷款申请被批准，约90%的贷款申请被拒绝。\nLending Club的收入来源为交易手续费、服务费和管理费。交易手续费是向借款人收取的费用；服务费是向投资者收取的费用；管理费是管理投资基金和其他管理账户的费用。交易手续费是Lending Club收入的主要来源。\n尽管被视为金融科技行业的先驱和最大的此类公司之一，LC在2016年初遇到了问题，难以吸引投资者，公司的一些贷款丑闻以及董事会对首席执行官雷诺拉普朗什披露信息的担忧导致其股价大幅下跌和拉普朗什辞职。\n2020年，LC收购了Radius Bank，并宣布将关闭其P2P借贷平台。现有账户持有人将继续对现有票据收取利息，直到每笔贷款还清或违约，但没有新贷款可用于个人投资。也不再可能像以前那样通过二级市场出售现有贷款。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目目标",
    "href": "project1_LC.html#项目目标",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.2 项目目标",
    "text": "7.2 项目目标\n本项目旨在利用Lending Club提供的历史贷款数据，构建机器学习模型以预测贷款是否会违约。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#数据简介",
    "href": "project1_LC.html#数据简介",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.3 数据简介",
    "text": "7.3 数据简介\nLending Club贷款数据，覆盖2007.6-2018.12时间段，包含以下几类信息：\n\n贷款基本信息：\n\nid：贷款唯一标识符\nissue_d：贷款发布时间\nloan_amnt：贷款金额\nterm：贷款期限（36或60个月）\nint_rate：贷款利率\ninstallment：每月还款额\ngrade & sub_grade：LC给出的信用评级\nloan_status：贷款状态（是否违约）\npurpose：贷款目的\n\n借款人信息：\n\nemp_title：工作职位\nemp_length：工作年限\nannual_inc：年收入\ndti：债务收入比(DTI)\nhome_ownership：房产拥有状态\n\n信用数据：\n\nfico_range_low & fico_range_high：FICO分数范围\nopen_acc：开放信用账户数\nrevol_bal：循环信用余额\nrevol_util：循环额度利用率\n\n\n原始数据集包含145个变量和约200万条记录。本项目将使用其中的子集进行分析。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#样本与变量选择",
    "href": "project1_LC.html#样本与变量选择",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.4 样本与变量选择",
    "text": "7.4 样本与变量选择\n\n时间范围：选择2013-2014年发放的、期限为3年的贷款数据。这些贷款在2018年底已全部结束，因此有完整的还款结果。\n特征选择原则：\n\n剔除所有贷后信息，因为这些信息在贷款发放时并不可得，包括：\n\n包含recover字段的变量（与回收相关）\n包含settlement字段的变量（与结算相关）\n包含pymnt字段的变量（与付款相关）\n以total_rec开始的变量（与收款总额相关）\n以out_prncp开始的变量（与未偿本金相关）\n\n只保留那些在贷款申请和审批过程中可获得的信息，以构建具有实际预测价值的模型",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#项目步骤建议仅供参考",
    "href": "project1_LC.html#项目步骤建议仅供参考",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.5 项目步骤建议（仅供参考）",
    "text": "7.5 项目步骤建议（仅供参考）\n\n7.5.1 数据清理与特征工程\n\n数据探索分析：\n\n计算各变量的基本统计量（均值、中位数、标准差等）\n检测并处理缺失值、异常值和不合法值\n分析目标变量的分布情况，评估类别不平衡程度\n\n数据预处理：\n\n缺失值处理：根据变量类型选择适当的填补方法（均值、中位数、众数或特殊值）\n异常值处理：识别并处理离群点（可使用箱线图、Z-score等方法）\n特征变换：将分类变量转换为哑变量，对数值变量进行标准化或归一化\n\n特征工程：\n\n特征选择：去除低方差特征、高度相关特征或具有较多缺失值的特征\n特征创建：根据业务理解创建新的特征（如各类比率、差值等）\n特征重要性评估：使用统计方法或模型预测能力评估特征重要性\n\n数据可视化：\n\n绘制变量分布图，分析变量与目标的关系\n使用散点图、热力图等展示变量间相关性\n生成变量重要性图表\n\n\n\n\n7.5.2 数据建模与模型评估\n\n数据集划分：\n\n训练集（60%）：用于模型训练\n验证集（20%）：用于超参数调优\n测试集（20%）：用于最终模型评估，模拟真实应用场景\n\n处理类别不平衡：\n\n尝试欠抽样（减少多数类样本）或过抽样（增加少数类样本）技术\n考虑SMOTE等合成样本生成方法\n调整类别权重或使用集成学习方法\n\n模型构建与选择： 尝试以下几种分类模型并进行比较：\n\nLogistic回归：基准模型，易于解释\n决策树：能够捕捉非线性关系，提供决策规则\n随机森林：降低过拟合风险，提高预测稳定性\n梯度提升树（如XGBoost、LightGBM）：通常具有较高的预测准确率\n\n模型调优：\n\n使用网格搜索或随机搜索方法确定最优超参数\n利用交叉验证评估模型稳定性\n根据验证集表现选择最佳模型配置\n\n模型评估：\n\n计算多种评估指标：\n\n混淆矩阵：TP、TN、FP、FN\n精度（Accuracy）：整体分类正确率\n查准率（Precision）：预测为违约中实际违约的比例\n查全率（Recall）：实际违约中被成功预测的比例\nF1分数：Precision和Recall的调和平均\nROC曲线与AUC值：评估模型在不同阈值下的性能\nKS统计量：衡量模型区分好坏客户的能力\n\n分析模型的业务价值：计算不同决策阈值下的潜在收益和损失\n\n模型解释：\n\n分析特征重要性\n部分依赖图或SHAP值分析，理解特征对预测的影响\n提出基于模型的业务洞见和建议",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#提交要求",
    "href": "project1_LC.html#提交要求",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.6 提交要求",
    "text": "7.6 提交要求\n\n项目报告：\n\n项目背景和目标的理解\n数据探索分析结果及发现\n特征工程和数据预处理的详细步骤\n模型构建、评估和比较的过程与结果\n最终模型的性能分析和业务意义解读\n项目总结与进一步改进建议\n报告长度建议不超过10页\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n说明对违约预测有显著影响的变量、特征工程、模型选择\n包含关键可视化图表\n演示时间控制在8分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据导入、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目报告和项目代码打包为一个ZIP文件\n截止日期：2025年4月20日23:59",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project1_LC.html#评分标准",
    "href": "project1_LC.html#评分标准",
    "title": "7  项目1：借贷违约风险评估",
    "section": "7.7 评分标准",
    "text": "7.7 评分标准\n\n项目报告（40分）\n\n背景与目标理解：准确阐述项目背景、业务逻辑和预测目标\n数据探索分析：缺失值/异常值处理得当，可视化分析深入，数据分布描述清晰\n特征工程：特征选择合理，创造有效新特征，编码转换方法正确\n模型构建与评估：模型选择恰当，评估指标完整，对比分析深入\n总结建议：结论有数据支撑，改进建议具有实操性\n\n代码质量（30分）\n\n完整性：包含数据清洗、特征工程、建模评估全流程\n可重复性：代码可直接运行并复现结果\n规范性：代码结构清晰，有详细注释说明\n数据处理：缺失值/异常值处理逻辑正确\n模型实现：正确使用机器学习库，参数设置合理\n\n模型性能（20分）\n\n基准模型：实现合理的基准模型（如逻辑回归）\n优化模型：通过特征工程/参数调优显著提升性能\n模型对比：尝试3种以上模型并进行横向比较\n\n课堂展示（10分）\n\n内容组织：逻辑清晰，重点突出，时间控制得当\n可视化呈现：图表专业，信息传达有效\n问答环节：准确回答评委提问",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>项目1：借贷违约风险评估</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html",
    "href": "project2a_tspred.html",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "",
    "text": "8.1 项目背景\n上海证券交易所(上交所)成立于1990年11月26日，是中国大陆第一家证券交易所，与深圳证券交易所、北京证券交易所并称为中国大陆三大证券交易所。上证综指(Shanghai Composite Index)是以上海证券交易所挂牌上市的所有股票为样本，以发行量为权数计算的加权综合股价指数，是反映上海证券市场整体走势的重要指标。\n上证综指自1991年7月15日正式发布，基日为1990年12月19日，基点为100点。作为中国股市最具代表性的指数之一，上证综指的走势不仅反映了中国资本市场的整体状况，也在很大程度上反映了中国宏观经济的发展态势。\n金融时间序列预测一直是金融数据分析和量化投资领域的重要研究方向。相比传统的截面数据分析，时间序列数据具有明显的时序依赖性，这使得时间序列预测面临更多的挑战。特别是金融市场数据，其高波动性、非线性特征以及受多种复杂因素影响的特点，进一步增加了预测的难度。\n随着机器学习技术的发展，各种先进的预测方法被应用于金融时间序列分析，从传统的ARIMA、GARCH模型，到现代的支持向量机(SVM)、随机森林、深度学习网络等，为金融时间序列预测提供了更多可能性。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目目标",
    "href": "project2a_tspred.html#项目目标",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.2 项目目标",
    "text": "8.2 项目目标\n本项目旨在利用机器学习方法对上证综指的收益率进行时间序列预测。具体目标包括：\n\n理解并处理金融时间序列数据的特性，如平稳性、季节性、趋势等\n构建和评估不同的时间序列预测模型\n比较传统统计方法与现代机器学习方法在金融时间序列预测中的表现\n探索能够提高预测准确性的特征工程方法\n分析预测结果的经济意义和实际应用价值",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据简介",
    "href": "project2a_tspred.html#数据简介",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.3 数据简介",
    "text": "8.3 数据简介\n本项目将使用上证综指的历史数据，时间范围为2000年1月至2023年12月，包含以下几类信息：\n\n基础价格数据（CSMAR、WIND）：\n\nDate：交易日期\nOpen：开盘价\nHigh：最高价\nLow：最低价\nClose：收盘价\nAdj Close：调整后收盘价\nVolume：成交量\n\n派生指标（CSMAR、WIND）：\n\nReturns：日收益率（当日收盘价相对前一日收盘价的百分比变化）\nVolatility：基于历史窗口计算的波动率\n\n技术指标（需自行构建）：\n\n移动平均线(MA)：不同时间窗口的简单移动平均和指数移动平均\n相对强弱指数(RSI)\n布林带(Bollinger Bands)\nMACD(Moving Average Convergence Divergence)\n\n宏观经济数据（CSMAR、WIND）：\n\n中国GDP增长率\nCPI（消费者价格指数）\n利率\n汇率（美元/人民币）",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#数据特点与挑战",
    "href": "project2a_tspred.html#数据特点与挑战",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.4 数据特点与挑战",
    "text": "8.4 数据特点与挑战\n金融时间序列数据具有以下特点，这些都给预测带来了挑战：\n\n非平稳性：金融市场数据通常表现出非平稳特性，即统计特性随时间变化\n高噪声：市场价格受多种随机因素影响，含有大量噪声\n异方差性：金融数据的波动性往往会随时间聚集，表现为波动聚类现象\n胖尾分布：收益率分布常常表现出比正态分布更胖的尾部，意味着极端事件发生的概率更高\n长期记忆和短期记忆：金融时间序列可能同时表现出短期和长期的相关性特征",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#项目步骤建议仅供参考",
    "href": "project2a_tspred.html#项目步骤建议仅供参考",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.5 项目步骤建议（仅供参考）",
    "text": "8.5 项目步骤建议（仅供参考）\n\n8.5.1 数据预处理与探索性分析\n\n数据获取与清洗：\n\n下载上证综指历史数据\n处理缺失值（如节假日）\n检测并处理异常值\n确保时间索引的连续性和正确性\n\n探索性数据分析：\n\n绘制时间序列图，观察长期趋势和季节性模式\n计算并分析基本统计量（均值、标准差、偏度、峰度等）\n检验序列的平稳性（ADF检验、KPSS检验等）\n分析自相关性（ACF和PACF图）\n检测异方差性（ARCH效应检验）\n\n特征工程：\n\n构建基于不同滞后期的特征\n计算技术分析指标作为特征（如移动平均、相对强弱指数等）\n引入宏观经济变量作为外生变量\n特征选择与降维（如主成分分析）\n考虑时间结构特征（如一周中的某天、月份效应等）\n\n数据转换：\n\n差分变换以实现平稳性\n标准化或归一化处理\n考虑非线性变换（如对数变换）\n处理数据频率不匹配问题（如将宏观经济月度数据转换为日度数据）\n\n\n\n\n8.5.2 模型构建与评估\n\n传统时间序列模型：\n\n自回归模型(AR)\n移动平均模型(MA)\n自回归移动平均模型(ARMA)\n自回归积分移动平均模型(ARIMA)\n广义自回归条件异方差模型(GARCH)及其变种\n\n机器学习模型：\n\n支持向量回归(SVR)\n随机森林(RF)\n梯度提升树(如XGBoost、LightGBM)\nK近邻回归(KNN)\n神经网络模型(如MLP、RNN、LSTM)\n\n集成方法与混合模型：\n\n组合不同模型的预测结果\n构建混合模型融合时间序列模型和机器学习模型的优势\n\n交叉验证策略：\n\n使用时间序列交叉验证方法（如时间滚动窗口法）\n避免数据泄露\n考虑不同长度的训练窗口和预测窗口\n\n评估指标：\n\n均方误差(MSE)\n平均绝对误差(MAE)\n平均绝对百分比误差(MAPE)\n方向准确率（预测涨跌方向的准确度）\n信息系数(IC)和累积信息系数(CIC)\n\n\n\n\n8.5.3 预测结果分析与应用\n\n模型解释性分析：\n\n分析特征重要性\n理解模型学习到的模式\n分析不同市场条件下的预测表现\n\n交易策略构建：\n\n基于预测结果设计简单交易策略\n回测策略表现\n考虑交易成本和滑点\n计算策略风险调整后收益（如夏普比率）\n\n鲁棒性分析：\n\n在不同市场环境下测试模型性能\n分析模型对极端事件的适应能力\n考虑参数变化对预测结果的影响",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#提交要求",
    "href": "project2a_tspred.html#提交要求",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.6 提交要求",
    "text": "8.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码（可以是多个Python文件）\n代码应包含从数据获取、清洗、特征工程到模型训练、评估的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2a_tspred.html#评分标准",
    "href": "project2a_tspred.html#评分标准",
    "title": "8  项目2：上证综指收益率时间序列预测",
    "section": "8.7 评分标准",
    "text": "8.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示数据分析、特征工程、模型构建和评估的完整流程\n技术理解：准确阐述金融时间序列特性及采用模型的优缺点\n结果分析：深入解读预测结果的经济意义和实际应用价值\n可视化呈现：包含时间序列分解图、ACF/PACF图、模型预测对比图等专业图表\n创新性：展示对模型局限性的思考或改进尝试\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含数据预处理、特征生成、模型训练、回测评估全流程\n时序处理：正确实现时间序列分割、滚动窗口验证，避免数据泄露\n可重复性：设置随机种子，保证结果可复现\n规范性：代码模块化设计，有清晰的函数注释和文档说明\n可视化：实现关键时序分析和预测结果的可视化功能\n\n模型性能（30分）\n\n基准模型：实现ARIMA/GARCH等传统时序模型作为基准\n优化模型：通过特征工程/模型融合提升预测精度\n模型多样性：至少包含3种不同类别模型（如ARIMA、LSTM、XGBoost）\n评估全面性：同时报告点预测精度（MSE）和方向准确性指标\n策略应用：设计基于预测结果的简单交易策略并评估其表现",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>项目2：上证综指收益率时间序列预测</span>"
    ]
  },
  {
    "objectID": "project2b_text.html",
    "href": "project2b_text.html",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "",
    "text": "9.1 项目背景\n随着数字化转型的加速，网络安全已成为现代企业面临的最重要风险之一。数据泄露、系统入侵、勒索软件攻击等安全事件不仅会导致直接的经济损失，还会损害企业声誉、客户信任，甚至引发监管处罚和法律诉讼。据相关统计，中国企业平均数据泄露成本持续上升，重大网络安全事件对企业造成的负面影响也日益加剧。\n中国证监会及相关监管机构近年来逐步加强了对上市公司信息披露的要求，特别是在企业风险管理方面。《公开发行证券的公司信息披露内容与格式准则》要求上市公司在年度报告中披露可能面临的各类风险，包括网络安全风险。2021年，《数据安全法》和《个人信息保护法》的实施进一步提高了企业对网络安全和数据保护的合规要求，使得上市公司在财务报表中更加详细地披露相关风险信息。\n这些文本披露为研究者提供了丰富的数据源，使我们能够通过自然语言处理和机器学习技术，从财务报表文本中提取、量化和分析企业的网络安全风险。这种基于文本的风险分析方法，不仅可以帮助投资者识别潜在的高风险企业，还可以帮助监管机构发现网络安全披露不足的公司，并为企业自身提供行业基准和改进方向。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目目标",
    "href": "project2b_text.html#项目目标",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.2 项目目标",
    "text": "9.2 项目目标\n本项目旨在通过对中国上市公司财务报表的文本分析，构建模型以评估和预测企业的网络安全风险水平。具体目标包括：\n\n掌握中文财务文本数据的获取和预处理方法\n学习和应用中文文本分析的核心技术，包括文本特征提取、情感分析和主题建模\n构建能够评估企业网络安全风险水平的预测模型\n分析不同行业、不同规模企业的网络安全风险特征和趋势\n探索网络安全风险与企业财务表现、股价波动等因素之间的关系",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据简介",
    "href": "project2b_text.html#数据简介",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.3 数据简介",
    "text": "9.3 数据简介\n本项目将使用中国A股上市公司的财务报表数据，时间范围为2018年至2023年，主要包含以下数据来源：\n\n上市公司公告文本（巨潮网）：\n\n年度报告文本\n半年度报告文本\n季度报告文本\n临时公告和重大事件公告\n\n网络安全事件数据（需自行获取）：\n\n国家互联网应急中心(CNCERT)发布的安全事件报告\n公开报道的重大网络安全事件\n行业安全报告中记录的数据泄露和系统入侵事件\n\n财务和市场数据（CSMAR、WIND）：\n\n股价和股票回报数据\n财务比率和业绩指标\n市值和行业分类\n\n文本特征数据（需自行构建）：\n\n中文网络安全相关词汇表和术语库\n行业特定风险指标\n中文情感和语调词典",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#数据特点与挑战",
    "href": "project2b_text.html#数据特点与挑战",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.4 数据特点与挑战",
    "text": "9.4 数据特点与挑战\n中文财务报表文本分析面临以下特点和挑战：\n\n中文文本特性：中文没有空格分词，需要特殊的分词技术；同时存在词义多样、同义词丰富等特点\n非结构化数据：财务报表文本是典型的非结构化数据，需要特殊的处理和特征提取方法\n专业术语丰富：财务和网络安全领域均有大量专业术语，普通NLP模型可能难以准确理解\n披露规范差异：与欧美市场相比，中国上市公司在风险披露方面的规范和实践存在差异\n行业差异：不同行业面临的网络安全风险类型和程度各不相同\n时间演变：网络安全威胁和披露要求随时间不断变化\n因果关系复杂：网络安全风险与企业表现之间的因果关系难以确定",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#项目步骤建议仅供参考",
    "href": "project2b_text.html#项目步骤建议仅供参考",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.5 项目步骤建议（仅供参考）",
    "text": "9.5 项目步骤建议（仅供参考）\n\n9.5.1 数据收集与预处理\n\n文本数据获取：\n\n从巨潮网下载上市公司年报、半年报、季报文件\n提取文件中与风险相关的章节（尤其是”风险因素”和”管理层讨论与分析”部分）\n识别并提取与网络安全相关的段落\n\n中文文本清洗与标准化：\n\n对PDF文件进行清洗\n中文分词处理\n去除停用词\n简繁体转换（如有必要）\n标点符号和特殊字符处理\n\n创建标记数据集：\n\n标记历史上发生过重大网络安全事件的企业\n根据事件严重程度和影响创建风险等级标签\n构建时间序列标签，反映风险随时间变化\n\n\n\n\n9.5.2 特征工程与文本分析\n\n网络安全关键词提取：\n\n基于行业专家知识和相关法规构建中文网络安全术语词典（如”数据泄露”、“勒索软件”、“网络攻击”等）\n计算网络安全相关术语的出现频率、密度和分布位置\n对比不同时期同一企业的网络安全关键词变化\n分析风险披露部分中网络安全词汇占比\n\n风险披露的复杂性与模糊性：\n\n计算风险描述的语言复杂度（如句长、复合句比例）\n分析模糊表述词汇使用（如”可能”、“或许”、“不确定”等）\n对比明确风险描述与模糊风险描述比例\n构建模糊性指数，衡量风险披露的具体程度\n\n财务报表特定结构特征：\n\n提取风险因素章节位置和长度\n分析网络安全风险在整体风险披露中的相对位置\n计算网络安全风险披露的篇幅占比\n构建章节重要性特征（如风险因素被放在报告前部的企业可能对风险更重视）\n\n\n\n\n9.5.3 模型构建与评估\n\n基础文本聚类分析：\n\n使用简单K-means聚类（sklearn库）将企业按风险披露特征分为几个组\n尝试不同的聚类数量（如3-5类），观察企业风险分组情况\n计算并可视化各聚类中心，理解不同企业组的风险特点\n使用词云图直观展示各类企业风险表述的关键词差异\n\n简单风险指标构建：\n\n统计每份报告中网络安全关键词出现的数量作为风险指标\n计算风险词汇占总词数的百分比，作为风险关注度指标\n设计简单的风险评分公式：结合关键词频率、模糊词使用程度等\n对比不同企业的风险指标，建立相对排名\n\n基础主题分析：\n\n使用Python的gensim库实现简单的LDA主题模型\n设置3-5个主题，识别网络安全风险中的主要话题\n分析不同企业的主题分布情况\n观察主题随时间的变化趋势\n\n简易验证方法：\n\n随机抽取样本进行人工检查，验证模型发现的模式是否合理\n与公开报道的网络安全事件进行对比（作为间接验证）\n绘制散点图，观察企业在风险指标上的分布是否符合预期\n使用统计检验（如t检验）比较不同行业企业的风险指标差异\n\n\n\n\n9.5.4 分析与应用\n\n跨行业比较分析：\n\n比较中国不同行业上市公司的网络安全风险特征\n识别高风险行业特征\n分析中国特色行业（如互联网、金融科技等）的风险特点\n\n风险与财务表现关系：\n\n分析网络安全风险与股价波动的关系\n研究风险披露与企业估值的关联\n测量网络安全事件对企业长期财务影响\n\n披露质量评估：\n\n评估中国企业网络安全风险披露的完整性\n分析披露内容与实际风险的一致性\n比较不同市场板块（如主板、科创板、创业板）企业的披露差异\n\n案例研究：\n\n选择典型企业进行深入分析\n研究重大网络安全事件前后的披露变化\n分析最佳实践和常见缺陷",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#提交要求",
    "href": "project2b_text.html#提交要求",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.6 提交要求",
    "text": "9.6 提交要求\n\n课堂展示Slides：\n\n简明扼要展示项目主要发现和结果\n包含关键可视化图表和案例分析\n演示时间控制在10分钟以内\n\n项目代码文件：\n\n提交完整的、有注释的Python代码\n代码应包含从数据获取、中文文本处理、特征工程到后续应用的全过程\n确保代码可重复运行，并包含必要的环境依赖说明\n\n提交方式与截止日期：\n\n通过学习通平台提交所有文件\n项目Slides和项目代码打包为一个ZIP文件\n截止日期：2025年6月8日23:59",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  },
  {
    "objectID": "project2b_text.html#评分标准",
    "href": "project2b_text.html#评分标准",
    "title": "9  项目2B：财务报表文本分析与企业网络安全风险评估",
    "section": "9.7 评分标准",
    "text": "9.7 评分标准\n\n课堂展示（40分）\n\n内容全面性：清晰展示中文文本数据获取、预处理、特征提取和风险评估的完整流程\n技术理解：准确阐述中文财务文本分析的特点和所采用NLP技术的优缺点\n结果分析：深入解读企业网络安全风险评估结果及其行业差异\n可视化呈现：包含关键词频统计图、主题分布图、风险评分对比图等专业可视化图表\n创新性：展示特色词典构建或针对中文财务文本的特殊处理方法\n时间控制：重点突出，在10分钟内完整呈现核心内容\n\n代码质量（30分）\n\n完整性：包含文本获取、中文分词、特征提取、模型构建和结果评估全流程\n中文处理：正确实现中文文本的分词、去停用词和特征提取流程\n可重复性：设置随机种子，保证结果可复现，包含必要的词典文件\n规范性：代码模块化设计，有清晰的函数注释和文档说明（特别是中文注释）\n可视化：实现文本分析关键结果的可视化功能，包括词云、主题分布等\n\n模型性能（30分）\n\n特征工程：构建全面的中文网络安全相关特征，包括关键词频率、语义特征等\n风险指标：成功构建有效的网络安全风险评估指标体系\n模型多样性：至少实现3种不同文本分析方法（如词频统计、主题模型、情感分析）\n评估全面性：从多角度（行业对比、时间趋势、案例分析）评估分析结果\n实际应用：将文本分析结果与企业财务表现或市场反应进行关联分析",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>项目2B：财务报表文本分析与企业网络安全风险评估</span>"
    ]
  }
]