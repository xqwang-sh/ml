<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; 监督学习（上） – 数据挖掘与机器学习课程讲义</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_supervised.html" rel="next">
<link href="./lab02_data.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ed6da6eef3892af8a4b5ed59bfb951f5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03_supervised.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">数据挖掘与机器学习课程讲义</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">前言</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_ml_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">机器学习基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab02_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_supervised.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_model_assess.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">模型评估与优化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lab03_titanic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">泰坦尼克号生存预测实践</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project1_LC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">项目1：借贷违约风险评估</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2a_tspred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">项目2A：上证综指收益率时间序列预测</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project2b_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">项目2B：财务报表文本分析与企业网络安全风险评估</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#监督学习简介" id="toc-监督学习简介" class="nav-link active" data-scroll-target="#监督学习简介"><span class="header-section-number">4.1</span> 监督学习简介</a></li>
  <li><a href="#监督学习模型详解" id="toc-监督学习模型详解" class="nav-link" data-scroll-target="#监督学习模型详解"><span class="header-section-number">4.2</span> 监督学习模型详解</a>
  <ul class="collapse">
  <li><a href="#回归问题描述" id="toc-回归问题描述" class="nav-link" data-scroll-target="#回归问题描述"><span class="header-section-number">4.2.1</span> 回归问题描述</a></li>
  <li><a href="#线性回归-linear-regression" id="toc-线性回归-linear-regression" class="nav-link" data-scroll-target="#线性回归-linear-regression"><span class="header-section-number">4.2.2</span> 线性回归 (Linear Regression)</a></li>
  <li><a href="#岭回归-ridge-regression" id="toc-岭回归-ridge-regression" class="nav-link" data-scroll-target="#岭回归-ridge-regression"><span class="header-section-number">4.2.3</span> 岭回归 (Ridge Regression)</a></li>
  <li><a href="#lasso-回归-lasso-regression" id="toc-lasso-回归-lasso-regression" class="nav-link" data-scroll-target="#lasso-回归-lasso-regression"><span class="header-section-number">4.2.4</span> Lasso 回归 (Lasso Regression)</a></li>
  <li><a href="#弹性网-elastic-net" id="toc-弹性网-elastic-net" class="nav-link" data-scroll-target="#弹性网-elastic-net"><span class="header-section-number">4.2.5</span> 弹性网 (Elastic Net)</a></li>
  <li><a href="#分类问题描述" id="toc-分类问题描述" class="nav-link" data-scroll-target="#分类问题描述"><span class="header-section-number">4.2.6</span> 分类问题描述</a></li>
  <li><a href="#类别不平衡问题" id="toc-类别不平衡问题" class="nav-link" data-scroll-target="#类别不平衡问题"><span class="header-section-number">4.2.7</span> 类别不平衡问题</a></li>
  <li><a href="#逻辑回归-logistic-regression" id="toc-逻辑回归-logistic-regression" class="nav-link" data-scroll-target="#逻辑回归-logistic-regression"><span class="header-section-number">4.2.8</span> 逻辑回归 (Logistic Regression)</a></li>
  <li><a href="#支持向量机-support-vector-machine-svm" id="toc-支持向量机-support-vector-machine-svm" class="nav-link" data-scroll-target="#支持向量机-support-vector-machine-svm"><span class="header-section-number">4.2.9</span> 支持向量机 (Support Vector Machine, SVM)</a></li>
  <li><a href="#决策树-decision-tree" id="toc-决策树-decision-tree" class="nav-link" data-scroll-target="#决策树-decision-tree"><span class="header-section-number">4.2.10</span> 决策树 (Decision Tree)</a></li>
  </ul></li>
  <li><a href="#总结" id="toc-总结" class="nav-link" data-scroll-target="#总结"><span class="header-section-number">4.3</span> 总结</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">监督学习（上）</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="监督学习简介" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="监督学习简介"><span class="header-section-number">4.1</span> 监督学习简介</h2>
<p>监督学习是机器学习的一个重要分支，它是指从<strong>带有标签的数据</strong>中自动学习规律和模式，并利用这些规律和模式对<strong>新数据进行预测和决策</strong>的过程。在监督学习中，我们拥有一个包含输入特征 <span class="math inline">\(\mathbf{x}\)</span> 和对应输出标签 <span class="math inline">\(y\)</span> 的数据集，模型的目标是学习一个从输入特征到输出标签的映射关系。监督学习在量化投资、金融科技等领域有广泛应用，例如：</p>
<ul>
<li><strong>风险评估</strong>：根据客户的历史信用数据（特征）预测其信用风险等级（标签）。</li>
<li><strong>欺诈检测</strong>：基于交易记录（特征）识别欺诈交易（标签）。</li>
<li><strong>量化交易</strong>：预测股票价格走势（标签）以辅助交易决策（特征）。</li>
<li><strong>客户细分</strong>：根据客户特征（特征）预测客户所属类别（标签），进行精准营销。</li>
</ul>
<p>由于课程时间有限，本讲义将重点介绍监督学习中的<strong>回归</strong>、<strong>分类</strong>和<strong>集成学习</strong>，以及它们在金融预测中的应用。</p>
</section>
<section id="监督学习模型详解" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="监督学习模型详解"><span class="header-section-number">4.2</span> 监督学习模型详解</h2>
<section id="回归问题描述" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="回归问题描述"><span class="header-section-number">4.2.1</span> 回归问题描述</h3>
<p>回归问题旨在通过由 <span class="math inline">\(K \times 1\)</span> 维向量 <span class="math inline">\(\mathbf{x}\)</span> 表示的 <span class="math inline">\(K\)</span> 个观测到的<strong>预测变量（特征）</strong>来预测<strong>连续数值型</strong>的结果 <span class="math inline">\(y\)</span>。 给定训练数据 <span class="math inline">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}\)</span>，其中 <span class="math inline">\(\mathbf{x}_i\)</span> 是第 <span class="math inline">\(i\)</span> 个样本的特征向量， <span class="math inline">\(y_i\)</span> 是对应的真实值， <span class="math inline">\(N\)</span> 是样本数量。我们的目标是找到一个函数 <span class="math inline">\(f\)</span>，使得对于新的输入 <span class="math inline">\(\mathbf{x}\)</span>，模型预测值 <span class="math inline">\(\hat{y} = f(\mathbf{x})\)</span> 尽可能接近真实值 <span class="math inline">\(y\)</span>。假设真实值 <span class="math inline">\(y_i\)</span> 与预测函数 <span class="math inline">\(f(\mathbf{x}_i)\)</span> 之间存在如下关系：</p>
<p><span class="math display">\[y_i = f(\mathbf{x}_i) + \epsilon_i\]</span></p>
<p>其中 <span class="math inline">\(\epsilon_i\)</span> 代表<strong>随机误差项</strong>，通常假设其服从均值为 0 的正态分布。在实际应用中，我们通常将观测值堆叠成矩阵和向量的形式，方便模型表达和计算：</p>
<ul>
<li><span class="math inline">\(N \times 1\)</span> 维<strong>结果向量</strong> <span class="math inline">\(\mathbf{y} = (y_1, y_2, ..., y_N)^T\)</span></li>
<li><span class="math inline">\(N \times K\)</span> 维<strong>特征矩阵</strong> <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N)^T\)</span>，每一行代表一个样本，每一列代表一个特征。</li>
<li><span class="math inline">\(N \times 1\)</span> 维<strong>误差向量</strong> <span class="math inline">\(\mathbf{\epsilon} = (\epsilon_1, \epsilon_2, ..., \epsilon_N)^T\)</span></li>
</ul>
<p>回归模型可以简洁地写为： <span class="math inline">\(\mathbf{y} = f(\mathbf{X}) + \mathbf{\epsilon}\)</span>。我们的目标是通过训练数据学习到函数 <span class="math inline">\(f\)</span> 的具体形式，从而能够对新的样本 <span class="math inline">\(\mathbf{x}\)</span> 进行预测。</p>
</section>
<section id="线性回归-linear-regression" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="线性回归-linear-regression"><span class="header-section-number">4.2.2</span> 线性回归 (Linear Regression)</h3>
<p>线性回归模型是最简单且应用广泛的回归模型。它假设结果变量 <span class="math inline">\(y\)</span> 与特征向量 <span class="math inline">\(\mathbf{x}\)</span> 之间存在<strong>线性关系</strong>。线性回归模型易于理解和实现，是许多复杂模型的基础。</p>
<p><strong>模型表达式:</strong></p>
<p>线性回归模型假设预测函数 <span class="math inline">\(f(\mathbf{x})\)</span> 是特征 <span class="math inline">\(\mathbf{x}\)</span> 的线性组合，模型表达式如下：</p>
<p><span class="math inline">\(y = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}\)</span></p>
<p>或者对于单个样本 <span class="math inline">\(i\)</span>，可以表示为：</p>
<p><span class="math inline">\(y_i = \mathbf{x}_i^T \mathbf{\beta} + \epsilon_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_K x_{iK} + \epsilon_i\)</span></p>
<p>其中： - <span class="math inline">\(\mathbf{\beta} = (\beta_0, \beta_1, ..., \beta_K)^T\)</span> 是 <span class="math inline">\((K+1) \times 1\)</span> 维<strong>回归系数向量</strong>，<span class="math inline">\(\beta_0\)</span> 是截距项（bias），<span class="math inline">\(\beta_1, ..., \beta_K\)</span> 是特征的系数。为了方便表示，我们通常在特征矩阵 <span class="math inline">\(\mathbf{X}\)</span> 中添加一列全为 1 的列向量，对应于截距项 <span class="math inline">\(\beta_0\)</span>。 - <span class="math inline">\(\mathbf{x}_i = (1, x_{i1}, x_{i2}, ..., x_{iK})^T\)</span> 是 <span class="math inline">\((K+1) \times 1\)</span> 维<strong>增广特征向量</strong>，包含了常数项 1 和原始特征。 - <span class="math inline">\(\epsilon_i\)</span> 是误差项。</p>
<p><strong>最优化方法：最小二乘法 (OLS)</strong></p>
<p>线性回归的目标是找到最优的回归系数 <span class="math inline">\(\mathbf{\beta}\)</span>，使得模型的预测值 <span class="math inline">\(\mathbf{X}\mathbf{\beta}\)</span> 与真实值 <span class="math inline">\(\mathbf{y}\)</span> 之间的<strong>误差平方和 (Sum of Squared Errors, SSE)</strong> 最小。最小二乘法 (Ordinary Least Squares, OLS) 是一种常用的求解线性回归模型参数的方法。其目标函数为：</p>
<p><span class="math display">\[\min_{\mathbf{\beta}} L(\mathbf{\beta}) = \min_{\mathbf{\beta}} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) = \min_{\mathbf{\beta}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^T \mathbf{\beta})^2\]</span></p>
<p>为了求解最优的 <span class="math inline">\(\mathbf{\beta}\)</span>，我们可以对目标函数 <span class="math inline">\(L(\mathbf{\beta})\)</span> 关于 <span class="math inline">\(\mathbf{\beta}\)</span> 求导，并令导数等于 0，得到<strong>正规方程 (Normal Equation)</strong>：</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}\mathbf{\beta} = \mathbf{X}^T\mathbf{y}\]</span></p>
<p>如果矩阵 <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> 可逆（即满秩），则可以得到<strong>普通最小二乘 (OLS) 估计量</strong>的解析解：</p>
<p><span class="math display">\[\hat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{y}\]</span></p>
<p><strong>高维环境下的过拟合问题与正则化:</strong></p>
<p>在高维环境中，当特征数量 <span class="math inline">\(K\)</span> 相对于观测数量 <span class="math inline">\(N\)</span> 来说较大时（例如 <span class="math inline">\(K &gt; N\)</span>，或 <span class="math inline">\(K\)</span> 接近 <span class="math inline">\(N\)</span>），OLS 估计可能会出现<strong>过拟合 (Overfitting)</strong> 问题。过拟合是指模型在训练集上表现得非常好（例如，训练误差很小），但在<strong>未见过的测试集</strong>上泛化能力很差，预测性能下降。这是因为在高维情况下，模型参数过多，容易捕捉到训练数据中的噪声和随机波动，而不是真实的 underlying pattern。</p>
<p>为了解决过拟合问题，提高模型的泛化能力，可以引入<strong>正则化 (Regularization)</strong> 方法。正则化通过在损失函数中添加<strong>惩罚项</strong>，限制模型复杂度，从而避免模型过度拟合训练数据。常用的正则化方法包括 <strong>岭回归 (Ridge Regression)</strong> 和 <strong>Lasso 回归 (Lasso Regression)</strong>。</p>
</section>
<section id="岭回归-ridge-regression" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="岭回归-ridge-regression"><span class="header-section-number">4.2.3</span> 岭回归 (Ridge Regression)</h3>
<p>岭回归是一种<strong>改进的线性回归方法</strong>，也称为 <strong><span class="math inline">\(L_2\)</span> 正则化线性回归</strong>。它通过在最小二乘法的损失函数中添加 <strong><span class="math inline">\(L_2\)</span> 范数惩罚项</strong>来对回归系数进行 <strong>shrinkage (收缩)</strong>，限制回归系数的大小，从而降低模型的复杂度和过拟合风险。岭回归特别适用于处理<strong>多重共线性</strong>问题，即特征之间存在高度相关性的情况。</p>
<p><strong>模型表达式:</strong></p>
<p>岭回归的目标函数为：</p>
<p><span class="math display">\[\min_{\mathbf{\beta}} L_{Ridge}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \lambda \mathbf{\beta}^T\mathbf{\beta} \right] \]</span></p>
<p>其中： - <span class="math inline">\(\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})\)</span> 是<strong>均方误差 (Mean Squared Error, MSE)</strong> 项，衡量模型预测值与真实值之间的平均误差平方。 - <span class="math inline">\(\lambda \mathbf{\beta}^T\mathbf{\beta} = \lambda ||\mathbf{\beta}||_2^2 = \lambda \sum_{j=0}^{K} \beta_j^2\)</span> 是 <strong><span class="math inline">\(L_2\)</span> 范数惩罚项</strong>，也称为 <strong>权重衰减项 (Weight Decay)</strong>。它惩罚回归系数 <span class="math inline">\(\mathbf{\beta}\)</span> 的平方和，迫使系数趋向于较小的值。 - <span class="math inline">\(\lambda \ge 0\)</span> 是<strong>正则化参数 (Regularization Parameter)</strong>，也称为 <strong>惩罚系数</strong>。它控制惩罚项的强度。<span class="math inline">\(\lambda\)</span> 越大，惩罚越强，回归系数越趋向于 0。当 <span class="math inline">\(\lambda = 0\)</span> 时，岭回归退化为普通线性回归。</p>
<p><strong>估计方法:</strong></p>
<p>类似于线性回归，我们可以对岭回归的目标函数 <span class="math inline">\(L_{Ridge}(\mathbf{\beta})\)</span> 关于 <span class="math inline">\(\mathbf{\beta}\)</span> 求导，并令导数等于 0，得到岭回归的估计结果：</p>
<p><span class="math display">\[\hat{\mathbf{\beta}}_{Ridge} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}_{K+1})^{-1} \mathbf{X}^T\mathbf{y}\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{I}_{K+1}\)</span> 是 <span class="math inline">\((K+1) \times (K+1)\)</span> 单位矩阵。通过向 <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> 添加对角矩阵 <span class="math inline">\(\lambda \mathbf{I}_{K+1}\)</span>（即”岭”），可以使得在求逆运算时，即使 <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> 接近奇异矩阵（例如，当存在多重共线性时），<span class="math inline">\((\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}_{K+1})\)</span> 仍然具有较好的可逆性，保证了解的稳定性。并且 <span class="math inline">\(\lambda \mathbf{I}_{K+1}\)</span> 的存在将导致回归系数 <span class="math inline">\(\hat{\mathbf{\beta}}_{Ridge}\)</span> 向零收缩。</p>
<p><strong>岭回归的特点:</strong></p>
<ul>
<li><strong><span class="math inline">\(L_2\)</span> 正则化</strong>：使用 <span class="math inline">\(L_2\)</span> 范数惩罚项，将回归系数向零收缩，但不会精确地变为 0。</li>
<li><strong>缓解多重共线性</strong>：通过引入正则化项，降低了模型对特征之间相关性的敏感度，可以缓解多重共线性问题，提高模型稳定性。</li>
<li><strong>降低过拟合风险</strong>：通过限制模型复杂度，有效降低过拟合风险，提高模型的泛化能力。</li>
<li><strong>无法进行特征选择</strong>：岭回归会缩小所有特征的系数，但不会将任何系数精确地设置为 0，因此无法进行特征选择。</li>
</ul>
</section>
<section id="lasso-回归-lasso-regression" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="lasso-回归-lasso-regression"><span class="header-section-number">4.2.4</span> Lasso 回归 (Lasso Regression)</h3>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) 回归是另一种常用的正则化线性回归方法，也称为 <strong><span class="math inline">\(L_1\)</span> 正则化线性回归</strong>。与岭回归不同，Lasso 回归使用 <strong><span class="math inline">\(L_1\)</span> 范数惩罚项</strong>进行正则化。 <span class="math inline">\(L_1\)</span> 正则化不仅可以进行系数 shrinkage，更重要的是，它具有 <strong>特征选择 (Feature Selection)</strong> 的能力，可以将一些不重要特征的回归系数压缩为 <strong>精确的 0</strong>，从而得到 <strong>稀疏模型 (Sparse Model)</strong>。稀疏模型更易于解释，并且可以提高模型的泛化能力。</p>
<p><strong>模型表达式:</strong></p>
<p>Lasso 回归的目标函数为：</p>
<p><span class="math display">\[\min_{\mathbf{\beta}} L_{Lasso}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \gamma \sum_{j=1}^{K} |\beta_j| \right]\]</span></p>
<p>其中： - <span class="math inline">\(\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})\)</span> 仍然是均方误差项。 - <span class="math inline">\(\gamma \sum_{j=1}^{K} |\beta_j| = \gamma ||\mathbf{\beta}_{1:K}||_1 = \gamma (|\beta_1| + |\beta_2| + ... + |\beta_K|)\)</span> 是 <strong><span class="math inline">\(L_1\)</span> 范数惩罚项</strong>，注意这里只惩罚了特征系数 <span class="math inline">\(\beta_1, ..., \beta_K\)</span>，<strong>不惩罚截距项 <span class="math inline">\(\beta_0\)</span></strong>。 <span class="math inline">\(L_1\)</span> 范数惩罚项迫使一些回归系数变为 0。 - <span class="math inline">\(\gamma \ge 0\)</span> 是 <strong>正则化参数</strong>，控制 <span class="math inline">\(L_1\)</span> 惩罚项的强度。<span class="math inline">\(\gamma\)</span> 越大，惩罚越强，更多的回归系数会被压缩为 0。</p>
<p><strong>估计方法:</strong></p>
<p>与岭回归不同，Lasso 回归的目标函数由于包含 <span class="math inline">\(L_1\)</span> 范数项，<strong>在 <span class="math inline">\(\beta_j = 0\)</span> 处不可导</strong>，因此 <strong>没有解析解</strong>。通常需要使用<strong>数值优化算法</strong>（如<strong>坐标轴下降法 (Coordinate Descent)</strong>、<strong>近端梯度下降法 (Proximal Gradient Descent)</strong>）进行求解。</p>
<p><strong>Lasso 回归的特点:</strong></p>
<ul>
<li><strong><span class="math inline">\(L_1\)</span> 正则化</strong>：使用 <span class="math inline">\(L_1\)</span> 范数惩罚项，不仅可以进行系数 shrinkage，还可以将一些不重要特征的回归系数压缩为精确的 0，实现<strong>特征选择</strong>。</li>
<li><strong>稀疏模型</strong>：Lasso 回归可以产生稀疏模型，即模型中只有少数特征的系数非零，这有助于模型解释和提高泛化能力。</li>
<li><strong>特征选择能力</strong>：在特征选择方面优于岭回归。Lasso 回归可以自动选择重要的特征，去除冗余和不相关的特征。</li>
<li><strong>适用于高维稀疏数据</strong>：Lasso 回归特别适用于处理高维稀疏数据，例如文本数据、基因数据等。</li>
</ul>
</section>
<section id="弹性网-elastic-net" class="level3" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="弹性网-elastic-net"><span class="header-section-number">4.2.5</span> 弹性网 (Elastic Net)</h3>
<p>弹性网 (Elastic Net) 是一种<strong>结合了岭回归和 Lasso 回归的正则化方法</strong>，可以看作是岭回归和 Lasso 回归的折衷。弹性网<strong>同时使用 <span class="math inline">\(L_1\)</span> 范数和 <span class="math inline">\(L_2\)</span> 范数惩罚项</strong>进行正则化。弹性网<strong>同时使用 <span class="math inline">\(L_1\)</span> 正则化和 <span class="math inline">\(L_2\)</span> 正则化</strong>，综合利用 <span class="math inline">\(L_1\)</span> 正则化的特征选择能力和 <span class="math inline">\(L_2\)</span> 正则化的稳定性和 shrinkage 能力。在某些情况下，弹性网的性能优于单独的岭回归和 Lasso 回归，尤其是在特征之间高度相关时，弹性网表现更稳定。</p>
<p><strong>模型表达式:</strong></p>
<p>弹性网的目标函数为：</p>
<p><span class="math display">\[\min_{\mathbf{\beta}} L_{ElasticNet}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \gamma_1 \sum_{j=1}^{K} |\beta_j| + \gamma_2 \mathbf{\beta}^T\mathbf{\beta} \right]\]</span></p>
<p>其中： - <span class="math inline">\(\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})\)</span> 是均方误差项。 - <span class="math inline">\(\gamma_1 \sum_{j=1}^{K} |\beta_j|\)</span> 是 <strong><span class="math inline">\(L_1\)</span> 范数惩罚项</strong>，用于特征选择和产生稀疏模型。 - <span class="math inline">\(\gamma_2 \mathbf{\beta}^T\mathbf{\beta}\)</span> 是 <strong><span class="math inline">\(L_2\)</span> 范数惩罚项</strong>，用于系数 shrinkage 和缓解多重共线性。 - <span class="math inline">\(\gamma_1 \ge 0\)</span> 和 <span class="math inline">\(\gamma_2 \ge 0\)</span> 分别是 <strong><span class="math inline">\(L_1\)</span> 正则化参数</strong> 和 <strong><span class="math inline">\(L_2\)</span> 正则化参数</strong>，控制两种惩罚项的强度。通常需要通过交叉验证等方法来选择合适的 <span class="math inline">\(\gamma_1\)</span> 和 <span class="math inline">\(\gamma_2\)</span> 值。</p>
<p><strong>弹性网的特点:</strong></p>
<ul>
<li><strong>结合 <span class="math inline">\(L_1\)</span> 和 <span class="math inline">\(L_2\)</span> 正则化</strong>：弹性网同时使用 <span class="math inline">\(L_1\)</span> 和 <span class="math inline">\(L_2\)</span> 范数惩罚项，结合了两者的优点。</li>
<li><strong>既可以进行特征选择，又可以进行系数 shrinkage</strong>：弹性网既可以像 Lasso 回归一样进行特征选择，将一些不重要特征的系数压缩为 0，又可以像岭回归一样进行系数 shrinkage，缩小系数的整体大小，提高模型稳定性。</li>
<li><strong>性能更稳定</strong>：在某些情况下，弹性网的预测性能和鲁棒性优于岭回归和 Lasso 回归。</li>
<li><strong>处理特征高度相关性</strong>：当特征之间高度相关时，Lasso 回归可能随机选择其中一个特征，而弹性网倾向于选择一组相关的特征，表现更稳定。</li>
</ul>
</section>
<section id="分类问题描述" class="level3" data-number="4.2.6">
<h3 data-number="4.2.6" class="anchored" data-anchor-id="分类问题描述"><span class="header-section-number">4.2.6</span> 分类问题描述</h3>
<p>分类问题旨在通过由 <span class="math inline">\(K \times 1\)</span> 维向量 <span class="math inline">\(\mathbf{x}\)</span> 表示的 <span class="math inline">\(K\)</span> 个观测到的<strong>预测变量（特征）</strong>来预测<strong>离散类别型</strong>的结果 <span class="math inline">\(y\)</span>。分类问题的目标是学习一个模型，将输入样本 <span class="math inline">\(\mathbf{x}\)</span> 划分到预定义的类别中。根据类别数量的不同，分类问题可以分为：</p>
<ul>
<li><strong>二分类 (Binary Classification)</strong>：预测结果 <span class="math inline">\(y\)</span> 只有两个类别，通常表示为 <span class="math inline">\(y \in \{0, 1\}\)</span> (或 <span class="math inline">\(y \in \{-1, +1\}\)</span>)。例如，判断邮件是否为垃圾邮件（是/否），预测用户是否会点击广告（点击/不点击），识别交易是否为欺诈交易（欺诈/正常）。</li>
<li><strong>多分类 (Multiclass Classification)</strong>：预测结果 <span class="math inline">\(y\)</span> 有两个以上的类别，表示为 <span class="math inline">\(y \in \{C_1, C_2, ..., C_L\}\)</span>，其中 <span class="math inline">\(C_i\)</span> 是类别标签， <span class="math inline">\(L \ge 3\)</span> 是类别数量。例如，图像分类（猫、狗、鸟、鱼等），文本分类（政治、经济、体育、娱乐等），客户类型分类（高价值客户、中价值客户、低价值客户）。</li>
</ul>
<p>对于多分类问题，常用的处理策略是 <strong>“拆解法” (Decomposition)</strong>，即将多分类任务拆解为若干个<strong>二分类任务</strong>求解。常见的拆解策略包括 <strong>一对一 (One-vs-One, OvO)</strong>、<strong>一对多 (One-vs-Rest, OvR)</strong> 和 <strong>多对多 (Many-vs-Many, MvM)</strong> 等。</p>
</section>
<section id="类别不平衡问题" class="level3" data-number="4.2.7">
<h3 data-number="4.2.7" class="anchored" data-anchor-id="类别不平衡问题"><span class="header-section-number">4.2.7</span> 类别不平衡问题</h3>
<p>在分类任务中，经常会遇到不同类别的训练样本数量差别很大的情况，即 <strong>类别不平衡 (Class Imbalance)</strong> 问题。例如，在欺诈检测、罕见病诊断、自然灾害预测等领域，<strong>少数类样本 (Minority Class)</strong>（如欺诈交易、患病样本、地震）的数量通常远<strong>远少于多数类样本 (Majority Class)</strong>（如正常交易、健康样本、非地震）。类别不平衡问题会严重影响模型的学习效果，使得模型更倾向于预测样本数量较多的类别，而对少数类别的识别率很低。</p>
<p><strong>类别不平衡的影响:</strong></p>
<ul>
<li><strong>模型偏向多数类</strong>：模型在训练过程中更容易学习到多数类样本的特征，而忽略少数类样本的特征，导致模型预测结果偏向多数类。</li>
<li><strong>整体分类精度虚高</strong>：由于多数类样本数量占优，即使模型将所有样本都预测为多数类，也可能获得较高的整体分类精度 (Accuracy)。但这种高精度是没有意义的，因为模型对少数类的识别能力很差。</li>
<li><strong>评估指标失效</strong>：常用的评估指标（如准确率 Accuracy）在类别不平衡数据集上可能失效，无法真实反映模型的性能。我们需要使用更合适的评估指标，例如 <strong>精确率 (Precision)</strong>、<strong>召回率 (Recall)</strong>、<strong>F1 值 (F1-score)</strong>、<strong>AUC 值 (Area Under ROC Curve)</strong> 等。</li>
</ul>
<p><strong>类别不平衡的解决方案:</strong></p>
<p>为了解决类别不平衡问题，提高模型对少数类别的识别能力，常用的解决方案包括：</p>
<ul>
<li><strong>再缩放 (Rescaling) / 阈值调整 (Threshold Adjustment)</strong>：不改变原始模型，而是<strong>调整分类阈值 (Classification Threshold)</strong>，使得模型在类别不平衡时也能做出合理的预测。例如，对于逻辑回归或 SVM 等输出概率的模型，默认的分类阈值通常为 0.5。当类别不平衡时，可以将预测为正例的阈值从 0.5 <strong>调整为更小的值</strong>，例如 <span class="math inline">\(\frac{m^{+}}{m^{-} + m^{+}}\)</span>, 其中 <span class="math inline">\(m^{+}\)</span> 和 <span class="math inline">\(m^{-}\)</span> 分别是正类（少数类）和负类（多数类）样本的数量。降低阈值会使得模型更容易将样本预测为正类，从而提高少数类的召回率。</li>
<li><strong>重采样 (Resampling)</strong>：通过<strong>改变训练集中不同类别样本的比例</strong>来缓解类别不平衡问题。重采样方法包括 <strong>欠抽样 (Undersampling)</strong> 和 <strong>过抽样 (Oversampling)</strong>。
<ul>
<li><strong>欠抽样 (Undersampling)</strong>：<strong>减少多数类样本的数量</strong>，随机删除一部分多数类样本，使得正负类样本数量接近平衡。欠抽样方法简单易行，但可能会<strong>丢失一部分多数类样本的信息</strong>，适用于数据量较大的情况。</li>
<li><strong>过抽样 (Oversampling)</strong>：<strong>增加少数类样本的数量</strong>，例如通过<strong>复制少数类样本</strong>或<strong>生成合成样本</strong>（如 <strong>SMOTE (Synthetic Minority Over-sampling Technique)</strong>）。过抽样方法可以保留所有原始多数类样本的信息，但可能会<strong>导致过拟合</strong>，适用于数据量较小的情况。SMOTE 算法通过在少数类样本之间进行插值生成新的合成样本，可以有效缓解过拟合问题。</li>
</ul></li>
<li><strong>阈值移动 (Threshold-moving)</strong>：这是一种<strong>代价敏感学习 (Cost-sensitive learning)</strong> 的思想。基于原始训练集进行学习，但在用训练好的分类器进行预测时，<strong>根据类别不平衡的程度调整决策阈值</strong>。例如，如果少数类样本的误分类代价更高，则可以将决策阈值向多数类方向移动，使得模型更倾向于将样本预测为少数类。</li>
<li><strong>代价敏感学习 (Cost-sensitive learning)</strong>：为不同类别的<strong>误分类设置不同的代价 (Cost)</strong>，使得模型在训练时更加关注少数类样本，<strong>最小化总的期望代价</strong>而不是最小化分类错误率。例如，可以使用<strong>代价矩阵 (Cost Matrix)</strong> 来定义不同误分类情况的代价，然后在训练过程中根据代价矩阵调整模型的学习策略。</li>
<li><strong>集成学习方法</strong>：一些集成学习方法，如 <strong>集成学习 (Ensemble Learning)</strong> 方法，例如 <strong>EasyEnsemble</strong>、<strong>BalanceCascade</strong> 等，通过将数据集划分为多个子集，在每个子集上训练基学习器，然后集成多个基学习器的预测结果，可以有效提高模型在类别不平衡数据集上的性能。</li>
</ul>
</section>
<section id="逻辑回归-logistic-regression" class="level3" data-number="4.2.8">
<h3 data-number="4.2.8" class="anchored" data-anchor-id="逻辑回归-logistic-regression"><span class="header-section-number">4.2.8</span> 逻辑回归 (Logistic Regression)</h3>
<p>逻辑回归 (Logistic Regression) 是一种广泛使用的<strong>二分类模型</strong>。虽然名字带有”回归”，但逻辑回归实际上是一种<strong>分类算法</strong>，主要用于解决二分类问题。逻辑回归模型简单高效，易于解释，是许多分类问题的 baseline 模型。</p>
<p><strong>模型表达式:</strong></p>
<p>逻辑回归模型基于<strong>线性回归</strong>的思想，但通过引入 <strong>Sigmoid 函数 (Sigmoid Function)</strong> 或 <strong>Logistic 函数</strong>，将线性回归的输出值<strong>映射到 <span class="math inline">\((0, 1)\)</span> 区间</strong>，使其具有概率意义，用于表示样本属于正类的概率。</p>
<p>逻辑回归模型的表达式如下：</p>
<p><span class="math display">\[P(y=1|\mathbf{x}; \mathbf{\beta}) = \sigma(\mathbf{x}^T \mathbf{\beta}) = \frac{1}{1 + e^{-\mathbf{x}^T \mathbf{\beta}}}\]</span></p>
<p>其中： - <span class="math inline">\(P(y=1|\mathbf{x}; \mathbf{\beta})\)</span> 表示给定特征向量 <span class="math inline">\(\mathbf{x}\)</span> 和模型参数 <span class="math inline">\(\mathbf{\beta}\)</span> 的条件下，样本属于正类 (y=1) 的<strong>概率</strong>。 - <span class="math inline">\(\mathbf{x} = (1, x_1, x_2, ..., x_K)^T\)</span> 是增广特征向量。 - <span class="math inline">\(\mathbf{\beta} = (\beta_0, \beta_1, ..., \beta_K)^T\)</span> 是模型参数，与线性回归中的回归系数类似。 - <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> 是 <strong>Sigmoid 函数</strong>，也称为 <strong>Logistic 函数</strong>。Sigmoid 函数将任意实数 <span class="math inline">\(z\)</span> 映射到 <span class="math inline">\((0, 1)\)</span> 区间，函数图像呈 S 形。当 <span class="math inline">\(z \rightarrow +\infty\)</span> 时，<span class="math inline">\(\sigma(z) \rightarrow 1\)</span>；当 <span class="math inline">\(z \rightarrow -\infty\)</span> 时，<span class="math inline">\(\sigma(z) \rightarrow 0\)</span>；当 <span class="math inline">\(z = 0\)</span> 时，<span class="math inline">\(\sigma(z) = 0.5\)</span>。</p>
<p>对于二分类问题，逻辑回归模型预测样本属于正类的概率 <span class="math inline">\(P(y=1|\mathbf{x}; \mathbf{\beta})\)</span>，则样本属于负类的概率为 <span class="math inline">\(P(y=0|\mathbf{x}; \mathbf{\beta}) = 1 - P(y=1|\mathbf{x}; \mathbf{\beta}) = 1 - \sigma(\mathbf{x}^T \mathbf{\beta}) = \sigma(-\mathbf{x}^T \mathbf{\beta}) = \frac{e^{-\mathbf{x}^T \mathbf{\beta}}}{1 + e^{-\mathbf{x}^T \mathbf{\beta}}} = \frac{1}{1 + e^{\mathbf{x}^T \mathbf{\beta}}}\)</span>。</p>
<p><strong>模型训练：最大似然估计 (Maximum Likelihood Estimation, MLE)</strong></p>
<p>逻辑回归模型的训练目标是<strong>最大化训练数据的似然函数 (Likelihood Function)</strong>，即找到一组模型参数 <span class="math inline">\(\mathbf{\beta}\)</span>，使得在给定这组参数下，训练数据出现的概率最大。对于二分类问题，逻辑回归的似然函数可以表示为：</p>
<p><span class="math display">\[L(\mathbf{\beta}) = \prod_{i=1}^{N} [P(y_i=1|\mathbf{x}_i; \mathbf{\beta})]^{y_i} [P(y_i=0|\mathbf{x}_i; \mathbf{\beta})]^{1-y_i} = \prod_{i=1}^{N} [\sigma(\mathbf{x}_i^T \mathbf{\beta})]^{y_i} [\sigma(-\mathbf{x}_i^T \mathbf{\beta})]^{1-y_i}\]</span></p>
<p>为了方便优化，通常将似然函数取对数，得到<strong>对数似然函数 (Log-Likelihood Function)</strong>：</p>
<p><span class="math display">\[\ell(\mathbf{\beta}) = \ln L(\mathbf{\beta}) = \sum_{i=1}^{N} [y_i \ln \sigma(\mathbf{x}_i^T \mathbf{\beta}) + (1-y_i) \ln \sigma(-\mathbf{x}_i^T \mathbf{\beta})] = \sum_{i=1}^{N} [y_i \ln \frac{1}{1 + e^{-\mathbf{x}_i^T \mathbf{\beta}}} + (1-y_i) \ln \frac{e^{-\mathbf{x}_i^T \mathbf{\beta}}}{1 + e^{-\mathbf{x}_i^T \mathbf{\beta}}}]\]</span></p>
<p>我们的目标是<strong>最大化对数似然函数 <span class="math inline">\(\ell(\mathbf{\beta})\)</span></strong>，等价于<strong>最小化负对数似然函数 (Negative Log-Likelihood Function)</strong>：</p>
<p><span class="math display">\[J(\mathbf{\beta}) = -\ell(\mathbf{\beta}) = - \sum_{i=1}^{N} [y_i \ln \sigma(\mathbf{x}_i^T \mathbf{\beta}) + (1-y_i) \ln \sigma(-\mathbf{x}_i^T \mathbf{\beta})]\]</span></p>
<p>负对数似然函数 <span class="math inline">\(J(\mathbf{\beta})\)</span> 也称为 <strong>交叉熵损失函数 (Cross-Entropy Loss Function)</strong> 或 <strong>Logistic Loss Function</strong>。</p>
<p><strong>最优化方法：梯度下降法 (Gradient Descent)</strong></p>
<p>逻辑回归模型通常使用<strong>梯度下降法 (Gradient Descent)</strong> 或其变种（如 <strong>随机梯度下降 (SGD)</strong>、<strong>小批量梯度下降 (Mini-batch GD)</strong>、<strong>Adam</strong> 等）来求解最优参数 <span class="math inline">\(\mathbf{\beta}\)</span>，最小化交叉熵损失函数 <span class="math inline">\(J(\mathbf{\beta})\)</span>。梯度下降法是一种迭代优化算法，通过不断沿着损失函数梯度 <strong>负方向</strong> 更新参数，逐步逼近最优解。</p>
<p><strong>决策边界 (Decision Boundary):</strong></p>
<p>逻辑回归模型的决策边界是线性的。当 <span class="math inline">\(\mathbf{x}^T \mathbf{\beta} = 0\)</span> 时，<span class="math inline">\(\sigma(\mathbf{x}^T \mathbf{\beta}) = 0.5\)</span>，模型预测样本属于正类和负类的概率均为 0.5。因此，<strong>线性方程 <span class="math inline">\(\mathbf{x}^T \mathbf{\beta} = 0\)</span> 定义了逻辑回归模型的决策边界</strong>，将特征空间划分为正类区域和负类区域。</p>
</section>
<section id="支持向量机-support-vector-machine-svm" class="level3" data-number="4.2.9">
<h3 data-number="4.2.9" class="anchored" data-anchor-id="支持向量机-support-vector-machine-svm"><span class="header-section-number">4.2.9</span> 支持向量机 (Support Vector Machine, SVM)</h3>
<p>支持向量机 (Support Vector Machine, SVM) 是一种强大且广泛应用于<strong>分类和回归问题</strong>的监督学习模型。SVM 的核心思想是<strong>找到一个最优超平面 (Optimal Hyperplane)</strong>，将不同类别的样本<strong>最大程度地分开</strong>，同时使得<strong>分类间隔 (Margin)</strong> 最大化。SVM 在<strong>高维空间</strong>和<strong>非线性分类</strong>问题中表现出色，通过<strong>核技巧 (Kernel Trick)</strong> 可以有效地处理非线性可分数据。</p>
<p><strong>线性可分支持向量机 (Linearly Separable SVM) / 硬间隔 SVM (Hard Margin SVM):</strong></p>
<p>对于<strong>线性可分 (Linearly Separable)</strong> 的数据集，即存在一个超平面可以将不同类别的样本完全分开的情况，我们可以构建<strong>线性可分支持向量机</strong>，也称为 <strong>硬间隔 SVM</strong>。硬间隔 SVM 旨在找到一个<strong>最大间隔超平面</strong>，将两类样本完全正确地分开，并且使得<strong>间隔最大化</strong>。间隔是指超平面到<strong>最近的样本点</strong>（称为 <strong>支持向量 (Support Vector)</strong>）的距离。</p>
<p><strong>模型表达式:</strong></p>
<p>给定线性可分的训练数据集 <span class="math inline">\(D = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}\)</span>，其中 <span class="math inline">\(y_i \in \{-1, +1\}\)</span>。线性可分 SVM 的目标是找到一个超平面 <span class="math inline">\((\mathbf{w}, b)\)</span>，使得：</p>
<ul>
<li><strong>正确分类</strong>: 所有样本都被正确分类，即对于 <span class="math inline">\(y_i = +1\)</span> 的样本，有 <span class="math inline">\(\mathbf{w}^T \mathbf{x}_i + b \ge +1\)</span>；对于 <span class="math inline">\(y_i = -1\)</span> 的样本，有 <span class="math inline">\(\mathbf{w}^T \mathbf{x}_i + b \le -1\)</span>。可以将两个不等式统一为： <span class="math inline">\(y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, ..., N\)</span>。</li>
<li><strong>间隔最大化</strong>: 最大化分类间隔 <span class="math inline">\(Margin = \frac{2}{||\mathbf{w}||}\)</span>，等价于最小化 <span class="math inline">\(||\mathbf{w}||^2 = \mathbf{w}^T \mathbf{w}\)</span>。</li>
</ul>
<p>因此，线性可分 SVM 的<strong>最优化问题</strong>可以表示为：</p>
<p><span class="math display">\[\min_{\mathbf{w}, b} \frac{1}{2} ||\mathbf{w}||^2 \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, ..., N\]</span></p>
<p>这是一个<strong>凸二次规划 (Convex Quadratic Programming, QP)</strong> 问题，可以使用现成的 QP 求解器求解。</p>
<p><strong>线性不可分支持向量机 (Linearly Inseparable SVM) / 软间隔 SVM (Soft Margin SVM):</strong></p>
<p>在实际应用中，很多数据集<strong>不是线性可分</strong>的，即不存在一个超平面可以将不同类别的样本完全分开。为了处理线性不可分数据，我们需要引入<strong>软间隔 SVM</strong>，也称为 <strong>线性支持向量机</strong>。软间隔 SVM 允许模型在一些样本上<strong>分类错误</strong>，但希望<strong>尽可能减少分类错误</strong>，同时<strong>保持间隔最大化</strong>。</p>
<p><strong>模型表达式:</strong></p>
<p>软间隔 SVM 通过引入<strong>松弛变量 (Slack Variables)</strong> <span class="math inline">\(\xi_i \ge 0\)</span>，允许一些样本不满足硬间隔约束 <span class="math inline">\(y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1\)</span>。松弛变量 <span class="math inline">\(\xi_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个样本<strong>违反约束的程度</strong>。软间隔 SVM 的最优化问题变为：</p>
<p><span class="math display">\[\min_{\mathbf{w}, b, \xi} \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^{N} \xi_i \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0, \quad i = 1, 2, ..., N\]</span></p>
<p>其中： - <span class="math inline">\(\frac{1}{2} ||\mathbf{w}||^2\)</span> 仍然是间隔最大化项。 - <span class="math inline">\(C \sum_{i=1}^{N} \xi_i\)</span> 是<strong>惩罚项</strong>，表示对<strong>误分类的惩罚</strong>。 <span class="math inline">\(\xi_i\)</span> 越大，误分类程度越高，惩罚越大。 - <span class="math inline">\(C &gt; 0\)</span> 是 <strong>惩罚参数 (Penalty Parameter)</strong>，也称为 <strong>正则化参数</strong>。 <span class="math inline">\(C\)</span> 控制对误分类的惩罚程度。 <span class="math inline">\(C\)</span> 越大，对误分类的惩罚越大，模型越倾向于减小误分类，但可能会导致间隔变小，容易过拟合； <span class="math inline">\(C\)</span> 越小，对误分类的惩罚越小，模型更容忍误分类，间隔可能更大，泛化能力可能更好。 <span class="math inline">\(C\)</span> 的选择需要通过交叉验证等方法进行调优。</p>
<p><strong>核函数 (Kernel Function):</strong></p>
<p>对于<strong>非线性可分 (Nonlinearly Separable)</strong> 的数据集，SVM 可以通过 <strong>核函数 (Kernel Function)</strong> 将数据<strong>映射到高维空间 (High-Dimensional Space)</strong>，使得在高维空间中数据变得<strong>线性可分</strong>，然后在高维空间中寻找最优超平面。<strong>核技巧 (Kernel Trick)</strong> 的强大之处在于，我们<strong>不需要显式地计算高维空间的特征向量</strong>，只需要定义一个<strong>核函数 <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j)\)</span></strong>，它可以计算<strong>原始空间中两个向量 <span class="math inline">\(\mathbf{x}_i\)</span> 和 <span class="math inline">\(\mathbf{x}_j\)</span> 映射到高维空间后的内积</strong>。常用的核函数包括：</p>
<ul>
<li><strong>线性核 (Linear Kernel)</strong>: <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j\)</span>。线性核实际上没有进行特征映射，适用于线性可分数据。</li>
<li><strong>多项式核 (Polynomial Kernel)</strong>: <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^T \mathbf{x}_j + r)^d\)</span>。多项式核可以将数据映射到多项式特征空间，适用于多项式关系的数据。其中 <span class="math inline">\(\gamma &gt; 0, r \ge 0, d \ge 1\)</span> 是核参数。</li>
<li><strong>高斯核 / RBF 核 (Gaussian Kernel / Radial Basis Function Kernel)</strong>: <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2)\)</span>。高斯核是最常用的核函数之一，可以将数据映射到<strong>无限维空间</strong>，适用于各种类型的数据，尤其是<strong>局部性模式</strong>的数据。其中 <span class="math inline">\(\gamma &gt; 0\)</span> 是核参数，控制核函数的宽度。</li>
<li><strong>Sigmoid 核 (Sigmoid Kernel)</strong>: <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i^T \mathbf{x}_j + r)\)</span>。Sigmoid 核类似于神经网络中的 Sigmoid 激活函数，SVM 使用 Sigmoid 核时，其行为类似于<strong>多层感知机神经网络</strong>。其中 <span class="math inline">\(\gamma &gt; 0, r &lt; 0\)</span> 是核参数。</li>
</ul>
<p><strong>最优化方法：对偶问题与 SMO 算法</strong></p>
<p>SVM 的优化问题（无论是硬间隔还是软间隔）通常转化为 <strong>对偶问题 (Dual Problem)</strong> 进行求解。求解对偶问题的好处包括： 1. <strong>更容易求解</strong>：对偶问题通常比原始问题更容易求解。 2. <strong>引入核函数</strong>：在对偶问题中，目标函数和约束条件只涉及到样本之间的内积，可以方便地引入核函数，将线性 SVM 扩展到非线性 SVM。</p>
<p>求解 SVM 对偶问题的高效算法是 <strong>SMO (Sequential Minimal Optimization) 算法</strong>。SMO 算法是一种<strong>启发式算法</strong>，它将<strong>大规模 QP 问题分解为一系列小规模 QP 子问题</strong>，通过<strong>迭代地优化两个变量</strong>，高效地求解 SVM 模型。</p>
</section>
<section id="决策树-decision-tree" class="level3" data-number="4.2.10">
<h3 data-number="4.2.10" class="anchored" data-anchor-id="决策树-decision-tree"><span class="header-section-number">4.2.10</span> 决策树 (Decision Tree)</h3>
<p>决策树 (Decision Tree) 是一种<strong>树形结构</strong>的<strong>分类或回归模型</strong>。决策树模型直观易懂，易于解释，并且可以处理<strong>类别型和数值型特征</strong>，无需进行特征缩放。决策树模型的核心思想是<strong>基于特征对数据集进行递归划分</strong>，构建一棵树状的决策规则，用于对新样本进行分类或预测。</p>
<p>决策树由<strong>节点 (Node)</strong> 和 <strong>有向边 (Directed Edge)</strong> 组成。节点分为两种类型： - <strong>内部节点 (Internal Node)</strong>：表示一个<strong>特征或属性</strong>的测试条件，用于决定样本的划分方向。 - <strong>叶节点 (Leaf Node / Terminal Node)</strong>：表示<strong>最终的决策结果</strong>，即类别标签（分类树）或预测值（回归树）。</p>
<p>有向边代表<strong>划分规则</strong>，从父节点指向子节点。从根节点到每个叶节点的路径都对应着一条<strong>决策规则</strong>。</p>
<p>决策树的学习过程主要包括三个步骤：<strong>特征选择</strong>、<strong>树的生成</strong> 和 <strong>树的剪枝</strong>。</p>
<p><strong>回归树 (Regression Tree):</strong></p>
<p>回归树 (Regression Tree) 用于<strong>预测连续数值型目标变量</strong>。例如，预测房价、股票价格等。</p>
<p><strong>模型构建:</strong></p>
<p>回归树的构建过程是一个<strong>递归的二叉树构建过程</strong>，也称为 <strong>CART (Classification and Regression Tree) 树</strong>。CART 树是一种二叉树，内部节点根据特征取值将数据集划分为两个子集，叶节点输出预测值。回归树的构建过程如下：</p>
<ol type="1">
<li><strong>选择划分特征和划分点</strong>：从所有特征和所有可能的划分点中，选择一个<strong>最优的特征 <span class="math inline">\(j\)</span> 和切分点 <span class="math inline">\(s\)</span></strong>，将当前节点的数据集划分为两个区域 <span class="math inline">\(R_1(j,s) = \{\mathbf{x}|\mathbf{x}_j \le s\}\)</span> 和 <span class="math inline">\(R_2(j,s) = \{\mathbf{x}|\mathbf{x}_j &gt; s\}\)</span>。</li>
<li><strong>最小化平方误差</strong>：选择最优划分属性 <span class="math inline">\(j\)</span> 和划分点 <span class="math inline">\(s\)</span> 的目标是<strong>最小化划分后的平方误差 (Squared Error)</strong>，即使得划分后的两个子区域内样本的<strong>目标变量值尽可能接近</strong>。对于给定的特征 <span class="math inline">\(j\)</span> 和切分点 <span class="math inline">\(s\)</span>，遍历所有可能的 <span class="math inline">\((j, s)\)</span> 对，计算划分后的平方误差，选择使得平方误差最小的 <span class="math inline">\((j, s)\)</span> 对作为最优划分。平方误差的计算公式为：</li>
</ol>
<p><span class="math display">\[\min_{j,s} \left[ \min_{c_1} \sum_{\mathbf{x}_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{\mathbf{x}_i \in R_2(j,s)} (y_i - c_2)^2\right]\]</span></p>
<p>其中 <span class="math inline">\(c_1\)</span> 和 <span class="math inline">\(c_2\)</span> 分别是区域 <span class="math inline">\(R_1(j,s)\)</span> 和 <span class="math inline">\(R_2(j,s)\)</span> 的<strong>预测值</strong>。对于给定的区域 <span class="math inline">\(R_m(j,s)\)</span>，最优的预测值 <span class="math inline">\(\hat{c}_m\)</span> 是该区域内样本<strong>目标变量的均值</strong>：</p>
<p><span class="math display">\[\hat{c}_m = \text{ave}(y_i|\mathbf{x}_i \in R_m(j,s)) = \frac{1}{|R_m(j,s)|} \sum_{\mathbf{x}_i \in R_m(j,s)} y_i\]</span></p>
<ol start="3" type="1">
<li><strong>递归划分</strong>：对划分后的两个子区域 <span class="math inline">\(R_1(j,s)\)</span> 和 <span class="math inline">\(R_2(j,s)\)</span>，<strong>递归地重复步骤 1 和 2</strong>，继续选择最优特征和切分点进行划分，直到满足<strong>停止条件</strong>。停止条件通常包括：
<ul>
<li>节点内样本数量小于某个<strong>预设阈值</strong>。</li>
<li>节点内样本的目标变量<strong>方差或平方误差小于某个阈值</strong>。</li>
<li>没有更多特征可用于划分，或所有特征都已用完。</li>
</ul></li>
<li><strong>生成叶节点</strong>：当满足停止条件时，将当前节点作为<strong>叶节点</strong>，并<strong>计算叶节点的预测值</strong>，通常为叶节点内样本目标变量的均值。</li>
</ol>
<p><strong>分类树 (Classification Tree):</strong></p>
<p>分类树 (Classification Tree) 用于<strong>预测离散类别型目标变量</strong>。例如，判断用户是否会流失、识别图像中的物体类别等。</p>
<p><strong>模型构建:</strong></p>
<p>分类树的构建过程与回归树类似，也是一个递归的二叉树构建过程。不同之处在于，分类树在<strong>选择最优特征和切分点</strong>时，使用的<strong>划分指标不同</strong>，以及<strong>叶节点的预测值类型不同</strong>。分类树常用的划分指标包括 <strong>信息增益 (Information Gain)</strong>、<strong>信息增益率 (Information Gain Ratio)</strong> 和 <strong>基尼指数 (Gini Index)</strong>。目标是使得划分后的子节点数据尽可能 <strong>“纯净” (Pure)</strong>，即属于<strong>同一类别的样本比例尽可能高</strong>。</p>
<p><strong>划分指标:</strong></p>
<ul>
<li><strong>信息增益 (Information Gain)</strong>：基于<strong>信息熵 (Entropy)</strong> 的划分指标。信息熵衡量了数据集的<strong>混乱程度</strong>或<strong>不确定性</strong>。信息增益表示<strong>使用特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 进行划分后，数据集 <span class="math inline">\(D\)</span> 的信息熵减少的程度</strong>。信息增益越大，说明使用特征 <span class="math inline">\(A\)</span> 划分数据集的效果越好。常用的基于信息增益的决策树算法是 <strong>ID3 算法</strong>。</li>
<li><strong>信息增益率 (Information Gain Ratio)</strong>：为了<strong>克服信息增益对取值数目较多的特征的偏好</strong>，C4.5 算法引入了信息增益率。信息增益率在信息增益的基础上，<strong>除以特征 <span class="math inline">\(A\)</span> 本身的熵</strong>，对特征取值数目较多的情况进行<strong>惩罚</strong>。常用的基于信息增益率的决策树算法是 <strong>C4.5 算法</strong>。</li>
<li><strong>基尼指数 (Gini Index)</strong>：基尼指数衡量了数据集的<strong>纯度</strong>。基尼指数越小，数据集纯度越高。CART 算法使用基尼指数作为分类树的划分指标。</li>
</ul>
<p><strong>叶节点预测值:</strong></p>
<p>分类树的叶节点<strong>输出类别标签</strong>，通常是叶节点内<strong>样本数量最多的类别</strong>（多数表决）。</p>
<p><strong>决策树的特点:</strong></p>
<ul>
<li><strong>优点</strong>：
<ul>
<li><strong>易于理解和解释</strong>：决策树模型直观易懂，决策规则清晰可见，易于向业务人员解释。</li>
<li><strong>可以处理类别型和数值型特征</strong>：无需对特征进行预处理，如独热编码、标准化等。</li>
<li><strong>无需特征缩放</strong>：决策树模型对特征的尺度不敏感，无需进行特征缩放。</li>
<li><strong>可以处理缺失值</strong>：决策树模型可以处理包含缺失值的数据。</li>
<li><strong>可以进行特征选择</strong>：决策树模型在构建过程中会自动选择重要的特征进行划分。</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li><strong>容易过拟合</strong>：决策树模型容易在训练集上过拟合，导致泛化能力差。可以通过<strong>剪枝 (Pruning)</strong> 等方法缓解过拟合问题。</li>
<li><strong>不稳定</strong>：决策树模型对训练数据敏感，训练数据的微小变化可能导致树结构发生很大变化。</li>
<li><strong>忽略特征之间的相关性</strong>：决策树模型在选择划分特征时，每次只考虑一个特征，忽略了特征之间的相关性。</li>
</ul></li>
</ul>
</section>
</section>
<section id="总结" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="总结"><span class="header-section-number">4.3</span> 总结</h2>
<p>本讲义主要介绍了监督学习的基本概念和常用模型，包括：</p>
<ul>
<li><strong>监督学习概述</strong>: 介绍了监督学习的定义、应用场景以及与量化投资的结合。</li>
<li><strong>回归模型</strong>: 详细讲解了线性回归和岭回归模型，包括模型表达式、最小二乘法、正则化以及模型特点。</li>
<li><strong>分类模型</strong>: 深入探讨了支持向量机 (SVM) 和决策树模型，包括模型原理、核函数、优化方法以及模型优缺点。</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./lab02_data.html" class="pagination-link" aria-label="金融数据获取与数据分析基础">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">金融数据获取与数据分析基础</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_supervised.html" class="pagination-link" aria-label="监督学习（下）">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">监督学习（下）</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>