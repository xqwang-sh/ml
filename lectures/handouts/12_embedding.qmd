---
title: "文本分析2：词向量与深度学习基础"
---

# 从稀疏到密集表示

## Bag of Words模型的局限性

上一讲中，我们学习了词袋模型（Bag of Words）和TF-IDF，这些是文本分析的基础方法。然而，这些方法存在明显局限性：

1. **丢失词序信息**：词袋模型完全忽略词语出现的顺序。例如"政府调控房价"和"房价调控政府"在词袋表示中是完全相同的
   
2. **语义鸿沟问题**：无法捕捉词与词之间的语义关系，如同义词、上下位词等
   
3. **维度灾难**：高维稀疏向量（维度等于词汇量大小）导致计算效率低下
   
4. **未登录词问题**：无法处理训练集中未出现过的词语

词袋模型的这些局限性促使研究者寻找更先进的文本表示方法，词向量（Word Embedding）正是这一探索的重要成果。

## 词向量的直觉理解

**词向量**（Word Embedding）是将词语映射到一个低维稠密实数向量空间的技术，通常维度在50-300之间。与词袋模型不同，词向量具有以下特点：

1. **稠密表示**：向量中的每个维度都有非零值
2. **语义编码**：向量的不同维度隐含地编码了词语的语义特征
3. **相似性可计算**：语义相近的词在向量空间中距离较近

例如，"银行"和"金融"在向量空间中距离较近，而"银行"和"蔬菜"则距离较远。

## 分布式假设：词向量的理论基础

词向量背后的核心理论是**分布式假设**（Distributional Hypothesis），这一理论由语言学家J.R. Firth在1957年提出：

> "You shall know a word by the company it keeps."（一个词的含义取决于它的伙伴词）

这一假设认为：**上下文相似的词，其语义也相似**。例如，"银行"和"金融机构"经常出现在相似的上下文中，因此它们可能具有相似的语义。

基于分布式假设，词向量学习的核心任务可以归纳为：学习一个映射函数，使得在语料库中上下文相似的词在向量空间中的位置也相近。

## 密集表示的数学性质

从数学角度看，密集表示的优势在于：

1. **降维性**：从稀疏高维空间（词汇量大小，如5万维）降至低维空间（如300维）
2. **连续性**：连续向量空间允许进行向量代数运算，如类比推理
3. **泛化能力**：能够更好地泛化到未见过的例子

数学上，稀疏向量与密集向量的对比如下：

- **稀疏向量**（Sparse Vector）：$\mathbf{v} = [0, 0, 1, 0, ..., 0, 2, 0]$，大多数元素为0
- **密集向量**（Dense Vector）：$\mathbf{v} = [0.2, -0.6, 0.5, 0.9, ..., -0.1, 0.3]$，大多数元素非0

稠密向量表示的直观优势可以通过一个简单的类比来理解：假设我们要描述一个人，可以使用二元特征（是/否问题，对应稀疏表示）如"是否戴眼镜"、"是否有胡子"等，也可以使用连续特征（对应稠密表示）如身高、体重、年龄等。连续特征通常能更精确、更紧凑地描述对象。

# Word2Vec原理讲解

## Word2Vec简介

**Word2Vec**是Mikolov等人于2013年提出的一种高效学习词向量的方法，它通过浅层神经网络从大规模语料库中学习词语的分布式表示。Word2Vec迅速成为NLP领域的里程碑技术，为后续深度学习在NLP中的应用奠定了基础。

Word2Vec的核心思想是：**通过预测上下文中的词来学习词语的向量表示**。基于这一思想，Word2Vec提出了两种模型：

1. **Skip-gram模型**：预测上下文词
2. **CBOW（Continuous Bag of Words）模型**：预测目标词

## Skip-gram模型详解

Skip-gram模型的目标是：**给定中心词，预测其上下文词**。

### 模型结构

Skip-gram模型的网络结构如下：

1. **输入层**：中心词的one-hot编码，维度为词汇量大小$|V|$
2. **隐藏层**：不含激活函数的全连接层，维度为词向量维度$d$
3. **输出层**：预测上下文词概率的softmax层，维度为词汇量大小$|V|$

其数学表示为：

$$p(w_o|w_i) = \frac{\exp(v_{w_o}^{\prime T} \cdot v_{w_i})}{\sum_{w=1}^{|V|} \exp(v_w^{\prime T} \cdot v_{w_i})}$$

其中：
- $w_i$是中心词
- $w_o$是上下文词
- $v_{w_i}$是中心词的词向量（输入向量）
- $v_{w_o}^{\prime}$是上下文词的词向量（输出向量）
- $|V|$是词汇表大小

### 训练过程

Skip-gram模型的训练过程如下：

1. 从语料库中抽取中心词$w_i$及其上下文窗口内的词$w_o$
2. 最大化预测上下文词的条件概率$p(w_o|w_i)$
3. 对所有词对$(w_i, w_o)$，优化目标函数：

$$J(\theta) = \frac{1}{T}\sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)$$

其中：
- $T$是语料库中的词数
- $c$是上下文窗口大小
- $\theta$是模型参数

## CBOW模型详解

CBOW（Continuous Bag of Words）模型与Skip-gram相反，其目标是：**给定上下文词，预测中心词**。

### 模型结构

CBOW模型的网络结构如下：

1. **输入层**：多个上下文词的one-hot编码，每个维度为词汇量大小$|V|$
2. **隐藏层**：不含激活函数的全连接层，维度为词向量维度$d$
3. **输出层**：预测中心词概率的softmax层，维度为词汇量大小$|V|$

CBOW模型首先对上下文词的向量取平均：

$$\hat{v} = \frac{1}{2c} \sum_{-c \leq j \leq c, j \neq 0} v_{w_{t+j}}$$

然后预测中心词的概率：

$$p(w_t|\hat{v}) = \frac{\exp(v_{w_t}^{\prime T} \cdot \hat{v})}{\sum_{w=1}^{|V|} \exp(v_w^{\prime T} \cdot \hat{v})}$$

### Skip-gram与CBOW对比

两种模型各有优缺点：

1. **Skip-gram**:
   - 更适合小型语料库
   - 对低频词表现更好
   - 计算复杂度较高

2. **CBOW**:
   - 训练速度更快
   - 对高频词表现更好
   - 在大型语料库上更稳定

## 负采样（Negative Sampling）技术

Word2Vec的一个主要计算瓶颈是softmax函数，其计算复杂度与词汇量成正比。为解决这一问题，Mikolov等人提出了**负采样**（Negative Sampling）技术。

### 负采样原理

负采样将多分类问题转化为二分类问题：

1. 对于真实的词对$(w_i, w_o)$，将其标记为正样本（标签为1）
2. 对于每个正样本，随机采样$k$个负样本$(w_i, w_n)$，其中$w_n$是随机词（标签为0）
3. 使用逻辑回归来判断词对是否真实共现

优化目标变为：

$$J(\theta) = \log \sigma(v_{w_o}^{\prime T} \cdot v_{w_i}) + \sum_{j=1}^{k} \mathbb{E}_{w_j \sim P_n(w)} [\log \sigma(-v_{w_j}^{\prime T} \cdot v_{w_i})]$$

其中：
- $\sigma$是sigmoid函数
- $P_n(w)$是负样本的噪声分布，通常为词频的3/4次方

### 负采样的优势

负采样技术带来的主要优势包括：

1. **计算效率**：将复杂度从$O(|V|)$降至$O(k)$，其中$k \ll |V|$（通常$k=5-20$）
2. **稀疏更新**：每次只更新少量词向量，加速收敛
3. **控制学习难度**：通过调整负样本数量控制任务难度

负采样是Word2Vec能够在大规模语料库上高效训练的关键技术之一。

## 词向量空间的语义特性

Word2Vec训练得到的词向量空间具有丰富的语义特性，这些特性使得词向量成为各种NLP任务的强大特征。

### 语义相关性

相似概念在向量空间中距离较近。例如：

- "银行"和"金融"距离近
- "苹果"(水果)和"橙子"距离近
- "苹果"(公司)和"微软"距离近

这种相似性可以通过余弦相似度定量衡量：

$$similarity(w_1, w_2) = \cos(\theta) = \frac{v_{w_1} \cdot v_{w_2}}{||v_{w_1}|| \cdot ||v_{w_2}||}$$

### 语义计算

词向量空间中最令人惊讶的特性是支持向量代数运算，可以进行"语义计算"：

$$v(\text{"king"}) - v(\text{"man"}) + v(\text{"woman"}) \approx v(\text{"queen"})$$

这意味着我们可以通过向量运算回答类比问题："man之于woman，相当于king之于什么？"

其他例子包括：
- $v(\text{"中国"}) - v(\text{"北京"}) + v(\text{"法国"}) \approx v(\text{"巴黎"})$
- $v(\text{"比特币"}) - v(\text{"数字"}) + v(\text{"实物"}) \approx v(\text{"黄金"})$

这些语义运算的存在表明词向量确实捕获了复杂的语义关系，而非简单的共现统计。

# 金融文本中应用Word2Vec

## 金融领域的词向量应用

在金融领域，词向量技术已被广泛应用于多种任务：

1. **情感分析**：分析金融新闻、社交媒体对市场情绪的影响
2. **风险评估**：从文本数据中提取风险信号
3. **主题发现**：自动识别财经报道中的热点话题
4. **市场预测**：结合文本特征进行市场走势预测

金融文本的特殊性（专业术语多、实体关系复杂）使得通用词向量模型可能表现不佳，因此针对金融领域训练的词向量至关重要。

## 训练金融领域词向量

以下我们将使用政府工作报告和其他财经语料训练Word2Vec模型，展示其在金融领域的应用。

# 预训练词向量模型比较与应用

在实际应用中，我们通常可以选择使用已有的预训练词向量模型，而不必从头开始训练。这些模型由大型组织或研究机构在海量文本上训练得到，具有更好的通用性和语义表示能力。下面我们将介绍几种常用的预训练词向量模型，并比较它们在不同应用场景下的表现。

## 中文预训练词向量模型

### 1. 腾讯AI Lab词向量（Chinese Word Vectors）

腾讯AI Lab发布的中文词向量是目前应用最广泛的中文预训练词向量之一。

- **训练语料**：由8亿多条句子、超过200亿词汇组成
- **词汇量**：约800万个词、词组和实体
- **向量维度**：200维
- **特点**：覆盖面广，质量高，适用于多领域任务

### 2. 哈工大/讯飞联合实验室词向量（HIT-SCIR Chinese Word Vectors）

- **训练语料**：人民日报语料库和其他新闻语料
- **词汇量**：约100万个词
- **向量维度**：300维
- **特点**：对专业术语和实体识别有较好的表现

### 3. 北师大/人大学者开发的词向量（Chinese Word Vectors）

- **训练语料**：基于百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等语料
- **词汇量**：约200万个词
- **向量维度**：300维
- **特点**：对百科类内容和常识性知识表示较好

## 英文预训练词向量模型

### 1. Word2Vec词向量

由Google在约1000亿单词的Google News数据集上训练的Word2Vec模型。

- **词汇量**：约300万个词和短语
- **向量维度**：300维
- **特点**：通用性强，被广泛应用于英文NLP研究和应用

### 2. GloVe（Global Vectors for Word Representation）

由斯坦福NLP小组开发的词向量模型，训练自维基百科和网络文本。

- **训练语料**：CommonCrawl（840B tokens）、Wikipedia（6B tokens）等
- **词汇量**：根据语料大小从40万到200万不等
- **向量维度**：50到300维不等
- **特点**：结合了全局矩阵分解和局部上下文窗口方法的优点

### 3. FastText

由Facebook AI Research开发，在维基百科语料上训练。

- **词汇量**：约200万个词
- **向量维度**：300维
- **特点**：利用词的子词信息，能更好地处理罕见词和未登录词

## 预训练模型的表现比较

不同预训练词向量模型在各种任务上的表现各有优劣。下面我们将从多个维度对它们进行比较：

### 1. 语义捕捉能力

通过测试几组典型的语义关系来比较不同模型：

### 2. 领域适应性

不同模型在特定领域（如金融、医疗、法律等）的表现评估：

### 3. 处理未登录词能力

FastText由于使用子词信息，对未登录词有独特优势：

三种预训练词向量模型在处理未登录词（Out-Of-Vocabulary，OOV）方面的能力存在显著差异：

1. **FastText**：
   - **技术原理**：将词表示为字符n-gram的集合，利用子词信息
   - **优势**：即使词不在词表中，仍可通过子词组合生成向量
   - **实际效果**：能有效处理形态变化（如复数、时态）和拼写变体
   - **适用场景**：专业领域术语多、新词频繁出现的应用，如医疗、法律和技术文本

2. **GloVe**：
   - **技术原理**：结合全局矩阵分解和局部上下文窗口方法
   - **优势**：能较好地表示低频词，但不能直接处理未登录词
   - **处理方案**：通常使用特殊的<UNK>向量或基于字符级特征构建未知词向量
   - **适用场景**：词汇相对稳定但关注低频词的场景，如学术文本分析

3. **Word2Vec**：
   - **技术原理**：纯粹基于词级别的分布式表示学习
   - **劣势**：完全无法处理训练集中未出现的词
   - **处理方案**：常用随机向量、零向量或所有词向量的平均值代替未登录词
   - **适用场景**：封闭域应用，词汇变化少的场景，如特定主题的文档分类

在实际测试中，当处理"区块链"、"元宇宙"等新兴词汇时，FastText能生成语义相关的向量，而Word2Vec和GloVe则完全无法处理。这一点在金融科技等快速发展的领域尤为重要。

### 4. 多语言词向量对齐

为了支持跨语言应用，可以将不同语言的词向量空间对齐：

## 预训练模型在不同任务中的选择指南

根据不同应用场景，我们推荐选择的预训练词向量模型：

| 应用场景 | 推荐中文模型 | 推荐英文模型 | 理由 |
|---------|------------|------------|------|
| 通用文本分类 | 腾讯AI Lab | GloVe 300d | 覆盖面广，向量维度适中 |
| 命名实体识别 | 哈工大词向量 | FastText | 对实体名称和罕见词有更好表现 |
| 情感分析 | 腾讯AI Lab | Word2Vec | 对语义细微差别表现更好 |
| 专业领域(如金融) | 领域特定模型 | 领域特定模型 | 通用模型对专业术语表示不足 |
| 处理网络文本 | 搜狗新闻词向量 | FastText | 对网络流行语和新词表现更好 |

## 词向量在实际应用中的评估

评估词向量质量是选择或训练词向量模型的重要一环。词向量评估通常分为两类：内在评估和外在评估。

### 内在评估(Intrinsic Evaluation)

内在评估直接测试词向量捕捉语言学特性的能力：

1. **词相似度任务**：
   - **WordSim-353**：包含353对词语，每对都有人工标注的相似度评分
   - **SimLex-999**：着重于真正的语义相似性（而非相关性）的数据集
   - **中文评估集**：如哈工大汉语词相似度数据集（HowNet-500）
   
   评估方法：计算词向量余弦相似度与人工评分的相关系数（Spearman或Pearson）

2. **类比关系任务**：
   - 测试模型是否捕捉语义关系，如"北京:中国::巴黎:法国"
   - 常见关系类型：国家-首都、国家-货币、动词时态、形容词比较级等
   
   评估方法：计算向量运算（如v("北京")-v("中国")+v("法国")）的结果与目标词v("巴黎")的接近度

3. **多语言对齐质量**：
   - 对于跨语言应用，评估词向量在不同语言间的语义一致性
   - 使用平行词典或自动翻译进行评估
   
   评估方法：翻译精度或跨语言检索正确率

### 外在评估(Extrinsic Evaluation)

外在评估测量词向量在实际NLP任务中的表现：

1. **下游任务性能**：
   - 文本分类：准确率、F1分数
   - 命名实体识别：精确率、召回率
   - 机器翻译：BLEU分数
   - 情感分析：准确率，特别是对微妙情感表达的捕捉能力

2. **迁移能力**：
   - 从一个领域到另一个领域的泛化能力
   - 从一种语言到另一种语言的迁移性能
   - 低资源场景下的表现

3. **资源效率**：
   - 训练时间和计算资源需求
   - 模型大小和内存占用
   - 推理速度

### 金融领域案例分析

在金融文本分析中，领域特定词向量通常优于通用词向量：

- 在市场情绪分析任务中，金融专用词向量比通用词向量提高8-15%的准确率
- 对专业术语和金融实体的识别准确率提升显著（>20%）
- 在金融新闻聚类任务中，专业词向量能发现更细粒度的主题区分

这种性能差异主要来自金融领域特有的语言使用模式和术语，如"牛市/熊市"、"做多/做空"等在通用语料中很少出现或具有不同含义的词。

## 选择或训练自己的词向量模型

在实际应用中，我们需要根据具体任务和数据特点选择合适的预训练模型或决定是否需要训练自己的模型：

1. **使用预训练模型的情况**：
   - 数据量有限，无法支持有效训练
   - 任务是通用领域，预训练模型已足够好
   - 计算资源有限
   - 需要快速开发原型系统

2. **训练自己的模型的情况**：
   - 有大量特定领域的文本数据
   - 应用领域有特殊术语或表达方式
   - 现有预训练模型表现不佳
   - 有足够的计算资源

3. **微调预训练模型的折中方案**：
   - 从预训练模型开始，用领域数据继续训练
   - 保留通用语言知识，同时学习领域特定表示
   - 资源需求适中，效果通常不错

## 结论

预训练词向量模型为NLP任务提供了便捷的起点，无需从头训练就能获得高质量的词语表示。根据我们的测试，腾讯AI Lab词向量和GloVe模型在通用任务中表现最佳，而FastText在处理未登录词方面具有明显优势。

对于金融文本分析，我们建议：如果数据量充足，可以在通用预训练模型基础上，使用金融领域文本进行进一步训练，以获得更符合领域特性的词向量表示；如果资源有限，可以选择腾讯AI Lab等高质量预训练模型作为基础，然后结合任务特点设计适当的特征工程。

# 案例比较：词频法与词向量在政府工作报告分析中的差异

基于上一讲中的政府工作报告分析案例，我们可以直观地比较词频法(Bag of Words/TF-IDF)和词向量(Word2Vec)在相同任务上的表现差异。以下我们在三个具体任务上进行比较分析：文本相似度计算、关键词提取和语义关联发现。

## 相同文本的两种表示方法

首先，让我们回顾两种方法对同一文档的不同表示方式：

**相似度计算结果分析**:

1. **TF-IDF相似度**：
   - 基于词汇重叠度，相同词汇出现越多，相似度越高
   - 对关键词敏感，但没有语义理解
   - 相似度变化跨度较大，更容易区分文档

2. **Word2Vec相似度**：
   - 基于语义空间的接近程度，能捕捉同义词和相关概念
   - 所有文档相似度普遍较高，因为它们都处于相似的语义空间
   - 变化更平滑，体现语义连续性

例如，即使2020年疫情报告的词汇与其他年份有明显不同，但在Word2Vec中相似度依然较高，因为整体语义主题（政府工作）是相似的。

## 关键词提取比较

两种方法在关键词提取上也有明显不同：

**关键词提取结果分析**:

1. **TF-IDF关键词**：
   - 提取文档中特有的、区分度高的词
   - 往往是该文档特有的专有名词或低频词
   - 关注"独特性"而非"重要性"

2. **Word2Vec关键词**：
   - 提取文档的语义中心词，与其他词语义联系最紧密的词
   - 往往是文档主题的核心词，且在语义网络中起枢纽作用
   - 关注"中心性"而非"频率"或"独特性"

例如，TF-IDF可能会提取"十四五"这样的特定术语，而Word2Vec可能会提取"发展"这样的核心概念词。

## 语义关联发现比较

词频法和词向量在发现词语关联关系上有本质区别：

**语义关联分析结果比较**:

1. **词频法（共现分析）**：
   - 基于词语在文本中出现的物理距离
   - 只能发现直接共现的关系，不能泛化
   - 需要大量文本才能得到可靠的统计结果
   - 无法发现从未共现但语义相关的词

2. **Word2Vec（语义网络）**：
   - 基于分布式表示学习的语义空间距离
   - 能发现间接关联，即使两个词从未共现
   - 能发现语义层次和类比关系
   - 受预训练语料的影响大

例如，在政府工作报告中，"环保"和"低碳"可能很少直接共现，但在Word2Vec的语义空间中会很接近。

## 主题和情感分析的差异

两种方法在主题建模和情感分析上也有明显区别：

**主题分析结果比较**:

1. **基于词频的LDA**：
   - 基于词语在文档中的共现统计构建主题
   - 主题是词语的概率分布，每个词有明确的主题概率
   - 结果可解释性强，但受制于表面统计

2. **基于Word2Vec的聚类**：
   - 基于文档在语义空间中的分布进行聚类
   - 主题体现为语义空间中的区域，边界更加模糊
   - 能发现更抽象的语义关联，但解释性较弱

## 综合比较与应用建议

通过以上案例比较，我们总结词频法和词向量在政府工作报告分析中的优缺点：

| 比较维度 | 词频法 (Bag of Words/TF-IDF) | 词向量 (Word2Vec) |
|---------|---------------------------|------------------|
| 数据表示 | 高维稀疏向量(~万维) | 低维稠密向量(~百维) |
| 语义捕捉 | 基于表面词频统计，无语义 | 基于分布式假设，有语义 |
| 计算复杂度 | 低，适合大规模文档 | 中等，训练需要时间 |
| 内存占用 | 大（稀疏矩阵）| 小（稠密向量）|
| 新词处理 | 无法处理未见词 | 也无法直接处理(FastText可以) |
| 相似度计算 | 仅基于词重叠 | 基于语义相似 |
| 关键词提取 | 偏向特有词 | 偏向中心词 |
| 语义关联 | 仅能发现共现关系 | 能发现间接语义关联 |
| 应用场景 | 文档分类、信息检索 | 语义搜索、推荐系统 |

### 应用建议

基于我们对政府工作报告的分析经验，针对不同任务推荐的方法：

1. **文档去重或精确匹配**：使用词频表示
2. **文档语义检索或推荐**：使用词向量表示
3. **特有术语或政策提取**：使用TF-IDF方法
4. **政策主题语义聚类**：使用Word2Vec
5. **综合分析**：可以同时使用两种方法并结合结果

在实际应用中，选择合适的文本表示方法往往取决于具体任务需求、可用资源和期望的结果特性。词频法和词向量并非互斥，而是互补的分析视角。

# 小结与进阶方向

## 词向量的优缺点

### 优点
1. **语义丰富**：捕获了词语间的语义关系
2. **维度可控**：典型地为50-300维，远低于词汇量大小
3. **泛化能力**：能处理未见过的词组合
4. **通用性**：可用于各种NLP任务的特征提取

### 局限性
1. **多义词问题**：无法区分同一个词的不同含义（如"苹果"可以是水果或公司）
2. **上下文依赖**：固定的词向量无法根据上下文调整
3. **预训练依赖**：需要大量语料预训练
4. **领域专一性**：通用领域训练的词向量在专业领域效果可能不佳

### 多义词问题的解决方案

针对词向量无法处理多义词的局限性，研究者提出了多种解决方案：

1. **基于聚类的多义词表示**：
   - 收集词的所有上下文向量
   - 对上下文向量进行聚类（如K-means）
   - 为每个聚类中心分配一个词义向量
   - 使用时根据上下文选择最相似的词义向量

2. **动态词表示模型**：
   - **ELMo (Embeddings from Language Models)**：基于双向LSTM，根据句子上下文动态生成词表示
   - **BERT (Bidirectional Encoder Representations from Transformers)**：基于Transformer架构，学习上下文敏感的词表示
   - **GPT (Generative Pre-trained Transformer)**：单向语言模型，生成考虑左侧上下文的表示

3. **词义消歧与表示结合**：
   - 先使用词义消歧算法确定词的当前含义
   - 再选择对应含义的词向量表示
   - 例如MSSG (Multi-Sense Skip-Gram)模型

这些技术已经在很大程度上克服了传统Word2Vec模型处理多义词的局限，代表了词表示技术的发展方向。

## 进阶方向

1. **上下文相关的词表示**：如ELMo、BERT等模型能根据上下文动态生成词表示
2. **多语言词向量**：跨语言的词向量对齐，支持多语言应用
3. **领域适应**：将通用词向量迁移到特定领域
4. **可解释性研究**：理解词向量空间的维度含义

## 本讲小结

本讲我们从Bag of Words模型的局限性出发，介绍了词向量的概念、原理和应用：

1. 词向量通过低维稠密向量表示词语，克服了传统方法的局限
2. Word2Vec通过Skip-gram和CBOW两种模型高效学习词向量
3. 负采样等技术大幅提高了训练效率
4. 词向量空间具有丰富的语义特性，支持相似性计算和向量代数运算
5. 在金融文本分析中，词向量可以发现政策热点、分析语义变化等
