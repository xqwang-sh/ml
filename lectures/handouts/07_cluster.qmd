---
title: "非监督学习：聚类 (Clustering)"
---

## 聚类 (Clustering)

### 核心概念

聚类是一种非监督学习技术，其目标是将数据集中的样本根据它们的相似性划分为若干个组，这些组被称为"簇" (Cluster)。聚类的核心思想是：

*   **簇内相似性最大化 (Maximize Intra-cluster Similarity):** 同一个簇内的数据点（样本）应该尽可能相似。
*   **簇间差异性最大化 (Maximize Inter-cluster Dissimilarity):** 不同簇之间的数据点应该尽可能不相似。

在金融领域，我们可以对各种对象进行聚类，例如：

*   **资产 (Assets):** 如股票、债券，根据它们的风险收益特征、价格波动模式等进行聚类。
*   **客户 (Customers):** 根据客户的交易行为、风险偏好、人口统计学特征等进行聚类。
*   **市场时期 (Market Regimes):** 根据市场波动性、相关性等指标对不同的市场阶段进行聚类。

### 常用算法简介

#### K-Means (K均值聚类)

*   **原理:** K-Means 是一种迭代算法，旨在将数据划分为预先指定的 K 个簇。它通过不断更新簇的中心点（质心）并将每个数据点分配给最近的质心来实现。
    1.  随机选择 K 个初始质心。
    2.  **分配步骤:** 将每个数据点分配给距离其最近的质心，形成 K 个簇。
    3.  **更新步骤:** 重新计算每个簇的质心（通常是簇内所有点的均值）。
    4.  重复步骤 2 和 3，直到质心不再发生显著变化或达到最大迭代次数。
*   **优点:**
    *   算法简单，容易理解和实现。
    *   计算效率高，处理大数据集速度较快。
*   **缺点:**
    *   需要预先指定簇的数量 K，而 K 值的选择往往比较困难。
    *   对初始质心的选择敏感，可能陷入局部最优解。
    *   对非球状簇、不同大小和密度的簇效果不佳。
    *   对异常值（Outliers）比较敏感。

#### 层次聚类 (Hierarchical Clustering)

*   **原理:** 层次聚类通过构建数据点之间的层次结构（通常表示为树状图，Dendrogram）来进行聚类。它有两种主要方式：
    *   **凝聚式 (Agglomerative):** 自底向上，开始时每个点自成一簇，然后逐步合并最相似的簇，直到所有点合并为一个簇。
    *   **分裂式 (Divisive):** 自顶向下，开始时所有点在一个簇，然后逐步分裂最不相似的簇，直到每个点自成一簇或满足停止条件。
*   **优点:**
    *   不需要预先指定簇的数量 K，可以根据树状图决定合适的簇数。
    *   可以揭示数据的层次结构关系。
    *   对簇的形状没有严格假设。
*   **缺点:**
    *   计算复杂度较高，特别是对于大数据集（通常为 O(n^2) 或 O(n^3)）。
    *   一旦合并或分裂完成，就无法撤销，可能导致次优结果。
    *   算法选择（如距离度量、合并策略）对结果影响较大。

#### DBSCAN (基于密度的聚类)

*   **原理:** DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一种基于密度的聚类算法，它将密度相连的点划分为一个簇，并且能够发现任意形状的簇，同时自动识别噪声点。
    1.  **核心概念:**
        *   **ε (Epsilon):** 邻域半径，定义点之间的"近"的概念。
        *   **MinPts:** 邻域内最少点数，用于判断密度。
        *   **核心点 (Core Point):** 在其 ε 邻域内至少有 MinPts 个点的点。
        *   **边界点 (Border Point):** 不是核心点，但在某个核心点的 ε 邻域内的点。
        *   **噪声点 (Noise Point):** 既不是核心点也不是边界点的点。
    2.  **算法步骤:**
        *   任选一个未被访问的点 p。
        *   标记 p 为已访问。
        *   如果 p 是核心点，创建一个新簇，并将 p 的所有密度可达点加入该簇。
        *   如果 p 不是核心点，标记为噪声点并继续。
        *   重复以上步骤，直到所有点都被访问。
*   **优点:**
    *   不需要预先指定簇的数量。
    *   能发现任意形状的簇，不限于球形簇。
    *   能自动识别和处理噪声点。
    *   对离群点不敏感。
*   **缺点:**
    *   参数选择（ε 和 MinPts）有时较为困难。
    *   对数据集中密度差异较大的簇效果不佳。
    *   计算复杂度较高（约为 O(n²)），但通常可以通过空间索引优化。
    *   不能处理高维空间中的"维度灾难"问题。

### 具体金融市场应用实例：股票板块轮动分析

**目标:** 利用聚类技术识别股票市场中可能存在的板块轮动现象或隐藏的股票群体特征。

**步骤:**

1.  **数据收集:**
    *   选择一个股票池，例如沪深300指数的成分股。
    *   收集这些股票在过去一段时间（如一年）的每日收益率数据。

2.  **数据预处理:**
    *   处理缺失值（如有）。
    *   可以考虑对收益率数据进行标准化，以消除量纲影响。

3.  **聚类分析 (使用 K-Means):**
    *   选择 K-Means 算法对股票进行聚类，输入数据为每只股票的日收益率序列（可以看作一个高维向量）。
    *   尝试不同的 K 值（例如，从 K=5 到 K=10）。可以使用肘部法则或轮廓系数等方法辅助选择最优的 K 值（见下文评估方法）。

4.  **结果解读与分析:**
    *   **簇成员分析:** 查看每个形成的簇中包含哪些股票。
        *   **传统行业板块验证:** 这些簇是否大致对应于已知的传统行业板块（如银行、地产、科技、消费、医药等）？
        *   **跨行业/因子聚类发现:** 是否出现了跨越传统行业的聚类？例如，一个簇可能同时包含高分红的银行股、公用事业股和部分稳定消费股，这可能揭示了一个基于"价值"或"低波动"因子的股票群体。另一个簇可能包含新能源、半导体和部分高端制造股票，揭示了基于"高成长性"或"科技主题"的群体。
    *   **簇表现分析:** 计算每个簇内股票在不同时间段（如最近一个月、一个季度）的平均收益率、整体波动性（如平均标准差）。
        *   **识别强势/弱势群体:** 比较各簇的表现，识别出近期表现显著优于或劣于市场平均水平的股票群体。例如，如果发现某个包含新能源和高端制造的簇近期平均收益率远超大盘指数，这可能指示了当前的市场热点和资金流向。
        *   **板块轮动证据:** 观察不同时间段内各簇表现的相对变化，可以为板块轮动的投资策略提供数据支持。例如，上个季度表现强势的"价值"簇本季度表现平平，而"成长"簇开始领涨。

**意义:** 通过聚类，可以超越传统的行业划分，从数据驱动的角度发现股票之间更深层次的关联性（如同涨同跌模式），为投资组合构建和择时策略提供新的视角。

### 评估方法 (详细介绍)

如何评估聚类效果的好坏，以及如何选择合适的 K 值？

#### 1. 轮廓系数 (Silhouette Coefficient)

轮廓系数是评估聚类质量的一种方法，它综合考虑了簇内的紧密度和簇间的分离度。

*   **计算方法:**
    *   对于数据集中的每个样本点 i：
        *   计算 a(i): 点 i 与同簇中其他所有点的平均距离（衡量簇内紧密度）。
        *   计算 b(i): 点 i 与距离最近的其他簇中所有点的平均距离（衡量簇间分离度）。
        *   轮廓系数 s(i) = (b(i) - a(i)) / max(a(i), b(i))。
    *   数据集的整体轮廓系数是所有样本点轮廓系数的平均值。

*   **轮廓系数解读:**
    *   值域范围：[-1, 1]
    *   接近 +1：表示样本与自己的簇很匹配，与相邻簇的差距很大，聚类效果好。
    *   接近 0：表示样本处于两个簇的边界附近。
    *   接近 -1：表示样本可能被分配到了错误的簇。
    *   通常认为，平均轮廓系数大于 0.5 是较好的聚类结果，大于 0.7 是优秀的聚类结果。

*   **使用方法:**
    *   对不同的簇数量 (K) 计算平均轮廓系数。
    *   选择平均轮廓系数最大的 K 值。
    *   可以额外检查每个簇的平均轮廓系数，以识别问题簇（轮廓系数较低的簇）。

*   **Python 实现:**
    ```python
    from sklearn.metrics import silhouette_score, silhouette_samples
    
    # 计算整体轮廓系数
    sil_score = silhouette_score(X, labels)
    
    # 计算每个样本的轮廓系数
    sil_samples = silhouette_samples(X, labels)
    
    # 计算每个簇的平均轮廓系数
    for i in range(n_clusters):
        cluster_sil = sil_samples[labels == i].mean()
        print(f"Cluster {i} silhouette coefficient: {cluster_sil:.3f}")
    ```

#### 2. 肘部法则 (Elbow Method)

肘部法则是一种启发式方法，用于确定 K-Means 中的最佳聚类数量 K。

*   **原理:**
    *   对不同的 K 值运行 K-Means 算法，计算每个 K 值对应的簇内平方和 (WCSS, Within-Cluster Sum of Squares)。WCSS 是每个点与其所属簇的中心点之间距离的平方和。
    *   绘制 K 值与 WCSS 的关系图。
    *   随着 K 值的增加，WCSS 总体呈下降趋势，但下降速率会逐渐变缓。
    *   寻找图中的"肘部"，即曲线下降速率发生明显变化的点。该点对应的 K 值被视为较佳选择。

*   **数学表示:**
    *   WCSS = Σ(xi - μj)²，其中 xi 是簇 j 中的点，μj 是簇 j 的中心点。
    *   在 scikit-learn 中，WCSS 可以通过 K-Means 的 `inertia_` 属性获取。

*   **选择肘部的依据:**
    *   当 K 值小于真实簇数时，增加 K 会显著减少 WCSS（曲线陡峭部分）。
    *   当 K 值超过真实簇数时，再增加 K 对 WCSS 的减少效果不再明显（曲线平缓部分）。
    *   肘部通常是曲线从陡峭变为平缓的转折点。

*   **Python 实现:**
    ```python
    from sklearn.cluster import KMeans
    import matplotlib.pyplot as plt
    
    wcss = []
    k_range = range(1, 11)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, wcss, marker='o')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('WCSS (Inertia)')
    plt.grid(True)
    plt.show()
    ```

*   **注意事项:**
    *   肘部法则是一种视觉方法，有时肘部可能不够明显，需要结合业务理解和其他评估指标（如轮廓系数）来确定最终的 K 值。
    *   如果数据中的簇数量不明显，曲线可能会很平滑，没有明显的肘部。
    *   该方法主要适用于球形簇，对于复杂形状的簇可能效果不佳。

#### 3. 戴维斯-布尔丁指数 (Davies-Bouldin Index, DBI)

戴维斯-布尔丁指数是另一种评估聚类质量的度量，它关注簇内相似度和簇间差异性的比率。

*   **计算方法:**
    *   对于每对簇 i 和 j，计算两个簇的"相似度" Rij = (Si + Sj) / Dij，其中：
        *   Si 和 Sj 分别是簇 i 和簇 j 的平均点到中心的距离（簇内散布度）。
        *   Dij 是簇 i 和簇 j 中心之间的距离（簇间距离）。
    *   对于每个簇 i，找到它与其他所有簇 j 的最大相似度 max(Rij)。
    *   DBI 是所有簇最大相似度的平均值。

*   **DBI 解读:**
    *   值越小越好，表示簇内紧密且簇间分离。
    *   理想的聚类会产生低的 DBI 值。

*   **使用方法:**
    *   对不同的簇数量 (K) 计算 DBI。
    *   选择 DBI 值最小的 K 值。

*   **Python 实现:**
    ```python
    from sklearn.metrics import davies_bouldin_score
    
    dbi = davies_bouldin_score(X, labels)
    print(f"Davies-Bouldin Index: {dbi:.3f}")
    ```

#### 4. 为 DBSCAN 选择参数

与 K-Means 不同，DBSCAN 不需要预先指定簇的数量，但需要选择两个关键参数：邻域半径 ε 和最小点数 MinPts。

*   **选择 MinPts:**
    *   一般经验法则是设置 MinPts ≥ 维度 + 1，例如对于二维数据，MinPts ≥ 3。
    *   更大的 MinPts 值可以减少噪声点的误判，但也可能导致小簇被视为噪声。
    *   常用值范围：3-10，但对于大型数据集可能需要更大的值。

*   **选择 ε (Epsilon):**
    *   **K-距离图:**
        1. 对每个点，计算到其第 k 个最近邻点 (k = MinPts) 的距离。
        2. 对这些距离进行升序排序，并绘制成图。
        3. 寻找曲线上的"拐点"，该点对应的距离值可作为 ε 的值。
    
    *   **Python 实现 K-距离图:**
        ```python
        from sklearn.neighbors import NearestNeighbors
        import numpy as np
        import matplotlib.pyplot as plt
        
        # 选择 MinPts 值，例如 4
        MinPts = 4
        
        # 找到每个点的 k 个最近邻
        nbrs = NearestNeighbors(n_neighbors=MinPts).fit(X)
        distances, indices = nbrs.kneighbors(X)
        
        # 距离排序
        distances = np.sort(distances[:, MinPts-1])
        
        # 绘制 K-距离图
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(distances)), distances)
        plt.title('K-Distance Plot (k={})'.format(MinPts))
        plt.xlabel('Points sorted by distance')
        plt.ylabel('Distance to k-th nearest neighbor')
        plt.grid(True)
        plt.show()
        ```

通过综合使用上述评估方法，我们可以更客观、定量地判断聚类算法的效果，选择合适的参数，从而获得更有意义的聚类结果。