---
title: "监督学习（上）"
---

## 监督学习简介

监督学习是机器学习的一个重要分支，它是指从**带有标签的数据**中自动学习规律和模式，并利用这些规律和模式对**新数据进行预测和决策**的过程。在监督学习中，我们拥有一个包含输入特征 $\mathbf{x}$ 和对应输出标签 $y$ 的数据集，模型的目标是学习一个从输入特征到输出标签的映射关系。监督学习在量化投资、金融科技等领域有广泛应用，例如：

- **风险评估**：根据客户的历史信用数据（特征）预测其信用风险等级（标签）。
- **欺诈检测**：基于交易记录（特征）识别欺诈交易（标签）。
- **量化交易**：预测股票价格走势（标签）以辅助交易决策（特征）。
- **客户细分**：根据客户特征（特征）预测客户所属类别（标签），进行精准营销。

由于课程时间有限，本讲义将重点介绍监督学习中的**回归**、**分类**和**集成学习**，以及它们在金融预测中的应用。

## 监督学习模型详解

### 回归问题描述

回归问题旨在通过由 $K \times 1$ 维向量 $\mathbf{x}$ 表示的 $K$ 个观测到的**预测变量（特征）**来预测**连续数值型**的结果 $y$。 给定训练数据 $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$，其中 $\mathbf{x}_i$ 是第 $i$ 个样本的特征向量， $y_i$ 是对应的真实值， $N$ 是样本数量。我们的目标是找到一个函数 $f$，使得对于新的输入 $\mathbf{x}$，模型预测值 $\hat{y} = f(\mathbf{x})$ 尽可能接近真实值 $y$。假设真实值 $y_i$ 与预测函数 $f(\mathbf{x}_i)$ 之间存在如下关系：

$$y_i = f(\mathbf{x}_i) + \epsilon_i$$

其中 $\epsilon_i$ 代表**随机误差项**，通常假设其服从均值为 0 的正态分布。在实际应用中，我们通常将观测值堆叠成矩阵和向量的形式，方便模型表达和计算：

- $N \times 1$ 维**结果向量** $\mathbf{y} = (y_1, y_2, ..., y_N)^T$
- $N \times K$ 维**特征矩阵** $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N)^T$，每一行代表一个样本，每一列代表一个特征。
- $N \times 1$ 维**误差向量** $\mathbf{\epsilon} = (\epsilon_1, \epsilon_2, ..., \epsilon_N)^T$

回归模型可以简洁地写为： $\mathbf{y} = f(\mathbf{X}) + \mathbf{\epsilon}$。我们的目标是通过训练数据学习到函数 $f$ 的具体形式，从而能够对新的样本 $\mathbf{x}$ 进行预测。

### 线性回归 (Linear Regression)

线性回归模型是最简单且应用广泛的回归模型。它假设结果变量 $y$ 与特征向量 $\mathbf{x}$ 之间存在**线性关系**。线性回归模型易于理解和实现，是许多复杂模型的基础。

**模型表达式:**

线性回归模型假设预测函数 $f(\mathbf{x})$ 是特征 $\mathbf{x}$ 的线性组合，模型表达式如下：

$y = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

或者对于单个样本 $i$，可以表示为：

$y_i = \mathbf{x}_i^T \mathbf{\beta} + \epsilon_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_K x_{iK} + \epsilon_i$

其中：
- $\mathbf{\beta} = (\beta_0, \beta_1, ..., \beta_K)^T$ 是 $(K+1) \times 1$ 维**回归系数向量**，$\beta_0$ 是截距项（bias），$\beta_1, ..., \beta_K$ 是特征的系数。为了方便表示，我们通常在特征矩阵 $\mathbf{X}$ 中添加一列全为 1 的列向量，对应于截距项 $\beta_0$。
- $\mathbf{x}_i = (1, x_{i1}, x_{i2}, ..., x_{iK})^T$ 是 $(K+1) \times 1$ 维**增广特征向量**，包含了常数项 1 和原始特征。
- $\epsilon_i$ 是误差项。

**最优化方法：最小二乘法 (OLS)**

线性回归的目标是找到最优的回归系数 $\mathbf{\beta}$，使得模型的预测值 $\mathbf{X}\mathbf{\beta}$ 与真实值 $\mathbf{y}$ 之间的**误差平方和 (Sum of Squared Errors, SSE)** 最小。最小二乘法 (Ordinary Least Squares, OLS) 是一种常用的求解线性回归模型参数的方法。其目标函数为：

$$\min_{\mathbf{\beta}} L(\mathbf{\beta}) = \min_{\mathbf{\beta}} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) = \min_{\mathbf{\beta}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^T \mathbf{\beta})^2$$

为了求解最优的 $\mathbf{\beta}$，我们可以对目标函数 $L(\mathbf{\beta})$ 关于 $\mathbf{\beta}$ 求导，并令导数等于 0，得到**正规方程 (Normal Equation)**：

$$\mathbf{X}^T\mathbf{X}\mathbf{\beta} = \mathbf{X}^T\mathbf{y}$$

如果矩阵 $\mathbf{X}^T\mathbf{X}$ 可逆（即满秩），则可以得到**普通最小二乘 (OLS) 估计量**的解析解：

$$\hat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{y}$$

**高维环境下的过拟合问题与正则化:**

在高维环境中，当特征数量 $K$ 相对于观测数量 $N$ 来说较大时（例如 $K > N$，或 $K$ 接近 $N$），OLS 估计可能会出现**过拟合 (Overfitting)** 问题。过拟合是指模型在训练集上表现得非常好（例如，训练误差很小），但在**未见过的测试集**上泛化能力很差，预测性能下降。这是因为在高维情况下，模型参数过多，容易捕捉到训练数据中的噪声和随机波动，而不是真实的 underlying pattern。

为了解决过拟合问题，提高模型的泛化能力，可以引入**正则化 (Regularization)** 方法。正则化通过在损失函数中添加**惩罚项**，限制模型复杂度，从而避免模型过度拟合训练数据。常用的正则化方法包括 **岭回归 (Ridge Regression)** 和 **Lasso 回归 (Lasso Regression)**。

### 岭回归 (Ridge Regression)

岭回归是一种**改进的线性回归方法**，也称为 **$L_2$ 正则化线性回归**。它通过在最小二乘法的损失函数中添加 **$L_2$ 范数惩罚项**来对回归系数进行 **shrinkage (收缩)**，限制回归系数的大小，从而降低模型的复杂度和过拟合风险。岭回归特别适用于处理**多重共线性**问题，即特征之间存在高度相关性的情况。

**模型表达式:**

岭回归的目标函数为：

$$\min_{\mathbf{\beta}} L_{Ridge}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \lambda \mathbf{\beta}^T\mathbf{\beta} \right] $$

其中：
- $\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})$ 是**均方误差 (Mean Squared Error, MSE)** 项，衡量模型预测值与真实值之间的平均误差平方。
- $\lambda \mathbf{\beta}^T\mathbf{\beta} = \lambda ||\mathbf{\beta}||_2^2 = \lambda \sum_{j=0}^{K} \beta_j^2$ 是 **$L_2$ 范数惩罚项**，也称为 **权重衰减项 (Weight Decay)**。它惩罚回归系数 $\mathbf{\beta}$ 的平方和，迫使系数趋向于较小的值。
- $\lambda \ge 0$ 是**正则化参数 (Regularization Parameter)**，也称为 **惩罚系数**。它控制惩罚项的强度。$\lambda$ 越大，惩罚越强，回归系数越趋向于 0。当 $\lambda = 0$ 时，岭回归退化为普通线性回归。

**估计方法:**

类似于线性回归，我们可以对岭回归的目标函数 $L_{Ridge}(\mathbf{\beta})$ 关于 $\mathbf{\beta}$ 求导，并令导数等于 0，得到岭回归的估计结果：

$$\hat{\mathbf{\beta}}_{Ridge} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}_{K+1})^{-1} \mathbf{X}^T\mathbf{y}$$

其中 $\mathbf{I}_{K+1}$ 是 $(K+1) \times (K+1)$ 单位矩阵。通过向 $\mathbf{X}^T\mathbf{X}$ 添加对角矩阵 $\lambda \mathbf{I}_{K+1}$（即"岭"），可以使得在求逆运算时，即使 $\mathbf{X}^T\mathbf{X}$ 接近奇异矩阵（例如，当存在多重共线性时），$(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}_{K+1})$ 仍然具有较好的可逆性，保证了解的稳定性。并且 $\lambda \mathbf{I}_{K+1}$ 的存在将导致回归系数 $\hat{\mathbf{\beta}}_{Ridge}$ 向零收缩。

**岭回归的特点:**

- **$L_2$ 正则化**：使用 $L_2$ 范数惩罚项，将回归系数向零收缩，但不会精确地变为 0。
- **缓解多重共线性**：通过引入正则化项，降低了模型对特征之间相关性的敏感度，可以缓解多重共线性问题，提高模型稳定性。
- **降低过拟合风险**：通过限制模型复杂度，有效降低过拟合风险，提高模型的泛化能力。
- **无法进行特征选择**：岭回归会缩小所有特征的系数，但不会将任何系数精确地设置为 0，因此无法进行特征选择。

### Lasso 回归 (Lasso Regression)

Lasso (Least Absolute Shrinkage and Selection Operator) 回归是另一种常用的正则化线性回归方法，也称为 **$L_1$ 正则化线性回归**。与岭回归不同，Lasso 回归使用 **$L_1$ 范数惩罚项**进行正则化。 $L_1$ 正则化不仅可以进行系数 shrinkage，更重要的是，它具有 **特征选择 (Feature Selection)** 的能力，可以将一些不重要特征的回归系数压缩为 **精确的 0**，从而得到 **稀疏模型 (Sparse Model)**。稀疏模型更易于解释，并且可以提高模型的泛化能力。

**模型表达式:**

Lasso 回归的目标函数为：

$$\min_{\mathbf{\beta}} L_{Lasso}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \gamma \sum_{j=1}^{K} |\beta_j| \right]$$

其中：
- $\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})$ 仍然是均方误差项。
- $\gamma \sum_{j=1}^{K} |\beta_j| = \gamma ||\mathbf{\beta}_{1:K}||_1 = \gamma (|\beta_1| + |\beta_2| + ... + |\beta_K|)$ 是 **$L_1$ 范数惩罚项**，注意这里只惩罚了特征系数 $\beta_1, ..., \beta_K$，**不惩罚截距项 $\beta_0$**。 $L_1$ 范数惩罚项迫使一些回归系数变为 0。
- $\gamma \ge 0$ 是 **正则化参数**，控制 $L_1$ 惩罚项的强度。$\gamma$ 越大，惩罚越强，更多的回归系数会被压缩为 0。

**估计方法:**

与岭回归不同，Lasso 回归的目标函数由于包含 $L_1$ 范数项，**在 $\beta_j = 0$ 处不可导**，因此 **没有解析解**。通常需要使用**数值优化算法**（如**坐标轴下降法 (Coordinate Descent)**、**近端梯度下降法 (Proximal Gradient Descent)**）进行求解。

**Lasso 回归的特点:**

- **$L_1$ 正则化**：使用 $L_1$ 范数惩罚项，不仅可以进行系数 shrinkage，还可以将一些不重要特征的回归系数压缩为精确的 0，实现**特征选择**。
- **稀疏模型**：Lasso 回归可以产生稀疏模型，即模型中只有少数特征的系数非零，这有助于模型解释和提高泛化能力。
- **特征选择能力**：在特征选择方面优于岭回归。Lasso 回归可以自动选择重要的特征，去除冗余和不相关的特征。
- **适用于高维稀疏数据**：Lasso 回归特别适用于处理高维稀疏数据，例如文本数据、基因数据等。

### 弹性网 (Elastic Net)

弹性网 (Elastic Net) 是一种**结合了岭回归和 Lasso 回归的正则化方法**，可以看作是岭回归和 Lasso 回归的折衷。弹性网**同时使用 $L_1$ 范数和 $L_2$ 范数惩罚项**进行正则化。弹性网**同时使用 $L_1$ 正则化和 $L_2$ 正则化**，综合利用 $L_1$ 正则化的特征选择能力和 $L_2$ 正则化的稳定性和 shrinkage 能力。在某些情况下，弹性网的性能优于单独的岭回归和 Lasso 回归，尤其是在特征之间高度相关时，弹性网表现更稳定。

**模型表达式:**

弹性网的目标函数为：

$$\min_{\mathbf{\beta}} L_{ElasticNet}(\mathbf{\beta}) = \min_{\mathbf{\beta}} \left[ \frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \gamma_1 \sum_{j=1}^{K} |\beta_j| + \gamma_2 \mathbf{\beta}^T\mathbf{\beta} \right]$$

其中：
- $\frac{1}{N} (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})$ 是均方误差项。
- $\gamma_1 \sum_{j=1}^{K} |\beta_j|$ 是 **$L_1$ 范数惩罚项**，用于特征选择和产生稀疏模型。
- $\gamma_2 \mathbf{\beta}^T\mathbf{\beta}$ 是 **$L_2$ 范数惩罚项**，用于系数 shrinkage 和缓解多重共线性。
- $\gamma_1 \ge 0$ 和 $\gamma_2 \ge 0$ 分别是 **$L_1$ 正则化参数** 和 **$L_2$ 正则化参数**，控制两种惩罚项的强度。通常需要通过交叉验证等方法来选择合适的 $\gamma_1$ 和 $\gamma_2$ 值。

**弹性网的特点:**

- **结合 $L_1$ 和 $L_2$ 正则化**：弹性网同时使用 $L_1$ 和 $L_2$ 范数惩罚项，结合了两者的优点。
- **既可以进行特征选择，又可以进行系数 shrinkage**：弹性网既可以像 Lasso 回归一样进行特征选择，将一些不重要特征的系数压缩为 0，又可以像岭回归一样进行系数 shrinkage，缩小系数的整体大小，提高模型稳定性。
- **性能更稳定**：在某些情况下，弹性网的预测性能和鲁棒性优于岭回归和 Lasso 回归。
- **处理特征高度相关性**：当特征之间高度相关时，Lasso 回归可能随机选择其中一个特征，而弹性网倾向于选择一组相关的特征，表现更稳定。

### 分类问题描述

分类问题旨在通过由 $K \times 1$ 维向量 $\mathbf{x}$ 表示的 $K$ 个观测到的**预测变量（特征）**来预测**离散类别型**的结果 $y$。分类问题的目标是学习一个模型，将输入样本 $\mathbf{x}$ 划分到预定义的类别中。根据类别数量的不同，分类问题可以分为：

- **二分类 (Binary Classification)**：预测结果 $y$ 只有两个类别，通常表示为 $y \in \{0, 1\}$ (或 $y \in \{-1, +1\}$)。例如，判断邮件是否为垃圾邮件（是/否），预测用户是否会点击广告（点击/不点击），识别交易是否为欺诈交易（欺诈/正常）。
- **多分类 (Multiclass Classification)**：预测结果 $y$ 有两个以上的类别，表示为 $y \in \{C_1, C_2, ..., C_L\}$，其中 $C_i$ 是类别标签， $L \ge 3$ 是类别数量。例如，图像分类（猫、狗、鸟、鱼等），文本分类（政治、经济、体育、娱乐等），客户类型分类（高价值客户、中价值客户、低价值客户）。

对于多分类问题，常用的处理策略是 **"拆解法" (Decomposition)**，即将多分类任务拆解为若干个**二分类任务**求解。常见的拆解策略包括 **一对一 (One-vs-One, OvO)**、**一对多 (One-vs-Rest, OvR)** 和 **多对多 (Many-vs-Many, MvM)** 等。

### 类别不平衡问题

在分类任务中，经常会遇到不同类别的训练样本数量差别很大的情况，即 **类别不平衡 (Class Imbalance)** 问题。例如，在欺诈检测、罕见病诊断、自然灾害预测等领域，**少数类样本 (Minority Class)**（如欺诈交易、患病样本、地震）的数量通常远**远少于多数类样本 (Majority Class)**（如正常交易、健康样本、非地震）。类别不平衡问题会严重影响模型的学习效果，使得模型更倾向于预测样本数量较多的类别，而对少数类别的识别率很低。

**类别不平衡的影响:**

- **模型偏向多数类**：模型在训练过程中更容易学习到多数类样本的特征，而忽略少数类样本的特征，导致模型预测结果偏向多数类。
- **整体分类精度虚高**：由于多数类样本数量占优，即使模型将所有样本都预测为多数类，也可能获得较高的整体分类精度 (Accuracy)。但这种高精度是没有意义的，因为模型对少数类的识别能力很差。
- **评估指标失效**：常用的评估指标（如准确率 Accuracy）在类别不平衡数据集上可能失效，无法真实反映模型的性能。我们需要使用更合适的评估指标，例如 **精确率 (Precision)**、**召回率 (Recall)**、**F1 值 (F1-score)**、**AUC 值 (Area Under ROC Curve)** 等。

**类别不平衡的解决方案:**

为了解决类别不平衡问题，提高模型对少数类别的识别能力，常用的解决方案包括：

- **再缩放 (Rescaling) / 阈值调整 (Threshold Adjustment)**：不改变原始模型，而是**调整分类阈值 (Classification Threshold)**，使得模型在类别不平衡时也能做出合理的预测。例如，对于逻辑回归或 SVM 等输出概率的模型，默认的分类阈值通常为 0.5。当类别不平衡时，可以将预测为正例的阈值从 0.5 **调整为更小的值**，例如 $\frac{m^{+}}{m^{-} + m^{+}}$, 其中 $m^{+}$ 和 $m^{-}$ 分别是正类（少数类）和负类（多数类）样本的数量。降低阈值会使得模型更容易将样本预测为正类，从而提高少数类的召回率。
- **重采样 (Resampling)**：通过**改变训练集中不同类别样本的比例**来缓解类别不平衡问题。重采样方法包括 **欠抽样 (Undersampling)** 和 **过抽样 (Oversampling)**。
    - **欠抽样 (Undersampling)**：**减少多数类样本的数量**，随机删除一部分多数类样本，使得正负类样本数量接近平衡。欠抽样方法简单易行，但可能会**丢失一部分多数类样本的信息**，适用于数据量较大的情况。
    - **过抽样 (Oversampling)**：**增加少数类样本的数量**，例如通过**复制少数类样本**或**生成合成样本**（如 **SMOTE (Synthetic Minority Over-sampling Technique)**）。过抽样方法可以保留所有原始多数类样本的信息，但可能会**导致过拟合**，适用于数据量较小的情况。SMOTE 算法通过在少数类样本之间进行插值生成新的合成样本，可以有效缓解过拟合问题。
- **阈值移动 (Threshold-moving)**：这是一种**代价敏感学习 (Cost-sensitive learning)** 的思想。基于原始训练集进行学习，但在用训练好的分类器进行预测时，**根据类别不平衡的程度调整决策阈值**。例如，如果少数类样本的误分类代价更高，则可以将决策阈值向多数类方向移动，使得模型更倾向于将样本预测为少数类。
- **代价敏感学习 (Cost-sensitive learning)**：为不同类别的**误分类设置不同的代价 (Cost)**，使得模型在训练时更加关注少数类样本，**最小化总的期望代价**而不是最小化分类错误率。例如，可以使用**代价矩阵 (Cost Matrix)** 来定义不同误分类情况的代价，然后在训练过程中根据代价矩阵调整模型的学习策略。
- **集成学习方法**：一些集成学习方法，如 **集成学习 (Ensemble Learning)** 方法，例如 **EasyEnsemble**、**BalanceCascade** 等，通过将数据集划分为多个子集，在每个子集上训练基学习器，然后集成多个基学习器的预测结果，可以有效提高模型在类别不平衡数据集上的性能。

### 逻辑回归 (Logistic Regression)

逻辑回归 (Logistic Regression) 是一种广泛使用的**二分类模型**。虽然名字带有"回归"，但逻辑回归实际上是一种**分类算法**，主要用于解决二分类问题。逻辑回归模型简单高效，易于解释，是许多分类问题的 baseline 模型。

**模型表达式:**

逻辑回归模型基于**线性回归**的思想，但通过引入 **Sigmoid 函数 (Sigmoid Function)** 或 **Logistic 函数**，将线性回归的输出值**映射到 $(0, 1)$ 区间**，使其具有概率意义，用于表示样本属于正类的概率。

逻辑回归模型的表达式如下：

$$P(y=1|\mathbf{x}; \mathbf{\beta}) = \sigma(\mathbf{x}^T \mathbf{\beta}) = \frac{1}{1 + e^{-\mathbf{x}^T \mathbf{\beta}}}$$

其中：
- $P(y=1|\mathbf{x}; \mathbf{\beta})$ 表示给定特征向量 $\mathbf{x}$ 和模型参数 $\mathbf{\beta}$ 的条件下，样本属于正类 (y=1) 的**概率**。
- $\mathbf{x} = (1, x_1, x_2, ..., x_K)^T$ 是增广特征向量。
- $\mathbf{\beta} = (\beta_0, \beta_1, ..., \beta_K)^T$ 是模型参数，与线性回归中的回归系数类似。
- $\sigma(z) = \frac{1}{1 + e^{-z}}$ 是 **Sigmoid 函数**，也称为 **Logistic 函数**。Sigmoid 函数将任意实数 $z$ 映射到 $(0, 1)$ 区间，函数图像呈 S 形。当 $z \rightarrow +\infty$ 时，$\sigma(z) \rightarrow 1$；当 $z \rightarrow -\infty$ 时，$\sigma(z) \rightarrow 0$；当 $z = 0$ 时，$\sigma(z) = 0.5$。

对于二分类问题，逻辑回归模型预测样本属于正类的概率 $P(y=1|\mathbf{x}; \mathbf{\beta})$，则样本属于负类的概率为 $P(y=0|\mathbf{x}; \mathbf{\beta}) = 1 - P(y=1|\mathbf{x}; \mathbf{\beta}) = 1 - \sigma(\mathbf{x}^T \mathbf{\beta}) = \sigma(-\mathbf{x}^T \mathbf{\beta}) = \frac{e^{-\mathbf{x}^T \mathbf{\beta}}}{1 + e^{-\mathbf{x}^T \mathbf{\beta}}} = \frac{1}{1 + e^{\mathbf{x}^T \mathbf{\beta}}}$。

**模型训练：最大似然估计 (Maximum Likelihood Estimation, MLE)**

逻辑回归模型的训练目标是**最大化训练数据的似然函数 (Likelihood Function)**，即找到一组模型参数 $\mathbf{\beta}$，使得在给定这组参数下，训练数据出现的概率最大。对于二分类问题，逻辑回归的似然函数可以表示为：

$$L(\mathbf{\beta}) = \prod_{i=1}^{N} [P(y_i=1|\mathbf{x}_i; \mathbf{\beta})]^{y_i} [P(y_i=0|\mathbf{x}_i; \mathbf{\beta})]^{1-y_i} = \prod_{i=1}^{N} [\sigma(\mathbf{x}_i^T \mathbf{\beta})]^{y_i} [\sigma(-\mathbf{x}_i^T \mathbf{\beta})]^{1-y_i}$$

为了方便优化，通常将似然函数取对数，得到**对数似然函数 (Log-Likelihood Function)**：

$$\ell(\mathbf{\beta}) = \ln L(\mathbf{\beta}) = \sum_{i=1}^{N} [y_i \ln \sigma(\mathbf{x}_i^T \mathbf{\beta}) + (1-y_i) \ln \sigma(-\mathbf{x}_i^T \mathbf{\beta})] = \sum_{i=1}^{N} [y_i \ln \frac{1}{1 + e^{-\mathbf{x}_i^T \mathbf{\beta}}} + (1-y_i) \ln \frac{e^{-\mathbf{x}_i^T \mathbf{\beta}}}{1 + e^{-\mathbf{x}_i^T \mathbf{\beta}}}]$$

我们的目标是**最大化对数似然函数 $\ell(\mathbf{\beta})$**，等价于**最小化负对数似然函数 (Negative Log-Likelihood Function)**：

$$J(\mathbf{\beta}) = -\ell(\mathbf{\beta}) = - \sum_{i=1}^{N} [y_i \ln \sigma(\mathbf{x}_i^T \mathbf{\beta}) + (1-y_i) \ln \sigma(-\mathbf{x}_i^T \mathbf{\beta})]$$

负对数似然函数 $J(\mathbf{\beta})$ 也称为 **交叉熵损失函数 (Cross-Entropy Loss Function)** 或 **Logistic Loss Function**。

**最优化方法：梯度下降法 (Gradient Descent)**

逻辑回归模型通常使用**梯度下降法 (Gradient Descent)** 或其变种（如 **随机梯度下降 (SGD)**、**小批量梯度下降 (Mini-batch GD)**、**Adam** 等）来求解最优参数 $\mathbf{\beta}$，最小化交叉熵损失函数 $J(\mathbf{\beta})$。梯度下降法是一种迭代优化算法，通过不断沿着损失函数梯度 **负方向** 更新参数，逐步逼近最优解。

**决策边界 (Decision Boundary):**

逻辑回归模型的决策边界是线性的。当 $\mathbf{x}^T \mathbf{\beta} = 0$ 时，$\sigma(\mathbf{x}^T \mathbf{\beta}) = 0.5$，模型预测样本属于正类和负类的概率均为 0.5。因此，**线性方程 $\mathbf{x}^T \mathbf{\beta} = 0$ 定义了逻辑回归模型的决策边界**，将特征空间划分为正类区域和负类区域。

### 支持向量机 (Support Vector Machine, SVM)

支持向量机 (Support Vector Machine, SVM) 是一种强大且广泛应用于**分类和回归问题**的监督学习模型。SVM 的核心思想是**找到一个最优超平面 (Optimal Hyperplane)**，将不同类别的样本**最大程度地分开**，同时使得**分类间隔 (Margin)** 最大化。SVM 在**高维空间**和**非线性分类**问题中表现出色，通过**核技巧 (Kernel Trick)** 可以有效地处理非线性可分数据。

**线性可分支持向量机 (Linearly Separable SVM) / 硬间隔 SVM (Hard Margin SVM):**

对于**线性可分 (Linearly Separable)** 的数据集，即存在一个超平面可以将不同类别的样本完全分开的情况，我们可以构建**线性可分支持向量机**，也称为 **硬间隔 SVM**。硬间隔 SVM 旨在找到一个**最大间隔超平面**，将两类样本完全正确地分开，并且使得**间隔最大化**。间隔是指超平面到**最近的样本点**（称为 **支持向量 (Support Vector)**）的距离。

**模型表达式:**

给定线性可分的训练数据集 $D = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$，其中 $y_i \in \{-1, +1\}$。线性可分 SVM 的目标是找到一个超平面 $(\mathbf{w}, b)$，使得：

- **正确分类**:  所有样本都被正确分类，即对于 $y_i = +1$ 的样本，有 $\mathbf{w}^T \mathbf{x}_i + b \ge +1$；对于 $y_i = -1$ 的样本，有 $\mathbf{w}^T \mathbf{x}_i + b \le -1$。可以将两个不等式统一为： $y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, ..., N$。
- **间隔最大化**:  最大化分类间隔 $Margin = \frac{2}{||\mathbf{w}||}$，等价于最小化 $||\mathbf{w}||^2 = \mathbf{w}^T \mathbf{w}$。

因此，线性可分 SVM 的**最优化问题**可以表示为：

$$\min_{\mathbf{w}, b} \frac{1}{2} ||\mathbf{w}||^2 \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, ..., N$$

这是一个**凸二次规划 (Convex Quadratic Programming, QP)** 问题，可以使用现成的 QP 求解器求解。

**线性不可分支持向量机 (Linearly Inseparable SVM) / 软间隔 SVM (Soft Margin SVM):**

在实际应用中，很多数据集**不是线性可分**的，即不存在一个超平面可以将不同类别的样本完全分开。为了处理线性不可分数据，我们需要引入**软间隔 SVM**，也称为 **线性支持向量机**。软间隔 SVM 允许模型在一些样本上**分类错误**，但希望**尽可能减少分类错误**，同时**保持间隔最大化**。

**模型表达式:**

软间隔 SVM 通过引入**松弛变量 (Slack Variables)** $\xi_i \ge 0$，允许一些样本不满足硬间隔约束 $y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1$。松弛变量 $\xi_i$ 表示第 $i$ 个样本**违反约束的程度**。软间隔 SVM 的最优化问题变为：

$$\min_{\mathbf{w}, b, \xi} \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^{N} \xi_i \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0, \quad i = 1, 2, ..., N$$

其中：
- $\frac{1}{2} ||\mathbf{w}||^2$ 仍然是间隔最大化项。
- $C \sum_{i=1}^{N} \xi_i$ 是**惩罚项**，表示对**误分类的惩罚**。 $\xi_i$ 越大，误分类程度越高，惩罚越大。
- $C > 0$ 是 **惩罚参数 (Penalty Parameter)**，也称为 **正则化参数**。 $C$ 控制对误分类的惩罚程度。 $C$ 越大，对误分类的惩罚越大，模型越倾向于减小误分类，但可能会导致间隔变小，容易过拟合； $C$ 越小，对误分类的惩罚越小，模型更容忍误分类，间隔可能更大，泛化能力可能更好。 $C$ 的选择需要通过交叉验证等方法进行调优。

**核函数 (Kernel Function):**

对于**非线性可分 (Nonlinearly Separable)** 的数据集，SVM 可以通过 **核函数 (Kernel Function)** 将数据**映射到高维空间 (High-Dimensional Space)**，使得在高维空间中数据变得**线性可分**，然后在高维空间中寻找最优超平面。**核技巧 (Kernel Trick)** 的强大之处在于，我们**不需要显式地计算高维空间的特征向量**，只需要定义一个**核函数 $K(\mathbf{x}_i, \mathbf{x}_j)$**，它可以计算**原始空间中两个向量 $\mathbf{x}_i$ 和 $\mathbf{x}_j$ 映射到高维空间后的内积**。常用的核函数包括：

- **线性核 (Linear Kernel)**: $K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j$。线性核实际上没有进行特征映射，适用于线性可分数据。
- **多项式核 (Polynomial Kernel)**: $K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^T \mathbf{x}_j + r)^d$。多项式核可以将数据映射到多项式特征空间，适用于多项式关系的数据。其中 $\gamma > 0, r \ge 0, d \ge 1$ 是核参数。
- **高斯核 / RBF 核 (Gaussian Kernel / Radial Basis Function Kernel)**: $K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2)$。高斯核是最常用的核函数之一，可以将数据映射到**无限维空间**，适用于各种类型的数据，尤其是**局部性模式**的数据。其中 $\gamma > 0$ 是核参数，控制核函数的宽度。
- **Sigmoid 核 (Sigmoid Kernel)**: $K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i^T \mathbf{x}_j + r)$。Sigmoid 核类似于神经网络中的 Sigmoid 激活函数，SVM 使用 Sigmoid 核时，其行为类似于**多层感知机神经网络**。其中 $\gamma > 0, r < 0$ 是核参数。

**最优化方法：对偶问题与 SMO 算法**

SVM 的优化问题（无论是硬间隔还是软间隔）通常转化为 **对偶问题 (Dual Problem)** 进行求解。求解对偶问题的好处包括：
1. **更容易求解**：对偶问题通常比原始问题更容易求解。
2. **引入核函数**：在对偶问题中，目标函数和约束条件只涉及到样本之间的内积，可以方便地引入核函数，将线性 SVM 扩展到非线性 SVM。

求解 SVM 对偶问题的高效算法是 **SMO (Sequential Minimal Optimization) 算法**。SMO 算法是一种**启发式算法**，它将**大规模 QP 问题分解为一系列小规模 QP 子问题**，通过**迭代地优化两个变量**，高效地求解 SVM 模型。

### 决策树 (Decision Tree)

决策树 (Decision Tree) 是一种**树形结构**的**分类或回归模型**。决策树模型直观易懂，易于解释，并且可以处理**类别型和数值型特征**，无需进行特征缩放。决策树模型的核心思想是**基于特征对数据集进行递归划分**，构建一棵树状的决策规则，用于对新样本进行分类或预测。

决策树由**节点 (Node)** 和 **有向边 (Directed Edge)** 组成。节点分为两种类型：
- **内部节点 (Internal Node)**：表示一个**特征或属性**的测试条件，用于决定样本的划分方向。
- **叶节点 (Leaf Node / Terminal Node)**：表示**最终的决策结果**，即类别标签（分类树）或预测值（回归树）。

有向边代表**划分规则**，从父节点指向子节点。从根节点到每个叶节点的路径都对应着一条**决策规则**。

决策树的学习过程主要包括三个步骤：**特征选择**、**树的生成** 和 **树的剪枝**。

**回归树 (Regression Tree):**

回归树 (Regression Tree) 用于**预测连续数值型目标变量**。例如，预测房价、股票价格等。

**模型构建:**

回归树的构建过程是一个**递归的二叉树构建过程**，也称为 **CART (Classification and Regression Tree) 树**。CART 树是一种二叉树，内部节点根据特征取值将数据集划分为两个子集，叶节点输出预测值。回归树的构建过程如下：

1. **选择划分特征和划分点**：从所有特征和所有可能的划分点中，选择一个**最优的特征 $j$ 和切分点 $s$**，将当前节点的数据集划分为两个区域 $R_1(j,s) = \{\mathbf{x}|\mathbf{x}_j \le s\}$ 和 $R_2(j,s) = \{\mathbf{x}|\mathbf{x}_j > s\}$。
2. **最小化平方误差**：选择最优划分属性 $j$ 和划分点 $s$ 的目标是**最小化划分后的平方误差 (Squared Error)**，即使得划分后的两个子区域内样本的**目标变量值尽可能接近**。对于给定的特征 $j$ 和切分点 $s$，遍历所有可能的 $(j, s)$ 对，计算划分后的平方误差，选择使得平方误差最小的 $(j, s)$ 对作为最优划分。平方误差的计算公式为：

$$\min_{j,s} \left[ \min_{c_1} \sum_{\mathbf{x}_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{\mathbf{x}_i \in R_2(j,s)} (y_i - c_2)^2\right]$$

其中 $c_1$ 和 $c_2$ 分别是区域 $R_1(j,s)$ 和 $R_2(j,s)$ 的**预测值**。对于给定的区域 $R_m(j,s)$，最优的预测值 $\hat{c}_m$ 是该区域内样本**目标变量的均值**：

$$\hat{c}_m = \text{ave}(y_i|\mathbf{x}_i \in R_m(j,s)) = \frac{1}{|R_m(j,s)|} \sum_{\mathbf{x}_i \in R_m(j,s)} y_i$$

3. **递归划分**：对划分后的两个子区域 $R_1(j,s)$ 和 $R_2(j,s)$，**递归地重复步骤 1 和 2**，继续选择最优特征和切分点进行划分，直到满足**停止条件**。停止条件通常包括：
    - 节点内样本数量小于某个**预设阈值**。
    - 节点内样本的目标变量**方差或平方误差小于某个阈值**。
    - 没有更多特征可用于划分，或所有特征都已用完。

4. **生成叶节点**：当满足停止条件时，将当前节点作为**叶节点**，并**计算叶节点的预测值**，通常为叶节点内样本目标变量的均值。

**分类树 (Classification Tree):**

分类树 (Classification Tree) 用于**预测离散类别型目标变量**。例如，判断用户是否会流失、识别图像中的物体类别等。

**模型构建:**

分类树的构建过程与回归树类似，也是一个递归的二叉树构建过程。不同之处在于，分类树在**选择最优特征和切分点**时，使用的**划分指标不同**，以及**叶节点的预测值类型不同**。分类树常用的划分指标包括 **信息增益 (Information Gain)**、**信息增益率 (Information Gain Ratio)** 和 **基尼指数 (Gini Index)**。目标是使得划分后的子节点数据尽可能 **"纯净" (Pure)**，即属于**同一类别的样本比例尽可能高**。

**划分指标:**

- **信息增益 (Information Gain)**：基于**信息熵 (Entropy)** 的划分指标。信息熵衡量了数据集的**混乱程度**或**不确定性**。信息增益表示**使用特征 $A$ 对数据集 $D$ 进行划分后，数据集 $D$ 的信息熵减少的程度**。信息增益越大，说明使用特征 $A$ 划分数据集的效果越好。常用的基于信息增益的决策树算法是 **ID3 算法**。
- **信息增益率 (Information Gain Ratio)**：为了**克服信息增益对取值数目较多的特征的偏好**，C4.5 算法引入了信息增益率。信息增益率在信息增益的基础上，**除以特征 $A$ 本身的熵**，对特征取值数目较多的情况进行**惩罚**。常用的基于信息增益率的决策树算法是 **C4.5 算法**。
- **基尼指数 (Gini Index)**：基尼指数衡量了数据集的**纯度**。基尼指数越小，数据集纯度越高。CART 算法使用基尼指数作为分类树的划分指标。

**叶节点预测值:**

分类树的叶节点**输出类别标签**，通常是叶节点内**样本数量最多的类别**（多数表决）。

**决策树的特点:**

- **优点**：
    - **易于理解和解释**：决策树模型直观易懂，决策规则清晰可见，易于向业务人员解释。
    - **可以处理类别型和数值型特征**：无需对特征进行预处理，如独热编码、标准化等。
    - **无需特征缩放**：决策树模型对特征的尺度不敏感，无需进行特征缩放。
    - **可以处理缺失值**：决策树模型可以处理包含缺失值的数据。
    - **可以进行特征选择**：决策树模型在构建过程中会自动选择重要的特征进行划分。
- **缺点**：
    - **容易过拟合**：决策树模型容易在训练集上过拟合，导致泛化能力差。可以通过**剪枝 (Pruning)** 等方法缓解过拟合问题。
    - **不稳定**：决策树模型对训练数据敏感，训练数据的微小变化可能导致树结构发生很大变化。
    - **忽略特征之间的相关性**：决策树模型在选择划分特征时，每次只考虑一个特征，忽略了特征之间的相关性。

## 总结

本讲义主要介绍了监督学习的基本概念和常用模型，包括：

* **监督学习概述**: 介绍了监督学习的定义、应用场景以及与量化投资的结合。
* **回归模型**: 详细讲解了线性回归和岭回归模型，包括模型表达式、最小二乘法、正则化以及模型特点。
* **分类模型**: 深入探讨了支持向量机 (SVM) 和决策树模型，包括模型原理、核函数、优化方法以及模型优缺点。

