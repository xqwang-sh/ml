{
  "hash": "980672efef2059d231d0d321b90d71f3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"泰坦尼克号生存预测实践\"\n---\n\n\n\n\n# 导言\n\n## 泰坦尼克号乘客数据\n\n泰坦尼克号是历史上著名的客轮，1912年首航时与冰山相撞沉没，造成了1500多人遇难。本实践课我们将使用泰坦尼克号乘客数据，尝试预测乘客在这场灾难中的生存情况。\n\n数据分为两个数据集：\n- 训练集 `train.csv`：有关键变量`Survived`显示是否生还\n- 测试集 `test.csv`：没有`Survived`变量\n\n变量定义如下表所示：\n\n| 变量 | 定义 | 取值说明 |\n|------|------|----------|\n| Survived | 生存状态 | 0 = 未生存, 1 = 生存 |\n| Pclass | 船票等级 | 1 = 一等舱, 2 = 二等舱, 3 = 三等舱 |\n| Sex | 性别 | |\n| Age | 年龄 | |\n| SibSp | 船上兄弟姐妹/配偶数量 | |\n| Parch | 船上父母/子女数量 | |\n| Ticket | 船票号码 | |\n| Fare | 票价 | |\n| Cabin | 客舱号码 | |\n| Embarked | 登船港口 | C = 瑟堡, Q = 皇后镇, S = 南安普顿 |\n\n## 机器学习的一般步骤\n\n### 探索性分析\n- 对变量进行初步描述性统计分析，用以检测空值、不合法值、异常值等\n- 数据可视化展示，发现变量之间的关系\n\n### 数据清理与特征工程\n- 特征工程：从现有特征中提取更有价值的信息\n- 填补缺失值：对缺失数据进行合理的估计和填补\n\n### 数据建模与模型选择\n- 建立分类模型（sklearn包）\n  - 在训练集上拟合模型\n  - 根据交叉验证的模型评估指标选择超参数\n\n- 备选模型包括：\n  - 逻辑回归模型\n  - 决策树模型\n  - 随机森林\n  - 提升树（XGBoost）\n  - 神经网络\n\n- 模型评估指标包括：\n  - 精度accuracy\n  - 查全率recall与F1\n\n# 探索性数据分析\n\n\n首先，我们需要读取数据，并对数据进行初步的探索分析。\n\n::: {#load-data .cell execution_count=2}\n``` {.python .cell-code}\n# 读取训练集和测试集\ntrain_data = pd.read_csv(\"data/train.csv\")\ntest_data = pd.read_csv(\"data/test.csv\")\n\n# 为数据集添加标记\ntrain_data['train'] = 1\ntest_data['train'] = 0\ntest_data['Survived'] = np.nan\n\n# 合并数据集\nall_data = pd.concat([train_data, test_data], axis=0)\n\nprint(f\"训练集样本数: {train_data.shape[0]}\")\nprint(f\"测试集样本数: {test_data.shape[0]}\")\nprint(f\"总样本数: {all_data.shape[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n训练集样本数: 891\n测试集样本数: 418\n总样本数: 1309\n```\n:::\n:::\n\n\n让我们先看看训练集的前几行数据，了解数据的基本结构：\n\n::: {#cell-head-data .cell execution_count=3}\n``` {.python .cell-code}\n# 查看训练集前5行\ntrain_data.head()\n```\n\n::: {#head-data .cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## 数据基本信息与统计描述\n\n我们需要了解数据的基本信息，包括各变量的数据类型和缺失情况：\n\n::: {#info-data .cell execution_count=4}\n``` {.python .cell-code}\n# 查看数据基本信息\ntrain_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \n 12  train        891 non-null    int64  \ndtypes: float64(2), int64(6), object(5)\nmemory usage: 90.6+ KB\n```\n:::\n:::\n\n\n对数值型变量进行统计描述，了解其分布特征：\n\n::: {#cell-describe-data .cell execution_count=5}\n``` {.python .cell-code}\n# 统计描述\ntrain_data.describe()\n```\n\n::: {#describe-data .cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n检查各变量的缺失值情况：\n\n::: {#cell-missing-values .cell execution_count=6}\n``` {.python .cell-code}\n# 检查缺失值\nprint(\"缺失值统计:\")\nall_data.isnull().sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n缺失值统计:\n```\n:::\n\n::: {#missing-values .cell-output .cell-output-display execution_count=6}\n```\nPassengerId       0\nSurvived        418\nPclass            0\nName              0\nSex               0\nAge             263\nSibSp             0\nParch             0\nTicket            0\nFare              1\nCabin          1014\nEmbarked          2\ntrain             0\ndtype: int64\n```\n:::\n:::\n\n\n从上面的分析可以看出：\n- Age（年龄）有一部分缺失\n- Cabin（船舱）缺失严重\n- Embarked（登船港口）有少量缺失\n\n## 生存情况分析\n\n首先我们看一下整体的生存率：\n\n::: {#survival-rate .cell execution_count=7}\n``` {.python .cell-code}\n# 分析生存率\nsurvival_rate = train_data['Survived'].mean() * 100\nprint(f\"总体生存率: {survival_rate:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n总体生存率: 38.38%\n```\n:::\n:::\n\n\n## 数据可视化分析\n\n通过可视化，我们可以更直观地了解不同特征与生存率之间的关系：\n\n::: {#cell-visualization .cell fig-height='10' fig-width='12' execution_count=8}\n``` {.python .cell-code}\n# 设置图形大小\nplt.figure(figsize=(15, 12))\n\n# 1. 性别与生存率\nplt.subplot(2, 2, 1)\nsns.countplot(x='Sex', hue='Survived', data=train_data)\nplt.title('性别与生存状况')\nplt.xlabel('性别')\nplt.ylabel('人数')\n\n# 2. 船票等级与生存率\nplt.subplot(2, 2, 2)\nsns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.title('船票等级与生存状况')\nplt.xlabel('船票等级')\nplt.ylabel('人数')\n\n# 3. 年龄分布与生存情况\nplt.subplot(2, 2, 3)\nsns.histplot(data=train_data, x='Age', hue='Survived', multiple='stack', bins=20)\nplt.title('年龄分布与生存状况')\nplt.xlabel('年龄')\nplt.ylabel('人数')\n\n# 4. 家庭规模与生存情况\nplt.subplot(2, 2, 4)\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\nsns.countplot(x='FamilySize', hue='Survived', data=train_data)\nplt.title('家庭规模与生存状况')\nplt.xlabel('家庭规模')\nplt.ylabel('人数')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/visualization-output-1.png){#visualization width=1431 height=1144}\n:::\n:::\n\n\n从以上可视化分析可以看出：\n\n1. **性别与生存率**：女性的生存率明显高于男性，这可能与\"妇女和儿童优先\"的救生原则有关。\n2. **船票等级与生存率**：一等舱乘客生存率最高，三等舱乘客生存率最低，表明社会经济地位可能影响了获救机会。\n3. **年龄分布与生存率**：儿童的生存率相对较高，而中年人的生存率较低。\n4. **家庭规模与生存率**：家庭规模中等(2-4人)的乘客生存率较高，而独自一人或家庭规模过大的乘客生存率较低。\n\n# 数据清理与特征工程\n\n在这一部分，我们将对数据进行清理和特征工程，为建模做准备。\n\n## 特征工程\n\n### 提取姓名中的头衔信息\n\n乘客的姓名中包含头衔信息（如Mr., Mrs., Miss等），这可能与社会地位和性别相关，进而影响生存率：\n\n::: {#cell-extract-title .cell execution_count=9}\n``` {.python .cell-code}\n# 提取姓名中的头衔\nall_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# 统计头衔分布\nprint(\"头衔分布:\")\nall_data['Title'].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n头衔分布:\n```\n:::\n\n::: {#extract-title .cell-output .cell-output-display execution_count=9}\n```\nTitle\nMr          757\nMiss        260\nMrs         197\nMaster       61\nRev           8\nDr            8\nCol           4\nMlle          2\nMajor         2\nMs            2\nLady          1\nSir           1\nMme           1\nDon           1\nCapt          1\nCountess      1\nJonkheer      1\nDona          1\nName: count, dtype: int64\n```\n:::\n:::\n\n\n有些头衔出现次数很少，我们将它们合并为\"Rare\"类别：\n\n::: {#cell-combine-title .cell execution_count=10}\n``` {.python .cell-code}\n# 合并稀有头衔\nrare_titles = ['Capt', 'Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir', 'Countess']\nall_data['Title'] = all_data['Title'].replace(rare_titles, 'Rare')\nall_data['Title'] = all_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n\n# 合并后的头衔分布\nprint(\"合并后的头衔分布:\")\nall_data['Title'].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n合并后的头衔分布:\n```\n:::\n\n::: {#combine-title .cell-output .cell-output-display execution_count=10}\n```\nTitle\nMr        757\nMiss      264\nMrs       198\nMaster     61\nRare       29\nName: count, dtype: int64\n```\n:::\n:::\n\n\n### 创建家庭规模特征\n\n我们将SibSp（兄弟姐妹/配偶数量）和Parch（父母/子女数量）相加，再加1（乘客自己），创建家庭规模特征：\n\n::: {#cell-family-size .cell execution_count=11}\n``` {.python .cell-code}\n# 创建家庭规模特征\nall_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n\n# 创建是否独自一人特征\nall_data['IsAlone'] = (all_data['FamilySize'] == 1).astype(int)\n\n# 查看是否独自一人的分布\nprint(\"是否独自一人的分布:\")\nall_data['IsAlone'].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n是否独自一人的分布:\n```\n:::\n\n::: {#family-size .cell-output .cell-output-display execution_count=11}\n```\nIsAlone\n1    790\n0    519\nName: count, dtype: int64\n```\n:::\n:::\n\n\n### 从Cabin提取信息\n\n虽然Cabin变量缺失严重，但我们仍可以提取是否有记录Cabin信息作为一个特征：\n\n::: {#cell-cabin-info .cell execution_count=12}\n``` {.python .cell-code}\n# 从Cabin提取信息：是否有Cabin记录\nall_data['HasCabin'] = (~all_data['Cabin'].isnull()).astype(int)\n\nprint(\"Cabin记录情况分布:\")\nall_data['HasCabin'].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCabin记录情况分布:\n```\n:::\n\n::: {#cabin-info .cell-output .cell-output-display execution_count=12}\n```\nHasCabin\n0    1014\n1     295\nName: count, dtype: int64\n```\n:::\n:::\n\n\n## 缺失值处理\n\n### 处理Age缺失值\n\n对年龄的缺失值，我们使用按Pclass和Sex分组的中位数进行填充：\n\n::: {#age-imputation .cell execution_count=13}\n``` {.python .cell-code}\n# Age缺失值填充（按Pclass和Sex分组的中位数）\nage_imputer = all_data.groupby(['Pclass', 'Sex'])['Age'].transform('median')\nall_data['Age'] = all_data['Age'].fillna(age_imputer)\n```\n:::\n\n\n### 处理Embarked缺失值\n\n对登船港口缺失值，使用众数填充：\n\n::: {#embarked-imputation .cell execution_count=14}\n``` {.python .cell-code}\n# Embarked缺失值用众数填充\nmost_common_embarked = all_data['Embarked'].mode()[0]\nall_data['Embarked'] = all_data['Embarked'].fillna(most_common_embarked)\n```\n:::\n\n\n### 处理Fare缺失值\n\n对票价缺失值，使用相同船票等级的中位数填充：\n\n::: {#cell-fare-imputation .cell execution_count=15}\n``` {.python .cell-code}\n# Fare缺失值用Pclass中位数填充\nfare_imputer = all_data.groupby('Pclass')['Fare'].transform('median')\nall_data['Fare'] = all_data['Fare'].fillna(fare_imputer)\n\n# 检查缺失值是否都已处理\nprint(\"缺失值处理后的统计:\")\nall_data[['Age', 'Embarked', 'Fare']].isnull().sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n缺失值处理后的统计:\n```\n:::\n\n::: {#fare-imputation .cell-output .cell-output-display execution_count=15}\n```\nAge         0\nEmbarked    0\nFare        0\ndtype: int64\n```\n:::\n:::\n\n\n## 数据转换\n\n将分类变量转换为数值型变量，便于模型处理：\n\n::: {#categorical-to-numerical .cell execution_count=16}\n``` {.python .cell-code}\n# 类别特征转换为数值\nall_data['Sex'] = all_data['Sex'].map({'male': 0, 'female': 1})\nall_data['Embarked'] = all_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\nall_data['Title'] = all_data['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\n```\n:::\n\n\n删除不需要的特征：\n\n::: {#cell-drop-features .cell execution_count=17}\n``` {.python .cell-code}\n# 删除不需要的特征\nall_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n\n# 查看处理后的数据结构\nall_data.head()\n```\n\n::: {#drop-features .cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>train</th>\n      <th>Title</th>\n      <th>FamilySize</th>\n      <th>IsAlone</th>\n      <th>HasCabin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# 数据建模与模型选择\n\n## 准备训练和测试数据\n\n::: {#prepare-train-test .cell execution_count=18}\n``` {.python .cell-code}\n# 准备训练和测试数据\ntrain_data = all_data[all_data['train'] == 1].drop('train', axis=1)\ntest_data = all_data[all_data['train'] == 0].drop(['train', 'Survived'], axis=1)\n\nX_train = train_data.drop('Survived', axis=1)\ny_train = train_data['Survived'].astype(int)\n\nprint(f\"训练特征形状: {X_train.shape}\")\nprint(f\"训练标签形状: {y_train.shape}\")\nprint(f\"测试特征形状: {test_data.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n训练特征形状: (891, 11)\n训练标签形状: (891,)\n测试特征形状: (418, 11)\n```\n:::\n:::\n\n\n## 模型训练与评估函数\n\n定义一个通用函数用于训练模型并评估性能：\n\n::: {#train-evaluate-function .cell execution_count=19}\n``` {.python .cell-code}\n# 模型训练与评估函数\ndef train_and_evaluate(model, X, y, cv=5, model_name=\"模型\"):\n    \"\"\"训练模型并评估性能\"\"\"\n    # 交叉验证评估\n    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n    precision = cross_val_score(model, X, y, cv=cv, scoring='precision')\n    recall = cross_val_score(model, X, y, cv=cv, scoring='recall')\n    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')\n    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n    \n    # 输出评估结果\n    print(f\"{model_name}交叉验证结果：\")\n    print(f\"准确率: {accuracy.mean():.4f} (±{accuracy.std():.4f})\")\n    print(f\"精确率: {precision.mean():.4f} (±{precision.std():.4f})\")\n    print(f\"召回率: {recall.mean():.4f} (±{recall.std():.4f})\")\n    print(f\"F1分数: {f1.mean():.4f} (±{f1.std():.4f})\")\n    print(f\"ROC AUC: {roc_auc.mean():.4f} (±{roc_auc.std():.4f})\")\n    \n    # 在全部训练数据上拟合模型\n    model.fit(X, y)\n    return model\n```\n:::\n\n\n## 逻辑回归模型\n\n::: {#cell-logistic-regression .cell execution_count=20}\n``` {.python .cell-code}\n# 逻辑回归模型\nlogreg_model = LogisticRegression(max_iter=1000, random_state=42)\nlogreg_fitted = train_and_evaluate(logreg_model, X_train, y_train, model_name=\"逻辑回归\")\n\n# 查看特征系数\nlogreg_coef = pd.DataFrame(\n    logreg_fitted.coef_[0],\n    index=X_train.columns,\n    columns=['系数']\n).sort_values('系数', ascending=False)\n\nprint(\"\\n逻辑回归系数（特征重要性）:\")\nlogreg_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n逻辑回归交叉验证结果：\n准确率: 0.8159 (±0.0139)\n精确率: 0.7708 (±0.0385)\n召回率: 0.7454 (±0.0315)\nF1分数: 0.7566 (±0.0159)\nROC AUC: 0.8613 (±0.0194)\n\n逻辑回归系数（特征重要性）:\n```\n:::\n\n::: {#logistic-regression .cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>系数</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sex</th>\n      <td>2.149786</td>\n    </tr>\n    <tr>\n      <th>HasCabin</th>\n      <td>0.690087</td>\n    </tr>\n    <tr>\n      <th>Title</th>\n      <td>0.489232</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>0.155761</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.002529</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>-0.025733</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>-0.042430</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>-0.268012</td>\n    </tr>\n    <tr>\n      <th>FamilySize</th>\n      <td>-0.282378</td>\n    </tr>\n    <tr>\n      <th>IsAlone</th>\n      <td>-0.461523</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>-0.827330</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## 决策树模型\n\n::: {#decision-tree .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\n# 决策树模型\n# 超参数网格搜索\nparam_grid = {\n    'max_depth': range(1, 20),\n    'min_samples_split': range(2, 20)\n}\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_grid = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\nprint(f\"决策树最佳参数: {dt_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {dt_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = range(1, 20)\ntest_scores = []\n\nfor depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=depth, \n                              min_samples_split=dt_grid.best_params_['min_samples_split'],\n                              random_state=42)\n    dt.fit(X_train, y_train)\n    # 使用交叉验证来评估\n    score = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('决策树性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\ndt_fitted = train_and_evaluate(dt_grid.best_estimator_, X_train, y_train, model_name=\"决策树\")\n\n# 特征重要性\ndt_importance = pd.DataFrame(\n    dt_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n决策树特征重要性:\")\ndt_importance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n决策树最佳参数: {'max_depth': 4, 'min_samples_split': 17}\n最佳交叉验证分数: 0.8260\n决策树交叉验证结果：\n准确率: 0.8260 (±0.0237)\n精确率: 0.8136 (±0.0331)\n召回率: 0.7159 (±0.1060)\nF1分数: 0.7549 (±0.0577)\nROC AUC: 0.8510 (±0.0399)\n\n决策树特征重要性:\n```\n:::\n\n::: {#decision-tree-1 .cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>重要性</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Title</th>\n      <td>0.661197</td>\n    </tr>\n    <tr>\n      <th>FamilySize</th>\n      <td>0.141799</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.131015</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>0.041912</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.012085</td>\n    </tr>\n    <tr>\n      <th>HasCabin</th>\n      <td>0.011992</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>IsAlone</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/decision-tree-output-3.png){#decision-tree-2 width=812 height=524}\n:::\n:::\n\n\n## 随机森林模型\n\n::: {#random-forest .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\n# 随机森林模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [10, 30, 50, 70, 100, 200],\n    'max_depth': [3, 5, 7, 10, 15, 20, None]\n}\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_grid = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nrf_grid.fit(X_train, y_train)\n\nprint(f\"随机森林最佳参数: {rf_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {rf_grid.best_score_:.4f}\")\n\n# 可视化n_estimators对模型性能的影响\nplt.figure(figsize=(10, 6))\nn_estimators = [10, 30, 50, 70, 100, 200]\ntest_scores = []\n\nfor n in n_estimators:\n    rf = RandomForestClassifier(n_estimators=n,\n                              max_depth=rf_grid.best_params_['max_depth'],\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(n_estimators, test_scores)\nplt.xlabel('n_estimators (树的数量)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随树数量的变化')\nplt.grid(True)\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 10, 15, 20]\ntest_scores = []\n\nfor depth in max_depths:\n    rf = RandomForestClassifier(n_estimators=rf_grid.best_params_['n_estimators'],\n                              max_depth=depth,\n                              random_state=42)\n    # 使用交叉验证来评估\n    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('随机森林性能随最大深度的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nrf_fitted = train_and_evaluate(rf_grid.best_estimator_, X_train, y_train, model_name=\"随机森林\")\n\n# 特征重要性\nrf_importance = pd.DataFrame(\n    rf_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\n随机森林特征重要性:\")\nrf_importance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n随机森林最佳参数: {'max_depth': 5, 'n_estimators': 10}\n最佳交叉验证分数: 0.8294\n随机森林交叉验证结果：\n准确率: 0.8294 (±0.0124)\n精确率: 0.7989 (±0.0240)\n召回率: 0.7453 (±0.0610)\nF1分数: 0.7690 (±0.0281)\nROC AUC: 0.8710 (±0.0378)\n\n随机森林特征重要性:\n```\n:::\n\n::: {#random-forest-1 .cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>重要性</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Title</th>\n      <td>0.367692</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>0.146653</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.097934</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.093234</td>\n    </tr>\n    <tr>\n      <th>HasCabin</th>\n      <td>0.089965</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>0.085205</td>\n    </tr>\n    <tr>\n      <th>FamilySize</th>\n      <td>0.052974</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.032125</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>0.018703</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.011941</td>\n    </tr>\n    <tr>\n      <th>IsAlone</th>\n      <td>0.003575</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/random-forest-output-3.png){#random-forest-2 width=818 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/random-forest-output-4.png){#random-forest-3 width=818 height=524}\n:::\n:::\n\n\n## XGBoost模型\n\n::: {#xgboost .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\n# XGBoost模型\n# 超参数网格搜索\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, 9],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n}\n\nxgb_model = xgb.XGBClassifier(random_state=42)\nxgb_grid = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\n\nprint(f\"XGBoost最佳参数: {xgb_grid.best_params_}\")\nprint(f\"最佳交叉验证分数: {xgb_grid.best_score_:.4f}\")\n\n# 可视化max_depth对模型性能的影响\nplt.figure(figsize=(10, 6))\nmax_depths = [3, 5, 7, 9, 11]\ntest_scores = []\n\nfor depth in max_depths:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=depth,\n        learning_rate=xgb_grid.best_params_['learning_rate'],\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(max_depths, test_scores)\nplt.xlabel('max_depth (最大深度)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随最大深度的变化')\nplt.grid(True)\n\n# 可视化learning_rate对模型性能的影响\nplt.figure(figsize=(10, 6))\nlearning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\ntest_scores = []\n\nfor lr in learning_rates:\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=xgb_grid.best_params_['n_estimators'],\n        max_depth=xgb_grid.best_params_['max_depth'],\n        learning_rate=lr,\n        random_state=42\n    )\n    # 使用交叉验证来评估\n    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n    test_scores.append(score)\n\nplt.plot(learning_rates, test_scores)\nplt.xlabel('learning_rate (学习率)')\nplt.ylabel('准确率')\nplt.title('XGBoost性能随学习率的变化')\nplt.grid(True)\n\n# 使用最佳参数训练模型并评估\nxgb_fitted = train_and_evaluate(xgb_grid.best_estimator_, X_train, y_train, model_name=\"XGBoost\")\n\n# 特征重要性\nxgb_importance = pd.DataFrame(\n    xgb_fitted.feature_importances_,\n    index=X_train.columns,\n    columns=['重要性']\n).sort_values('重要性', ascending=False)\n\nprint(\"\\nXGBoost特征重要性:\")\nxgb_importance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXGBoost最佳参数: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}\n最佳交叉验证分数: 0.8440\nXGBoost交叉验证结果：\n准确率: 0.8440 (±0.0229)\n精确率: 0.8236 (±0.0284)\n召回率: 0.7571 (±0.0667)\nF1分数: 0.7870 (±0.0394)\nROC AUC: 0.8686 (±0.0345)\n\nXGBoost特征重要性:\n```\n:::\n\n::: {#xgboost-1 .cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>重要性</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Title</th>\n      <td>0.545888</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>0.111701</td>\n    </tr>\n    <tr>\n      <th>FamilySize</th>\n      <td>0.095393</td>\n    </tr>\n    <tr>\n      <th>HasCabin</th>\n      <td>0.082403</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>0.035206</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.034001</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.027042</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.025975</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.022535</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>0.019856</td>\n    </tr>\n    <tr>\n      <th>IsAlone</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/xgboost-output-3.png){#xgboost-2 width=818 height=524}\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/xgboost-output-4.png){#xgboost-3 width=812 height=523}\n:::\n:::\n\n\n## 神经网络模型\n\n在本节中，我们将从简单的全连接神经网络开始，逐步增加网络复杂度，并展示模型从欠拟合到过拟合的演变过程，最后通过引入正则化技术来解决过拟合问题。\n\n首先，我们需要对特征进行标准化，这对神经网络的训练非常重要：\n\n::: {#nn-standardization .cell execution_count=24}\n``` {.python .cell-code}\n# 对特征进行标准化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ntest_data_scaled = scaler.transform(test_data)\n\n# 划分训练集和验证集\nX_train_nn, X_val, y_train_nn, y_val = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42\n)\n\nprint(f\"神经网络训练集形状: {X_train_nn.shape}\")\nprint(f\"神经网络验证集形状: {X_val.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n神经网络训练集形状: (712, 11)\n神经网络验证集形状: (179, 11)\n```\n:::\n:::\n\n\n### 简单全连接网络 - 可能欠拟合\n\n我们先从一个非常简单的全连接网络开始，看看其性能如何：\n\n::: {#nn-simple .cell execution_count=25}\n``` {.python .cell-code}\n# 定义简单的全连接网络\ndef create_simple_nn():\n    model = keras.Sequential([\n        layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练简单神经网络\nsimple_nn = create_simple_nn()\nsimple_nn.summary()  # 显示模型结构\n\n# 训练模型\nsimple_history = simple_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nsimple_train_loss, simple_train_acc = simple_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nsimple_val_loss, simple_val_acc = simple_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"简单网络 - 训练集准确率: {simple_train_acc:.4f}\")\nprint(f\"简单网络 - 验证集准确率: {simple_val_acc:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 8)                 96        \n                                                                 \n dense_1 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 105 (420.00 Byte)\nTrainable params: 105 (420.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n简单网络 - 训练集准确率: 0.8287\n简单网络 - 验证集准确率: 0.7989\n```\n:::\n:::\n\n\n绘制简单网络的学习曲线：\n\n::: {#cell-nn-simple-curves .cell fig-height='5' fig-width='12' execution_count=26}\n``` {.python .cell-code}\n# 绘制简单网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(simple_history.history['accuracy'])\nplt.plot(simple_history.history['val_accuracy'])\nplt.title('简单网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(simple_history.history['loss'])\nplt.plot(simple_history.history['val_loss'])\nplt.title('简单网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/nn-simple-curves-output-1.png){#nn-simple-curves width=1142 height=473}\n:::\n:::\n\n\n### 中等复杂度网络 - 适度拟合\n\n现在，让我们增加网络的复杂度，添加更多层和更多神经元：\n\n::: {#nn-medium .cell execution_count=27}\n``` {.python .cell-code}\n# 定义中等复杂度的网络\ndef create_medium_nn():\n    model = keras.Sequential([\n        layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练中等复杂度神经网络\nmedium_nn = create_medium_nn()\nmedium_nn.summary()  # 显示模型结构\n\n# 训练模型\nmedium_history = medium_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\nmedium_train_loss, medium_train_acc = medium_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nmedium_val_loss, medium_val_acc = medium_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"中等网络 - 训练集准确率: {medium_train_acc:.4f}\")\nprint(f\"中等网络 - 验证集准确率: {medium_val_acc:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 16)                192       \n                                                                 \n dense_3 (Dense)             (None, 8)                 136       \n                                                                 \n dense_4 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 337 (1.32 KB)\nTrainable params: 337 (1.32 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n中等网络 - 训练集准确率: 0.8511\n中等网络 - 验证集准确率: 0.8045\n```\n:::\n:::\n\n\n绘制中等复杂度网络的学习曲线：\n\n::: {#cell-nn-medium-curves .cell fig-height='5' fig-width='12' execution_count=28}\n``` {.python .cell-code}\n# 绘制中等网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(medium_history.history['accuracy'])\nplt.plot(medium_history.history['val_accuracy'])\nplt.title('中等网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(medium_history.history['loss'])\nplt.plot(medium_history.history['val_loss'])\nplt.title('中等网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/nn-medium-curves-output-1.png){#nn-medium-curves width=1142 height=472}\n:::\n:::\n\n\n### 复杂网络 - 可能过拟合\n\n现在，我们进一步增加网络复杂度，使其具有更多层和更多神经元，观察是否会出现过拟合：\n\n::: {#nn-complex .cell execution_count=29}\n``` {.python .cell-code}\n# 定义复杂网络\ndef create_complex_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练复杂神经网络\ncomplex_nn = create_complex_nn()\ncomplex_nn.summary()  # 显示模型结构\n\n# 训练模型\ncomplex_history = complex_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,  # 增加训练轮次以观察过拟合\n    batch_size=16,  # 减小批量大小\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# 评估性能\ncomplex_train_loss, complex_train_acc = complex_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\ncomplex_val_loss, complex_val_acc = complex_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"复杂网络 - 训练集准确率: {complex_train_acc:.4f}\")\nprint(f\"复杂网络 - 验证集准确率: {complex_val_acc:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 64)                768       \n                                                                 \n dense_6 (Dense)             (None, 32)                2080      \n                                                                 \n dense_7 (Dense)             (None, 16)                528       \n                                                                 \n dense_8 (Dense)             (None, 8)                 136       \n                                                                 \n dense_9 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 3521 (13.75 KB)\nTrainable params: 3521 (13.75 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n复杂网络 - 训练集准确率: 0.8975\n复杂网络 - 验证集准确率: 0.7989\n```\n:::\n:::\n\n\n绘制复杂网络的学习曲线：\n\n::: {#cell-nn-complex-curves .cell fig-height='5' fig-width='12' execution_count=30}\n``` {.python .cell-code}\n# 绘制复杂网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(complex_history.history['accuracy'])\nplt.plot(complex_history.history['val_accuracy'])\nplt.title('复杂网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(complex_history.history['loss'])\nplt.plot(complex_history.history['val_loss'])\nplt.title('复杂网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/nn-complex-curves-output-1.png){#nn-complex-curves width=1142 height=473}\n:::\n:::\n\n\n### 正则化网络 - 解决过拟合\n\n现在，我们将为复杂网络添加正则化技术，包括 Dropout 和 BatchNormalization，以解决过拟合问题：\n\n::: {#nn-regularized .cell execution_count=31}\n``` {.python .cell-code}\n# 定义带有正则化的复杂网络\ndef create_regularized_nn():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(32, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(16, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(8, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# 创建并训练带有正则化的神经网络\nregularized_nn = create_regularized_nn()\nregularized_nn.summary()  # 显示模型结构\n\n# 添加早停策略\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)\n\n# 训练模型\nregularized_history = regularized_nn.fit(\n    X_train_nn, y_train_nn,\n    epochs=100,\n    batch_size=16,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n# 评估性能\nregularized_train_loss, regularized_train_acc = regularized_nn.evaluate(X_train_nn, y_train_nn, verbose=0)\nregularized_val_loss, regularized_val_acc = regularized_nn.evaluate(X_val, y_val, verbose=0)\n\nprint(f\"正则化网络 - 训练集准确率: {regularized_train_acc:.4f}\")\nprint(f\"正则化网络 - 验证集准确率: {regularized_val_acc:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_10 (Dense)            (None, 64)                768       \n                                                                 \n batch_normalization (Batch  (None, 64)                256       \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_11 (Dense)            (None, 32)                2080      \n                                                                 \n batch_normalization_1 (Bat  (None, 32)                128       \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_12 (Dense)            (None, 16)                528       \n                                                                 \n batch_normalization_2 (Bat  (None, 16)                64        \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_13 (Dense)            (None, 8)                 136       \n                                                                 \n batch_normalization_3 (Bat  (None, 8)                 32        \n chNormalization)                                                \n                                                                 \n dropout_3 (Dropout)         (None, 8)                 0         \n                                                                 \n dense_14 (Dense)            (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 4001 (15.63 KB)\nTrainable params: 3761 (14.69 KB)\nNon-trainable params: 240 (960.00 Byte)\n_________________________________________________________________\n正则化网络 - 训练集准确率: 0.8427\n正则化网络 - 验证集准确率: 0.8101\n```\n:::\n:::\n\n\n绘制正则化网络的学习曲线：\n\n::: {#cell-nn-regularized-curves .cell fig-height='5' fig-width='12' execution_count=32}\n``` {.python .cell-code}\n# 绘制正则化网络的学习曲线\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(regularized_history.history['accuracy'])\nplt.plot(regularized_history.history['val_accuracy'])\nplt.title('正则化网络准确率')\nplt.ylabel('准确率')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(regularized_history.history['loss'])\nplt.plot(regularized_history.history['val_loss'])\nplt.title('正则化网络损失')\nplt.ylabel('损失')\nplt.xlabel('轮次')\nplt.legend(['训练集', '验证集'], loc='upper right')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/nn-regularized-curves-output-1.png){#nn-regularized-curves width=1142 height=472}\n:::\n:::\n\n\n### 模型比较\n\n我们来比较不同复杂度和正则化策略下神经网络的性能：\n\n::: {#cell-nn-comparison .cell fig-height='6' fig-width='12' execution_count=33}\n``` {.python .cell-code}\n# 比较不同网络的性能\nnn_models = ['简单网络', '中等网络', '复杂网络', '正则化网络']\ntrain_accuracy = [simple_train_acc, medium_train_acc, complex_train_acc, regularized_train_acc]\nval_accuracy = [simple_val_acc, medium_val_acc, complex_val_acc, regularized_val_acc]\n\nplt.figure(figsize=(12, 6))\nx = np.arange(len(nn_models))\nwidth = 0.35\n\nplt.bar(x - width/2, train_accuracy, width, label='训练集准确率')\nplt.bar(x + width/2, val_accuracy, width, label='验证集准确率')\n\nplt.ylabel('准确率')\nplt.title('不同神经网络模型的性能比较')\nplt.xticks(x, nn_models)\nplt.legend()\n\n# 显示差距\nfor i in range(len(nn_models)):\n    gap = train_accuracy[i] - val_accuracy[i]\n    plt.text(i, 0.5, f'差距: {gap:.4f}', ha='center')\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/nn-comparison-output-1.png){#nn-comparison width=1142 height=567}\n:::\n:::\n\n\n### 最终神经网络模型\n\n基于上述实验，我们选择性能最好的正则化网络作为最终的神经网络模型：\n\n::: {#final-nn-model .cell execution_count=34}\n``` {.python .cell-code}\n# 使用正则化网络作为最终模型\nnn_model = regularized_nn\n\n# 在完整训练集上重新训练\nX_full_scaled = scaler.transform(X_train)\nnn_model.fit(X_full_scaled, y_train, epochs=50, batch_size=16, verbose=0)\n\n# 生成最终预测\nnn_pred = (nn_model.predict(X_full_scaled) > 0.5).astype(int).flatten()\nnn_accuracy = accuracy_score(y_train, nn_pred)\nprint(f\"最终神经网络在训练集上的准确率: {nn_accuracy:.4f}\")\n\n# 将正则化网络用于测试集预测\ntest_pred_nn = (nn_model.predict(test_data_scaled) > 0.5).astype(int).flatten()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/28 [>.............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 0s 313us/step\n最终神经网络在训练集上的准确率: 0.8563\n\r 1/14 [=>............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 0s 323us/step\n```\n:::\n:::\n\n\n### 神经网络模型小结\n\n通过这一系列实验，我们观察到：\n\n1. **简单网络**：容易欠拟合，训练集和验证集性能都不够理想\n2. **中等网络**：提高了模型复杂度，性能有所改善\n3. **复杂网络**：进一步增加复杂度，在训练集表现良好但可能在验证集上表现下降，出现过拟合\n4. **正则化网络**：通过添加Dropout和BatchNormalization等正则化技术，在保持模型复杂度的同时有效减轻了过拟合，使训练集和验证集的性能差距减小\n\n这个过程展示了神经网络建模中的一个关键问题：如何在模型复杂度和泛化能力之间取得平衡。正则化技术是解决这一问题的有效工具。\n\n# 模型评估与比较\n\n在这一部分，我们将对所有训练好的模型进行评估和比较，选出性能最好的模型。\n\n::: {#cell-model-comparison .cell execution_count=35}\n``` {.python .cell-code}\n# 在整个训练集上再次训练各模型\nlogreg_model = LogisticRegression(max_iter=1000, random_state=42)\nlogreg_model.fit(X_train, y_train)\n\ndt_model = DecisionTreeClassifier(**dt_grid.best_params_, random_state=42)\ndt_model.fit(X_train, y_train)\n\nrf_model = RandomForestClassifier(**rf_grid.best_params_, random_state=42)\nrf_model.fit(X_train, y_train)\n\nxgb_model = xgb.XGBClassifier(**xgb_grid.best_params_, random_state=42)\nxgb_model.fit(X_train, y_train)\n\n# 在训练集上的预测\ny_pred_logreg = logreg_model.predict(X_train)\ny_pred_dt = dt_model.predict(X_train)\ny_pred_rf = rf_model.predict(X_train)\ny_pred_xgb = xgb_model.predict(X_train)\ny_pred_nn = (nn_model.predict(X_train_scaled) > 0.5).astype(int).flatten()\n\n# 计算各种评估指标\nmodels = ['逻辑回归', '决策树', '随机森林', 'XGBoost', '神经网络']\npredictions = [y_pred_logreg, y_pred_dt, y_pred_rf, y_pred_xgb, y_pred_nn]\n\nmetrics_df = pd.DataFrame(index=models, columns=['准确率', '精确率', '召回率', 'F1分数'])\n\nfor i, model_name in enumerate(models):\n    acc = accuracy_score(y_train, predictions[i])\n    prec = precision_score(y_train, predictions[i])\n    rec = recall_score(y_train, predictions[i])\n    f1 = f1_score(y_train, predictions[i])\n    \n    metrics_df.loc[model_name] = [acc, prec, rec, f1]\n\nprint(\"各模型在训练集上的性能比较:\")\nmetrics_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/28 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 0s 1ms/step\n各模型在训练集上的性能比较:\n```\n:::\n\n::: {#model-comparison .cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>准确率</th>\n      <th>精确率</th>\n      <th>召回率</th>\n      <th>F1分数</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>逻辑回归</th>\n      <td>0.824916</td>\n      <td>0.785276</td>\n      <td>0.748538</td>\n      <td>0.766467</td>\n    </tr>\n    <tr>\n      <th>决策树</th>\n      <td>0.839506</td>\n      <td>0.80805</td>\n      <td>0.763158</td>\n      <td>0.784962</td>\n    </tr>\n    <tr>\n      <th>随机森林</th>\n      <td>0.857464</td>\n      <td>0.854785</td>\n      <td>0.75731</td>\n      <td>0.803101</td>\n    </tr>\n    <tr>\n      <th>XGBoost</th>\n      <td>0.920314</td>\n      <td>0.924765</td>\n      <td>0.862573</td>\n      <td>0.892587</td>\n    </tr>\n    <tr>\n      <th>神经网络</th>\n      <td>0.856341</td>\n      <td>0.902256</td>\n      <td>0.701754</td>\n      <td>0.789474</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n可视化模型性能比较：\n\n::: {#model-comparison-plot .cell fig-height='6' fig-width='12' execution_count=36}\n``` {.python .cell-code}\n# 绘制性能比较图\nplt.figure(figsize=(12, 6))\nmetrics_df.plot(kind='bar', figsize=(12, 6))\nplt.title('各模型性能比较')\nplt.ylabel('得分')\nplt.xlabel('模型')\nplt.legend(loc='lower right')\nplt.tight_layout()\n```\n\n::: {#model-comparison-plot-1 .cell-output .cell-output-display}\n```\n<Figure size 1152x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab03_titanic_files/figure-html/model-comparison-plot-output-2.png){#model-comparison-plot-2 width=1143 height=567}\n:::\n:::\n\n\n# 测试集预测\n\n最后，我们使用训练好的模型对测试集进行预测：\n\n::: {#test-prediction .cell execution_count=37}\n``` {.python .cell-code}\n# 为所有模型生成预测\ntest_pred_logreg = logreg_model.predict(test_data)\ntest_pred_dt = dt_model.predict(test_data)\ntest_pred_rf = rf_model.predict(test_data)\ntest_pred_xgb = xgb_model.predict(test_data)\ntest_pred_nn = (nn_model.predict(test_data_scaled) > 0.5).astype(int).flatten()\n\n# 使用随机森林的预测作为最终结果（可以根据交叉验证结果选择最佳模型）\nfinal_predictions = test_pred_rf\n\n# 创建提交文件\nsubmission = pd.DataFrame({\n    'PassengerId': pd.read_csv(\"data/test.csv\")['PassengerId'],\n    'Survived': final_predictions\n})\n\nsubmission.to_csv('titanic_submission.csv', index=False)\nprint(\"提交文件已保存为 'titanic_submission.csv'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/14 [=>............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 0s 557us/step\n提交文件已保存为 'titanic_submission.csv'\n```\n:::\n:::\n\n\n各模型预测结果的比较：\n\n::: {#cell-prediction-comparison .cell execution_count=38}\n``` {.python .cell-code}\n# 各模型预测结果的比较\ntest_predictions = pd.DataFrame({\n    'PassengerId': pd.read_csv(\"data/test.csv\")['PassengerId'],\n    '逻辑回归': test_pred_logreg,\n    '决策树': test_pred_dt,\n    '随机森林': test_pred_rf,\n    'XGBoost': test_pred_xgb,\n    '神经网络': test_pred_nn\n})\n\n# 检查不同模型预测的一致性\nagreement = test_predictions.iloc[:, 1:].sum(axis=1)\nprint(\"模型预测一致性统计:\")\nprint(agreement.value_counts())\n\nprint(\"\\n各模型预测示例 (前10行):\")\ntest_predictions.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型预测一致性统计:\n0    231\n5    112\n4     28\n2     23\n3     14\n1     10\nName: count, dtype: int64\n\n各模型预测示例 (前10行):\n```\n:::\n\n::: {#prediction-comparison .cell-output .cell-output-display execution_count=38}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>逻辑回归</th>\n      <th>决策树</th>\n      <th>随机森林</th>\n      <th>XGBoost</th>\n      <th>神经网络</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>897</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>898</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>899</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>901</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# 总结\n\n在这个实践中，我们通过对泰坦尼克号乘客数据的分析，建立了多个机器学习模型来预测乘客在灾难中的生存情况。主要步骤包括：\n\n1. **数据探索**：通过统计分析和可视化，我们发现了一些关键特征，如性别、船票等级和年龄对生存率有显著影响。\n\n2. **特征工程**：我们从原始数据中提取了更有价值的特征，如头衔、家庭规模等，并处理了缺失值。\n\n3. **模型建立与评估**：我们构建了多种机器学习模型，包括逻辑回归、决策树、随机森林、XGBoost和神经网络，并通过交叉验证评估了它们的性能。\n\n4. **模型比较与选择**：通过比较不同模型的性能指标，我们选择了最佳模型用于最终预测。\n\n5. **测试集预测**：最后，我们使用选择的模型对测试集进行了预测，并生成了提交文件。\n\n这个实践展示了机器学习在实际问题中的应用过程，从数据探索到模型部署的完整流程。通过这个实践，我们不仅掌握了机器学习的基本技术，还了解了如何将这些技术应用到实际问题中。 \n\n",
    "supporting": [
      "lab03_titanic_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}