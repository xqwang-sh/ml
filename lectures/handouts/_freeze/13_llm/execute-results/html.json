{
  "hash": "d32fa915037db2c26892d3c9626d506c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"文本分析3：大语言模型及其应用\"\n---\n\n\n\n\n\n\n# 从静态向量到动态表示\n\n## Word2Vec的局限性\n\n上一讲中，我们学习了Word2Vec等词向量技术，它通过分布式表示极大提升了NLP的表示能力。然而，静态词向量仍然存在明显局限性：\n\n1. **一词一向量问题**：每个词只对应一个固定的向量，无法处理一词多义。例如\"苹果\"在\"我吃了一个苹果\"和\"苹果公司发布新产品\"中的含义完全不同。\n   \n2. **上下文无关**：词向量无法捕捉词语在特定上下文中的含义变化。例如\"银行存款\"和\"河流的银行\"中，\"银行\"的含义有很大差异。\n   \n3. **长距离依赖问题**：无法捕捉句子中相距较远的词之间的依赖关系。例如\"他说中文，因为他在中国生活了很多年\"中，第二个\"他\"与第一个\"他\"指代相同。\n   \n4. **表达能力有限**：固定维度的向量难以编码复杂的语言知识和语法结构。\n\n这些局限性促使研究者探索更先进的表示方法，能够根据上下文动态调整词语的表示。这一探索最终导致了BERT等基于Transformer的语言模型的诞生。\n\n## 上下文感知的词表示\n\n**上下文感知的词表示**（Contextualized Word Representations）是指词语的向量表示会根据其所处的上下文动态变化。与静态词向量不同，它具有以下特点：\n\n1. **动态表示**：同一个词在不同上下文中具有不同的向量表示\n2. **语义消歧**：能够根据上下文区分多义词的不同含义\n3. **句法感知**：能够捕捉词语在句子中的句法功能\n4. **长距离依赖**：能够建模句子中远距离词语之间的关系\n\n这种表示方法的核心思想是：**一个词的含义不仅取决于它自身，更取决于它的上下文环境**。\n\n## 语言模型：理解上下文的基础\n\n上下文感知表示的关键在于**语言模型**（Language Model）。语言模型是一种能够计算文本序列概率的模型，其基本任务是预测序列中的下一个词：\n\n$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n\n不同类型的语言模型处理上下文的方式不同：\n\n1. **传统n-gram语言模型**：只考虑有限历史，如$P(w_t | w_{t-2}, w_{t-1})$\n   \n2. **循环神经网络(RNN)语言模型**：通过隐藏状态递归编码全部历史\n   \n3. **双向语言模型**：同时考虑左侧和右侧上下文\n   \n4. **Transformer语言模型**：通过注意力机制直接建模所有位置间的依赖关系\n\n预训练语言模型的出现为NLP带来了革命性变化，它通过在大规模语料上无监督预训练，学习通用的语言表示，然后再针对下游任务进行微调。\n\n## 从ELMo到BERT的演进\n\n上下文感知的词表示技术的发展经历了几个里程碑：\n\n### ELMo (2018)\n\nELMo (Embeddings from Language Models) 是上下文词表示的早期尝试，由Peters等人在2018年提出。其特点包括：\n\n- 使用双层双向LSTM结构\n- 将前向和后向语言模型结合\n- 使用不同层的表示的加权组合作为最终表示\n- 有效解决了一词多义问题\n\nELMo的表示公式为：\n\n$$ELMo_k^{task} = E(R_k; \\Theta^{task}) = \\gamma^{task} \\sum_{j=0}^{L} s_j^{task} \\mathbf{h}_{k,j}^{LM}$$\n\n其中，$\\mathbf{h}_{k,j}^{LM}$是第k个词在第j层的表示。\n\n### GPT (2018)\n\nOpenAI的GPT (Generative Pre-Training) 模型采用了单向Transformer结构：\n\n- 仅使用前向语言模型（只看左侧上下文）\n- 基于Transformer解码器架构\n- 首次展示了大规模预训练+微调的范式\n\nGPT采用的预训练目标是预测下一个词：\n\n$$L(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n\n### BERT (2018)\n\nBERT (Bidirectional Encoder Representations from Transformers) 由Google在2018年提出，成为上下文词表示的里程碑工作：\n\n- 使用双向Transformer编码器\n- 采用掩码语言模型(Masked LM)预训练\n- 同时使用下一句预测(NSP)任务\n- 极大提升了NLP任务的性能上限\n\nBERT的预训练目标是预测被掩码的词：\n\n$$L(\\mathcal{D}) = \\sum_{i \\in \\mathcal{M}} \\log P(w_i | w_{\\neg \\mathcal{M}}; \\Theta)$$\n\n其中，$\\mathcal{M}$是被掩码的词的位置集合。\n\n这一演进体现了以下趋势：\n- 从浅层网络到深层Transformer架构\n- 从单向上下文到双向上下文\n- 从特征提取器到通用语言模型\n- 从任务相关到预训练-微调范式\n\n接下来，我们将深入理解BERT模型的内部工作原理。\n\n# BERT原理深度解析\n\n## Transformer架构：BERT的基础\n\nBERT建立在Transformer架构之上，这是由Vaswani等人在2017年提出的一种完全基于注意力机制的神经网络结构。在深入BERT之前，我们需要先理解Transformer的基本组件。\n\n### 自注意力机制\n\n**自注意力**（Self-Attention）是Transformer的核心组件，它允许模型在处理某个位置时，考虑序列中所有位置的信息。其计算过程如下：\n\n1. 将输入向量$X$分别转换为查询(Query)、键(Key)和值(Value)三个矩阵：\n   $$Q = XW^Q, K = XW^K, V = XW^V$$\n\n2. 计算注意力得分并归一化：\n   $$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n\n3. 其中，$\\sqrt{d_k}$是缩放因子，用于防止梯度消失。\n\n自注意力机制的优势在于：\n- 可以捕捉任意距离的依赖关系\n- 计算复杂度相对RNN低\n- 允许并行计算\n\n### 多头注意力\n\n为了增强模型的表示能力，Transformer使用了**多头注意力**（Multi-Head Attention）：\n\n$$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n\n其中，$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n\n多头注意力允许模型:\n- 在不同子空间中学习不同的关注模式\n- 同时关注位置和语义信息\n- 提供更丰富的特征表示\n\n### 位置编码\n\n由于自注意力机制本身不包含位置信息，Transformer引入了**位置编码**（Positional Encoding）来将序列顺序信息注入模型：\n\n$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$\n$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$\n\n其中，$pos$是位置，$i$是维度。\n\n### 前馈神经网络\n\nTransformer中每个子层还包含一个**前馈神经网络**（Feed-Forward Network），由两个线性变换组成：\n\n$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n\n### Transformer编码器结构\n\n一个完整的Transformer编码器层包含：\n1. 多头自注意力机制\n2. 层归一化（Layer Normalization）\n3. 前馈神经网络\n4. 残差连接（Residual Connection）\n\n这些组件按以下方式组合：\n$$\\hat{h} = LayerNorm(x + MultiHeadAttention(x))$$\n$$h = LayerNorm(\\hat{h} + FFN(\\hat{h}))$$\n\nBERT使用了Transformer的编码器部分，通常包含12层（BERT-Base）或24层（BERT-Large）。\n\n## BERT模型详解\n\nBERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，旨在学习深层的双向语言表示。\n\n### BERT的输入表示\n\nBERT的输入由三种嵌入的总和组成：\n\n1. **词嵌入**（Token Embeddings）：WordPiece词表中的词元对应的嵌入\n2. **段嵌入**（Segment Embeddings）：区分句子对中的第一句和第二句\n3. **位置嵌入**（Position Embeddings）：表示词元在序列中的位置\n\n每个输入序列以特殊标记`[CLS]`开始，以`[SEP]`分隔不同句子。\n\n![BERT输入表示](https://d2l.ai/_images/bert-input.svg)\n\n### BERT的预训练任务\n\nBERT通过两个无监督任务进行预训练：\n\n1. **掩码语言模型（Masked Language Model，MLM）**：\n   - 随机掩盖输入中15%的词元\n   - 其中80%用`[MASK]`替换，10%用随机词替换，10%保持不变\n   - 训练模型预测被掩盖的原始词元\n   - 这使得BERT能够学习双向上下文表示\n\n2. **下一句预测（Next Sentence Prediction，NSP）**：\n   - 给定两个句子，预测第二句是否是第一句的真实后续\n   - 训练数据中50%是真实的连续句子，50%是随机句子对\n   - 这使得BERT能够理解句子间的关系\n\n### BERT的模型变体\n\nBERT有两个主要变体：\n\n1. **BERT-Base**：\n   - 12层Transformer编码器\n   - 12个注意力头\n   - 768维隐藏层\n   - 1.1亿参数\n\n2. **BERT-Large**：\n   - 24层Transformer编码器\n   - 16个注意力头\n   - 1024维隐藏层\n   - 3.4亿参数\n\n### BERT的微调方式\n\n预训练后的BERT可以通过简单的任务特定层进行微调，适用于多种下游任务：\n\n1. **序列级任务**（如分类）：使用`[CLS]`标记的最终隐藏状态\n2. **词元级任务**（如NER）：使用每个词元的最终隐藏状态\n3. **句子对任务**（如问答）：同时输入问题和段落，识别答案跨度\n\n微调过程通常只需要少量标注数据和训练轮次，极大地降低了NLP任务的门槛。\n\n## BERT的内部工作机制\n\n通过深入分析BERT的内部表示，研究者发现BERT的不同层捕捉了不同类型的语言知识：\n\n### 层次化语言知识\n\n1. **底层**（1-4层）：捕捉表面语法特征、词性、局部依赖等\n2. **中层**（5-8层）：编码短语级语义和共指关系\n3. **高层**（9-12层）：处理长距离依赖和更抽象的语义关系\n\n### 注意力头的专业化\n\nBERT的不同注意力头专注于不同类型的语言信息：\n\n1. **语法头**：关注句法依赖关系\n2. **语义头**：关注语义相关的词\n3. **共指头**：关注指代同一实体的表达\n\n### BERT的表示空间\n\nBERT的表示空间表现出interessting的性质：\n\n1. **各向异性**：嵌入向量集中在狭窄的锥体中，而非均匀分布\n2. **语义区分**：相似概念在表示空间中形成聚类\n3. **线性结构**：某些语义关系可以通过向量差来表示\n\n这些特性使得BERT能够有效地编码复杂的语言知识，并为下游任务提供丰富的特征表示。\n\n## BERT的后续演进\n\nBERT发布后，研究者提出了许多改进版本，主要集中在以下几个方向：\n\n### 预训练任务优化\n\n1. **RoBERTa**：移除NSP任务，使用更大批量和更多数据训练\n2. **ALBERT**：参数共享和分解嵌入，降低模型大小\n3. **ELECTRA**：用判别式替换检测训练，提高效率\n\n### 知识增强\n\n1. **KnowBERT**：集成知识库信息\n2. **ERNIE**：加入实体和短语级掩码\n3. **FinBERT**：针对金融领域的专业知识训练\n\n### 模型架构改进\n\n1. **SpanBERT**：掩盖连续的文本片段而非单个词\n2. **XLNet**：使用排列语言模型，解决掩码带来的预训练-微调不一致\n3. **DeBERTa**：解耦注意力机制，增强位置编码\n\n这些改进进一步推动了预训练语言模型的发展，为下一代更强大的模型如GPT系列奠定了基础。\n\n# 从BERT到大语言模型\n\n## Transformer架构的扩展\n\n虽然BERT在NLP领域带来了巨大进步，但它仍然存在一些局限性，如无法进行生成任务和处理长文本。为了克服这些限制，研究者们对Transformer架构进行了多方面扩展。\n\n### 编码器-解码器结构\n\n**编码器-解码器**（Encoder-Decoder）结构是机器翻译等序列到序列任务的标准架构：\n\n1. **编码器**：处理输入序列，生成上下文表示\n2. **解码器**：基于编码器输出生成目标序列\n3. **交叉注意力**：解码器通过注意力机制访问编码器的输出\n\n代表模型：\n- **T5**：将所有NLP任务统一为文本到文本的转换\n- **BART**：通过降噪自编码器预训练\n\n### 仅解码器架构\n\n**仅解码器**（Decoder-only）架构专注于生成任务，通过自回归方式预测下一个词：\n\n1. **单向自注意力**：每个位置只能看到其前面的位置\n2. **自回归生成**：逐词生成输出序列\n3. **缩放规模**：通过扩大模型规模提升能力\n\n代表模型：\n- **GPT系列**：从GPT-1到GPT-4，规模和能力不断增长\n- **LLaMA**：开源的大型语言模型，有效降低了资源需求\n\n### 长距离建模\n\n处理长文本的能力是大语言模型的关键挑战之一，研究者提出了多种解决方案：\n\n1. **稀疏注意力**：如Longformer，只关注局部窗口和全局标记\n2. **循环机制**：如Transformer-XL，跨段传递隐藏状态\n3. **线性复杂度**：如Linformer，通过低秩近似降低计算量\n4. **扩展上下文窗口**：如DeepSeek，将上下文窗口扩展到128K\n\n## 大型语言模型的关键创新\n\n大型语言模型（LLMs）相比传统BERT模型有几个关键创新：\n\n### 规模扩展\n\n深度学习研究表明，模型规模与性能呈现\"幂律\"关系，增加参数量能带来显著性能提升：\n\n1. **从亿到千亿参数**：BERT-Large有3.4亿参数，而GPT-4估计有超过1万亿参数\n2. **计算资源增长**：训练大模型需要数千GPU/TPU，消耗数百万美元\n3. **预训练数据扩展**：从GB级语料到TB级语料\n\n### 涌现能力\n\n大语言模型最惊人的特性是**涌现能力**（Emergent Abilities）——在达到一定规模后突然出现的能力：\n\n1. **指令跟随**：理解并执行自然语言指令\n2. **思维链推理**：通过分步骤推理解决复杂问题\n3. **上下文学习**：从少量示例中学习新任务\n4. **多模态理解**：结合文本与图像等多种模态信息\n\n### 提示工程与思维链推理\n\n大语言模型的使用方式也发生了革命性变化：\n\n1. **提示工程**（Prompt Engineering）：\n   - 通过精心设计的提示引导模型行为\n   - 不同于传统的微调范式\n   - 允许灵活调整模型输出\n\n2. **思维链推理**（Chain-of-Thought）：\n   - 让模型先生成推理过程，再给出结论\n   - 显著提高模型解决复杂问题的能力\n   - 公式：$\\text{Prompt} + \\text{思考过程} \\to \\text{更准确的结果}$\n\n3. **上下文学习**（In-context Learning）：\n   - 在提示中包含示例，引导模型学习模式\n   - 无需参数更新，即可适应新任务\n   - 示例：给出几个情感分类示例，模型可泛化到新文本\n\n## 代表性大型语言模型\n\n### GPT系列\n\n由OpenAI开发的GPT（Generative Pre-trained Transformer）系列是大型语言模型的代表：\n\n1. **GPT-1**（2018）：\n   - 1.17亿参数\n   - 首次展示预训练+微调范式\n   - 在多个NLP任务上获得突破\n\n2. **GPT-2**（2019）：\n   - 15亿参数\n   - 展示了零样本学习能力\n   - 文本生成质量有显著提升\n\n3. **GPT-3**（2020）：\n   - 1750亿参数\n   - 展示了惊人的少样本学习能力\n   - 可以执行之前未见过的任务\n\n4. **GPT-4**（2023）：\n   - 参数规模未公开，估计超过1万亿\n   - 多模态能力，支持图像输入\n   - 接近人类专家水平的表现\n\n### 开源大型语言模型\n\n除了GPT系列，开源社区也开发了多种高性能大语言模型：\n\n1. **LLaMA系列**：\n   - 由Meta AI开发\n   - 参数规模从7B到65B不等\n   - 性能接近闭源商业模型\n   - 衍生了许多优秀模型如Vicuna和Alpaca\n\n2. **国产大模型**：\n   - **ChatGLM**：清华大学与智谱AI合作开发的双语模型\n   - **DeepSeek**：深度求索开发，专注长序列处理\n   - **Qwen**：阿里云开发，性能优异的开源模型\n\n3. **多模态模型**：\n   - **CLIP**：连接图像和文本的表示学习\n   - **GPT-4V**：具有视觉理解能力的GPT-4变体\n   - **Gemini**：Google的多模态大语言模型\n\n## 大语言模型的金融应用\n\n大语言模型在金融领域有广泛的应用潜力：\n\n### 信息提取与分析\n\n1. **报告解析**：\n   - 自动提取财报中的关键财务指标\n   - 总结长篇研报要点\n   - 识别风险披露声明\n\n2. **市场情感分析**：\n   - 分析新闻报道的市场情绪\n   - 提取投资者情绪信号\n   - 预测市场波动\n\n3. **事件提取**：\n   - 从财经新闻中识别重大事件\n   - 构建事件知识图谱\n   - 分析事件之间的因果关系\n\n### 金融文本生成\n\n1. **研究报告生成**：\n   - 基于数据自动生成财务分析\n   - 创建行业趋势报告\n   - 生成个股评论\n\n2. **监管合规**：\n   - 生成合规声明和披露\n   - 检查文档是否符合监管要求\n   - 自动更新合规文件\n\n3. **客户交互**：\n   - 智能金融顾问\n   - 个性化投资建议\n   - 金融知识普及\n\n### 无监督学习辅助\n\n1. **文本聚类**：\n   - 通过嵌入向量聚类发现主题\n   - 识别相似公告和报告\n   - 发现市场关注热点\n\n2. **异常检测**：\n   - 识别异常金融叙述\n   - 发现财报中的可疑部分\n   - 预警潜在风险信号\n\n3. **主题提取**：\n   - 无监督发现文档主题\n   - 总结长文本的核心观点\n   - 追踪主题随时间的演变\n\n# 基于BERT的政府工作报告分析\n\n## BERT与Word2Vec的实验对比\n\n在本节中，我们将使用政府工作报告数据，对比BERT与Word2Vec在文本表示上的差异。这一对比将帮助我们理解上下文感知表示相对于静态词向量的优势。\n\n### 数据准备与预处理\n\n与上一讲类似，我们首先加载和预处理政府工作报告数据：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jieba\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nfrom transformers import BertModel, BertTokenizer\n\n# 加载政府工作报告数据\ngovreport = pd.read_csv(\"labs/NLP/data/govreport.csv\")\n\n# 设置中文显示\nplt.rcParams['font.sans-serif'] = ['Songti SC']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n# 加载停用词\nwith open(\"labs/NLP/data/ChineseStopWords.txt\", 'r', encoding='utf-8') as f:\n    stop_words = {line.strip() for line in f}\n\n# 文本清洗并分词\ndef preprocess_text(text):\n    # 去除标点符号和数字\n    text = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', text)\n    # 分词\n    words = jieba.cut(text)\n    # 去除停用词和空白\n    return [word for word in words if word.strip() and word not in stop_words]\n\n# 处理所有文档\ncorpus = []\nyears = []\nfor _, row in govreport.iterrows():\n    words = preprocess_text(row['texts'])\n    corpus.append(words)\n    years.append(row['Year'])\n\nprint(f\"语料库包含{len(corpus)}篇文档\")\n\n# 加载预训练的Word2Vec模型（假设我们已经训练好了，这里直接加载）\nfrom gensim.models import Word2Vec\nw2v_model = Word2Vec.load(\"gov_report_word2vec.model\")\n```\n:::\n\n\n\n\n### 加载BERT模型\n\n我们将使用中文预训练BERT模型：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 加载预训练的中文BERT模型\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\nmodel = BertModel.from_pretrained('bert-base-chinese')\nmodel.eval()  # 设置为评估模式\n\n# 定义函数获取BERT词向量\ndef get_bert_embeddings(text, tokenizer, model, max_length=512):\n    # 对文本进行分词和转换\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length, padding='max_length')\n    \n    # 获取BERT输出\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # 提取最后一层的隐藏状态（词向量）\n    last_hidden_states = outputs.last_hidden_state\n    \n    # 提取[CLS]标记的向量作为句子表示\n    cls_embedding = last_hidden_states[:, 0, :].numpy()\n    \n    # 提取所有词元的向量\n    token_embeddings = last_hidden_states.numpy()\n    \n    return {\n        'cls_embedding': cls_embedding,  # 句子表示\n        'token_embeddings': token_embeddings,  # 词元表示\n        'tokens': tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])  # 对应的词元\n    }\n```\n:::\n\n\n\n\n### 多义词表示对比\n\n我们将对比BERT和Word2Vec在处理多义词上的能力差异：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 选择一个多义词进行分析，如\"发展\"\nambiguous_word = \"发展\"\n\n# 从报告中找出含有该词的不同上下文\ncontexts = []\nfor i, doc in enumerate(corpus):\n    # 将分词列表转换为文本\n    text = \"\".join(doc)\n    # 在原文中寻找该词\n    if ambiguous_word in text:\n        # 寻找包含目标词的短句\n        sentences = re.split(r'[，。！？；]', text)\n        for sentence in sentences:\n            if ambiguous_word in sentence and 10 < len(sentence) < 50:\n                contexts.append((years[i], sentence))\n                if len(contexts) >= 5:  # 只取5个上下文示例\n                    break\n    if len(contexts) >= 5:\n        break\n\n# 1. Word2Vec的表示（静态）\nw2v_vector = w2v_model.wv[ambiguous_word]\n\n# 2. BERT的表示（动态，依赖上下文）\nbert_vectors = []\nfor year, context in contexts:\n    # 获取BERT表示\n    bert_result = get_bert_embeddings(context, tokenizer, model)\n    # 找到目标词的位置\n    target_positions = [i for i, token in enumerate(bert_result['tokens']) \n                        if ambiguous_word in token and '##' not in token]\n    \n    if target_positions:\n        # 提取目标词在该上下文中的表示\n        target_idx = target_positions[0]\n        bert_vector = bert_result['token_embeddings'][0, target_idx, :]\n        bert_vectors.append((year, context, bert_vector))\n\n# 计算BERT表示之间的相似度\nif bert_vectors:\n    bert_similarities = np.zeros((len(bert_vectors), len(bert_vectors)))\n    for i in range(len(bert_vectors)):\n        for j in range(len(bert_vectors)):\n            bert_similarities[i, j] = cosine_similarity(\n                bert_vectors[i][2].reshape(1, -1), \n                bert_vectors[j][2].reshape(1, -1)\n            )[0, 0]\n    \n    # 可视化BERT表示的相似度矩阵\n    plt.figure(figsize=(10, 8))\n    plt.imshow(bert_similarities, cmap='YlOrRd')\n    plt.colorbar(label='余弦相似度')\n    plt.title(f'\"{ambiguous_word}\"在不同上下文中的BERT表示相似度')\n    \n    # 设置坐标轴标签\n    context_labels = [f\"{year}: {context[:10]}...\" for year, context, _ in bert_vectors]\n    plt.xticks(range(len(context_labels)), context_labels, rotation=45, ha='right')\n    plt.yticks(range(len(context_labels)), context_labels)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nWord2Vec中'{ambiguous_word}'的静态表示是固定的，无法区分不同上下文\")\n    print(f\"而BERT可以为'{ambiguous_word}'在不同上下文中生成不同的表示\")\n    print(\"相似度矩阵展示了这些表示之间的差异，相似度较低的对应不同语义用法\")\n```\n:::\n\n\n\n\n### 生成句子向量比较\n\n我们将对比Word2Vec和BERT生成的句子向量在文本相似度任务上的表现：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 为每个报告生成句子向量\n# 1. 基于Word2Vec (简单平均)\ndef get_w2v_sentence_vector(tokens, model):\n    valid_tokens = [t for t in tokens if t in model.wv]\n    if not valid_tokens:\n        return np.zeros(model.vector_size)\n    return np.mean([model.wv[t] for t in valid_tokens], axis=0)\n\n# 2. 基于BERT的[CLS]标记\ndef get_bert_sentence_vector(text, tokenizer, model):\n    result = get_bert_embeddings(text, tokenizer, model)\n    return result['cls_embedding'][0]\n\n# 生成向量\nw2v_doc_vectors = [get_w2v_sentence_vector(doc, w2v_model) for doc in corpus]\nbert_doc_vectors = []\n\nfor i, _ in govreport.iterrows():\n    # 取报告的前512个字符（BERT输入长度限制）\n    text_sample = govreport.iloc[i]['texts'][:512]\n    bert_vec = get_bert_sentence_vector(text_sample, tokenizer, model)\n    bert_doc_vectors.append(bert_vec)\n\n# 计算报告之间的相似度\nw2v_similarities = cosine_similarity(w2v_doc_vectors)\nbert_similarities = cosine_similarity(bert_doc_vectors)\n\n# 可视化报告相似度矩阵比较\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# Word2Vec相似度矩阵\nim1 = axes[0].imshow(w2v_similarities, cmap='Blues')\naxes[0].set_title('Word2Vec: 报告相似度矩阵')\naxes[0].set_xticks(range(len(years)))\naxes[0].set_yticks(range(len(years)))\naxes[0].set_xticklabels(years)\naxes[0].set_yticklabels(years)\nplt.colorbar(im1, ax=axes[0])\n\n# BERT相似度矩阵\nim2 = axes[1].imshow(bert_similarities, cmap='Reds')\naxes[1].set_title('BERT: 报告相似度矩阵')\naxes[1].set_xticks(range(len(years)))\naxes[1].set_yticks(range(len(years)))\naxes[1].set_xticklabels(years)\naxes[1].set_yticklabels(years)\nplt.colorbar(im2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n\n\n### 关键词提取对比\n\n比较BERT和Word2Vec在关键词提取任务上的差异：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 基于BERT的关键词提取\ndef extract_bert_keywords(text, tokenizer, model, top_n=10):\n    # 将文本分成短句\n    sentences = re.split(r'[，。！？；]', text)\n    sentences = [s for s in sentences if len(s) > 5]\n    \n    if not sentences:\n        return []\n    \n    # 对每个句子获取词元表示\n    all_tokens = []\n    all_embeddings = []\n    \n    for sentence in sentences[:20]:  # 限制句子数量\n        result = get_bert_embeddings(sentence, tokenizer, model)\n        tokens = result['tokens']\n        embeddings = result['token_embeddings'][0]\n        \n        # 过滤掉特殊标记和重复词\n        valid_indices = []\n        valid_tokens = []\n        for i, token in enumerate(tokens):\n            if token not in ['[CLS]', '[SEP]', '[PAD]'] and '##' not in token:\n                if token not in valid_tokens:\n                    valid_indices.append(i)\n                    valid_tokens.append(token)\n        \n        all_tokens.extend([tokens[i] for i in valid_indices])\n        all_embeddings.extend([embeddings[i] for i in valid_indices])\n    \n    if not all_tokens:\n        return []\n    \n    # 计算每个词与文档中心的余弦相似度\n    doc_center = np.mean(all_embeddings, axis=0)\n    token_scores = []\n    \n    for i, token in enumerate(all_tokens):\n        if token in stop_words or len(token) < 2:\n            continue\n        score = cosine_similarity([all_embeddings[i]], [doc_center])[0][0]\n        token_scores.append((token, score))\n    \n    # 按分数排序并返回前N个关键词\n    unique_tokens = {}\n    for token, score in token_scores:\n        if token not in unique_tokens or score > unique_tokens[token]:\n            unique_tokens[token] = score\n    \n    sorted_tokens = sorted(unique_tokens.items(), key=lambda x: x[1], reverse=True)\n    return sorted_tokens[:top_n]\n\n# 选择2023年报告进行分析\nreport_idx = govreport[govreport['Year'] == 2023].index[0]\nreport_text = govreport.loc[report_idx, 'texts']\nreport_tokens = corpus[report_idx]\n\n# Word2Vec关键词（使用上一讲的中心性方法）\ndef get_w2v_central_words(tokens, model, top_n=10):\n    # 过滤不在模型中的词\n    valid_tokens = [t for t in tokens if t in model.wv]\n    \n    if not valid_tokens:\n        return []\n    \n    # 计算中心性\n    word_centrality = {}\n    for word in set(valid_tokens):\n        if len(word) < 2:  # 跳过单字词\n            continue\n        similarities = [model.wv.similarity(word, t) for t in valid_tokens if t != word]\n        if similarities:\n            word_centrality[word] = sum(similarities) / len(similarities)\n    \n    # 返回中心性最高的词\n    return sorted(word_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n# 提取关键词\nw2v_keywords = get_w2v_central_words(report_tokens, w2v_model)\nbert_keywords = extract_bert_keywords(report_text[:2000], tokenizer, model)  # 限制文本长度\n\n# 打印结果比较\nprint(\"Word2Vec提取的关键词:\")\nfor word, score in w2v_keywords:\n    print(f\"{word}: {score:.4f}\")\n\nprint(\"\\nBERT提取的关键词:\")\nfor word, score in bert_keywords:\n    print(f\"{word}: {score:.4f}\")\n```\n:::\n\n\n\n\n## 使用预训练模型进行主题分析\n\n接下来，我们探索如何使用BERT进行政府工作报告的主题分析。\n\n### 基于BERT表示的文本聚类\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# 对BERT文档向量进行聚类\nn_clusters = 3\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\ncluster_labels = kmeans.fit_predict(bert_doc_vectors)\n\n# 将聚类结果与年份对应\ncluster_df = pd.DataFrame({\n    'Year': years,\n    'Cluster': cluster_labels\n})\n\n# 对结果进行可视化（使用PCA降维）\npca = PCA(n_components=2)\nbert_2d = pca.fit_transform(bert_doc_vectors)\n\nplt.figure(figsize=(12, 8))\nfor cluster in range(n_clusters):\n    # 找出属于当前簇的点\n    cluster_points = bert_2d[cluster_labels == cluster]\n    cluster_years = [years[i] for i in range(len(years)) if cluster_labels[i] == cluster]\n    \n    # 绘制点\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}', alpha=0.7)\n    \n    # 添加年份标签\n    for i, point in enumerate(cluster_points):\n        plt.annotate(cluster_years[i], xy=point, fontsize=10)\n\nplt.title('基于BERT表示的政府工作报告聚类')\nplt.xlabel('PCA维度1')\nplt.ylabel('PCA维度2')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 分析每个簇的主题特征\nfor cluster in range(n_clusters):\n    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster]\n    cluster_years = [years[i] for i in cluster_indices]\n    \n    print(f\"\\n簇 {cluster} 包含年份: {', '.join(map(str, cluster_years))}\")\n    \n    # 合并该簇的所有文档\n    cluster_text = \" \".join([govreport.iloc[i]['texts'][:500] for i in cluster_indices])\n    \n    # 提取该簇的主题词\n    cluster_keywords = extract_bert_keywords(cluster_text, tokenizer, model, top_n=15)\n    print(\"主题词:\")\n    for word, score in cluster_keywords:\n        print(f\"  {word}: {score:.4f}\")\n```\n:::\n\n\n\n\n### 文本主题随时间的演变分析\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 对每年报告提取主题词，分析主题随时间的演变\nyears_list = sorted(set(years))\nyearly_topics = {}\n\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts'][:2000]  # 限制长度\n    \n    # 提取主题词\n    topics = extract_bert_keywords(text, tokenizer, model, top_n=10)\n    yearly_topics[year] = topics\n\n# 跟踪某些关键词随时间的变化\nfocus_words = ['创新', '发展', '改革', '民生', '科技', '数字', '经济']\nword_trends = {word: [] for word in focus_words}\n\nfor year in years_list:\n    # 创建当年主题词的字典\n    year_word_scores = {word: score for word, score in yearly_topics[year]}\n    \n    # 记录焦点词的出现情况\n    for word in focus_words:\n        if word in year_word_scores:\n            word_trends[word].append((year, year_word_scores[word]))\n        else:\n            word_trends[word].append((year, 0))\n\n# 可视化关键词趋势\nplt.figure(figsize=(14, 8))\nfor word, trend in word_trends.items():\n    years_data = [t[0] for t in trend]\n    scores = [t[1] for t in trend]\n    plt.plot(years_data, scores, marker='o', linewidth=2, label=word)\n\nplt.title('政府工作报告中关键概念的重要性变化')\nplt.xlabel('年份')\nplt.ylabel('主题重要性得分')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.xticks(years_list)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n\n\n### 语义相似度异常点检测\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# 使用局部异常因子(LOF)算法检测异常点\nlof = LocalOutlierFactor(n_neighbors=2, contamination=0.1)\noutliers = lof.fit_predict(bert_doc_vectors)\n\n# 异常点是-1，正常点是1\nanomaly_indices = [i for i, label in enumerate(outliers) if label == -1]\nanomaly_years = [years[i] for i in anomaly_indices]\n\nprint(f\"检测到的异常报告: {', '.join(map(str, anomaly_years))}\")\n\n# 可视化异常点\nplt.figure(figsize=(12, 8))\nplt.scatter(bert_2d[:, 0], bert_2d[:, 1], c=['red' if x == -1 else 'blue' for x in outliers], \n            alpha=0.7, label=['Anomaly' if x == -1 else 'Normal' for x in outliers])\n\n# 添加年份标签\nfor i, point in enumerate(bert_2d):\n    plt.annotate(years[i], xy=point, fontsize=10)\n\nplt.title('政府工作报告语义异常检测')\nplt.xlabel('PCA维度1')\nplt.ylabel('PCA维度2')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 分析异常报告的特点\nif anomaly_indices:\n    for idx in anomaly_indices:\n        year = years[idx]\n        print(f\"\\n{year}年报告被检测为异常，分析其特点:\")\n        \n        # 提取该报告的特有词汇\n        text = govreport.iloc[idx]['texts'][:2000]\n        keywords = extract_bert_keywords(text, tokenizer, model, top_n=10)\n        \n        print(\"特征词:\")\n        for word, score in keywords:\n            print(f\"  {word}: {score:.4f}\")\n        \n        # 计算与其他报告的平均相似度\n        similarities = []\n        for i in range(len(bert_doc_vectors)):\n            if i != idx:\n                sim = cosine_similarity([bert_doc_vectors[idx]], [bert_doc_vectors[i]])[0][0]\n                similarities.append((years[i], sim))\n        \n        avg_similarity = sum(sim for _, sim in similarities) / len(similarities)\n        print(f\"与其他报告的平均相似度: {avg_similarity:.4f}\")\n        \n        # 找出最不相似的报告\n        least_similar = min(similarities, key=lambda x: x[1])\n        print(f\"最不相似的报告是{least_similar[0]}年，相似度为{least_similar[1]:.4f}\")\n```\n:::\n\n\n\n\n# 使用大语言模型进行金融文本分析\n\n在本节中，我们将探索如何利用强大的大语言模型（LLMs）进行金融文本分析。我们将介绍如何使用预训练的金融专业模型FinBERT以及通用大模型如DeepSeek等，实现无监督的文本理解任务。\n\n## 使用FinBERT进行金融文本分类\n\nFinBERT是一种针对金融领域进行微调的BERT变体，专门为金融文本分析而设计。\n\n### 安装与加载FinBERT\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 安装所需库\n# !pip install transformers sentencepiece matplotlib\n# !pip install finbert\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 加载FinBERT模型\ntokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\nmodel.eval()  # 设置为评估模式\n```\n:::\n\n\n\n\n### 金融情感分析\n\nFinBERT的一个主要用途是金融文本的情感分析，可以将文本分类为正面、负面或中性：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef analyze_sentiment(text, model, tokenizer):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    \n    # FinBERT情感标签: 0=negative, 1=neutral, 2=positive\n    sentiment_labels = [\"负面\", \"中性\", \"正面\"]\n    scores = predictions[0].numpy()\n    \n    results = []\n    for i in range(len(sentiment_labels)):\n        results.append((sentiment_labels[i], float(scores[i])))\n    \n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n# 测试几个金融相关的文本片段\ntest_texts = [\n    \"今年以来，我国经济稳中向好，经济增长好于预期，通胀水平保持稳定。\",\n    \"受外部需求减弱影响，出口增长放缓，企业经营压力加大，就业形势更加严峻。\",\n    \"科技创新成为经济高质量发展的强大动力，数字经济蓬勃发展。\",\n    \"金融风险明显增加，部分企业债务违约，需要加强风险防控。\",\n    \"资本市场改革持续推进，投资者信心有所恢复，市场预期逐步改善。\"\n]\n\n# 分析情感\nsentiments = []\nfor text in test_texts:\n    sentiment = analyze_sentiment(text, model, tokenizer)\n    sentiments.append(sentiment)\n    print(f\"文本: {text}\")\n    for label, score in sentiment:\n        print(f\"  {label}: {score:.4f}\")\n    print()\n\n# 可视化情感分析结果\nfig, ax = plt.subplots(figsize=(12, 8))\nx = np.arange(len(test_texts))\nwidth = 0.25\n\n# 提取各情感得分\nnegative_scores = [sentiment[2][1] for sentiment in sentiments]\nneutral_scores = [sentiment[1][1] for sentiment in sentiments]\npositive_scores = [sentiment[0][1] for sentiment in sentiments]\n\n# 绘制条形图\nax.bar(x - width, positive_scores, width, label='正面')\nax.bar(x, neutral_scores, width, label='中性')\nax.bar(x + width, negative_scores, width, label='负面')\n\nax.set_ylabel('情感得分')\nax.set_title('FinBERT金融文本情感分析')\nax.set_xticks(x)\nax.set_xticklabels([f'文本{i+1}' for i in range(len(test_texts))])\nax.legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n\n\n### 对政府工作报告进行金融情感分析\n\n接下来，我们使用FinBERT分析政府工作报告中的金融相关段落：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 从政府工作报告中提取金融相关段落\ndef extract_finance_paragraphs(text, keywords=None):\n    if keywords is None:\n        keywords = ['经济', '金融', '财政', '税收', '货币', '银行', '债务', '投资', \n                  '证券', '股市', '外汇', '通胀', '增长', '风险', '改革']\n    \n    # 将文本分成段落\n    paragraphs = text.split('\\n')\n    \n    # 过滤出含有金融关键词的段落\n    finance_paragraphs = []\n    for para in paragraphs:\n        if len(para) < 10:  # 跳过短段落\n            continue\n        if any(keyword in para for keyword in keywords):\n            finance_paragraphs.append(para)\n    \n    return finance_paragraphs\n\n# 按年份分析政府工作报告的金融情感\nyearly_sentiments = {}\n\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取金融段落\n    finance_paras = extract_finance_paragraphs(text)\n    \n    # 不分析段落过少的年份\n    if len(finance_paras) < 3:\n        continue\n    \n    # 分析每个段落的情感\n    para_sentiments = []\n    for para in finance_paras[:10]:  # 限制段落数量\n        sentiment = analyze_sentiment(para, model, tokenizer)\n        para_sentiments.append(sentiment)\n    \n    # 计算平均情感分数\n    avg_positive = np.mean([s[0][1] for s in para_sentiments])\n    avg_neutral = np.mean([s[1][1] for s in para_sentiments])\n    avg_negative = np.mean([s[2][1] for s in para_sentiments])\n    \n    yearly_sentiments[year] = {\n        \"正面\": avg_positive,\n        \"中性\": avg_neutral,\n        \"负面\": avg_negative\n    }\n\n# 可视化情感随时间的变化\nplt.figure(figsize=(14, 8))\nyears = sorted(yearly_sentiments.keys())\npositive_scores = [yearly_sentiments[year][\"正面\"] for year in years]\nneutral_scores = [yearly_sentiments[year][\"中性\"] for year in years]\nnegative_scores = [yearly_sentiments[year][\"负面\"] for year in years]\n\nplt.plot(years, positive_scores, 'g-', marker='o', linewidth=2, label='正面')\nplt.plot(years, neutral_scores, 'b-', marker='s', linewidth=2, label='中性')\nplt.plot(years, negative_scores, 'r-', marker='^', linewidth=2, label='负面')\n\nplt.title('政府工作报告金融段落情感变化趋势')\nplt.xlabel('年份')\nplt.ylabel('情感强度')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.xticks(years)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n\n\n## 使用通用大语言模型的零样本分类\n\n除了专业领域模型，我们还可以利用通用大语言模型的强大能力进行零样本分类，无需额外训练。\n\n### 安装与设置大语言模型\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 安装所需库\n# !pip install openai\n# !pip install deepseek\n\n# 导入必要的库\nimport openai\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom deepseek import DeepSeekAPI\n\n# 设置API密钥（请使用自己的API密钥）\n# openai.api_key = \"your-api-key-here\"  # GPT-4/3.5\n# deepseek_api = DeepSeekAPI(\"your-deepseek-api-key\")  # DeepSeek\n```\n:::\n\n\n\n\n### 零样本文本分类\n\n我们可以使用大语言模型进行零样本分类，无需提供训练数据：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef classify_text_with_gpt(text, categories, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型进行零样本文本分类\"\"\"\n    prompt = f\"\"\"请将以下文本分类到这些类别之一: {', '.join(categories)}。\n    只需回复类别名称，不要添加任何解释或标点符号。\n    \n    文本: {text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个精确的文本分类助手。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,\n    )\n    \n    return response.choices[0].message.content.strip()\n\ndef classify_text_with_deepseek(text, categories, model=\"deepseek-chat\"):\n    \"\"\"使用DeepSeek模型进行零样本文本分类\"\"\"\n    prompt = f\"\"\"请将以下文本分类到这些类别之一: {', '.join(categories)}。\n    只需回复类别名称，不要添加任何解释或标点符号。\n    \n    文本: {text}\n    \"\"\"\n    \n    response = deepseek_api.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个精确的文本分类助手。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 示例：将政府工作报告段落分类为不同政策领域\npolicy_categories = [\"经济发展\", \"科技创新\", \"民生改善\", \"环境保护\", \"改革开放\", \"风险防控\"]\n\n# 从多个年份的报告中选取段落\nsample_paragraphs = []\nsample_years = [2019, 2020, 2021, 2022, 2023]\n\nfor year in sample_years:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取段落\n    paragraphs = [p for p in text.split('\\n') if len(p) > 50 and len(p) < 200]\n    if paragraphs:\n        # 随机选择一个段落\n        import random\n        selected_para = random.choice(paragraphs)\n        sample_paragraphs.append((year, selected_para))\n\n# 使用大语言模型进行分类\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\nclassification_results = []\nfor year, para in sample_paragraphs:\n    # category = classify_text_with_gpt(para, policy_categories)\n    category = classify_text_with_deepseek(para, policy_categories)\n    classification_results.append((year, para[:50] + \"...\", category))\n\n# 打印分类结果\nprint(\"大语言模型零样本分类结果:\")\nfor year, para_preview, category in classification_results:\n    print(f\"{year}年段落: {para_preview}\")\n    print(f\"分类: {category}\\n\")\n\"\"\"\n```\n:::\n\n\n\n\n### 使用大模型生成金融文本摘要\n\n大语言模型在摘要生成方面表现出色，可以用于提取政府工作报告中的金融政策要点：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef generate_summary_with_gpt(text, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型生成摘要\"\"\"\n    prompt = f\"\"\"请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:\n\n    {text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的金融政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=300\n    )\n    \n    return response.choices[0].message.content.strip()\n\ndef generate_summary_with_deepseek(text, model=\"deepseek-chat\"):\n    \"\"\"使用DeepSeek模型生成摘要\"\"\"\n    prompt = f\"\"\"请总结以下政府工作报告中的金融政策要点，以简洁的要点形式列出，最多5条:\n\n    {text}\n    \"\"\"\n    \n    response = deepseek_api.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的金融政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=300\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 为近几年的报告生成金融政策摘要\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\npolicy_summaries = {}\nfor year in [2021, 2022, 2023]:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 提取金融段落\n    finance_paras = extract_finance_paragraphs(text)\n    finance_text = \"\\n\".join(finance_paras[:10])  # 限制输入长度\n    \n    # 生成摘要\n    # summary = generate_summary_with_gpt(finance_text)\n    summary = generate_summary_with_deepseek(finance_text)\n    policy_summaries[year] = summary\n\n# 打印摘要结果\nfor year, summary in policy_summaries.items():\n    print(f\"{year}年政府工作报告金融政策要点:\")\n    print(summary)\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\"\"\"\n```\n:::\n\n\n\n\n## 使用大语言模型进行高级文本分析\n\n大语言模型还可以用于更复杂的无监督文本分析任务。\n\n### 提示式主题建模\n\n使用提示工程（Prompt Engineering）引导大语言模型进行主题发现：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef extract_topics_with_gpt(texts, n_topics=5, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用GPT模型进行提示式主题建模\"\"\"\n    # 合并文本，但限制长度以适应API限制\n    combined_text = \"\\n\\n\".join(texts)\n    if len(combined_text) > 4000:\n        combined_text = combined_text[:4000] + \"...\"\n    \n    prompt = f\"\"\"作为一个文本挖掘专家，请分析以下多个文本片段，识别其中的{n_topics}个主要主题。\n    每个主题请提供一个简短标题和3-5个关键词。\n    请仅输出主题和关键词，不要有其他解释。\n    \n    文本片段:\n    {combined_text}\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的文本挖掘专家。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.2,\n        max_tokens=500\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 从多个年份的报告中提取金融段落\nall_finance_paras = []\nfor year in years_list[-5:]:  # 只取最近5年\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    finance_paras = extract_finance_paragraphs(text)\n    all_finance_paras.extend(finance_paras[:5])  # 每年取5个段落\n\n# 使用大语言模型进行主题建模\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\ntopics = extract_topics_with_gpt(all_finance_paras)\nprint(\"大语言模型识别的主题:\")\nprint(topics)\n\"\"\"\n```\n:::\n\n\n\n\n### 思维链分析\n\n使用思维链（Chain-of-Thought）技术引导大语言模型进行深度分析：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef analyze_with_cot(text, question, model=\"gpt-3.5-turbo\"):\n    \"\"\"使用思维链技术进行深度分析\"\"\"\n    prompt = f\"\"\"请分析以下政府工作报告文本，回答问题。\n    请先逐步思考，然后给出最终答案。\n    \n    文本:\n    {text}\n    \n    问题: {question}\n    \n    逐步思考:\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一个专业的政策分析师。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=800\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# 示例：分析最新报告中的政策转变\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\n# 获取2023年报告\nlatest_idx = govreport[govreport['Year'] == 2023].index[0]\nlatest_text = govreport.loc[latest_idx, 'texts']\n\n# 限制文本长度\nanalysis_text = latest_text[:3000]\n\n# 定义分析问题\nanalysis_question = \"这份政府工作报告对金融风险防控政策有哪些新的调整？这些调整与前几年相比有何变化？\"\n\n# 进行思维链分析\nanalysis_result = analyze_with_cot(analysis_text, analysis_question)\nprint(\"思维链分析结果:\")\nprint(analysis_result)\n\"\"\"\n```\n:::\n\n\n\n\n### 嵌入空间与向量检索\n\n大语言模型的嵌入向量可用于高级语义检索：\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_embedding_with_openai(text, model=\"text-embedding-ada-002\"):\n    \"\"\"获取OpenAI的文本嵌入向量\"\"\"\n    if not text.strip():\n        return np.zeros(1536)  # OpenAI embeddings are 1536-dimensional\n    \n    # 限制文本长度\n    if len(text) > 8000:\n        text = text[:8000]\n    \n    response = openai.Embedding.create(\n        input=text,\n        model=model\n    )\n    \n    embedding = response['data'][0]['embedding']\n    return np.array(embedding)\n\n# 为政府工作报告段落生成嵌入向量\n# 注意：实际运行时取消注释以下代码，但需要API密钥\n\"\"\"\n# 准备段落\nall_paragraphs = []\nfor year in years_list:\n    idx = govreport[govreport['Year'] == year].index[0]\n    text = govreport.loc[idx, 'texts']\n    \n    # 分段\n    paragraphs = [p for p in text.split('\\n') if len(p) > 50]\n    for para in paragraphs[:10]:  # 每年最多10个段落\n        all_paragraphs.append({\n            'year': year,\n            'content': para\n        })\n\n# 生成嵌入向量\nfor i, para in enumerate(all_paragraphs):\n    embedding = get_embedding_with_openai(para['content'])\n    all_paragraphs[i]['embedding'] = embedding\n    \n# 计算相似度矩阵\nn_paras = len(all_paragraphs)\nsimilarity_matrix = np.zeros((n_paras, n_paras))\n\nfor i in range(n_paras):\n    for j in range(n_paras):\n        if i == j:\n            similarity_matrix[i, j] = 1.0\n        else:\n            similarity_matrix[i, j] = cosine_similarity(\n                [all_paragraphs[i]['embedding']], \n                [all_paragraphs[j]['embedding']]\n            )[0, 0]\n\n# 找出每个段落最相似的其他段落\nfor i, para in enumerate(all_paragraphs):\n    similar_indices = np.argsort(similarity_matrix[i])[-3:-1][::-1]  # 排除自身，取前2个\n    \n    print(f\"{para['year']}年段落:\")\n    print(para['content'][:100] + \"...\\n\")\n    \n    print(\"最相似的段落:\")\n    for idx in similar_indices:\n        sim_para = all_paragraphs[idx]\n        sim_score = similarity_matrix[i, idx]\n        print(f\"- {sim_para['year']}年 (相似度: {sim_score:.4f}):\")\n        print(sim_para['content'][:100] + \"...\\n\")\n    \n    print(\"-\"*80 + \"\\n\")\n\"\"\"\n```\n:::\n\n\n\n\n## 应用大语言模型的最佳实践\n\n在金融文本分析中应用大语言模型时，应注意以下几点：\n\n### 提示设计技巧\n\n1. **明确任务界定**：\n   - 清晰指定分析目标和期望输出格式\n   - 使用领域专业术语增强精确性\n\n2. **思维链设计**：\n   - 引导模型分步思考复杂问题\n   - 要求模型先分析再总结\n\n3. **角色设定**：\n   - 指定模型扮演\"金融分析师\"等专业角色\n   - 增强输出的专业性和针对性\n\n### 金融特定优化\n\n1. **上下文补充**：\n   - 提供行业背景信息增强理解\n   - 明确时间线帮助模型理解经济周期\n\n2. **多模型比较**：\n   - 使用通用模型和金融专业模型对比结果\n   - 综合优势获得更全面分析\n\n3. **人机协作**：\n   - 将模型输出作为专业分析的起点\n   - 关键决策仍需人类专家判断\n\n### 局限性与注意事项\n\n1. **事实准确性**：\n   - 大语言模型可能产生\"幻觉\"，输出虚构内容\n   - 关键数据和结论需要人工验证\n\n2. **偏见风险**：\n   - 模型可能继承训练数据中的偏见\n   - 金融分析需要客观中立\n\n3. **时效性限制**：\n   - 模型知识截止日期后的事件需要通过提示补充\n   - 定期更新分析以反映最新情况\n\n# 小结与进阶方向\n\n## 从静态向量到大语言模型的演进\n\n本讲我们从Word2Vec的局限性出发，介绍了BERT等Transformer模型的原理，以及大语言模型的应用：\n\n1. **表示方法演进**：从静态词向量到上下文感知的动态表示\n2. **架构演进**：从浅层神经网络到深层Transformer架构\n3. **规模演进**：从百万参数到千亿参数\n4. **应用演进**：从特征提取到端到端文本理解与生成\n\n## 无监督学习的新范式\n\n大语言模型为无监督学习带来了新的范式：\n\n1. **零样本学习**：无需额外标注数据，直接分类新数据\n2. **上下文学习**：通过提示中的示例引导模型学习模式\n3. **涌现能力**：模型规模增长带来质的飞跃\n4. **提示工程**：通过设计提示引导模型行为\n\n## 金融文本分析的未来方向\n\n大语言模型在金融文本分析中的未来方向包括：\n\n1. **多模态融合**：结合文本、数值、图表等多种数据\n2. **实时适应**：持续学习最新市场信息和政策变化\n3. **可解释性增强**：提高模型决策的透明度\n4. **领域知识增强**：融入更多金融专业知识\n\n## 进阶学习资源\n\n1. **理论深入**：\n   - 《Attention Is All You Need》 - Transformer原始论文\n   - 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》\n\n2. **实践教程**：\n   - Hugging Face Transformers 库文档\n   - OpenAI GPT API 文档\n\n3. **金融NLP资源**：\n   - FinBERT 和 FinGPT 项目\n   - 金融领域预训练模型集合\n\n## 本讲小结\n\n本讲我们从Word2Vec的局限性出发，介绍了BERT和Transformer架构的原理，以及大语言模型在金融文本分析中的应用：\n\n1. 从静态词向量到动态上下文表示的演进\n2. Transformer架构与自注意力机制的工作原理\n3. BERT等预训练模型的内部结构和应用方法\n4. 大语言模型的关键创新与涌现能力\n5. 实践案例：使用BERT和大语言模型分析政府工作报告\n\n通过这些内容，我们理解了现代NLP技术在金融文本分析中的强大能力，以及如何将这些技术应用于实际金融分析任务。\n\n# 参考资料\n\n1. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n3. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n4. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n5. Yang, Y., Uy, M. C. S., & Huang, A. (2020). FinBERT: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097.\n6. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\n",
    "supporting": [
      "13_llm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}