{
  "hash": "7deeee50edf1bd160744802096cd24b4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"聚类算法实践\"\n---\n\n\n\n\n## 1. 环境设置\n\n首先导入必要的库和设置绘图参数：\n\n::: {#5b50c280 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.datasets import make_blobs, make_moons\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.mixture import GaussianMixture\nfrom scipy import stats\n\n# 设置随机种子\nnp.random.seed(42)\n\n# 设置matplotlib参数\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# 创建保存图像的函数\nfrom pathlib import Path\nIMAGES_PATH = Path() / \"images\" / \"clustering\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n```\n:::\n\n\n## 2. K均值聚类\n\n### 2.1 分类 vs 聚类\n\n我们先来看一个简单的例子，展示分类和聚类的区别：\n\n::: {#d638199d .cell execution_count=2}\n``` {.python .cell-code}\n# 加载鸢尾花数据集\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target\n\nplt.figure(figsize=(9, 3.5))\n\n# 左侧：带有类别标签的数据（分类问题）\nplt.subplot(121)\nplt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\nplt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\nplt.xlabel(\"花瓣长度\")\nplt.ylabel(\"花瓣宽度\")\nplt.grid()\nplt.legend()\n\n# 右侧：不带标签的数据（聚类问题）\nplt.subplot(122)\nplt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\nplt.xlabel(\"花瓣长度\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\nplt.grid()\n\nsave_fig(\"classification_vs_clustering_plot\")\nplt.show()\n```\n:::\n\n\n### 2.2 创建和拟合K均值模型\n\n让我们生成一些数据点并应用K均值聚类：\n\n::: {#04cc649f .cell execution_count=3}\n``` {.python .cell-code}\n# 生成模拟数据\nblob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n                         [-2.8,  2.8], [-2.8,  1.3]])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nX, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\n\n# 可视化数据\nplt.figure(figsize=(8, 4))\nplt.scatter(X[:, 0], X[:, 1], c='k', s=1)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\nsave_fig(\"blobs_plot\")\nplt.show()\n\n# 使用K-Means聚类\nk = 5\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\nprint(\"预测的簇标签（前10个）:\", y_pred[:10])\nprint(\"簇中心:\", kmeans.cluster_centers_)\n```\n:::\n\n\n### 2.3 绘制决策边界（沃罗诺伊图）\n\n::: {#963895d2 .cell execution_count=4}\n``` {.python .cell-code}\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                           show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans, X)\nsave_fig(\"voronoi_plot\")\nplt.show()\n```\n:::\n\n\n### 2.4 软聚类 vs 硬聚类\n\nK-means通常执行硬聚类（每个点属于一个簇），但我们也可以查看每个点到各个簇中心的距离：\n\n::: {#adcb6271 .cell execution_count=5}\n``` {.python .cell-code}\n# 创建新的测试点\nX_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n\n# 计算到每个簇中心的距离\ndistances = kmeans.transform(X_new)\nprint(\"每个测试点到各个簇中心的距离:\")\nprint(distances.round(2))\n\n# 预测簇标签\ny_pred_new = kmeans.predict(X_new)\nprint(\"预测的簇标签:\", y_pred_new)\n```\n:::\n\n\n### 2.5 K均值算法的步骤演示\n\n我们来看看K均值算法的迭代过程：\n\n::: {#f5bba41d .cell execution_count=6}\n``` {.python .cell-code}\nkmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n                      random_state=5)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n                      random_state=5)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=3,\n                      random_state=5)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n\nplt.figure(figsize=(10, 8))\n\nplt.subplot(321)\nplot_data(X)\nplot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.tick_params(labelbottom=False)\nplt.title(\"更新质心（初始随机）\")\n\nplt.subplot(322)\nplot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n                         show_ylabels=False)\nplt.title(\"标记实例\")\n\nplt.subplot(323)\nplot_decision_boundaries(kmeans_iter1, X, show_centroids=False,\n                         show_xlabels=False)\nplot_centroids(kmeans_iter2.cluster_centers_)\n\nplt.subplot(324)\nplot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,\n                         show_ylabels=False)\n\nplt.subplot(325)\nplot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\nplot_centroids(kmeans_iter3.cluster_centers_)\n\nplt.subplot(326)\nplot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n\nsave_fig(\"kmeans_algorithm_plot\")\nplt.show()\n```\n:::\n\n\n### 2.6 K均值的变异性问题\n\nK均值对初始质心的位置很敏感，不同的初始点可能导致不同的结果：\n\n::: {#76f1b6e1 .cell execution_count=7}\n``` {.python .cell-code}\ndef plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(10, 3.2))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2)\n\nkmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\nkmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n\nplot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n                          \"解决方案 1\",\n                          \"解决方案 2（使用不同的随机初始化）\")\n\nsave_fig(\"kmeans_variability_plot\")\nplt.show()\n\n# 查看惯性值\nprint(\"解决方案1的惯性:\", kmeans_rnd_init1.inertia_)\nprint(\"解决方案2的惯性:\", kmeans_rnd_init2.inertia_)\n```\n:::\n\n\n### 2.7 确定最佳K值\n\n#### 肘部法则\n\n::: {#9424b3fd .cell execution_count=8}\n``` {.python .cell-code}\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"惯性\")\nplt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n             arrowprops=dict(facecolor='black', shrink=0.1))\nplt.text(4.5, 650, \"肘部\", horizontalalignment=\"center\")\nplt.axis([1, 8.5, 0, 1300])\nplt.grid()\nsave_fig(\"inertia_vs_k_plot\")\nplt.show()\n```\n:::\n\n\n#### 轮廓系数\n\n::: {#3c62edc9 .cell execution_count=9}\n``` {.python .cell-code}\nsilhouette_scores = [silhouette_score(X, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"轮廓分数\")\nplt.axis([1.8, 8.5, 0.55, 0.7])\nplt.grid()\nsave_fig(\"silhouette_score_vs_k_plot\")\nplt.show()\n```\n:::\n\n\n#### 轮廓图分析\n\n::: {#83242403 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"簇\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"轮廓系数\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n\nsave_fig(\"silhouette_analysis_plot\")\nplt.show()\n```\n:::\n\n\n### 2.8 K均值的局限性\n\nK均值在处理非球形或不同密度的簇时效果不佳：\n\n::: {#3d39fabb .cell execution_count=11}\n``` {.python .cell-code}\n# 生成一个更难聚类的数据集\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX_difficult = np.r_[X1, X2]\n\n# 两种不同初始化方式的K-means\nkmeans_good = KMeans(n_clusters=3,\n                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n                     n_init=1, random_state=42)\nkmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmeans_good.fit(X_difficult)\nkmeans_bad.fit(X_difficult)\n\nplt.figure(figsize=(10, 3.2))\n\nplt.subplot(121)\nplot_decision_boundaries(kmeans_good, X_difficult)\nplt.title(f\"惯性 = {kmeans_good.inertia_:.1f}\")\n\nplt.subplot(122)\nplot_decision_boundaries(kmeans_bad, X_difficult, show_ylabels=False)\nplt.title(f\"惯性 = {kmeans_bad.inertia_:.1f}\")\n\nsave_fig(\"bad_kmeans_plot\")\nplt.show()\n```\n:::\n\n\n### 2.9 图像分割应用\n\n让我们用K均值进行图像分割：\n\n::: {#067e0888 .cell execution_count=12}\n``` {.python .cell-code}\n# 加载图像\nimport PIL\n\n# 若没有图像，可直接使用代码中的示例图像链接下载\nimport urllib.request\nhoml3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\nfilename = \"ladybug.png\"\nfilepath = IMAGES_PATH / filename\nif not filepath.is_file():\n    print(\"下载\", filename)\n    url = f\"{homl3_root}/images/unsupervised_learning/{filename}\"\n    urllib.request.urlretrieve(url, filepath)\n\n# 读取图像并应用K-means\nimage = np.asarray(PIL.Image.open(filepath))\nprint(\"图像形状:\", image.shape)\n\nX_img = image.reshape(-1, 3)\nsegmented_imgs = []\nn_colors = (10, 8, 6, 4, 2)\n\nfor n_clusters in n_colors:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X_img)\n    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_imgs.append(segmented_img.reshape(image.shape))\n\n# 显示原始图像和分割后的图像\nplt.figure(figsize=(10, 5))\nplt.subplots_adjust(wspace=0.05, hspace=0.1)\n\nplt.subplot(2, 3, 1)\nplt.imshow(image)\nplt.title(\"原始图像\")\nplt.axis('off')\n\nfor idx, n_clusters in enumerate(n_colors):\n    plt.subplot(2, 3, 2 + idx)\n    plt.imshow(segmented_imgs[idx] / 255)\n    plt.title(f\"{n_clusters} 种颜色\")\n    plt.axis('off')\n\nsave_fig('image_segmentation_plot', tight_layout=False)\nplt.show()\n```\n:::\n\n\n## 3. DBSCAN聚类\n\nDBSCAN是一种基于密度的聚类算法，特别适合处理形状不规则的簇。\n\n### 3.1 应用DBSCAN算法\n\n::: {#18e06aa3 .cell execution_count=13}\n``` {.python .cell-code}\n# 创建一个新月形数据集\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\n# 应用DBSCAN\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\n# 查看结果\nprint(\"标签（前10个）:\", dbscan.labels_[:10])\nprint(\"核心样本索引（前10个）:\", dbscan.core_sample_indices_[:10])\n```\n:::\n\n\n### 3.2 可视化DBSCAN结果\n\n::: {#320184f1 .cell execution_count=14}\n``` {.python .cell-code}\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\n# 试两个不同的epsilon值\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nsave_fig(\"dbscan_plot\")\nplt.show()\n```\n:::\n\n\n### 3.3 基于DBSCAN的分类\n\n可以使用DBSCAN找到的核心点构建分类器：\n\n::: {#f1045e81 .cell execution_count=15}\n``` {.python .cell-code}\n# 使用核心点构建KNN分类器\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan2.components_, dbscan2.labels_[dbscan2.core_sample_indices_])\n\n# 预测新点的类别\nX_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\nprint(\"预测标签:\", knn.predict(X_new))\nprint(\"预测概率:\", knn.predict_proba(X_new).round(2))\n\n# 可视化分类边界\nplt.figure(figsize=(6, 3))\nplot_decision_boundaries(knn, X, show_centroids=False)\nplt.scatter(X_new[:, 0], X_new[:, 1], c=\"b\", marker=\"+\", s=200, zorder=10)\nsave_fig(\"cluster_classification_plot\")\nplt.show()\n```\n:::\n\n\n## 4. 使用聚类进行半监督学习\n\n聚类算法可以应用于半监督学习场景：\n\n::: {#10d61bb9 .cell execution_count=16}\n``` {.python .cell-code}\n# 加载digits数据集\nfrom sklearn.datasets import load_digits\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test = X_digits[:1400], X_digits[1400:]\ny_train, y_test = y_digits[:1400], y_digits[1400:]\n\n# 仅使用少量标记数据训练\nn_labeled = 50\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nprint(f\"使用{n_labeled}个标记样本的准确率:\", log_reg.score(X_test, y_test))\n\n# 使用K-means找到代表性数字\nk = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = X_digits_dist.argmin(axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\n# 绘制代表性数字\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n               interpolation=\"bilinear\")\n    plt.axis('off')\n\nsave_fig(\"representative_images_plot\", tight_layout=False)\nplt.show()\n\n# 手动标记这些代表性数字（这里使用预定义的标签）\ny_representative_digits = np.array([\n    1, 3, 6, 0, 7, 9, 2, 4, 8, 9,\n    5, 4, 7, 1, 2, 6, 1, 2, 5, 1,\n    4, 1, 3, 3, 8, 8, 2, 5, 6, 9,\n    1, 4, 0, 6, 8, 3, 4, 6, 7, 2,\n    4, 1, 0, 7, 5, 1, 9, 9, 3, 7\n])\n\n# 使用代表性数字训练模型\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nprint(\"使用代表性数字的准确率:\", log_reg.score(X_test, y_test))\n\n# 将标签传播到同一簇中的所有实例\ny_train_propagated = np.empty(len(X_train), dtype=np.int64)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n\n# 使用扩展的训练集\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train, y_train_propagated)\nprint(\"使用标签传播后的准确率:\", log_reg.score(X_test, y_test))\n\n# 移除异常值\npercentile_closest = 99\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist > cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\nprint(\"移除异常值后的准确率:\", log_reg.score(X_test, y_test))\n\n# 评估传播标签的质量\nprint(\"传播标签的准确率:\", \n      (y_train_partially_propagated == y_train[partially_propagated]).mean())\n```\n:::\n\n\n## 5. 总结与比较\n\n### 5.1 K均值优缺点：\n\n- **优点**：\n  - 算法简单，易于实现\n  - 计算效率高，适合大数据集\n  - 当簇呈球形且大小相近时效果好\n\n- **缺点**：\n  - 需要预先指定K值\n  - 对初始质心选择敏感\n  - 无法处理非球形簇\n  - 对异常值敏感\n\n### 5.2 DBSCAN优缺点：\n\n- **优点**：\n  - 不需要预先指定簇的数量\n  - 能识别任意形状的簇\n  - 能自动识别噪声点\n  - 对异常值不敏感\n\n- **缺点**：\n  - 参数选择（ε和MinPts）较为困难\n  - 对密度差异大的簇效果不佳\n  - 计算复杂度较高\n\n### 5.3 选择合适的聚类算法：\n\n- 如果簇形状规则、大小相似，且需要明确的簇数量，选择K均值\n- 如果簇形状不规则、有噪声点，且不知道簇的确切数量，选择DBSCAN\n- 对于复杂数据集，可能需要尝试多种算法并比较结果\n\n",
    "supporting": [
      "lab07_cluster_files"
    ],
    "filters": [],
    "includes": {}
  }
}